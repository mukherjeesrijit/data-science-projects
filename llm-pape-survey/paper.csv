Document Title,Authors,Publication Title,Date Added To Xplore,Publication Year,Abstract,IEEE Terms,PDF Link,Document Identifier,Medical,Disease,Modality,Model,Explainibility,Local/Global
Detection of Intracranial Hemorrhage by Artificial Intelligence and Deep Learning Methods,D. Veselov; N. Andriyanov; L. Trung,2024 X International Conference on Information Technology and Nanotechnology (ITNT),09-Jul-24,2024,"This paper addresses the task of intracranial hemorrhage detection and classification based on the RSNA 2019 brain computed tomography (CT) dataset. Several popular neural network (NN) architectures based on convolutional neural networks (CNNs) have been considered and interpretation of the neural network classifier by GradCAM method has been shown. To efficiently process the dataset, various radiology information processing methods were considered to create the resulting training sample. In addition, the training sample was reduced to 10% of the total dataset to avoid technical limitations and improve performance. The main contribution of the work is the creation and testing of deep learning algorithms for the detection of acute intracranial hemorrhage and its five subtypes. Various steps related to data cleaning, augmentation and preprocessing have also been applied in the presented work.",Training;Deep learning;Computed tomography;Predictive models;Radiology;Convolutional neural networks;Hemorrhaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10582393,IEEE Conferences,,,,,,
Diving Deep into Bone Anomalies on the FracAtlas Dataset Using Deep Learning and Explainable AI,F. Akhlaq; S. Ali; A. S. Imran; S. M. Daudpota; Z. Kastrati,2024 International Conference on Engineering & Computing Technologies (ICECT),08-Jul-24,2024,"Medical image analysis has undergone significant advancements with the integration of machine learning techniques, particularly in the realm of bone anomaly detection. The availability of recent datasets and the lack of benchmarking and explainability components provide numerous opportunities in this domain. This study proposes a benchmarking approach to a recently published FracAtlas dataset utilizing state-of-the-art deep-learning models coupled with explainable artificial intelligence (XAI) having two distinct modules. The first module involves the binary classification of fractures in different body parts and explains the decision-making process of the best-performing model using an XAI technique known as EigenCAM. EigenCAM generates heatmaps on every layer of the YOLOv8m model to explain how the model reached a conclusion and localizes the fracture using a heatmap. To verify the heatmap, we also detected fractures using the YOLOv8m detection model, which achieved a mAP@O.5 of 59.5%, outperforming the baseline results on this dataset. The second module involves a multi-class classification task to categorize images into one of the five anatomical regions. The best-performing model for binary classification is the YOLOv8m model, with an accuracy of 83.1%, whereas the best-performing model for multi-class classification is the YOLOv8s, achieving an accuracy of 96.2%.",Heating systems;Deep learning;Accuracy;Image analysis;Explainable AI;Computational modeling;Decision making,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581288,IEEE Conferences,,,,,,
A Fusion Deep Learning Model of ResNet and Vision Transformer for 3D CT Images,C. Liu; C. Sun,IEEE Access,,2024,"The outbreak of COVID-19 has had a serious impact on the safety of human life and property. Rapid and effective diagnosis is the key to the prevention and treatment of the virus. In this study, we introduce a new fusion model called â€œReswinâ€, which was trained by 3D CT data to diagnose COVID-19. The model combines two mainstream computer vision models, Resnet 3D (a convolutional neural network) and Video Swin Transformer (a vision transformer neural network), which use a soft voting method. We compared our proposed model Reswin with ResNet 3D-50, Swin-T, MViT, R2+1D-50, SlowFast-50, X3D, and CSN101, which are state-of-the-art deep learning models used for the classification of 3D images. The Reswin model achieved an accuracy of 0.9099, precision of 0.9266, F1 score of 0.9425, AUC of 0.9541, and AUPR of 0.9861 in binary classification, and an accuracy of 0.8655, precision of 0.8580, and F1 score of 0.8620 in triple classification. Reswin provides a new solution for 3D CT image classification tasks and new ideas for the development of deep learning in 3D medical imaging.",COVID-19;Transformers;Task analysis;Predictive models;Deep learning;Biomedical imaging;X3D;Three-dimensional displays;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10584487,IEEE Early Access Articles,,,,,,
Early Detection and Classification of Diabetic Retinopathy through Analysis of Retinal Medical Images Using Deep Learning,Y. Siyah; K. Minaoui; F. Z. E. Biach; S. Saoudi,"2024 IEEE 12th International Symposium on Signal, Image, Video and Communications (ISIVC)",04-Jul-24,2024,"In this paper, we propose a model based on deep learning (DL) frameworks for the detection and classification of diabetic retinopathy (DR), along with the assessment of its severity. DL algorithms have been applied to fundus photographs, which are images of the back of the eye, to automatically detect and classify signs of DR. These algorithms can analyze retinal images and identify anomalies, including microaneurysms, hemorrhages, exudates, as well as fluid accumulation in the macula, all of which serve as indicators of the presence of diabetic retinopathy. We utilized the APTOS 2019 dataset sourced from Kaggle, comprising high-resolution retinal images processed with Gaussian filters for diabetic retinopathy detection. Experiments were conducted on six different models, including Visual Geometry Group 16 (VGG16), Visual Geometry Group 19 (VGG19), Residual Network 50 (ResNet50), Inception Version 3 (InceptionV3), basic Convolutional Neural Networks (CNN), and Mobile Network Version 2 (MobileNetV2). The results obtained demonstrate high accuracy, suggesting that this model could contribute to expediting and enhancing the diabetic retinopathy diagnostic process. By automating the detection of anomalies in retinal images, the system proposes an intelligent approach that may help alleviate the workload on medical personnel.",Geometry;Deep learning;Diabetic retinopathy;Visualization;Retina;Classification algorithms;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10577805,IEEE Conferences,,,,,,
Chest Cancer Classification from Chest CT-Scan Images using Deep Learning,H. Bumpenje; Rahmadwati; Z. Abidin,"2024 International Conference on Smart Computing, IoT and Machine Learning (SIML)",04-Jul-24,2024,"Chest cancer is fatal, encompassing various tumors. This includes lung cancer as the most common, originating from tissues of the lungs. All chest cancer types present unique challenges, leaving individuals and healthcare professionals searching for solutions. The conventional fight against chest cancer includes surgery to remove the cancerous tissue, radiation therapy to target remaining cancer cells, and chemotherapy to attack cancer cells throughout the body. While these methods have seen advancements, innovative strategies are still required to aid doctors in classifying the different chest cancer types. Deep learning is currently showing potential, where researchers are developing sophisticated algorithms capable of analyzing medical images, identifying subtle patterns, and distinguishing between various cancer types. This research investigates the application of DenseNet121, a deep learning architecture with a combination of 2 data augmentation techniques namely, albumentations and mixup for classifying chest cancer types based on chest CT-scan images. Four image types were classified including adenocarcinoma, large cell carcinoma, squamous cell carcinoma and normal. The performance of the trained model for this research was based on accuracy, precision, recall and F1-score parameters. It was noted that DenseNet121 achieved an accuracy of 82.14% while combining albumentations and mixup, 78.64% with only mixup, 77.78% with only albumentations and 58.69% without data augmentation, indicating that the proposed model performed better. Combining albumentations and mixup took advantage of the strengths of both augmentation techniques to increase the size and the diversity of training datasets, thereby favoring regularization to reduce the overfitting challenge faced by deep learning models. However, further research is still required to evaluate the findings of this research on larger and other kinds of datasets and most importantly, compare them to the real-world scenarios that might introduce variations with a wider variety of cases.",Deep learning;Training;Accuracy;Squamous cell carcinoma;Diversity reception;Surgery;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578120,IEEE Conferences,,,,,,
Brain Tumor Classification using Multi-Resolution Averaged Spatial Attention Features with CBAM and Convolutional Neural Networks,M. C. Binish; R. S. Swarun Raj; V. Thomas,2024 1st International Conference on Trends in Engineering Systems and Technologies (ICTEST),03-Jul-24,2024,"Medical imaging plays a vital role in diagnosis and understanding of different medical conditions, especially with regard to neurology. The automation of medical imaging analysis has been substantially improved over the last few years due to the integration of advanced technologies like Deep Learning. Developing a system for the classification of brain tumors is the primary objective. It involves training the deep learning model of a dataset from brain scans that are labeled with tumor types. Recently, pre-trained models have been applied where features are typically collected from the very bottom layer after the flatten layer that differs between natural and medical images. This work suggests a multi-level feature extraction using spatial features and channel averaging methods for early brain tumor identification in order to address this issue and achieve a classification accuracy of 96.70% for the test data",Training;Deep learning;Accuracy;Feature extraction;Brain modeling;Data models;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10576086,IEEE Conferences,,,,,,
Enhanced MobileNet Architecture with Residual Blocks for Improved CT Image Classification,A. S; S. E. M,2024 1st International Conference on Trends in Engineering Systems and Technologies (ICTEST),03-Jul-24,2024,"This research introduces a novel modification to the MobileNet architecture, incorporating residual blocks to enhance its efficacy in the classification of computed tomography (CT) images, specifically for distinguishing malignant and benign cases. Comparative evaluations against MobileNet V1 and V2 were conducted using a diverse CT image dataset. Our proposed modified MobileNet consistently outperformed both baseline architectures across key evaluation metrics, including precision, recall, F1 score, confusion matrix, and overall accuracy. Additionally, the study investigated the impact of preprocessing techniques on classification performance. Results demonstrated a substantial improvement in accuracy when preprocessing was applied, underscoring its importance in optimizing deep learning models for medical image analysis. The incorporation of residual blocks into the MobileNet architecture proved instrumental in capturing intricate features, contributing to the observed performance boost compared to standard MobileNet architectures. This research not only highlights the significance of architectural modifications but also emphasizes the critical role of preprocessing in refining deep learning models for medical image classification. In conclusion, our findings suggest that the modified MobileNet architecture with residual blocks holds promise for enhanced CT image classification, with potential applications in early disease detection and clinical decision support systems.",Deep learning;Representation learning;Analytical models;Accuracy;Computed tomography;Refining;Computer architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10576085,IEEE Conferences,,,,,,
Enhanced Pneumonia Detection Through Advanced AI-Driven Hybrid Models: A Comparative Study of Deep Learning Architectures,H. Joseph; J. Anitha,2024 1st International Conference on Trends in Engineering Systems and Technologies (ICTEST),03-Jul-24,2024,"Pneumonia is an infection which is affected on lungs. The reason for infection can by bacteria, virus or fungi in humans. This is characterized by a severe cough with phlegm, chills, and fever. Pneumonia continues to be a significant global health concern. It is the necessity of early and accurate diagnosis is prudent to improve patient outcomes and reduce mortality rates. Now a days with the integration of advanced technologies, mainly artificial intelligence (AI) and medical imaging, we have observed promising results in enhancing pneumonia detection. The objective was to evaluate and examine different deep learning models to recognize pneumonia affected X-ray pictures in various instances, thereby identifying the best model. The proposed platform collects and processes datasets, builds different learning models for pneumonia classification, compares the models, and selects the best model. It delves into the potential of AI-driven models to analyze vast amounts of data, leading to faster and more accurate diagnoses. A comparative examination of each model in different settings may offer a deeper knowledge of each model's effectiveness in diagnosing pneumonia, resulting selection of deep learning models for pneumonia detection easier. In this study, we assess the effectiveness of pre-trained CNN models as feature extractors, followed by a variety of classifiers for distinguishing abnormal and normal chest X-rays. Statistical results show that pretrained CNN models combined with supervised classifier algorithms can be highly useful in analyzing chest X-ray images, particularly for detecting pneumonia. According to the results, the ResNet-50 model performed better on large datasets.",Deep learning;Analytical models;Pneumonia;Accuracy;Neural networks;Lung;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10576088,IEEE Conferences,,,,,,
Uni4Eye++: A General Masked Image Modeling Multi-modal Pre-training Framework for Ophthalmic Image Classification and Segmentation,Z. Cai; L. Lin; H. He; P. Cheng; X. Tang,IEEE Transactions on Medical Imaging,,2024,"A large-scale labeled dataset is a key factor for the success of supervised deep learning in most ophthalmic image analysis scenarios. However, limited annotated data is very common in ophthalmic image analysis, since manual annotation is time-consuming and labor-intensive. Self-supervised learning (SSL) methods bring huge opportunities for better utilizing unlabeled data, as they do not require massive annotations. To utilize as many unlabeled ophthalmic images as possible, it is necessary to break the dimension barrier, simultaneously making use of both 2D and 3D images as well as alleviating the issue of catastrophic forgetting. In this paper, we propose a universal self-supervised Transformer framework named Uni4Eye++ to discover the intrinsic image characteristic and capture domain-specific feature embedding in ophthalmic images. Uni4Eye++ can serve as a global feature extractor, which builds its basis on a Masked Image Modeling task with a Vision Transformer architecture. On the basis of our previous work Uni4Eye, we further employ an image entropy guided masking strategy to reconstruct more-informative patches and a dynamic head generator module to alleviate modality confusion. We evaluate the performance of our pre-trained Uni4Eye++ encoder by fine-tuning it on multiple downstream ophthalmic image classification and segmentation tasks. The superiority of Uni4Eye++ is successfully established through comparisons to other state-of-the-art SSL pre-training methods. Our code is available at Github1.",Task analysis;Training;Medical diagnostic imaging;Image reconstruction;Transformers;Head;Three-dimensional displays,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10580962,IEEE Early Access Articles,,,,,,
AI-Powered Brain Imaging for Early Neurodegenerative Disease Detection,P. B. Kalashetty; A. R. Punekar; S. H. Lokhande; H. Nawale,"2024 MIT Art, Design and Technology School of Computing International Conference (MITADTSoCiCon)",02-Jul-24,2024,"Neurodegenerative diseases, such as Alzheimer's, pose significant challenges to healthcare, underscoring the urgent need for precise and swift diagnostic tools. This research project aims to utilize Convolutional Neural Networks (CNN) alongside brain imaging methods to detect neurodegenerative diseases at an early stage. Drawing insights from existing literature, the objective is to devise an advanced system for timely detection. The endeavor begins with an extensive review of current research on neurodegenerative diseases, with a particular emphasis on the hippocampus role in Alzheimer's disease. The research study titled 'MRI of the hippocampus in the early stages of Alzheimer's disease' underscores the potential of magnetic resonance imaging (MRI) in early detection. Our project integrates sophisticated deep learning methodologies, including 3D CNNs and transfer learning, to augment diagnostic precision. Moreover, we introduce a pioneering approach, the Multiplane Convolutional Neural Network (MP-CNN), to leverage multiple MRI planes for a more comprehensive feature set. By combining machine learning, MRI, and deep learning, our project aims to advance early detection of neurodegenerative diseases, with a specific focus on Alzheimer's, offering a robust, effective, and efficient system.",Neuroimaging;Deep learning;Three-dimensional displays;Reviews;Magnetic resonance imaging;Transfer learning;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575866,IEEE Conferences,,,,,,
Multiclass Brain Tumor Detection using Deep Transfer Learning,A. V. Deshmukh; M. R. Bendre,"2024 MIT Art, Design and Technology School of Computing International Conference (MITADTSoCiCon)",02-Jul-24,2024,"Brain tumors, identified as abnormal growth of cell inside the brain, necessitate accurate categorization for tailored treatment plans. This classification aids in determining the severity of the condition and guiding therapeutic decisions. Iimaging technologies in medical like Magnetic Resonance Imaging (MRI) are instrumental in identifying brain pathologies without emitting ionizing radiation. Deep learning, a subset of AI, enhances brain tumor detection from MRI scans, improving prediction rates. Digital image processing is vital for medical image analysis and diagnosis. Brain tumors manifest in various forms, categorized as benign or malignant based on their tissue characteristics. Convolutional Neural Networks (CNNs), particularly VGG-16, Inception-v3, and ResNet-50 are prominent in brain tumor analysis, segmentation, and classification. These models are compared for their efficacy in identifying and forecasting brain tumor cells. Common brain tumor types include glioma, meningioma, and pituitary tumors, each requiring specific treatment approaches.",Image segmentation;Accuracy;Image analysis;Magnetic resonance imaging;Transfer learning;Feature extraction;Real-time systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575380,IEEE Conferences,,,,,,
Brain Tumor Detection Using AIML,S. Kumari; P. Bharti; G. Dixit; V. Rohilla; M. Kumar; C. Choudhary,"2024 MIT Art, Design and Technology School of Computing International Conference (MITADTSoCiCon)",02-Jul-24,2024,"Computer vision, a subfield of AI, specifically deals with allowing computers to construe and understand visual information from the world, often through digital images or videos. It is also very helpful in the healthcare because it detects the diseases through digital images. It involves developing algorithms and models that can recognize objects, patterns, and features within images, allowing machines to observe and extract meaningful information from visual data. ""Deep learning is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain"". Deep learning produces outstanding results in a range of fields, including biomedicine, education, and the detection of financial fraud, among others, due to its ability to handle vast volumes of data sets. It has proven to operate admirably and has the ability to use MRI scans to detect brain cancers for precise prognosis. This research project's main objective is to give a thorough summary of the studies and judgements made recently on the identification and classification of brain cancers using MRI scans. The first step in the classification and diagnosis of brain tumors is a rapid review of previous research publications using artificial intelligence and machine learning. The conclusions offered in this paper will provide researchers with a thorough comparison of recent studies as well as a novel viewpoint on the effectiveness of various AIML and Deep Learning approaches. With an emphasis on AI/ML and deep learning techniques, the major objective of this research project is to present a comprehensive overview of recent studies and findings addressing the detection and classification of brain tumors using MRI data. An review of past studies in the field, including the most recent advancements in AI and deep learning, is provided at the outset of this study.",Deep learning;Visualization;Reviews;Magnetic resonance imaging;Digital images;Computed tomography;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575020,IEEE Conferences,,,,,,
Comparison of Clustering Algorithms for Segregating the Planar Sections of Brain Tumour MRIs,A. Jhariya; D. Parekh; J. Lobo; A. Mogal; A. Bongale; R. Jayaswal,"2024 MIT Art, Design and Technology School of Computing International Conference (MITADTSoCiCon)",02-Jul-24,2024,"Clustering is a technique that involves grouping data into clusters based on the similarity of their characteristics. In images these can be visual features like color, texture, shape, intensity and so on. Unsupervised learning takes place here as the data is clustered without any prior knowledge of their labels or categories. MRI (Magnetic Resonance Imaging) tests capture medical images using magnetic fields and radio waves. These images are used for diagnostic purposes to detect and analyze the internal structure of organs and tissues in the human body. Clustering the planar section of these medical scans can help detect the various types of diseases or condition. This paper potrays a comparitive study by evaluating the efficiency of various clustering algorithms like K-Means, Agglomerative, DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies). The aim is to find out the most appropriate algorithm for effectively identifying and clustering the different planar sections of Brain Tumour MRI scans.",Visualization;Shape;Magnetic resonance imaging;Clustering algorithms;Data visualization;Surgery;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575146,IEEE Conferences,,,,,,
Elevating Ocular Diagnosis: Harnessing the Power of EfficientNet for Eye Disease Classification,V. Mohith; K. Raja; I. R. Oviya,2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT),02-Jul-24,2024,"In response to the evolving landscape of healthcare technology this study addresses the need for automated diagnostic tools in the field of ophthalmology. By utilizing deep learning methods specifically leveraging the EfficientNetV2S framework we achieve an overall accuracy rate of 94.6% in classifying images depicting common eye conditions like cataract, diabetic retinopathy, glaucoma, and normal cases. Our model exhibits capabilities across a diverse spectrum of medical conditions due to meticulous preprocessing and fine-tuning procedures. The results highlight the potential for automated technology integration in healthcare settings, which might improve patient care and treatment plans. This paper highlights the use of advanced neural network designs in the diagnosis of ocular illnesses, offering insights at the nexus of learning and medical image processing.",Glaucoma;Deep learning;Diabetic retinopathy;Accuracy;Computational modeling;Neural networks;Medical treatment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10574627,IEEE Conferences,,,,,,
Prediction of gender from structural MRI images using Multiscale ShuffleNet Extreme Learning Machine,B. F. Stanley; R. Jeen Retna Kumar; V. Gnanaprakash; P. Bini Palas; J. G. B. Patturose; D. J. Joel Devadass Daniel,2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT),02-Jul-24,2024,"In recent years, the application of machine learning techniques to medical imaging data has shown promising results in various clinical tasks. One such task is the prediction of gender from structural MRI (magnetic resonance imaging) images, which holds potential implications for personalized medicine and understanding neurobiological differences between genders. In this study, we propose a novel approach utilizing Multiscale ShuffleNet Extreme Learning Machine (MSEL), a fusion of Multiscale feature extraction and ShuffleNet architecture with Extreme Learning Machine classifier. We demonstrate the effectiveness of our method on a large dataset of structural MRI images, employing state-of-the-art preprocessing techniques and feature extraction methods. Our results indicate significant accuracy and robustness in gender prediction, outperforming existing methodologies. Furthermore, we conduct comprehensive analyses to investigate the contribution of different components in our proposed framework, shedding light on the underlying mechanisms of gender-related brain structural differences. Overall, our study presents a promising avenue for utilizing advanced machine learning techniques in neuroimaging research, with potential applications in clinical diagnostics and personalized healthcare.",Neuroimaging;Accuracy;Extreme learning machines;Magnetic resonance imaging;Computational modeling;Computer architecture;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10574550,IEEE Conferences,,,,,,
Bias Inheritance and its Amplification in GAN-Based Synthetic Data Augmentation for Skin Lesion Classification,M. Jindal; B. Singh,2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT),02-Jul-24,2024,"Even with the increasing availability of high-quality public datasets, the shortage of training samples remains a primary hurdle in deep learning skin analysis. Generative Adversarial Networks (GANs) present a promising solution by generating synthetic skin lesion images that closely resemble real images. Numerous studies have investigated the potential of GANs in medical contexts, yet their applicability remains uncertain due to prevalent biases in biomedical datasets and associated risks. Initial experiments on skin lesion datasets have yielded unreliable outcomes, prompting the fundamental question: ""Is GAN truly beneficial for biomedical classification tasks?"" Additionally, concerns about the amplification of dataset biases by AI models warrant further investigation into the inherent biases inherited by GANs. Considering these gaps, this study conducts experiments on skin lesion classification using four GAN architectures to assess their effectiveness in data generation, both conditional and unconditional. Among these models, StyleGAN2, when unconditionally applied, produced samples that contributed to classification improvement. Moreover, the visual analysis of generated samples revealed that GANs tend to amplify the existing biases with high distribution while suppressing those with low distribution. These findings highlight the limited efficacy of GAN-based augmentation for skin lesion classification in specific scenarios. Thus, caution is advised against the widespread adoption of GANs in medical applications due to computational costs and associated risks.",Training;Visualization;Medical services;Learning (artificial intelligence);Generative adversarial networks;Skin;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10574714,IEEE Conferences,,,,,,
An Explainable Ensemble Classifier for Infrared Breast Cancer Detection,K. Raghavan; B. Sivaselvan,2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT),02-Jul-24,2024,"This study introduces an ensemble classifier that combines the benefits of three pre-trained neural networks: Xception, InceptionV3, and ResNet101. A squeeze-and-excite attention mechanism and Grad-CAM improve the classifier, enabling explainable AI. Using infrared images to detect breast cancer is complicated. Our approach addresses two key medical imaging issues: precise diagnosis and transparency in artificial intelligence decision-making. Using complementary network features and attention-integrated Grad-CAM to improve interpretability, our solution is comprehensive and practical. The ensemble classifier had 95.4% accuracy, demonstrating its ability to improve breast cancer detection. Explainable artificial intelligence (AI) techniques also give clinicians valuable insights into the modelâ€™s predictions, boosting trust and making AI tools more accessible to use in clinical settings. We used the Database For Mastology Research (DMR-IR) dataset to validate and implement our model in practice. This research signifies a notable advancement in the utilisation of AI in healthcare, providing a route towards diagnostic tools that are more precise, transparent, and user-friendly.",Visualization;Accuracy;Explainable AI;Decision making;Learning (artificial intelligence);Predictive models;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10574722,IEEE Conferences,,,,,,
Customized Deep Learning Framework with Advanced Sampling Techniques for Lung Cancer Detection using CT Scans,T. Mahmmod; N. Ayesha; M. Mujahid; A. Rehman,2024 Seventh International Women in Data Science Conference at Prince Sultan University (WiDS PSU),02-Jul-24,2024,"Lung cancer is a devastating disease that kills around five million people annually, making it a major global threat. Timely recognition is critical to improving patient survival. With a focus on pulmonary nodules in medical imaging, CT scans are essential for the screening and identification of lung cancer. This paper introduces a customized deep-learning framework for lung cancer detection using CT Scans. To achieve an accurate and efficient early-stage lung cancer diagnosis, the model makes use of a large dataset of chest CT images for training and validation. The experimental results show that the model outperforms state-of-the-art methods for lung cancer categorization. The model provides a trustworthy and efficient means of early detection of lung cancer, which has the potential to revolutionize the field of lung cancer diagnostics. The deep framework outperforms existing approaches and highlights the need to apply transfer learning to medical image analysis. Because of the techniqueâ€™s extraordinary effectiveness, lung cancer research may be significantly impacted, and improvements in early diagnosis and therapy may result.",Training;Deep learning;Accuracy;Computed tomography;Transfer learning;Lung cancer;Medical treatment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10571877,IEEE Conferences,,,,,,
Advancing Brain Tumor Detection in MRI: Leveraging NAFNET Denoising for Enhanced CNN Performance,S. Ramu; V. Sanjay Kumaran; M. Vijay; A. Balamurugan,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"Image denoising is a vital process in the realm of medical imaging and the open-source image denoising algorithms such as NAFNET (Non-Linear Activation Free Network) can be used in improving the quality of brain MRI scans. The objective is to reduce noise in MRI images using NAFNET, thus improving the ability of subsequent CNN models to detect brain tumors. A comparative analysis was conducted between NAFNET and various traditional denoising algorithms. Different evaluation metrics were employed to demonstrate NAFNETâ€™s effectiveness in reducing image noise while preserving important details. Additionally, the impact of NAFNET-enhanced denoising on CNN-based brain tumor detection models was evaluated using various assessment metrics. The results highlight the potential of leveraging existing open source denoising algorithms like NAFNET to improve the detailing, accuracy in scan images and enhance the overall quality of CNN models, particularly in detecting brain tumors from MRI scans.",Measurement;Image quality;Accuracy;Magnetic resonance imaging;Noise reduction;Noise;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575654,IEEE Conferences,,,,,,
A CNN Approach for Brain Tumor Detection Using Diverse Optimizers,A. Revathi; R. N. Venkataraman; M. Moola,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"The ability to recognize brain tumors via medical imaging is crucial for brain tumor treatment. This proposed work focused on a technique for detecting brain tumors by employing Convolutional Neural Networks (CNNs) to achieve precise and prompt diagnosis of brain tumors. This frame utilizes a variety of optimizers such as Stochastic Gradient Descent, RMSprop, Adam (Adaptive Moment Estimation), Adadelta, Adagrad, Adamax, and Nadam. The modelâ€™s performance is evaluated using accuracy, sensitivity, specificity, precision, and recall as the primary metrics. Leveraging a dataset of 6951 MRI images, results reveal varying performance across optimizers, with a peak accuracy of 98.8% emphasizing the need for careful selection based on specific requirements and dataset characteristics. Advancing brain tumor identification, aiming to enhance early intervention and patient outcomes through the integration of cutting-edge technology, particularly Convolutional Neural Networks (CNNs), which offer high precision in analyzing intricate patterns within medical imaging data.",Accuracy;Magnetic resonance imaging;Transfer learning;Stochastic processes;Computer architecture;Brain modeling;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575750,IEEE Conferences,,,,,,
Knee Osteoarthritis Detection Using X-Rays and DNN,V. Sinha; E. Veemaraj,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"Knee osteoarthritis (OA) is a prevalent degenerative joint disease that affects millions worldwide, leading to pain, disability, and reduced quality of life. Early detection and intervention are crucial for effective management and prevention of disease progression. In this paper, we propose a novel approach for knee OA detection using X-ray images and deep neural networks (DNNs). Specifically, we investigate the performance of two popular DNN architectures: EfficientNet-B5 and DenseNet121. We begin by pre-processing a dataset of knee X-ray images to extract relevant features and mitigate noise. Subsequently, we fine-tune the pre-trained EfficientNet-B5 and DenseNet121 models using transfer learning techniques to adapt them to the task of knee OA detection. We then evaluate and compare the performance of these models in terms of accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC). Our experimental results demonstrate the efficacy of both EfficientNet-B5 and DenseNet121 architectures in knee OA detection, outperforming traditional machine learning approaches. However, we observe variations in their performance metrics, indicating nuanced differences in their ability to capture and discriminate pathological features in knee X-ray images. Furthermore, we conduct an in-depth analysis to understand the factors contributing to the performance disparities between the two models. Overall, our study contributes to the growing body of research aimed at leveraging DNNs for medical image analysis tasks, particularly in the domain of musculoskeletal disorders. The insights gained from this research can potentially aid clinicians in early diagnosis and personalized treatment planning for knee OA patients, ultimately improving patient outcomes and healthcare efficiency.",Adaptation models;Image analysis;Sensitivity;Transfer learning;Artificial neural networks;Feature extraction;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575881,IEEE Conferences,,,,,,
Exploring CNN Architectures for Seizure Detection from MRI during Pregnancy,G. Nayak; N. M. Padhy; T. K. Mishra,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"Deep Learning (DL) represents a subset that has revolutionized traditional machine learning. Unlike earlier methods relying on handcrafted feature extraction, DL automates both feature extraction and classification. This innovation has significantly advanced applications in various medical domains, including the diagnosis of epileptic seizures. This research provides a comparative overview of studies concentrating on automated epileptic seizure detection using leading convolutional neural networks (CNN) architectures. It explores diverse architectures proposed for the automatic diagnosis of epileptic seizures through MRI, encompassing an analysis of rehabilitation systems utilizing DL. The proposed work investigates the application of four primitive CNN architectures on the identification of onset of seizure. The proposed architectures are validated through benchmark MRI samples. Experimental evaluation is made on samples of pregnant females. Overall rate of accuracy stands at 85%, 80%, 90%, and 95% respectively.",Pregnancy;Deep learning;Neurological diseases;Technological innovation;Accuracy;Magnetic resonance imaging;Computer architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575568,IEEE Conferences,,,,,,
Identification of Harvestable Tomatoes using Yolov8,A. Rajesh; S. Muppala; A. Jeyasekar,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"The implementation of tomato picking robots holds immense importance for greenhouse tomato cultivation, as it significantly reduces labor requirements and enhances production efficiency. This study explores the application of Yolov8, a state-of-the-art object detection algorithm, for the precise identification of harvestable tomatoes in greenhouse settings. The yolov8 model is trained and fine-tuned using a comprehensive dataset comprising diverse tomato varieties that are ripe and unripe. The proposed methodology harnesses the power of deep learning to enable real-time detection of ripe tomatoes, facilitating timely and efficient harvesting practices. Leveraging the yolov8 architecture, the system demonstrates exceptional accuracy and speed, making it suitable for high-throughput greenhouse operations. Various challenges, including occlusion and varying lighting conditions, are addressed through robust training strategies, ensuring the modelâ€™s reliability in practical farming scenarios. The most advanced visual tomato identification technology focuses mostly on ripe tomatoes because they stand out from the backdrop in terms of color. Furthermore, the study evaluates the performance of the model against traditional computer vision methods, highlighting the superior accuracy and efficiency of the deep learning approach. The predictive accuracy of ripe and unripe tomatoes was found to be 97.23% when segmentation is applied in Yolov8. The integration of this advanced technology into greenhouse practices promises to revolutionize the way harvestable produce is identified and harvested, paving way for sustainable and efficient agricultural processes in future.",Deep learning;Training;Visualization;Accuracy;Computational modeling;Production;Object detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575283,IEEE Conferences,,,,,,
A Comparative Study of Machine Learning and Deep Learning Algorithms for Early Detection of Brain Tumours from MRI Images,C. Puvanadevi; T. Menakadevi,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"Detection of brain tumours at early stages is critical for timely intervention and improved patient outcomes. Magnetic Resonance Imaging (MRI) techniques in medical imaging plays an essential role in facilitating the detection and diagnosis of brain tumours. Over the years, advancements in machine learning and deep learning algorithms have significantly enhanced the accuracy and efficiency of tumour detection from MRI images. However, there remains a need for a comprehensive comparative study to evaluate the performance of these algorithms. The current research work presents a comprehensive comparative study aimed at evaluating the efficacy of such algorithms for early detection of brain tumours from MRI images. A novel algorithm, DT-CNN, is proposed, which combines decision trees (DT) with convolutional neural networks (CNN) to leverage the strengths of both techniques. The study encompasses simulation analyses, employing suitable metrics to assess the performance of DT-CNN alongside existing algorithms. Various simulation metrics such as precision, recall, accuracy and $F 1$-score are applied for rigorous evaluation. Results demonstrate that DT-CNN exhibits promising performance in terms of sensitivity and overall classification accuracy when compared to established algorithms. Moreover, DT-CNN demonstrates robustness in handling complex MRI image data, showcasing its potential as an effective tool for early detection of brain tumours. The study affords valued visions into the strengths and limitations of ML and DL algorithms in the context of medical imaging, facilitating informed decision-making for healthcare professionals.",Deep learning;Machine learning algorithms;Accuracy;Magnetic resonance imaging;Decision making;Classification algorithms;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575673,IEEE Conferences,,,,,,
"A Comparative Analysis of Pancreatic Tumor Detection using VGG16, ResNet, and DenseNet",G. Deepthi; A. M. Anusha Bamini; Y. J. Praveen,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"This study explores the transformative potential of image classification algorithms like VGG16, ResNet, and DenseNet, for the early detection of pancreatic tumors using medical imaging. One of the main causes of cancer-related deaths globally is pancreatic cancer. However, pancreatic cancer detected at early stages can be cured. This study navigates through the evolution of CNNs in medical image analysis and their specific applications in pancreatic cancer detection. Pancreatic cancer poses a significant global health challenge due to its high mortality rates, often attributed to late-stage detection. The primary objective is to enhance diagnostic accuracy and enable timely intervention. The obtained results demonstrated promising accuracy levels for each model. VGG16 achieved an accuracy of 96%, ResNet demonstrated an accuracy of 98.31%, and DenseNet showcased an accuracy of 99%. Precision values for VGG16, ResNet, and DenseNet were 95%, 96%, and 95.2%, respectively. Recall values were 98% for VGG16, 95% for ResNet, and 97% for DenseNet. The F1 score for 98% for VGG16, 97% for ResNet, and 98.1% for DenseNet. The study concludes that VGG16, ResNet, and DenseNet are valuable tools for pancreatic tumor detection, with each architecture exhibiting unique advantages. By aiding in the early detection of pancreatic cancer, the suggested models may enhance patient outcomes. The research provides insights into the comparative performance of these deep learning architectures, guiding future developments in medical image analysis.",Accuracy;Navigation;Computational modeling;Pancreatic cancer;Deep architecture;Computer architecture;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575802,IEEE Conferences,,,,,,
Revolutionizing Lung Cancer Prognosis Through CNN-based Predictive Models,A. Sharma; S. Mittal,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"Lung cancer is a matter of great concern, and unfortunately, it is the leading cause of mortality worldwide. Doctors are using many kinds of imaging scans like CT scans and surgical biopsies for the detection of lung cancer. In recent years, Deep Learning (DL) algorithms have produced encouraging results in automated prediction and classification tasks for medical imaging. Early prediction can resolve many problems, and the mortality rate can be lower after using various DL models. This research uses two models of DL, Sequential, and DenseNet for the prediction of lung cancer. This article uses chest X-ray scans for image preprocessing and feature selection before the modelâ€™s training, testing, and validation. After this process model is implemented with the different libraries and gives authenticated results. The DenseNet model gave the effective results than the sequential model with 95.86% accuracy.",Deep learning;Training;Accuracy;Computational modeling;Biological system modeling;Lung cancer;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575136,IEEE Conferences,,,,,,
Detection of Multi Stage Diabetes Foot Ulcer using Deep Learning Techniques,C. Giridhar; B. Akhila; S. P. Kumar; G. L. Sumalata,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"The detection of diabetes foot ulcers (DFUs) is crucial for timely intervention and prevention of severe complications in diabetic patients. One common effect of diabetes is diabetic foot ulcers that occurs when a wound or sore develops on the foot of a person with diabetes. It is typically caused by a various factor, including inadequate blood circulation, nerve injury (neuropathy), and impaired immunological function. Deep learning techniques have emerged as promising tools for automating DFU detection, offering advantages in accuracy and efficiency over traditional methods. This paper presents a unique deep learning approach for DFU detection using convolutional neural networks (CNNs) and image processing techniques. The proposed system utilizes a dataset of (DFUC 2021) foot images acquired through various imaging modalities, including infrared, thermal, and color imaging, to train the CNN model. The trained model demonstrates high accuracy in identifying DFUs by classifying them and analyzing key visual features such as wound size, wound type, color, and texture. Evaluation results on multiclass (3 class) classification of a diverse dataset of diabetic foot images (DFUC 2021) provide encouraging performance measures, such as a high F1 score, accuracy, and precision, indicating how well the suggested method works for DFU detection. The proposed system indicates that the DenseNet201 attains the greatest F 1 score of 98%, 98% and 97% for Ischemia, None and infection stage, respectively. Overall, the integration of deep learning techniques into DFU detection holds significant potential for advancing diabetic care as well as lowering the chance of problems.",Deep learning;Training;Analytical models;Visualization;Accuracy;Medical services;Wounds,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575186,IEEE Conferences,,,,,,
A Comprehensive Review of Fracture Detection and Identification from Medical Images using Deep Learning Methods,T. Praveen Kumar; K. Suthendran,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"The detection and identification of fractures have significant importance in the field of medical imaging, as they play a crucial role in facilitating precise diagnoses and prompt development of treatment strategies. Over the course of the last decade, there has been a notable progression in deep learning methods, demonstrating their effectiveness in a range of medical image analysis, such as fracture diagnosis and classification. This review study offers a full and extensive examination of recent advancements in using deep learning techniques for the purpose of detecting and identifying fractures from medical imaging. This study conducts a comprehensive examination of the prevailing methodology, commonly used datasets, inherent obstacles, and probable future trajectories within this rapidly developing field. The objective of this work is to provide significant insights into the transformational effects of deep learning on the interpretation of medical images linked to fractures. This research seeks to contribute to the improvement of patient care and diagnosis by examining the influence of Deep Learning (DL) in this field.",Deep learning;Image analysis;Reviews;Trajectory;Artificial intelligence;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575018,IEEE Conferences,,,,,,
Ocular Disease Recognition using EfficientNet,N. Balakrishna; M. B. Mukesh Krishnan; E. V. R. Sai; S. V. Ranganath; K. Sonika; L. G. Priyanka,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),02-Jul-24,2024,"Ocular diseases present a significant public health concern globally, warranting precise and efficient diagnostic methodologies. This research aims to develop an approach for identifying ocular diseases by classifying eight different eye conditions using fundus images. Leveraging transfer learning with EfficientNet, a powerful convolutional neural network architecture which is employed to learn discriminative features from the images, facilitating precise classification of various ocular diseases. Performance evaluations on the Ocular disease dataset ODIR highlight the practicality and effectiveness of our scheme. In addition to diagnostics, this combination has the potential for application in the fields of telemedicine and telehealth, enabling rapid and reliable diagnosis of eye diseases, especially in restricted or remote areas. The combination of Transfer Learning and CNN not only improves the accuracy of diagnosis but also simplifies the analysis process, helping to increase the efficiency of treatment and improve the patientâ€™s outcome effect.",Performance evaluation;Image recognition;Telemedicine;Transfer learning;Imaging;Eye diseases;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575282,IEEE Conferences,,,,,,
Multi-Label Chest X-Ray Image Classification with Single Positive Labels,J. Xiao; S. Li; T. Lin; J. Zhu; X. Yuan; D. D. Feng; B. Sheng,IEEE Transactions on Medical Imaging,,2024,"Deep learning approaches for multi-label Chest X-ray (CXR) images classification usually require large-scale datasets. However, acquiring such datasets with full annotations is costly, time-consuming, and prone to noisy labels. Therefore, we introduce a weakly supervised learning problem called Single Positive Multi-label Learning (SPML) into CXR images classification (abbreviated as SPML-CXR), in which only one positive label is annotated per image. A simple solution to SPML-CXR problem is to assume that all the unannotated pathological labels are negative, however, it might introduce false negative labels and decrease the model performance. To this end, we present a Multi-level Pseudo-label Consistency (MPC) framework for SPML-CXR. First, inspired by the pseudo-labeling and consistency regularization in semi-supervised learning, we construct a weak-to-strong consistency framework, where the model prediction on weakly-augmented image is treated as the pseudo label for supervising the model prediction on a strongly-augmented version of the same image, and define an Image-level Perturbation-based Consistency (IPC) regularization to recover the potential mislabeled positive labels. Besides, we incorporate Random Elastic Deformation (RED) as an additional strong augmentation to enhance the perturbation. Second, aiming to expand the perturbation space, we design a perturbation stream to the consistency framework at the feature-level and introduce a Feature-level Perturbation-based Consistency (FPC) regularization as a supplement. Third, we design a Transformer-based encoder module to explore the sample relationship within each mini-batch by a Batch-level Transformer-based Correlation (BTC) regularization. Extensive experiments on the CheXpert and MIMIC-CXR datasets have shown the effectiveness of our MPC framework for solving the SPML-CXR problem.",Transformers;Image classification;Annotations;Perturbation methods;Pathology;Diseases;Correlation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579876,IEEE Early Access Articles,,,,,,
Semi-Supervised Framework for Dual Encoder Attention Network: Classification of Retinopathy in Optical Coherence Tomography Images,H. Qiu; J. Zheng,2024 16th International Conference on Computer and Automation Engineering (ICCAE),01-Jul-24,2024,"Due to the high inter-class similarity and intra-class differences in retinal optical coherence tomography (OCT) images, subtle pathological attributes can serve as important dis-criminative clues, and semi-supervised medical image tasks still face challenges. To address this challenge, a new semi-supervised framework for medical image classification is proposed. This method introduces a deep feature fusion classification method based on dual encoders. The different branches of the encoder can extract different feature information, and their features are fused to achieve information complementarity, so that the model can learn subtle pathological attributes in medical images. To improve the model's discrimination and classification performance, an adaptive attention (AA) mechanism is introduced to capture key regions. The proposed method is evaluated on a public OCT dataset. Only 10% of labeled data is used for training, achieving an accuracy of 96.02 %, which is equivalent to the performance of fully supervised models trained with 100% labeled data. Experimental results show that the performance of this method is better than that of supervised benchmark methods and other semi-supervised methods.",Adaptation models;Pathology;Optical coherence tomography;Semisupervised learning;Feature extraction;Retina;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10569181,IEEE Conferences,,,,,,
DCDiff: Dual-Granularity Cooperative Diffusion Models for Pathology Image Analysis,J. Fan; T. Lv; P. Wang; X. Hong; Y. Liu; C. Jiang; J. Ni; L. Li; X. Pan,IEEE Transactions on Medical Imaging,,2024,"Whole Slide Images (WSIs) are paramount in the medical field, with extensive applications in disease diagnosis and treatment. Recently, many deep-learning methods have been used to classify WSIs. However, these methods are inadequate for accurately analyzing WSIs as they treat regions in WSIs as isolated entities and ignore contextual information. To address this challenge, we propose a novel Dual-Granularity Cooperative Diffusion Model (DCDiff) for the precise classification of WSIs. Specifically, we first design a cooperative forward and reverse diffusion strategy, utilizing fine-granularity and coarse-granularity to regulate each diffusion step and gradually improve context awareness. To exchange information between granularities, we propose a coupled U-Net for dual-granularity denoising, which efficiently integrates dual-granularity consistency information using the designed Fine- and Coarse-granularity Cooperative Aware (FCCA) model. Ultimately, the cooperative diffusion features extracted by DCDiff can achieve cross-sample perception from the reconstructed distribution of training samples. Experiments on three public WSI datasets show that the proposed method can achieve superior performance over state-of-the-art methods. The code is available at https://github.com/hemo0826/DCDiff.",Feature extraction;Noise;Noise reduction;Image analysis;Diffusion processes;Training;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10577168,IEEE Early Access Articles,,,,,,
Detection of Foot-Ulcer from Digital Photographs using MobileNet Variants with Features Fusion,S. Ramadasan; R. Augasthega; K. Vijayakumar; S. Prabha,2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM),28-Jun-24,2024,"Diabetes is a chronic illness usually caused by elevated blood glucose levels. If left untreated, diabetes can have serious repercussions. Diabetic foot ulcers (DFUs) is a chief complications of diabetes, which can result in foot wounds and, if left untreated, can amputation of a limb or leg. A visual examination by the physician is typically used to identify the DFU at the clinical level and assess its severity. This research aims to create a computer program that can identify DFU from digital photos taken using a camera. The stages of the developed technique are as follows: gathering and processing DFU images, extracting features using a selected deep-learning (DL) scheme, reducing and fusing features, and classifying the results using five-fold cross-validation. In this research, the benchmark DFU images were taken into consideration, and MobileNet (MN) and its variations were used to carry out the classification task. Using the individual deep-features, the proposed DFU detection task is first carried out. Afterwards, the selected features are serially fused to obtain a new feature vector, and the proposed work is repeated. The study's experimental results validate the developed technique's merit, as the K-Nearest Neighbor classifier achieves 97% detection accuracy.",Visualization;Accuracy;Medical services;Wounds;Feature extraction;Vectors;Diabetes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568675,IEEE Conferences,,,,,,
NNXG: Privacy based image processing in Pneumonia Detection from Chest X-ray using Modified Neural Network Architecture and XGBoost,V. Nanammal; J. Jebastine; B. v. R J,2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM),28-Jun-24,2024,"Pneumonia stands as a prevalent affliction claiming lives of both the young and the elderly worldwide, particularly prominent in underdeveloped regions. Its impact resonates through families, encompassing generations. This respiratory ailment ensues from microbial invasion of the lungs, necessitating prompt diagnosis for effective treatment. While conventional methods like chest X-rays aid in identification, challenges arise in discerning subtle anomalies or differentiating them from similar conditions. Hence, the integration of computer-aided diagnostic tools becomes imperative. Herein, we propose an innovative approach to pneumonia detection, amalgamating traditional deep learning networks with an XGBoost Classifier. Moreover, we advocate for the incorporation of digital watermarking technology to safeguard medical image privacy, integrity, and management. The retrieval algorithm ensures seamless access to concealed data when needed. Extensive validation of diverse algorithms based on performance and reliability culminates in conclusive findings and recommendations for future exploration.",Privacy;Pneumonia;Watermarking;Mathematics;Classification algorithms;Reliability;Older adults,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568581,IEEE Conferences,,,,,,
Development of a Novel Brain Tumor Classification Methodology Using Modified Deep Learning Principles,P. P. Mohan; G. Ramkumar,2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM),28-Jun-24,2024,"Currently, tumours rank as the second most common cancer kind. A great number of people are at risk because of cancer. In order to diagnose tumours like brain tumours, the medical sector requires a method that is quick, automated, efficient, and dependable. Treatment relies heavily on detection. Medical professionals will keep a patient safe if a tumour can be detected accurately. There are a number of image processing methods utilized by this programme. Many tumour patients have been saved because to this software, which allows physicians to give the right treatment. Unregulated cell growth is the hallmark of a tumour. As they multiply, brain tumour cells engulf all the nutrition that should be going to healthy brain cells and tissues, leading to brain failure. At present, doctors find out where the cancer is and how big it is by manually examining magnetic resonance pictures of the patient's brain. Not only is this an extremely time-consuming process, but it also leads to erroneous tumour detection. Modified Learning for Brain Tumour Classification (MLBTC) is a new tumour classification model that we presented in this paper. To evaluate its effectiveness, it is cross-validated using the current Convolutional Neural Network deep learning technology. Its purpose is to intelligently detect brain cancers. Using the most popular deep learning architectures, the suggested approaches intelligently classify brain tumours. This study aims to evaluate and analyze deep learning technologies with the purpose of guiding academics and medical professionals towards strong systems that identify brain tumours.",Deep learning;Microprocessors;Magnetic resonance;Medical services;Computer architecture;Software;Mathematics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568707,IEEE Conferences,,,,,,
Deep Learning-Based Detection and Severity Analysis of Corona and Pneumonia Using CNN with Wolf Optimization Algorithm,M. B. Kutti; T. Karthick,2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM),28-Jun-24,2024,"Effective identification and severity analysis of two serious respiratory diseases-pneumonia and corona-are desperately needed, and this study attempts to fill that gap. Early diagnosis and treatment of many conditions depend critically on prompt intervention. Diagnoses are made slowly and subjectively by existing procedures, which frequently rely on human interpretation of medical imaging data. The existing methods for severity analysis do not have a cohesive structure. Our study presents an all-encompassing strategy to get around these restrictions. First, to improve the quality of the images, grayscale-based pre-processing is applied. Next, marker-based watershed segmentation is used to precisely locate anomalies. Feature extraction and categorization are done using a convolutional neural network, or CNN. Specifically, for severity analysis, we optimize the CNN parameters by including the Wolf Optimization Algorithm (WOA). Grayscale pre-processing guarantees the best possible picture quality; marker-based watershed segmentation improves the accuracy of lesion identification; CNN streamlines feature extraction; and WOA supports the study of disease severity. Python is used in the study's implementation, which takes advantage of its many machine learning frameworks and tools. The outcomes show how well the suggested methodology works for precisely identifying and evaluating the levels of corona and pneumonia. By integrating WOA with CNN, the shortcomings of current techniques are addressed and diagnostic accuracy and severity evaluation are improved. When compared to alternative approaches like CNN-SVM, Bayesian CNN, DCNN with BPNN, the suggested CNN with WOA (99.3%) shows an average accuracy gain of about 11.68%, demonstrating higher performance in disease diagnosis and severity analysis. This innovative method offers a methodical and effective framework for better patient treatment, which has the potential to advance the field of respiratory disease diagnostics.",Training;Learning systems;Accuracy;Pneumonia;Watersheds;Corona;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568600,IEEE Conferences,,,,,,
Brain Tumor Detection: Deploying Stochastic Gradient Descent Classifier in a Web App,B. v. R J; M. J; V. K S,2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM),28-Jun-24,2024,"The early detection of brain tumors is crucial for effective treatment and patient outcomes. In this paper, we propose a novel approach for brain tumor detection by deploying a Stochastic Gradient Descent (SGD) Classifier within a user-friendly web application. Our method aims to provide accessible and efficient brain tumor detection capabilities to both clinicians and patients. By leveraging machine learning technology and web-based infrastructure, our system enables users to upload medical images, preprocess them, and obtain predictions for the presence of brain tumors. Through a detailed analysis of the development process, including data preprocessing, model training, web app integration, and deployment, we demonstrate the feasibility and effectiveness of our approach. The system's performance is evaluated through experiments on a test dataset, showcasing high accuracy and efficiency in brain tumor detection. By comparing our system with existing approaches, we highlight its potential to contribute significantly to the field of medical imaging and healthcare. The deployment of our web application facilitates widespread access to brain tumor detection capabilities, potentially leading to improved early diagnosis and treatment of brain-related diseases. Overall, our research presents a promising solution for enhancing brain tumor detection through the seamless integration of machine learning and web technologies.",Training;Visualization;Accuracy;Target recognition;System performance;Stochastic processes;Machine learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568624,IEEE Conferences,,,,,,
Oral Cancer Detection Using Convolutional Neural Networks,T. Jagadesh; K. P; K. A; L. V; J. B,2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM),28-Jun-24,2024,"In developing nation oral cancer makes the significant threat to human life. The existing system uses deep learning technique for detection of oral cancer in medical imaging. The two powerful tools of deep learning techniques are Convolutional Neural Networks and Deep Belief Network [1]. PSOBER- Particle Swarm Optimization and AI-Biruni Earth Radius a hybrid optimization algorithm is used to optimize the CNN and DBN. To overcome the outperformance of the existing method standard dataset of biomedical images are used for demonstration of the promising results. To test and validate the significance and stability one-way ANOVA and Wilcoxon signed-rank is used. For screening oral cancer detection in clinical setting this technique is used. However, the accuracy and to find the early detection of oral cancer and to make the less suffering of the patients the further research is needed. The proposed method of oral cancer detection using deep learning technique includes the enhancement of the images. In this method salt and pepper noise detection is used to find the accurate results using segmentation of the cells for thresholding. Back Propagation Neural Networks (BPNN) is used for the classification of the oral cancer. Tumor partition and features extraction are done in the proposed method. Feeding the loss backward through neural networks layers to fine tune the weights and the BPNN involves the error rate of forward propagation. Back propagation neural networks contain two signals. They are Error signal and Back signal.",Deep learning;Backpropagation;Accuracy;Neural networks;Cancer detection;Feature extraction;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568599,IEEE Conferences,,,,,,
Multi-Class Gaze Detection in a Dynamic Environment,A. Lochbihler; B. Wallace; K. Van Benthem; C. Herdman; W. Sloan; K. Brightman; F. Knoefel; S. Marshall; R. Goubran,2024 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),28-Jun-24,2024,"Developing AI tools to identify areas of interest within a dynamic field of view is essential for objective behavioural evaluation of drivers. Video image classification and specifically image segmentation is a key technology to allow for the possibility of physiological and behavioural measurement of drivers. For example, to understand driver attention, one must measure where a driver is looking when driving and this requires segmentation of their field of view into relevant areas of interest, such as windows, mirrors, and dashboard. The present work addresses the challenge of dynamic field of view classification and shows the impact of transfer learning, a new AI tool, on segmentation accuracy. Results from this study demonstrate that transfer learning improves predictive performance by 0.02 to 0.20 Dice when large training sets were used. The resulting performance was >0.80 Dice for all classification tests of driver attention segmentation. This work showed that transfer learning also supported the use of smaller training sets while still providing adequate performance. This finding is key for applications where labeled training data is limited or costly to create. The present results expand the application space for deep learning-based image segmentation models.",Training;Image segmentation;Transfer learning;Training data;Physiology;Object recognition;Mirrors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560882,IEEE Conferences,,,,,,
AttentionLUNet: A Hybrid Model for Parkinsonâ€™s Disease Detection using MRI Brain,A. R. Palakayala; P. Kuppusamy,IEEE Access,,2024,"Magnetic Resonance Imaging (MRI) is a medical imaging method used to visualize the brainâ€™s anatomy, evaluate its function, and identify any abnormalities or disorders without the need for surgical intervention. Parkinsonâ€™s Disease (PD) is a condition of gradual nervous decline that effects the neurological system and the bodily functions regulated by the nerves. The impact on Quality of Life (QOL) is significant, resulting in stigma, deterioration of cognitive function, and increased limitations in mobility, including activities of daily living. Hence, early-stage diagnosis and classification of PD is crucial. This study introduces a new Deep Neural Network architecture, designed by combining the LeNet and U-Net models (LUNet) with added attention and/or residual modules for the identification of PD. The MR Images underwent pre-processing and augmentation to facilitate the precise and efficient training of Deep Learning (DL) models. The proposed model was trained using 2000 enhanced images, while validation and testing was conducted on a set of 500 untrained data. The final model is assessed using various statistical evaluation metrics and compared with LeNet-5, U-Net model along with its variants and existing works. The overall accuracies of LeNet, U-Net, and the Proposed model were 95.92 %, 97.6 %, and 99.58 % respectively.",Magnetic resonance imaging;Brain modeling;Image segmentation;Computer architecture;Computational modeling;Diseases;Image processing;Neurological diseases;Parkinson's disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10574812,IEEE Early Access Articles,,,,,,
HiCervix: An Extensive Hierarchical Dataset and Benchmark for Cervical Cytology Classification,D. Cai; J. Chen; J. Zhao; Y. Xue; S. Yang; W. Yuan; M. Feng; H. Weng; S. Liu; Y. Peng; J. Zhu; K. Wang; C. Jackson; H. Tang; J. Huang; X. Wang,IEEE Transactions on Medical Imaging,,2024,"Cervical cytology is a critical screening strategy for early detection of pre-cancerous and cancerous cervical lesions. The challenge lies in accurately classifying various cervical cytology cell types. Existing automated cervical cytology methods are primarily trained on databases covering a narrow range of coarse-grained cell types, which fail to provide a comprehensive and detailed performance analysis that accurately represents real-world cytopathology conditions. To overcome these limitations, we introduce HiCervix, the most extensive, multi-center cervical cytology dataset currently available to the public. HiCervix includes 40,229 cervical cells from 4,496 whole slide images, categorized into 29 annotated classes. These classes are organized within a three-level hierarchical tree to capture fine-grained subtype information. To exploit the semantic correlation inherent in this hierarchical tree, we propose HierSwin, a hierarchical vision transformer-based classification network. HierSwin serves as a benchmark for detailed feature learning in both coarse-level and fine-level cervical cancer classification tasks. In our comprehensive experiments, HierSwin demonstrated remarkable performance, achieving 92.08% accuracy for coarse-level classification and 82.93% accuracy averaged across all three levels. When compared to board-certified cytopathologists, HierSwin achieved high classification performance (0.8293 versus 0.7359 averaged accuracy), highlighting its potential for clinical applications. This newly released HiCervix dataset, along with our benchmark HierSwin method, is poised to make a substantial impact on the advancement of deep learning algorithms for rapid cervical cancer screening and greatly improve cancer prevention and patient outcomes in real-world clinical settings.",Feature extraction;Hospitals;Accuracy;Task analysis;Correlation;Cervical cancer;Pathology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10571965,IEEE Early Access Articles,,,,,,
Deep Learning Approaches to Gastrointestinal Image Classification: Evaluating VGG19 and EfficientNetB0 Models,S. Padmakala,"2024 7th International Conference on Devices, Circuits and Systems (ICDCS)",26-Jun-24,2024,"This research reports on the comparison of a VGG19 and an EfficientNetB0 models for the identification of gastrointestinal images.Â At the same time that deep learning is becoming the cornerstone of medical imaging applications and as such, our research will aim to select the most accurate, fast, and robust model out of them.Â By incorporating a Kaggle dataset which include the whole array of gastrointestinal disorders, we resorted to the processes of data augmentation, preparation, and model experimental work involving the training of the model and its testing.Â It was seen that the functionalities of EfficientNetB0(Test Accuracy-84.4% and Test Loss-0.554)- were much better (in respect to accuracy in classification and computational efficiency) than those of VGG19(Test Accuracy-83.5% and Test Loss-0.905).Â It is a clear proof of the high generalizability of EfficientNetB0 model from different datasets, probably due to an optimal arrangement of three factors (depth, width and resolution).Â This work concludes with empathizing the possible use of, EfficientNetB0 in the healthcare space when it comes to diagnosing gastrointestinal problems which therefore results in better patient outcomes.Â Along with this, future work will look into convergence of these models into clinical practice and envisioning them for medical image classification through the integration of deep learning-based technology.",Deep learning;Training;Analytical models;Visualization;Accuracy;Computational modeling;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560955,IEEE Conferences,,,,,,
Precision MRI Brain Tumor Identification: Leveraging Advanced Techniques for Accurate Classification,D. D. G; S. Doss; S. S; S. C. N,2024 International Conference on Computing and Data Science (ICCDS),26-Jun-24,2024,"A vital difficulty in medical imaging is the diagnosis of brain tumors using MRI, which can occasionally be impeded by noise and inaccuracies in the imaging process. In this study, we provide a novel method to enhance the accuracy and reliability of brain tumor detection from MRI data by combining advanced Denoising and classification approaches with convolutional neural networks. The cv2.fastNIMeansDenoising technique from the OpenCV library is used in the denoising step to efficiently remove noise while maintaining significant picture properties. The preprocessed images are then fed into a VGG16 classification model, which is well-known for performing exceptionally well in image recognition tasks. Our strategy attempts to obtain greater classification accuracy than conventional methods by utilizing the spatial correlations exhibited and the discriminative powers of VGG16. Experimental results on benchmark MRI brain tumor datasets illustrate the efficacy of the proposed technique, with notable improvements in tumor identification accuracy and robustness against noise. With neuroimaging diagnostics, this novel approach may improve clinical procedures and patient outcomes.",Accuracy;Magnetic resonance imaging;Noise reduction;Noise;Brain modeling;Robustness;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560447,IEEE Conferences,,,,,,
A Study on Ethical Considerations in Automated Lung Ultrasound Analysis,A. G. E. Thomas; J. S. Duela,2024 International Conference on Computing and Data Science (ICCDS),26-Jun-24,2024,"Artificial intelligence has transformed lung ultrasound analysis in medical imaging, enhancing both accuracy and efficiency. However, this technological evolution introduces a complex array of ethical challenges that are pivotal to address. The use of AI tools in healthcare raises concerns about bias, accountability, transparency, and patient privacy. This paper provides an in-depth exploration of these ethical issues, particularly emphasizing how biases in AI algorithms could potentially lead to disparities in healthcare outcomes. It examines the difficulties in attributing responsibility for misdiagnoses in the context of AI, highlighting the blurred lines between clinician and algorithmic decision-making. Furthermore, the paper addresses the critical need for transparent AI systems that remain interpretable to healthcare professionals, balancing the intricacies of deep learning models with the necessity for clarity in medical decision-making. Privacy concerns are also paramount, given the sensitive nature of patient data used in training and implementing these AI systems. This paper proposes an ethical framework for using AI in lung ultrasound diagnostics, based on a thorough review of current practices and research. This framework emphasizes interdisciplinary collaboration and the development of standardized guidelines to ensure ethically responsible integration of AI, thereby safeguarding patient trust and upholding the quality of care in the era of digital medicine.",Training;Ethics;Privacy;Ultrasonic imaging;Reviews;Decision making;Lung,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560375,IEEE Conferences,,,,,,
A Comprehensive Technique Toward Pneumonia Diagnosis via X-Ray Images Through Deep Learning,D. Kothapalli; V. M. S. V. K. Alapati; A. Mokshagna; G. K. Kumar; J. S. P. Reddy,2024 International Conference on Computing and Data Science (ICCDS),26-Jun-24,2024,"Early identification is crucial for improved results in pneumonia, which is described by inflammation of the lungs. The principal objective of this investigation is to design and implement an advanced deep learning scheme for the purpose of detecting pneumonia. Development of a system incorporating a diverse array of chest X-ray images depicting both healthy and pneumonia-stricken patients is the aim of this research. Models being trained to recognize certain attributes that indicate the presence of pneumonia. Architectures like CNN, ResNet-50, DenseNet-121, and Inception Net V3 are capable of extracting hierarchical features. Through extensive training on a substantial dataset, the parameters of the model are refined, leading to enhanced capability in distinguishing between individuals with normal conditions and those with pneumonia. The effectiveness of the model in identifying pneumonia is assessed through the utilization of metrics including accuracy, sensitivity, and specificity. The efficiency of deep learning models is substantiated by empirical evidence, which underscores their capability to develop robust pneumonia detection systems that enable prompt medical intervention.",Deep learning;Training;Pneumonia;Accuracy;Neural networks;Sensitivity and specificity;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560414,IEEE Conferences,,,,,,
Exploring the Experimental Effectiveness of a Hybrid Deep Learning Approach in Detecting Thyroid Disease using Intelligent Predictions,K. Anitha; V. Lavanya; P. Chitra; A. S. Valarmathy; B. Alekhya; R. Y. P,2024 International Conference on Computing and Data Science (ICCDS),26-Jun-24,2024,"In the realm of machine learning, addressing class imbalance is a prevalent challenge, especially in datasets where certain classes are underrepresented. Synthetic Minority Over-sampling Technique (SMOTE) emerges as a powerful solution, strategically generating synthetic instances for minority classes, thereby rebalancing the dataset. By carefully selecting parameters and employing appropriate evaluation metrics, SMOTE enhances the overall classification performance, ensuring accurate representation of minority classes essential for real-world applications. Bidirectional Feature Elimination (BiDFE) stands out as a sophisticated technique for this purpose. BiDFE iteratively identifies and retains the most relevant features pertinent to thyroid disease classification, striking a balance between adding informative features and removing redundant ones. This method enhances the interpretability and generalization of the model, ensuring its robustness and efficiency in handling thyroid disease data. The Proposed Model TDIP (Thyroid Disease Identification and Prediction), a novel hybrid approach integrating Long Short-Term Memory (LSTM) and Support Vector Machine (SVM) algorithms. Leveraging LSTM's temporal learning capabilities and SVM's discriminative power, TDIP achieves an impressive accuracy of 97% in thyroid disease classification.",Support vector machines;Accuracy;Sensitivity;Vectors;Robustness;Classification algorithms;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560401,IEEE Conferences,,,,,,
Revolutionizing Alzheimer's Disease Prediction Using EfficientNetB6,V. S. Praneeth; N. Gowtham; S. RamaChandran; R. Jansi,"2024 Tenth International Conference on Bio Signals, Images, and Instrumentation (ICBSII)",26-Jun-24,2024,"Alzheimer's disease (AD) presents a pressing challenge globally, demanding the development of accurate predictive models for early diagnosis and intervention. In this research, we propose a novel prediction framework harnessing deep learning methodologies, specifically employing the advanced EfficientNetB6 algorithm, on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset comprising approximately 20,000 MRI images. Our aim is to differentiate between MRI images indicating normal cognitive function and those signaling AD progression, with a target of achieving 97.78% accuracy and a 98.21% F1 score. Moreover, our model categorizes images into various stages, including mild cognitive impairment (MCI), facilitating early identification. The proposed model's high accuracy and precision hold the potential to transform early detection efforts, allowing for timely interventions and thereby improving patients' quality of life. In addition to leveraging the EfficientNetB6 model for image feature extraction and LSTM networks for sequential data analysis, the proposed hybrid architecture incorporates dense layers to facilitate feature fusion and abstraction. The dense layers enable the model to combine the extracted image and sequential features, capturing complex relationships between different modalities and enhancing the model's discriminative power. By integrating dense layers, the model gains the flexibility to learn higher-level representations, further refining the extracted features for improved classification performance. This holistic approach, combining convolutional, recurrent, and dense layers, offers a comprehensive solution for multi- modal data analysis, showcasing the potential of deep learning techniques in addressing complex real-world problems.",Neuroimaging;Deep learning;Accuracy;Data analysis;Magnetic resonance imaging;Transforms;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10564023,IEEE Conferences,,,,,,
Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Classification and Lateralization Analysis,C. Zhu; Y. Tan; S. Yang; J. Miao; J. Zhu; H. Huang; D. Yao; C. Luo,IEEE Transactions on Medical Imaging,,2024,"Available evidence suggests that dynamic functional connectivity can capture time-varying abnormalities in brain activity in resting-state cerebral functional magnetic resonance imaging (rs-fMRI) data and has a natural advantage in uncovering mechanisms of abnormal brain activity in schizophrenia (SZ) patients. Hence, an advanced dynamic brain network analysis model called the temporal brain category graph convolutional network (Temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis module, DSF-BrainNet, was designed to construct dynamic synchronization features. Subsequently, a revolutionary graph convolution method, TemporalConv, was proposed based on the synchronous temporal properties of features. Finally, the first modular test tool for abnormal hemispherical lateralization in deep learning based on rs-fMRI data, named CategoryPool, was proposed. This study was validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average accuracies, respectively, outperforming the baseline model and other state-of-the-art methods. The ablation results also demonstrate the advantages of TemporalConv over the traditional edge feature graph convolution approach and the improvement of CategoryPool over the classical graph pooling approach. Interestingly, this study showed that the lower-order perceptual system and higher-order network regions in the left hemisphere are more severely dysfunctional than in the right hemisphere in SZ, reaffirmings the importance of the left medial superior frontal gyrus in SZ. Our code was available at: https://github.com/swfen/Temporal-BCGCN.",Convolution;Brain;Feature extraction;Brain modeling;Mental disorders;Medical diagnostic imaging;Synchronization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10570443,IEEE Early Access Articles,,,,,,
Oral Cancer Detection Using Deep Learning,R. Chavva; J. P. S; Mathu,2024 International Conference on Science Technology Engineering and Management (ICSTEM),25-Jun-24,2024,"Use of deep learning methods, and more especially the DenseNet architecture, for the purpose of oral cancer detection is the primary goal of this research. Oral cancer, thrush, lichens, hairy tongue, leukoplakia, and healthy tongue photos are all part of the oral conditions dataset. The dataset includes oral images from both healthy individuals and those with illness. We added new classification layers after fine-tuning a pre-trained DenseNet169 model that had been trained on ImageNet using transfer learning. Many methods of data augmentation were used to improve model resilience. A performance comparison using the LeNet model was done. The DenseNet based model demonstrated exceptional performance, attaining 94.08% accuracy, 94.16% precision, 94.7% recall, and 94.7% F1 score. By comparison, the LeNet model performed less, with an F1 score of 63.01%, recall of 64.03%, accuracy of 64.02%, and precision of 64.06%. Medical image analysis relies on data preparation, model selection, and assessment methods this study shows that deep learning might be useful for detecting oral cancer. Improving the model's performance for real-world deployment in oral cancer detection might be achieved by more tuning and testing.",Deep learning;Analytical models;Accuracy;Tongue;Transfer learning;Cancer detection;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561172,IEEE Conferences,,,,,,
Infant Brain Tumor Detection Using Deep Learning,B. S. Kumar Reddy; G. N. R; R. C,2024 International Conference on Science Technology Engineering and Management (ICSTEM),25-Jun-24,2024,"The difficulties in early detection and the potentially fatal nature of brain tumors in infants make them a major medical concern. The goal of this research is to create and test deep learning models that can automatically categorize pictures of brain tumors in infants. With a comparative investigation of their performance, two different neural network designs, DenseNet and AlexNet, are used for this purpose. The data set utilized for this analysis consists of pictures of brain tumors in infants, broken down into four types: pituitary, glioma, meningioma, and non-tumor. For data processing, model creation, and assessment, the project makes use of Python-based libraries including NumPy, pandas, seaborn, OpenCV, and TensorFlow. We use the state-of-the-art DenseNet201 architecture for our deep learning model. To improve their generalizability and robustness, both models are preprocessed and data augmented. A variety of measures, including as accuracy, precision, recall, and F1-score, are used to evaluate the model's performance during training and validation on the dataset. The findings show that DenseNet201 is more successful than AlexNet in classifying newborn brain tumors, with a weighted F1-score of 0.9703 and an accuracy of 97.048% on the validation set. Early detection and intervention have the potential to improve patient outcomes in instances of newborn brain tumors, and our project's accomplishment in creating efficient and accurate deep learning models for infant brain tumor categorization bears this promise. Improvements to the models and their use in clinical settings to assist doctors with diagnosis might be in the works for the future.",Deep learning;Pediatrics;Accuracy;Uncertainty;Medical services;Brain modeling;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560909,IEEE Conferences,,,,,,
AI Based Brain Tumor Detection Using Deep Learning,B. Harish; E. Jangam,2024 International Conference on Science Technology Engineering and Management (ICSTEM),25-Jun-24,2024,"Research centered on leveraging advanced technology identifying abnormalities. Specifically, the study implements UNet and ResUNet algorithms for segmentation, as well as ResNet and Transformers for classification, aiming to expedition. The methodology entails preprocessing the input data, training the deep comprehensive and fine-tuning networks optimize performance in discerning various tumor types. Through extensive testing and validation on diverse datasets, the findings showcase improved accuracy and novelty in diagnostic capabilities. This innovative approach not only streamlines the diagnostic process, reducing the time required for precise diagnosis, but also diminishes the potential for human error, thereby elevating the overall standard of neurology healthcare delivery.",Deep learning;Training;Brain cancer;Medical services;Transformers;System identification;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560895,IEEE Conferences,,,,,,
Redefining Diagnosis: A Novel Approach to Detect Brain Hemorrhages using Modified Learning Principles with Artificial Intelligence Association,N. V. RamaKrishna G.; J. J. Babu; R. Shankari; E. Dilipkumar; C. N. Ravi; T. Vanaja,"2024 1st International Conference on Innovative Sustainable Technologies for Energy, Mechatronics, and Smart Systems (ISTEMS)",25-Jun-24,2024,"Numerous studies have concentrated on developing computer-aided diagnostic methods. They work on the principle of fast and accurate diagnosis by processing and analysis of pictures of various human body parts. In order to determine if a brain haemorrhage is present in Computed Topography (CT) images of the brain, this research follows the aforementioned technique. In addition, the kind of haemorrhage is determined. An AI-assisted prediction scheme, AI-based Brain Haemorrhages Detection (AIBHD), is presented in this paper. In order to determine how well the suggested model works, it is cross-validated with regard to the conventional learning scheme, Random Forest Classifier (RFC). Image preprocessing, segmentation, feature extraction, and classification are some of the steps that make up the developed system. The trials that were carried out yielded really encouraging outcomes. A brain haemorrhage is a form of stroke in which blood clots form in the brain's surrounding tissues as a result of an arterial burst. This method classifies brain haemorrhages using computed tomography (CT) scans. The suggested approach, AIBHD, is used for picture segmentation after preprocessing and morphological procedures. The classification model takes this extracted picture as input and outputs details on the area and percentage of haemorrhage. The results of this study improve the likelihood of identifying haemorrhage in a picture and categorize it accordingly. On average, the system's accuracy in haemorrhage classification is determined to be 97.34%.",Training;Image segmentation;Accuracy;Mechatronics;Computed tomography;Computational modeling;Surfaces,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560202,IEEE Conferences,,,,,,
SyNet:Medical image anomaly detection with noise synthesis network,J. Liu; W. Liang; Y. Chen; K. Jin,2024 IEEE 10th Conference on Big Data Security on Cloud (BigDataSecurity),25-Jun-24,2024,"Medical image anomaly detection refers to machine learning techniques to analyze and identify lesions and abnormalities in them. However, in medical images, anomaly samples are usually sparse, which can lead to supervised learning not being able to obtain sufficient training. While existing unsupervised learning is mainly based on reconstruction and generation as well as feature embedding, these methods do not take into account the problem of data type bias during domain migration and often face the problem of loose judgment boundaries. To this end, this paper proposes a novel unsupervised learning method, SyNet, based on noisy anomaly synthesis. First, the problem at domain migration is well solved by aggregating the range of features within the feature map, fusing the multiscale features at the intermediate level, and adding feature adapters. Then, anomaly samples are synthesized by adding noise in the feature space, which is more efficient and stable than generating samples in the image space. Finally, normal and abnormal data are distinguished by training the discriminator. In this paper, experiments are conducted on ISIC and brain MRI datasets, and SyNet achieves nearly 20% improvement in AUC, ACC and other metrics compared with the current mainstream methods, and also has good inference efficiency.",Training;Magnetic resonance imaging;Noise;Supervised learning;Security;Noise measurement;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10565014,IEEE Conferences,,,,,,
Dual-Masked Autoencoders: Application to Multi-Labeled Pediatric Thoracic Diseases,T. Yoon; D. Kang,IEEE Access,27-Jun-24,2024,"Pediatric thoracic diseases present significant health risks to children. While chest X-rays are commonly used for diagnosing thoracic diseases, interpreting pediatric images comes with unique challenges such as anatomical variations, developmental differences, and potential artifacts. Deep learning offers promise in addressing these challenges, yet its effectiveness is hindered by the limited availability of pediatric chest X-ray data. To overcome this limitation, we introduce the dual-masked autoencoders (dual-MAE) algorithm, consisting of online and target networks with encoder and decoder modules. These networks are optimized by minimizing three losses: between the reconstructed image of the online network and the target network, between the input image and the reconstructed image of the online network, and between the input image and the reconstructed image of the target network. To learn efficiently from pediatric chest X-rays, we employ a two-step training strategy: pretraining the dual-MAE model on adult chest X-rays, then fine-tuning it on pediatric X-rays for diagnosing multi-labeled pediatric thoracic diseases. The proposed model exhibited superior performance with the highest mean AUC score (0.752), surpassing the ResNet-34 (0.669) and ViT-S (0.645) trained from scratch. Additionally, the dual-MAE model outperformed the ResNet-34 (0.697) and ViT-S (0.638), both pretrained on the ImageNet dataset and then fine-tuned on pediatric chest X-rays. Despite being pretrained on a significantly smaller number of X-rays compared to the ImageNet dataset, our model demonstrated better performance. Furthermore, it outperformed the ResNet-34 (0.712), ViT-S (0.673), and vanilla MAE method (0.735), all pretrained on adult chest X-rays and fine-tuned on pediatric chest X-rays. Even with only 50% of labeled pediatric chest X-ray images, dual-MAE demonstrated comparable performance to that of the vanilla MAE method and outperformed ResNet-34 and ViT-S fine-tuned with 100% labeled pediatric chest X-ray images.",Biomedical imaging;X-ray imaging;Diseases;Image reconstruction;Pneumonia;Deep learning;Accuracy;Encoding;Thorax;Pediatrics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10570434,IEEE Journals,,,,,,
Visual Geometry Group Architectures for Brain Cancer Diagnosis using MRI Scans,S. Dhanalakshmi; S. Arulselvi,2024 4th International Conference on Innovative Practices in Technology and Management (ICIPTM),24-Jun-24,2024,"The Visual Geometry Group Architecture Technique (VGGAT) is a powerful deep learning brain tumor classification system that uses MRI images. This research work analyzes its ability to effectively diagnose brain scan malignancies. In order to learn complicated features from raw data, VGGAT makes use of Convolutional Neural Network (CNN) architecture. VGGAT acquires the ability to distinguish between healthy tissue and malignant tissue with a high degree of precision via the process of training on a huge dataset of annotated MRI images. Using thorough assessment on separate test sets, VGGAT displays strong performance in classification tasks, exceeding standard approaches. This is accomplished via the analysis of data. The purpose is to emphasize the usefulness of VGGAT in supporting doctors with accurate and fast diagnosis of brain cancer, which in turn facilitates early intervention and better patient outcomes. Results proved that the proposed VGG system has achieved with 94.29% overall accuracy with 94.2% sensitivity and 94.4% specificity.",Geometry;Training;Visualization;Accuracy;Magnetic resonance imaging;Brain cancer;Computer architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10563607,IEEE Conferences,,,,,,
Utilizing Customized 3D U-Net Framework for the Classification and Segmentation of Multi-regional Brain Tumors in Volumetric MRI Images,M. M. T. Titu; M. M. Mary; M. F. Ahamed; T. M. Oishee; M. M. Hasan,2024 3rd International Conference on Advancement in Electrical and Electronic Engineering (ICAEEE),24-Jun-24,2024,"Precise diagnosis and treatment planning depend heavily on the segmentation and classification of brain tumors in medical pictures. The diagnostic and therapeutic challenges posed by brain tumors necessitate innovative solutions within the domain of medical imaging. This research endeavors to address this pressing need through the introduction of a pioneering approach to multi-regional tumor classification and segmentation. Utilizing volumetric MRI data, our study introduces a bespoke 3D U-Net framework meticulously tailored to the intricacies of brain tumor analysis. Our framework offers holistic solutions to two foundational objectives: classification and segmentation. In the classification facet, we strive to discriminate between various brain tumor types. Concurrently, our segmentation component aspires to precisely delineate tumor boundaries and partition distinct tumor regions. We used the BraTS 2020 dataset, and dice and Jaccard coefficient functions were employed in the model optimization process. The custom-tailored architecture of 3D U-Net, purpose-built for the nuances of brain tumor imaging, ensures resilient performance while preserving vital spatial information essential for accurate tumor delineation. Leveraging deep learning methodologies and a comprehensive dataset of volumetric MRI scans, our model showcases outstanding accuracy in classifying diverse tumor types and in the precise segmentation of tumor regions with a dice score are 88.1%, 79.4%, and 76.0% for WT dice, TC dice and ET dice. And Jaccard Scores are 79.4% for WT Jaccard, 68.2% for TC Jaccard and 64.3% for ET Jaccard. The proposed framework holds the potential to alleviate subjectivity in diagnostic processes, enhance the precision of treatment planning, and ultimately ameliorate patient outcomes.",Deep learning;Training;Image segmentation;Three-dimensional displays;Accuracy;Magnetic resonance imaging;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561700,IEEE Conferences,,,,,,
An Ensemble Approach of Transfer Learning and Vision Transformer to Identify COVID-19 from Chest X-rays,F. S. Tamim; Z. S. Taheri; S. S. Niloy; A. Dey; A. Das,2024 3rd International Conference on Advancement in Electrical and Electronic Engineering (ICAEEE),24-Jun-24,2024,"COVID-19 pandemic has presented global health with previously unknown challenges that call for quick and precise diagnostic procedures. Conventional RT-PCR technique for COVID-19 diagnosis is not only time-consuming and laborious but also the success rate is not up to the mark. This respiratory system infection can also be diagnosed using chest X-rays but COVID-19 X-rays have textures that are very similar to pneumonia. So, it is very crucial to differentiate COVID-19 X-rays from pneumonia. In this study, the potential of cutting-edge techniques, including the vision transformer, transfer learning models (InceptionResNetV2, VGG19, VGG16, DenseNet121, and InceptionV3), and scratch CNN has been investigated in classifying chest X-rays into COVID-19, pneumonia, and normal cases. For that purpose, a comprehensive large dataset comprising about 13,959 images was curated. Furthermore, an ensemble model has been developed employing a judicious majority voting approach to seamlessly integrate the predictive outputs of seven distinct models. The findings of our investigation revealed that the resultant ensemble model surpassed the individual models in performance exhibiting enhanced efficacy in correctly identifying COVID-19, pneumonia, and normal cases from chest X-rays. The exceptional accuracy, F1-score, and precision of 98.93%, 98.84%, and 98.85% respectively, show the ensemble modelâ€™s potential significance and relevance in the domain of medical diagnostics.",COVID-19;Pneumonia;Transfer learning;X-rays;Predictive models;Transformers;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561636,IEEE Conferences,,,,,,
A New Benchmark on Musculoskeletal Abnormality Recognition System using Deep Transfer Learning Model,T. S. Mime; D. Bala; M. A. Hossain; M. A. Rahman; M. S. Hossain; M. I. Abdullah,2024 3rd International Conference on Advancement in Electrical and Electronic Engineering (ICAEEE),24-Jun-24,2024,"Musculoskeletal disorders are abnormalities of the bones, muscles, and joints that affect the great majority of people worldwide. Radiographic scans are the most widely used technique for detecting these aberrations as part of medical diagnostics. Early-stage detection of anomalies in radiographs is crucial for the patient. Moreover, bone fracture classification is costly, more time-consuming, and requires more effort. These reasons have made the deep learning-based classifier model a reliable alternative. In this study, we have applied a deep transfer learning technique to the problem of classifying such anomalies in radiographs of the musculoskeletal system. For multiclass recognition of radiological images into seven categories, the suggested model has been executed on the widely used MURA dataset, which includes 40,561 radiograph images. We utilized the DenseNet121 pre-trained model with some optimization in the suggested model. Our proposed model achieved the highest accuracy of $96.62 \%$. The new model achieved a precision of $96.73 \%$, a recall of $96.62 \%$, an $F 1$-score of $96.60 \%$, and a Cohen Kappa score of $96.06 \%$. We attain better performances that approximately $3 \%$ exceed the previous study and are also comparable to state-of-the-art performances.",Accuracy;Image recognition;Magnetic resonance imaging;Computed tomography;Transfer learning;Bones;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561721,IEEE Conferences,,,,,,
Alleviation of Health Data Poverty for Skin Lesions using ACGAN: Systematic Review,A. Ravikumar; H. Sriraman; C. Chadha; V. K. Chattu,IEEE Access,,2024,"Skin-based infections are one of the primary causes of the global disease burden. Digital Health Technologies powered by data science models have the potential to revolutionize global health care. Health data poverty refers to the failure of individual people, teams, or communities to profit in research or development owing to a deficiency of representative data. Generative Adversarial Network-based synthetic images can be viable solutions to health data poverty since timely detection and frequent monitoring are extremely critical for the survival of the patients. This study aims to investigate the possibility of obtaining photo - realistic dermatoscopic images of Skin Lesions via Generative Adversarial Networks (GAN), followed by distributing the images to augment the existing dataset to further enhance the performance of a Convolutional Neural Network for the task of classification. The medical and technological publications in six databases: PubMed, Web of Science, IEEE Xplore, Science Direct, Scopus, and Google Scholar were investigated. A Deep Learning pipeline has been created and a set of deep learning models such as VGG16 (Visual Geometry Group 16), DenseNet, Xception, and Inception-ResNet v2 have been assembled. We have used condition-based generative adversarial networks (GANs) besides the traditional data augmentation approaches such as rotation and scaling. To highlight the image features that eventually lead to classification are highlighted using a Local Interpretable Model-Agnostic Explanation (LIME) strategy. It was inferred from the results of the classification that DenseNet-201 with GAN Augmentation was the best individual model, with an accuracy of around 82%, while models such as VGG-16 and SVM (Support Vector Machine) were unable to compete. It was also observed that starting with the pre-trained ImageNet weights sped up the convergence and prevented models from over fitting in the absence of the regularization effect of augmented data. However, the exploitation of the data was still not perfectly optimal, as over fitting with data augmentation and early stopping was observed, which can be used by more extensive data augmentation techniques. The GAN augmentation showed to reduce the data imbalance and increase the data percentage of the less representative classes. A data augmentation approach based on synthetic data that has been obtained from GAN helps us to classify images of lesions of the skin with high accuracy. We can also infer from the results obtained that, enriching the data with GAN-produced data samples results in a significant performance increase. In the field of medical imaging, where particularly large training datasets are not available, novel data augmentation and generation procedures can be beneficial.",Skin;Generative adversarial networks;Lesions;Data models;Melanoma;Electronic healthcare;Data augmentation;Machine learning;Scalability;Deep learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568151,IEEE Early Access Articles,,,,,,
Optimizing Lung Opacity Classification in Chest X-ray Images through Transfer Learning on VGG19 CNN Model,M. Singla; K. S. Gill; D. Upadhyay; S. Devliyal,2024 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES),21-Jun-24,2024,"This research work presents an innovative method for classifying lung opacities in chest X-ray images, utilizing transfer learning with a focus on Keras and the VGG19 model. Within the framework of the RSNA Pneumonia Detection Challenge dataset, our investigation thoroughly explores various data preprocessing steps, encompassing dataset overview, splitting, and balancing. Of particular note, Keras is tailored to ensure compatibility with DICOM, addressing the challenges associated with processing chest X-ray images. The implementation of straightforward data augmentation techniques enhances the dataset, thereby bolstering the robustness of the developed model. The paper offers a detailed breakdown of the pretrained VGG19 model, elucidating additional layers tailored to optimize performance for the task of lung opacity classification. Evaluation of the model entails a comprehensive analysis of architecture, parameters, and performance metrics such as precision, recall, and F1-score. The study culminates with insights into test-time augmentation strategies, underscoring the significance of predicting each image twice to enhance reliability. Overall, this research contributes to the advancement of medical imaging by showcasing the effectiveness of transfer learning in conjunction with Keras for precise and efficient lung opacity classification in chest X-ray images.",Pneumonia;Transfer learning;Refining;Lung;Robustness;Planning;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561338,IEEE Conferences,,,,,,
Charting New Frontiers using Transfer Learning in OCT Image Analysis for Retinal Health,M. Agarwal; K. S. Gill; D. Upadhyay; S. Dangi,2024 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES),21-Jun-24,2024,"The project aims to address the pressing necessity of early-stage diagnosis of retinal illnesses by the use of groundbreaking techniques in medical image processing. The study showcases the efficacy of transfer learning in the medical domain, utilising a dataset of less than 1,000 retinal OCT images, despite the substantial volume of over 30 million OCT scans conducted annually. The dataset has been meticulously assembled utilising a hierarchical grading process that incorporates input from undergraduate students, medical students, ophthalmologists, and senior retinal experts. The classification has four categories: DME, CNV, NORMAL, and DRUSEN. The methodology employed entails the utilisation of a pre-trained ResNet18 Convolutional Neural Network. The primary focus is on fine-tuning the last layer to accurately target the goal of categorising retinal illness. The study demonstrates a remarkable degree of accuracy, achieving a 94% success rate. This emphasises the potential of transfer learning as a powerful approach for early detection of retinal disorders with high effectiveness and precision. This work not only provides a useful addition to the field of medical imaging, but also acts as a comprehensive reference for those new to the subject who are interested in applying Convolutional Neural Networks and transfer learning.",Accuracy;Image recognition;Image analysis;Optical coherence tomography;Transfer learning;Pressing;Retina,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561346,IEEE Conferences,,,,,,
Detection and Analysis of Covid-19 from CT Scan images using Deep Learning based CNN Technique,V. Bhargava; K. H. Sikha; V. N. Raj; N. M. Babu; V. A. Kumar; N. Srinivasulu,2024 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES),21-Jun-24,2024,"In order to cure the COVID-19 pandemic, quick and precise COVID-19 identification from CT images that leverage deep identification of affected individuals is essential. This essay suggests a thorough method for learning methods. The processes covered by the suggested methodology are feature extraction, edge detection, preprocessing, segmentation, input image acquisition, and deep learning-based classification. First, as input, CT scans of possible instances are obtained. These pictures go through preprocessing to improve quality and reduce noise, and then they are segmented to separate lung areas. Then, lung borders are drawn using edge detection techniques for accurate feature extraction. Next, characteristics pertinent to COVID-19 infection are taken out of lung regions that have been segmented. These retrieved features are used to train deep learning models, like convolution neural networks (CNNs), which classify images into positive (COVID-19 infected) or negative (non-infected) categories. The effectiveness of the suggested method in correctly detecting COVID-19 instances from CT images is shown by the experimental findings. The deep learning model helps with effective disease diagnosis and management by achieving high accuracy, sensitivity, and specificity in differentiating between positive and negative cases. The suggested technique offers a quick and accurate way to screen and diagnose patients, making it a potentially useful tool for medical professionals fighting the COVID-19 pandemic.",COVID-19;Deep learning;Image segmentation;Accuracy;Pandemics;Computed tomography;Image edge detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561277,IEEE Conferences,,,,,,
A Deep Learning Approach for Blood Cells Classification Using CNN and Transfer Learning Models,A. G. Karegowda; A. Leena Rani; Aishwarya,2024 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES),21-Jun-24,2024,"Blood cell image classification stands as a cornerstone in modern healthcare, playing a pivotal role in the diagnosis and treatment of a wide array of medical conditions. The precise and efficient categorization of blood cells holds the potential to exert a profound influence on patient care and medical research alike. The traditional methods faced challenges with accuracy and robustness due to the complexity and variability of blood cell images. This work, propose a comprehensive approach for the classification of four types of blood cells: Eosinophils, Lymphocytes, Monocytes, and Neutrophils using Convolutional Neural Networks (CNNs) and transfer learning models. We employ four state-of-the-art pre-trained models: Xception, VGG16, MobileNetV2, and ResNet50V2. The primary objective is to accurately classify blood cell types, crucial for various medical diagnostic applications. Our experiments showâ€™s promising results, with the MobileNetV2 model achieved an impressive Training Accuracy of 98.22%, Validation Accuracy 86.89%, and Test Accuracy of 87.00%.",Training;Accuracy;Medical conditions;Transfer learning;Robustness;Mobile applications;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561449,IEEE Conferences,,,,,,
Deep Learning Based Blood Cell Identification and Subtype Classification,V. Kalpana; V. Y. Reddy; B. Balaji; A. Vanaja; C. V. K. Reddy,2024 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES),21-Jun-24,2024,"Precise identification and counting of blood cells, including Red Blood Cells (RBCs), White Blood Cells (WBCs), and Platelets, are essential in medical diagnostics. Conventional approaches depend on human procedures, however current progress in deep learning provides more efficient options. This research introduces a thorough method for analysing blood cells, utilizing advanced neural network-based structures for both counting blood cells and classifying WBCs. The BCCD dataset is used to improve accuracy and overcome obstacles such as low resolution and overlapping cells by applying different preprocessing techniques. An optimised VGG16 model with Keras is used to classify several types of WBCs by including normalization and data augmentation approaches. The results of the experiment demonstrate the efficacy of the proposed methodology, indicating a positive potential for strengthening diagnostic accuracy and efficiency in clinical practice, ultimately improving patient care standards.",Deep learning;White blood cells;Electric potential;Accuracy;Red blood cells;Imaging;Cells (biology),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561342,IEEE Conferences,,,,,,
A Comprehensive Liver Tumor Detection and Stages Classification Using Deep Learning and Image Processing Techniques,P. Prakash; K. Bhagavan; P. Apsiya; C. Chandana,2024 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES),21-Jun-24,2024,"This research presents a deep learning framework designed to automatically detect and classify liver tumors in CT images, leveraging Convolutional Neural Networks (CNNs). The suggested method includes a sequence of pre-processing steps, such as resizing images and enhancing contrast through histogram equalization. Additionally, a bilateral filter is applied for noise removal, followed by K-means image-based segmentation for improved localization. The CNN is then employed for binary classification, distinguishing between benign and malignant tumors with an accuracy of 98.88%. If the CNN identifies a tumor as malignant, a secondary CNN-based classification system is employed to further categorize the malignant tumors into different stages: Early Stage, Intermediate Stage, and Metastatic Stage with an accuracy of 98.72%. This multi-step approach not only automates tumor detection but also provides a finer-grained analysis of malignant cases, offering valuable insights into the progression of liver tumors. The methodology combines advanced image processing techniques with deep learning classification, showcasing a comprehensive framework for efficient and detailed liver tumor analysis in medical imaging.",Deep learning;Image segmentation;Histograms;Accuracy;Malignant tumors;Computed tomography;Noise,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10561272,IEEE Conferences,,,,,,
Visual Question Answering,A. Nada; M. Chen,"2024 International Conference on Computing, Networking and Communications (ICNC)",21-Jun-24,2024,"Visual question answering (VQA) is an artificial intelligence (AI) and computer vision (CV) comprehensive task to answer questions about the visual content of an image, such as â€œwhat color is the bus?â€ or â€œhow many people are in the photo?â€ VQA has shown great potential and importance in various domains, ranging from medical imaging applications, autonomous driving, to virtual assistants and search engines. This study develops a framework to tackle VQA research challenges by adopting and extending recent breakthroughs in attention techniques, natural language processing, and image classification models. In addition, different from other previous work that uses static question embedding, we investigate how alternative dynamic embedding models enhance the effectiveness of VQA task. The work is evaluated using the latest developed VQA v2 dataset with a 9% improvement over the results obtained with static word embedding. We also deployed the model as a cloud based VQA system to facilitate VQA tasks in real-life applications.",Visualization;Computer vision;Image coding;Computational modeling;Virtual assistants;Optical character recognition;Search engines,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556344,IEEE Conferences,,,,,,
Advanced Monkeypox Classification using EfficientNet B3: A Promising Method,P. Shourie; V. Anand; D. Upadhyay; V. Singh; S. Gupta; G. Sunil,"2024 International Conference on E-mobility, Power Control and Smart Systems (ICEMPS)",21-Jun-24,2024,"A rare and potentially fatal viral disease, monkeypox predominantly impacts non-human primates, including humans. It is critical to establish an early and precise diagnosis of monkeypox in order to contain outbreaks and administer opportune treatment effectively. Deep learning and convolutional neural networks (CNNs) have demonstrated considerable potential in the domain of medical image diagnosis in recent times. The current research paper introduces an innovative methodology for categorizing images of monkeypox utilizing the cutting-edge deep learning framework, EfficientNetB3. Outstanding performance has been exhibited by EfficientNetB3, a CNN architecture renowned for its high efficiency and accuracy across a range of image classification tasks. This study presents the adaptation and fine-tuning of EfficientNetB3 to classify monkeypox. Data augmentation techniques are implemented to improve the model's capacity to extrapolate to diverse variations present in images of monkeypox. A substantial collection of images about monkeypox, comprising laboratory samples and clinical photographs, is utilized by the proposed model. In terms of accurately and precisely classifying images of monkeypox, experimental results demonstrate the efficacy of our method. Several metrics are employed to assess the efficacy of the mode. The performance of the trained EfficientNetB3 model in differentiating monkeypox from various skin conditions and infections is encouraging, as this capability is critical for the timely and precise diagnosis of the disease.",Deep learning;Adaptation models;Instruments;Computational modeling;Rendering (computer graphics);Skin;Computational efficiency,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10559284,IEEE Conferences,,,,,,
An Investigation on Machine Learning Models in Classification and Identifications Cervical Cancer Using MRI Scan,J. Singh; K. T. Tan; D. Sahu; K. Upreti; P. R. Kshirsagar; M. Elangovan,"2024 International Conference on E-mobility, Power Control and Smart Systems (ICEMPS)",21-Jun-24,2024,"This study analyzes the effectiveness of machine learning models in the classification of cervical cancer using a dataset of 900 cancer and 200 non-cancer images gathered from online resources and hospitals. The dataset, covering both CT and MRI images, undergoes rigorous preprocessing, including standardization, normalization, and noise reduction, to enhance its quality for model training. Four machine learning models, namely VGG16, CNN, KNN, and RNN, are recruited to predict cancer and non-cancer cases. During the testing phase, VGG16 emerges as the most accurate, achieving an impressive accuracy of 95.44%, followed by CNN at 92.3%, KNN at 89.99%, and RNN at 86.233%. Performance parameters, such as precision, recall, F1 score, and accuracy, are fully analyzed, providing insights into each model's strengths and capabilities. These discoveries not only contribute to the advancement of cervical cancer diagnostic techniques but also underscore the potential of machine learning in medical imaging. The study emphasizes the relevance of model selection and provides a framework for future research endeavors seeking to enhance the accuracy and performance of cervical cancer diagnosis through the merger of advanced computational techniques with standard diagnostic practices.",Training;Analytical models;Accuracy;Computational modeling;Magnetic resonance imaging;Machine learning;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10559361,IEEE Conferences,,,,,,
CNN-O-ELMNet: Optimized Lightweight and Generalized Model for Lung Disease Classification and Severity Assessment,S. Agarwal; K. V. Arya; Y. K. Meena,IEEE Transactions on Medical Imaging,,2024,"The high burden of lung diseases on healthcare necessitates effective detection methods. Current Computer-aided design (CAD) systems are limited by their focus on specific diseases and computationally demanding deep learning models. To overcome these challenges, we introduce CNN-O-ELMNet, a lightweight classification model designed to efficiently detect various lung diseases, surpassing the limitations of disease-specific CAD systems and the complexity of deep learning models. This model combines a convolutional neural network for deep feature extraction with an optimized extreme learning machine, utilizing the imperialistic competitive algorithm for enhanced predictions. We then evaluated the effectiveness of CNN-O-ELMNet using benchmark datasets for lung diseases: distinguishing pneumothorax vs. non-pneumothorax, tuberculosis vs. normal, and lung cancer vs. healthy cases. Our findings demonstrate that CNN-O-ELMNet significantly outperformed (p < 0.05) state-of-the-art methods in binary classifications for tuberculosis and cancer, achieving accuracies of 97.85% and 97.70%, respectively, while maintaining low computational complexity with only 2481 trainable parameters. We also extended the model to categorize lung disease severity based on Brixia scores. Achieving a 96.20% accuracy in multi-class assessment for mild, moderate, and severe cases, makes it suitable for deployment in lightweight healthcare devices.",Lung cancer;Computational modeling;Accuracy;Feature extraction;Lung;Solid modeling;Computed tomography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10562310,IEEE Early Access Articles,,,,,,
From Local to Global: An Anatomy-Aware Deep Learning Approach for Deformable Registration of Brain MR Images,H. Liu; Y. Pu; J. Ni; L. Wang; X. Wang; D. Tao; Y. Yang,2024 7th International Conference on Advanced Algorithms and Control Engineering (ICAACE),18-Jun-24,2024,"Deformable registration serves as the foundation for longitudinal and population-based medical image analysis. In cross-sectional studies of brain magnetic resonance imaging (MRI), accurately aligning the regions of interest (ROIs) of various subjects, especially in the context of specific disease analysis, poses a challenge due to the complexity of brain structures and individual variations. The existing research predominantly utilizes the convolutional neural networks (CNNs) for learning global deformable registration, with little attention paid to the independent performance of neural networks on specific anatomical tissues. To address above problem, this paper proposed an anatomy-aware deep learning approach from local optimization to global information for medical deformable registration. In the method, the information of special brain anatomical tissue structure is selected to optimize the registration accuracy for the target region, and the anatomical perception constraint is designed to capture the information of these structures, so as to ensure the physiological rationality of the final registration deformation field. Compared with the recently developed state-of-the-art approaches, the proposed registration method can achieve the highest registration accuracy on both specific anatomical tissues and the overall image texture, and maintain image smoothness.",Deep learning;Brain;Image texture;Accuracy;Image analysis;Magnetic resonance imaging;Physiology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549474,IEEE Conferences,,,,,,
Optimizing Inception-V3 for Brain Tumor Classification Using Hybrid Precision Training and Cosine Annealing Learning Rate,J. Wang; L. He; X. Zhou,2024 7th International Conference on Advanced Algorithms and Control Engineering (ICAACE),18-Jun-24,2024,"In the realm of computational medical imaging, the precise classification of brain tumors from magnetic resonance (MR) images represents a quintessential challenge, demanding high fidelity in detection and categorization due to its critical implications for treatment planning and patient prognosis. This study pioneers the refinement of the Inception architecture, leveraging hybrid precision training and cosine annealing learning rate adjustment to enhance its computational efficiency and diagnostic accuracy in brain tumor classification tasks. Through a rigorous experimental framework, the modified Inception model exhibited exemplary performance, achieving an overall accuracy of 96.94% on a diverse test set, with precision metrics peaking at 99.32% for specific tumor categories. These results not only underscore the model's robust generalization capabilities but also its potential to set new standards in the automated interpretation of medical images. The comparative analysis conducted against established convolutional neural networks (CNNs) like AlexNet, VGG, and ResNet further corroborates the superiority of the proposed model, thereby cementing its status as a pivotal contribution to the field.",Training;Image recognition;Annealing;Shape;Computational modeling;Brain modeling;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548577,IEEE Conferences,,,,,,
A Review on Computational Methods Based on Deep Learning and Transfer Learning Techniques for Malaria Detection,A. M. Zakariya; M. F. Adak,"2024 10th International Conference on Automation, Robotics and Applications (ICARA)",18-Jun-24,2024,"Malaria continues to pose a significant health challenge worldwide, especially in most of under developing countries where most often there is limited healthcare resources. A death toll of nearly a million every year from two decades back is recorded by the World Health Organization. Doctors employ microscopic examination of patient's blood to identify the presence of malaria parasites in its initial phases, yet this method encounters issues related to both time consumption and accuracy. The need for precise, prompt, and effective detection of malaria parasites is crucial in the fight against this infectious disease. This review aims to highlight the recent most effective both preprocessing and classification methodologies used for detecting malaria parasites.",Deep learning;Training;Accuracy;Reviews;Malaria;Microscopy;Transfer learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10553171,IEEE Conferences,,,,,,
Improving Transfer Learning Performance for Abnormality Detection in Brain MRI Images Using Feature Optimization Techniques,Y. A. Nizamli; A. Y. Filatov,2024 XXVII International Conference on Soft Computing and Measurements (SCM),17-Jun-24,2024,"Accurate and early diagnosis of brain tumors plays a vital role in determining appropriate treatment and increasing survival rates. Manual diagnosis using MRI images is often prone to errors and can be a tedious and time-consuming procedure. As a result, automated computer-aided systems that utilize artificial intelligence and machine learning techniques are widely used. However, accurately diagnosing the type of brain tumor remains a significant and complex challenge for todayâ€™s medical imaging research community. In this paper, we present an enhanced transfer learning approach that aims to differentiate brain tumor types using MRI scans while achieving state-of-the-art performance. In the proposed approach, the pre-trained VGG-19 network with fixed weights is used to map MRI scans to a high-level numerical representation. The extracted features are then passed to a support vector machine algorithm to classify brain tumors into one of three major categories: glioma, meningioma, and pituitary. To further improve the performance of the model, the effectiveness of utilizing feature optimization techniques based on the radial basis similarity function and L1-penalty logistic regression was investigated. For evaluation, the benchmark Figshare dataset containing 3064 MRI images was used after a series of processing operations. The proposed system achieved a high overall accuracy of 98.53%, outperforming other analogues.",Logistic regression;Accuracy;Magnetic resonance imaging;Computational modeling;Transfer learning;Feature extraction;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554161,IEEE Conferences,,,,,,
Tailored Convolutional Neural Network Applied to Fragility Fracture Classification Using Ultrasonic Guided Wave Spectrum Images,W. Flores; D. DÃ­az; A. Aguilera; R. Olivares; R. MuÃ±oz; J. -G. Minonzio,2024 IEEE UFFC Latin America Ultrasonics Symposium (LAUS),17-Jun-24,2024,"Osteoporosis affects millions worldwide, significantly impairing quality of life and imposing substantial eco-nomic burdens on healthcare systems. Annually, osteoporosis leads to millions of fragility fractures, with hip fractures projected to double within the next few decades. Currently, dual-energy X-ray absorptiometry (DXA) is the standard diagnos-tic method, offering quick scans with low radiation exposure. However, its effectiveness is limited by several factors including the influence of bone size and patient-specific conditions, high costs, and accessibility issues. Alternative diagnostic methods such as trabecular bone score (TBS), computed tomography (CT), and magnetic resonance imaging (MRI) have been explored, yet each presents its own set of challenges. This study investigates the potential of advanced imaging techniques like quantitative ultrasound axial transmission technology to provide effective and safer alternatives to DXA. This research evaluates three CNN models, including ResNet and a BDAT-Net optimized and not optimized, to assess their effectiveness in medical image classification and their potential to best osteoporosis diagnostics.",Training;Osteoporosis;Accuracy;Magnetic resonance imaging;Computed tomography;Biological system modeling;Market research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10553069,IEEE Conferences,,,,,,
Tnc-Net: Automatic Classification for Thyroid Nodules Lesions Using Convolutional Neural Network,J. Cao; Y. Zhu; X. Tian; J. Wang,IEEE Access,20-Jun-24,2024,"Automatic and accurate classification of thyroid nodules is of great significance to doctors for clinical diagnosis and subsequent treatment recommendations. Since there are no obvious features between benign and malignant nodules, enlarging or reducing the image will result in blurred edges and image distortion, thus limiting the accuracy of clinical diagnosis. Furthermore, the prevalence of sample class imbalance in medical images poses significant challenges to applying convolutional neural networks in thyroid nodule classification methods. This paper proposes a network Tnc-Net for thyroid nodule classification. The network backbone can adapt to the problem of small data volume, capture global features with the help of simple channel attention, and effectively extract image information. The branch network supplements the feature extraction from the backbone network, and the information extracted from the backbone and branch networks is effectively utilized through the fusion module. In addition, this article designs training strategies suitable for this network to deal with category imbalance, improve model classification performance, and make classification results more clinically referenceable. The method test accuracy is 0.902, which exceeds other classic deep learning models in classification. This result demonstrates the effectiveness of our method in achieving automatic classification of thyroid nodules.",Thyroid;Feature extraction;Training;Medical diagnostic imaging;Accuracy;Convolutional neural networks;Cancer;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10557604,IEEE Journals,,,,Yes,No,Yes
Pancreatic Tumor Recognition from CT Images through Advanced Deep Learning Techniques,D. Mitrea; R. Brehar; R. Itu; S. Nedevschi; M. Socaciu; R. Badea,"2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",14-Jun-24,2024,"Cancer constitutes a major affection nowadays, leading to death in many situations. Pancreatic malignant tumors represent the fourth most frequent cause of cancer related death in United States and Europe. The most trustworthy cancer diagnosis method is the biopsy, but this technique raises several risks, the most relevant one being the danger of tumoral spread through the human body. Thus, in the context of our research, we employ computerized methods for achieving highly accurate abdominal tumor recognition and segmentation within medical images. In the current approach, we performed the automatic recognition of pancreatic malignant tumors within Computed Tomography (CT) images, with the aid of Convolutional Neural Networks (CNN), by employing last generation architectures, as well as their improved versions, respectively combinations of these structures at classifier and decision level. The classification performance was assessed through specific metrics, an accuracy above 98% being achieved.",Image recognition;Accuracy;Computed tomography;Malignant tumors;Neural networks;Convolutional neural networks;Robots,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554139,IEEE Conferences,,,,,,
Advanced Brain Tumor Diagnosis using Haralick Texture Features and Multiclass SVM in MRI Imaging,S. Karimullah; M. M. Basha; F. Shaik; S. Gundala; A. Stan; O. P. Stan,"2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",14-Jun-24,2024,"This study employs contemporary computer techniques and magnetic resonance imaging (MRI) to tackle the critical issue of brain tumor classification. The proposed technique involves obtaining databases, preprocessing the data, extracting features, applying machine learning for classification, and conducting a thorough performance evaluation. The method enhances the visibility of tumors and enhances the characteristics of images by acquiring a diverse collection of MRI images and applying a series of preprocessing operations, including Otsu Binarization, Gray Level Thresholding, Morphological Operations, and Principal Component Analysis (PCA). Subsequently, the Haralick texture characteristics are extracted to depict the textural patterns observed in the pictures. The concept is based on utilizing a multiclass Support Vector Machine (SVM) to achieve accurate tumor classification. Our proposed technique demonstrated efficacy in accurately classifying brain tumors, as evidenced by an impressive accuracy rate of 99.764% and near-perfect ROC curve performance, with area under the curve (AUC) values close to 1. The findings emphasize the potential practicality of the technique and contribute to the advancement of computational methods and medical imaging in the field of brain tumor detection.",Support vector machines;Training;Accuracy;Thresholding (Imaging);Magnetic resonance imaging;Machine learning;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554101,IEEE Conferences,,,,,,
"On the Feasibility of Deep Learning Classification from Raw Signal Data in Radiology, Ultrasonography and Electrophysiology",S. Enyedi,"2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",14-Jun-24,2024,"Medical imaging is a very useful tool in healthcare, various technologies being employed to non-invasively peek inside the human body. Deep learning with neural networks in radiology was welcome â€“ albeit cautiously â€“ by the radiologist community. Most of the currently deployed or researched deep learning solutions are applied on already generated images of medical scans, use the neural networks to aid in the generation of such images, or use them for identifying specific substance markers in spectrographs. This paperâ€™s author posits that if the neural networks were trained directly on the raw signals from the scanning machines, they would gain access to more nuanced information than from the already processed images, hence the training â€“ and later, the inferences â€“ would become more accurate. The paper presents the main current applications of deep learning in radiography, ultrasonography, and electrophysiology, and discusses whether the proposed neural network training directly on raw signals is feasible.",Deep learning;Training;Neural networks;Ultrasonography;Medical services;Radiology;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554095,IEEE Conferences,,,,,,
Enhanced Deep Learning Model Performance in 3D Multimodal Brain Tumor Segmentation with Gabor Filter,N. Ahmad; Y. -T. Chen,2024 10th International Conference on Applied System Innovation (ICASI),13-Jun-24,2024,"Medical image segmentation plays a crucial role in identifying and analyzing anatomical structures in medical images. This requires an accurate medical segmentation tool to delineate and quantitatively analyze the target regions, diagnose any abnormality, and assist in treatment planning. Deep learning approaches have emerged as a promising solution for automating medical segmentation. However, challenges arise when dealing with the complex shapes and spatial variations of some target regions, especially in 3D MRI scans. To deal with such transformations, specific techniques are required to properly analyze and preprocess the dataset and perform image filtering to provide better features for improved prediction performance of deep learning architectures. This study focuses on improving brain tumor segmentation in multimodal 3D MRI images. We observed significant improvements in multimodal brain tumor segmentation results (accuracy, IoU, and mIoU) using an optimized 3D Gabor filter, which helps extract meaningful features. Multichannel input images were preprocessed to remove noise and select an appropriate resolution to reduce computational complexity. An improvement in mean Intersection over Union (mIoU) from 0.714 to 0.804 and accuracy from 0.982 to 0.991 were achieved, which shows a major improvement. This work contributes to the field of medical image segmentation by offering an improved and efficient approach for brain tumor analysis in 3D MRI scans, potentially aiding in diagnosis and treatment planning.",Deep learning;Image segmentation;Solid modeling;Three-dimensional displays;Magnetic resonance imaging;Feature extraction;Planning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10547816,IEEE Conferences,,,,,,
A Novel Black-Box Complementary Explanation Approach for Thorax Multi-Label Classification,K. Bouabdallah; A. Drif; L. Kaderali,"2024 4th International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)",13-Jun-24,2024,"Recently, Explainable Artificial Intelligence (XAI) has addressed a critical issue of applying deep learning models for clinical use by providing results that medical experts can understand in various medical contexts such as Thoracic disease classification. Most of the works that provide explainability are categorized as attribution-based approaches. These methods try to explain the output using the input features, which is insufficient for clinical use. However, the explainability based on the attribution methods is insufficient for medical experts since it does not provide how the essential features contribute to the output. This paper proposes a Discriminative Attention Guided Convolutional Neural Network framework (DAG-CNN) that relies on a case-based similarities approach for Thoracic disease diagnosis. The proposed framework for medical imaging diagnostic is based on the paradigm of the twin-system approach for finding example-based explanations. Our framework extracts global features of the whole image and local features of a lesion area corresponding to a specific disease. Then, it compares those features with the case-based diagnosis to extract the most similar cases as an explanation. Experiments performed on ChestX-ray14 yielded competitive results when compared to other state-of-the-art papers. DAG-CNN also shows the best results for pneumonia and hernia. In the explanation phase, we get a high score of 0.93 accuracy compared to similar cases. This result shows that the complementary explanation approach looks deeply into the discrepancies between outputted features and similar cases (images in the archive), which can be used for the diagnosis. The proposed framework provides an efficient similar image explanation based on important features contributing to the diagnosis.",Analytical models;Pneumonia;Explainable AI;Feature extraction;Thorax;Lesions;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548809,IEEE Conferences,,,,,,
Automatic Localization and Classification of Brain Tumors using Deep Residual Networks and MRI Images,D. Lamrani; M. A. Bouqentar; M. A. Mahjoubi; S. Hamida; B. Cherradi; L. Bahatti,"2024 4th International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)",13-Jun-24,2024,"Automatic segmentation tools can serve as valuable aids for healthcare professionals providing additional information for clinical decisionâ€“making. This can include assessing disease severity, predicting outcomes, and tailoring treatment strategy. Indeed, brain tumor segmentation poses challenges in tumor shapes, size, and locations, as well as the complex and intricate nature of brain anatomy. Traditional segmentation methods may struggle with theses complexities. Deep neural networks, particularly those employing residual blocks help mitigate the vanishing gradient issue, allowing an effective training of deep networks, by enabling the learning of complex features by aiding the network in capturing subtle details crucial for accurate segmentation. This paper proposes a reliable encoder â€“decoder neural network based on U-net architecture and residual blocks performing the multiclass segmentation of brain tumors. The experimental study will be carried on BRATS 2020 dataset representing different tumour shapes, locations, sizes and image intensities.",Training;Image segmentation;Brain;Shape;Magnetic resonance imaging;Medical services;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548396,IEEE Conferences,,,,,,
FPGA implementation of a Convolutional Neural Network for Alzheimer's disease classification,S. B. h. Salah; M. Chouchene; F. E. Sayadi,"2024 21st International Multi-Conference on Systems, Signals & Devices (SSD)",12-Jun-24,2024,"In the field of medicine, the analysis and processing of images, particularly in the context of Alzheimer's disease (currently incurable), plays a critical role. Positron emission tomography (PET), essential in neuroscience research, reveals brain activity through analysing scans. Precise classifications of brain PET data are crucial for accelerating the diagnosis of Alzheimer's disease, allowing for earlier intervention and potentially better prognoses. To unravel the complexities of PET data in Alzheimer's diagnosis, we will deploy advanced CNN-based classification techniques. However, it is important to be aware of the computational and memory requirements of these techniques. The unique combination of high performance and low power consumption in SoC-FPGAs makes them perfect for resource-constrained environments. This opens doors for deploying advanced techniques like CNNs for medical image analysis. This paper proposes an Alzheimer's disease classification system using convolutional neural networks (CNNs) implemented on a Pynq_Z2 board. By evaluating this technique in terms of accuracy and cost through simulations, we were able to demonstrate its efficiency and validate the feasibility of the proposed design approach",Training;Analytical models;Refining;Real-time systems;Convolutional neural networks;Alzheimer's disease;Positron emission tomography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549753,IEEE Conferences,,,,,,
Enhancing Kidney Stone Diagnosis: A Fusion Approach of FCM and CNN for Precise Detection,K. Ramesh Chandra; T. Harsha Manoj; V. Harshitha; S. Divya; P. Swarnalatha; S. Kareem,2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),12-Jun-24,2024,"Kidney stones, characterized by the accumulation of mineral and organic substances within the renal system, pose a significant health concern globally. Timely and accurate diagnosis is paramount for effective treatment and prevention of complications. Medical imaging, particularly computed tomography (CT) scans, plays a crucial role in identifying and characterizing kidney stones. However, the inherent variability in stone appearance, shape, and size, coupled with the potential presence of artifacts in imaging data, poses challenges to traditional diagnostic methods. Hence, this work focuses on addressing these challenges and dedicated to optimizing the detection of kidney stones by employing a synergistic approach that integrates image processing and deep learning techniques. The proposed work involves initial pre-processing steps, utilizing median filtering to eliminate noise and adaptive histogram equalization (AHE) for enhancing the quality of CT images of kidneys. Subsequently, convolutional neural networks (CNNs) are employed as a deep learning algorithm for classification, with a primary focus on accurately distinguishing between normal and abnormal images. In the event of an abnormal classification indicating the presence of a kidney stone, a secondary step is implemented. This step involves the application of fuzzy C-means (FCM) and level set segmentation techniques to achieve precise detection and localization of the kidney stone within the image. The proposed approach achieves an accuracy of 99.45%, which is 13% improvised when compared to the existing approaches.",Deep learning;Location awareness;Histograms;Accuracy;Filtering;Computed tomography;Noise,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549262,IEEE Conferences,,,,,,
Pediatric oral health detection using Swin transformer,S. Bhat; G. K. Birajdar; M. D. Patil,2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),12-Jun-24,2024,"Oral health exams in children are important for a variety of reasons, including their impact on a child's overall well-being and development. Traditional dental health detection methods still need to be improved, particularly in actual diag-nostic environments. Artificial intelligence (AI) has the potential to have a substantial impact on pediatric dentistry by providing innovative solutions to numerous difficulties in children's oral healthcare. By analyzing X-rays or intra-oral scans, AI can aid in the early detection of disorders like tooth decay, and developmental anomalies, when correction is most successful. AI can provide additional insight by analyzing X-rays and data, allowing dentists to make more accurate treatment plan decisions. Transformer is an instance of a deep neural network that was first employed for natural language processing. Its main foundation is the self-attention mechanism. In this study, we assess the swin transformer model's effectiveness in pediatric oral health detection. The data classes were enhanced as a pre-processing step using the fuzzy color approach, and the images that were structured with original X-rays were stacked. The proposed system achieved an accuracy of 85.34 percent. The result obtained implies that swin transformer has prospects in medical imaging and pre-training may be a useful strategy for enhancing its model's accuracy on oral health detection tasks.",Pediatrics;Computational modeling;Unified modeling language;X-rays;Transformers;Feature extraction;Dentistry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548432,IEEE Conferences,,,,,,
A Comprehensive Multimodal Analysis for Detecting Pulmonary Infiltrates in Chest X-Ray Image,U. Sakthi; K. V. Vaishnave; A. Krishnan A M,2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),12-Jun-24,2024,"This research paper delves into the critical task of detecting pulmonary infiltrates indicative of pneumonia in Chest X-ray images through a comprehensive multimodal analysis. This study focuses on evaluating the performance of a diverse set of models on Chest X-ray datasets, offering a comparative analysis and contribute meaningfully to pneumonia detection advancement through leveraging cutting-edge models, it's essential to scrutinize the strengths and limitations of each model. By employing rigorous evaluation criteria, we can discern which models offer the most promise in terms of accuracy, efficiency, scalability, and interpretability. The dataset used for experimentation encompasses a broad spectrum of chest X-ray images, ensuring a representative sample that captures the nuances of real-world clinical scenarios. The proposed multimodal analysis considers not only the visual features present in the X-ray images but also incorporates textual and contextual information. This holistic approach aims to enhance the overall diagnostic accuracy by leveraging complementary modalities. The results obtained from the experiments shed light on the strengths and weaknesses of the selected models, paving the way for informed decisions in deploying pneumonia detection systems. This research contributes to the evolving landscape of medical image analysis by presenting a comprehensive multimodal approach for detecting pulmonary infiltrates in Chest X-ray images. The comparative study not only aids in identifying optimal models for pneumonia detection but also highlights the potential for improved diagnostic accuracy.",Analytical models;Visualization;Pneumonia;Image analysis;Computational modeling;Lung;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549653,IEEE Conferences,,,,,,
An Extensive Examination of Methods for Detecting and Classifying Malignant and Benign Skin Cancers Using Deep Learning Techniques,R. Pasumarthy; L. Gondi,2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),12-Jun-24,2024,"Skin cancer is a pressing health concern responsible for numerous fatalities in today's world, often diagnosed in its advanced stages, making treatment planning and patient survival challenging. Medical image interpretation, such as MRI as well as CT scans, is crucial for skin cancer diagnosis, but the manual analysis is costly, time-consuming, and can be influenced by biases. DL (Deep learning), a cutting-edge technology, is gaining attention as it allows for the automatic evaluation and interpretation of medical images, reducing the need for human intervention. Deep learning approaches have demonstrated remarkable outcomes in various domains by automatically extracting intrinsic image features, excluding the need for manual feature extraction. This review delves into the application of DL approaches in the diagnosis & detection of cancer, with a particular focus on skin cancer. Detecting melanoma from samples of dermoscopic skin is a formidable task, but employing a machine vision tool, like a deep learning approach, can address some of these challenges. The study introduces an automated melanoma classifier on the basis of DCNN (Deep Convolutional Neural Network) to accurately differentiate malignant from benign melanoma. It also outlines the various deep learning models and steps involved in utilizing these models for skin cancer detection. Recent advancements in the skin cancer detection Deep Learning (DL) methods are explored and summarized, shedding light on the critical challenges that have been faced in accurately detecting the cancer of skin in its early stages. For this research, dermoscopic images from the International Skin Imaging Collaboration datastores (ISIC 2016, ISIC 2017, and ISIC 2020), containing diverse cancer samples, were obtained and evaluated based on metrics like precision, accuracy, specificity, recall, and F1-score.",Deep learning;Image segmentation;Melanoma;Object segmentation;Manuals;Feature extraction;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548394,IEEE Conferences,,,,,,
Hybrid Deep Learning Based GRU Model for Classifying the Lung Cancer from CT Scan Images,R. R. Gondkar; S. R. Gondkar; K. S; S. Balan R V,2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),12-Jun-24,2024,"Lung cancer is a potentially fatal condition, posing significant challenges for early detection and treatment within the healthcare domain. Despite extensive efforts, the etiology and cure of cancer remain elusive. However, early detection offers hope for effective treatment. This study explores the application of image processing techniques, including noise reduction, feature extraction, and identification of cancerous regions within the lung, augmented by patient medical history data. Leveraging machine learning and image processing, this research presents a methodology for precise lung cancer categorization and prognosis. While computed tomography (CT) scans are a cornerstone of medical imaging, diagnosing cancer solely through CT scans remains challenging even for seasoned medical professionals. The emergence of computer-assisted diagnostics has revolutionized cancer detection and diagnosis. This study utilizes lung images from the Lung Image Database Consortium (LIDC-IDRI) and evaluates various image preprocessing filters such as median, Gaussian, Wiener, Otsu, and rough body area filters. Subsequently, feature extraction employs the Karhunen-Loeve (KL) methodology, followed by lung tumor classification using a hybrid model comprising a One-Dimensional Convolutional Neural Network (1D-CNN) and a Gated Recurrent Unit (GRU). Experimental findings demonstrate that the proposed model achieves a sensitivity of 99.14%, specificity of 90.00%, F -measure of 95.24%, and accuracy of 95%.",Deep learning;Solid modeling;Filters;Computed tomography;Lung cancer;Lung;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548266,IEEE Conferences,,,,,,
Automated Brain Tumor Detection and Classification Using Convolutional Neural Networks,M. Nagraju; M. Asritha Sai; K. Vamsi; N. Dhanush; K. U. Kiran,2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),12-Jun-24,2024,"Manual inspection is a commonly used method for diagnosing brain tumors. However, it is known to be time-consuming and subjective, which can be a drawback of this approach. This solution aims to address the problem at hand by presenting a research-backed approach that suggests an automated method for diagnosing and classifying brain cancer. The proposed method relies on convolutional neural networks (CNN) to achieve accurate results. In this approach, convolutional neural networks (CNNs) are used for preprocessing, classification, and feature extraction. Improved data quality is achieved through the use of high-resolution brain MRI images from multiple datasets, along with the implementation of state-of-the-art image preprocessing techniques. The CNN architecture has been meticulously designed to improve sensitivity and adaptability by incorporating an attention mechanism and multi-scale feature fusion. The proposed method achieves an impressive accuracy rate of 99.76% in identifying and classifying brain tumors, surpassing the accuracy of traditional human review procedures. Based on the research conducted, it has been discovered that the implementation of automated neuroimaging diagnostics has the potential to greatly enhance the accuracy and efficiency of brain cancer diagnosis. With an accuracy rate of 99.76%, the proposed method outperforms traditional human review procedures in identifying and classifying brain cancers.",Neuroimaging;Sensitivity;Reviews;Magnetic resonance imaging;Brain cancer;Manuals;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548635,IEEE Conferences,,,,,,
Enhancing Acute Lymphoblastic Leukemia Classification with a Rapid and Effective CNN Model,J. R. Devi; P. S. Kadiyala; S. Lavu; N. Kasturi; L. Kosuri,2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),12-Jun-24,2024,"Leukemias are malignant disorders of the hematopoietic stem cell compartment, characteristically associated with increased numbers of white cells in the bone marrow and/or peripheral blood. The course of leukemia may vary from a few days or weeks to many years, depending on the type (i.e., acute and chronic), which can be further classified into Acute lymphoblastic leukemia (ALL), Acute myeloid leukemia (AML), Chronic lymphocytic leukemia (CLL) and Chronic myeloid leukemia (CML). Given the high prevalence of ALL, a definite diagnosis necessitates intrusive, costly, and time-consuming tests. The nonspecific nature of the signs and symptoms associated with ALL often leads to misdiagnosis, further complicating the situation. In this study, a robust model is proposed that classifies the different subtypes of ALL. Our comprehensive diagnosis system achieved an accuracy of 97.85 % and demonstrated superior performance in comparison to other state-of-the-art approaches.",Training;Adaptation models;Computational modeling;Refining;Stem cells;Feature extraction;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549348,IEEE Conferences,,,,,,
An Improved Of Deep Convolutional Neural Network For Early Breast Cancer Detection,A. k. Mohammed Jawad Khudhur,"2024 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)",12-Jun-24,2024,"Breast cancer is a huge global health challenge, ranking as the second-largest cause of mortality worldwide. Early identification is crucial for effective treatment and better survival rates. Various screening methods, including mammography, ultrasonography, and thermography, have been developed to aid in the early diagnosis of breast cancer. Leveraging image processing techniques and deep learning algorithms, radiologists can boost the accuracy of detecting breast problems. This paper suggests the use of an upgraded Deep Convolutional Neural Network (DCNN) for the early and accurate identification and diagnosis of breast cancer. Setting itself apart from earlier approaches, this work leverages a DCNN with 12 layered processing layers, leading to better detection and diagnosis accuracy. The Mini Mammographic Database (MIAS) serves as the dataset for testing the proposed system's performance. Results reveal that the Deep CNN achieves exceptional accuracy, reaching 99.1%. Furthermore, comparative analysis against related research demonstrates the advantages of the proposed DCNN-based technique. This research contributes to expanding the area of breast cancer diagnosis and emphasises the potential of deep learning to enhance diagnosis and improve.",Deep learning;Training;Databases;System performance;Ultrasonography;Breast cancer;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550830,IEEE Conferences,,,,,,
GAN-Augmented Breast Cancer Classification with Histogram-Based Gradient Boosting,S. Kayikci; T. Khoshgoftaar,"2024 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)",12-Jun-24,2024,"The main method for screening for breast cancer is mammography, which aims to lower the risk of breast cancer death by early detection. Deep learning techniques have been found to show good predictive results with many medical image datasets. Even so, extensive data augmentation approaches are necessary for accurate computer-assisted diagnosis due to the scarcity of annotated medical images. After being trained on photos, Generative Adversarial Networks (GANs) can produce new images that resemble real images and have a lot of genuine features. The realism and representativeness of the generated features can vary depending on the factors like quality of training data, training parameters, evaluation metrics, and application domain. We used a Histogram-Based Gradient Booster classifier to improve the traditional deep learning methods and encourage the use of GANs for data augmentation. When compared to the baseline, the results demonstrate that GANs increased recall without lowering precision and enhanced classification performance on the malignant class without sacrificing performance on the benign class.",Deep learning;Training;Measurement;Human computer interaction;Training data;Data augmentation;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550539,IEEE Conferences,,,,,,
Unravelling the Mysteries of Lung Cancer: Harnessing the Power of Deep Learning for Detection and Classification,M. Dhayalini; B. R. A. Ponmozhi,"2024 International Conference on Recent Advances in Electrical, Electronics, Ubiquitous Communication, and Computational Intelligence (RAEEUCCI)",12-Jun-24,2024,"Lung cancer is the leading cause of cancer- related death worldwide. Identifying lung nodules is essential in the early detection of lung cancer, which can progress to tumors. Computed tomography (CT) can help with the diagnosis of lung diseases. The primary goal is identifying malignant lung nodules and categorizing lung cancer as benign or malignant. Various image processing, biomarker-based, and machine-automated methods are used to detect lung cancer, but medical practitioners face challenges in achieving accuracy and early diagnosis. This work proposes a novel approach for early and accurate lung cancer diagnosis using the LeNet-5 algorithm. Initially, a median filter is used for preprocessing. Cancerous nodules in lung images were segmented using a hybrid fuzzy with a genetic algorithm (GA). The proposed LeNet-5 is implemented in MATLAB using the dataset obtained by the Lung Image Database Consortium and the Image Database Resource Initiative. Various performance parameters are compared and evaluated with existing algorithms and cutting-edge methods. The experimental outcomes demonstrate that the suggested algorithm has higher classification accuracy (99.2%) than other methods.",Image segmentation;Image databases;Computed tomography;Lung cancer;Lung;Filtering algorithms;Sensitivity and specificity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10547834,IEEE Conferences,,,,,,
An Enhanced Architecture for Brain Tumor Segmentation with a Novel Hybrid NF-RCNN and Adam Optimizer,N. Lavanya; S. Nagasundaram,"2024 International Conference on Recent Advances in Electrical, Electronics, Ubiquitous Communication, and Computational Intelligence (RAEEUCCI)",12-Jun-24,2024,"Magnetic resonance imaging (MRI) of the brain for the purpose of manual tumor diagnosis is laborious and timeconsuming. A diagnostic by an expert is also necessary in every case. The need for accurate diagnosis and classification of brain tumors led to the development of several computer- controlled procedures. In order to segregate brain tumors, this research suggests a new filter. The approach used a hybrid model that included NasNet and FRCNN for segmentation of the brain tumor image, and it was optimized using Adam optimizer. For image filtering, SWT enhanced Median filter was utilized. Outperforming state-of-the-art approaches presented in literature, the suggested strategy shows promising outcomes. The proposed model segments brain tumor image with 92.19% accuracy on tumor region and 99.79% accuracy on background region.",Deep learning;Image segmentation;Magnetic resonance imaging;Manuals;Computer architecture;Filtering algorithms;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10547725,IEEE Conferences,,,,,,
Transformative Advances in Cervical Cancer Diagnosis Leveraging Colposcopy Imaging with ViT,H. S; R. V R; G. G; J. P. D; M. K,"2024 International Conference on Recent Advances in Electrical, Electronics, Ubiquitous Communication, and Computational Intelligence (RAEEUCCI)",12-Jun-24,2024,"This project addresses the urgent need for early and precise detection of cervical cancer types and stages, leveraging Vision Transformer models in deep learning. Key objectives include model development, user interface creation, ethical compliance, and real-world validation. By integrating various imaging tests, it offers a comprehensive approach to detection. Automation enhances efficiency, prioritizing ethical considerations like patient privacy. Validation in clinical settings ensures safety and efficacy, contributing significantly to healthcare advancements. Its scalable framework and research contributions underscore its broad impact on cervical cancer detection. Ultimately, it aims to enhance diagnostic accuracy and patient outcomes, advancing AI in healthcare.",Ethics;Privacy;Image segmentation;Medical services;User interfaces;Transformers;Safety,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10547840,IEEE Conferences,,,,,,
Distinguishing Kidney Tumor Types Using Radiomics Features and Deep Features,R. Magherini; M. Servi; Y. Volpe; R. Campi; F. Buonamici,IEEE Access,20-Jun-24,2024,"Despite technological advances in diagnostic imaging, to distinguish the type of renal tumor without performing a biopsy is still an unsolved challenge. In particular, this is even more striking in the case of clear cell renal cell carcinoma and small oncocytomas. To tackle this problem, a fully automated tool is proposed that can provide decision support for physicians to distinguish between these two types of masses in the most critical cases. In this work three approaches for the development of this tool are implemented and compared, specifically two approaches are based on the use of radiomic features and one on the use of deep features. The nnU-net is exploited to achieve tumor segmentation necessary to obtain the different types of features. The architectures are trained and tested by combining two different datasets, the public dataset KiTS2019 and data from the Careggi University Hospital. The best method is able to obtain 73.77% balanced accuracy, 94.59% sensitivity, 52.94% specificity and 86.84% accuracy.",Tumors;Feature extraction;Radiomics;Computed tomography;Cancer;Deep learning;Classification algorithms;Kidney stones;Computer aided diagnosis;Deep learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552759,IEEE Journals,,,,No,No,No
InsightBrain: CNN-Based Binary Classification of Brain Tumor Images,G. Sharma; V. Anand; R. Chauhan; H. S. Pokhariya; S. Gupta; G. Sunil,2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT),11-Jun-24,2024,"When a brain tumor is in its most advanced stages, it is considered a highly malignant medical condition with an adverse outcome. To improve patient survival rates and ensure proper medical attention, accurate diagnosis is essential. Computer-aided tumor detection systems, especially convolutional neural networks (CNNs), have shown encouraging outcomes in this field by utilizing deep learning developments. CNNs outperform traditional neural network architectures by extracting robust features automatically from input data. This paper presents a methodology for the binary classification of cerebral tumors using CNN architecture. Furthermore, MRI scans are subjected to data augmentation procedures to improve generalization, expand the size of the dataset, and reduce the likelihood of overfitting. The results demonstrate that the suggested CNN architecture produces the best classification accuracy, up to 97.29% along with an overall precision of 98% and recall of 97.85%. which helps in proving the effectiveness of CNN-based methods for recognizing and categorization of brain tumors in MRI scans. The medical field may find great use for this suggested architecture in helping to detect brain tumors early on. By streamlining the health evaluation procedure for patients, this technology might be put into use, which could lower the death rates linked to brain tumors.",Deep learning;Training;Magnetic resonance imaging;Computer architecture;Streaming media;Brain modeling;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10551200,IEEE Conferences,,,,,,
Multi-Model CNN Approaches for Prediction of Brain Tumor Using MRI Images,U. Sakthi; V. Bhardwaj; N. K. Tripathi,2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT),11-Jun-24,2024,"This research paper focuses on its quickly and accurately diagnosis of brain tumors by using Convolutional Neural Networks (CNN) on MRI images. Because they can have serious consequences, brain tumors are a serious health risk that should be detected as soon as possible. The importance of early diagnosis drives this investigation of cutting-edge technological solutions, especially those that make use of AI and ML. In this work, the research uses the most recent deep learning models i.e. Convolutional Neural Networks (CNN) utilizing Residual Networks (ResNet50), Visual Geometry Group 16 (VGG16) and U â€“ Shaped Neural Network (U-Net) to accurately identify brain tumors, with an emphasis on anticipating their early stages of development. The proposed method aims to improve accuracy and enable early detection by building upon previous research on Magnetic Resonance Imaging (MRI) scans of brain tumor datasets. The suggested approach tends to boost accuracy and facilitate early detection as well by looking into prior MRI studies on brain tumor image datasets. Via integrating AI with medical imaging, this work is an important contribution to the ongoing process of developing an integrated approach to a resolution of the contemporary global health challenge. The recent research on brain tumor detection provides the ray of hope for the future, and it also emphasizes the need for an algorithm approach for better brain tumors management. From the built on a well-defined design VGG16 was the best among all different models, accuracy reached even to 96.6%. The constructive and unmatched accuracy of VGG16 shows the strength of AI and medical imaging, which will make it easier to use different measures to fight this conflict that is spreading all over the world.",Magnetic resonance imaging;Computational modeling;Predictive models;Brain modeling;Convolutional neural networks;Artificial intelligence;Biological neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10551204,IEEE Conferences,,,,,,
Brain Tumour Detection Using Deep Learning Techniques,S. Paul; B. K. Soni; B. Baranidharan,2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT),11-Jun-24,2024,"Brain tumor detection is one of the most crucial problems in medical imaging solving which machine learning and, recently, deep learning techniques have demonstrated promising outcomes. This article explores deep learning algorithms for brain tumor detection in the proposed work. For this, the pre-trained architectures such as CNNs EfficientNetB1, VGG16, ResNet50, Inception, and MobileNetV2 are employed. The well-curated dataset of approximately 3400 MRI images of four tumor classes is used to train and evaluate the model using the performance metrics precision, accuracy, F1 score, and recall. EfficientNetB1, which is characterized by the efficient scaling properties, shows results-based on high accuracy and efficiency of 97.72%, and an ensemble and combination with SCNN demonstrates results of 98% accuracy, which improves the work overall. The study also delves into interpretability, elucidating the fusion of features extracted by the ensemble model. Overall, the research highlights the efficacy of ensemble learning in medical diagnosis, showcasing the potential of combining DL techniques to advance healthcare applications and increase the precision of brain tumor detection.",Deep learning;Magnetic resonance imaging;Transfer learning;Computer architecture;Brain modeling;Feature extraction;Search problems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10551009,IEEE Conferences,,,,,,
RM-DenseNet: An Enhanced DenseNet Framework with Residual Model for Breast Cancer Classification Using Mammographic Images,S. Kishore Khan; A. Kanamarlapudi; A. R. Singh,2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT),11-Jun-24,2024,"Breast cancer continues to be a significant health challenge and is the most common cancer in women, with millions of new cases diagnosed each year. Early diagnosis of breast cancer is crucial for improving treatment outcomes and survival rates. Traditional methods of interpretation heavily rely on human expertise, which is time-consuming and often leads to missed diagnoses or false positives. Researchers have developed deep-learning models to analyze mammograms and other medical images for early detection of breast cancer. The proposed model, which combines the DenseNet121 with a residual model (RM-DenseNet), aims to improve feature reuse and learning in deep networks, potentially leading to better performance in classifying breast cancer images. The effectiveness of preprocessing techniques, including Gaussian blur for denoising and horizontal flipping for data augmentation, enhances model robustness and generalization capabilities. This research uses a Dataset CBIS-DDSM, which contains digital mammography images and annotations for various abnormalities, including masses and calcifications. The proposed method is compared with state-of-the-art techniques such as AlexNet, VGG16, and ResNet50 and achieves a better accuracy of 96.50 %. The research outcomes show that the proposed framework can potentially improve patient outcomes and contribute to the advancement in the classification of breast cancer using mammographic Images.",Analytical models;Computational modeling;Noise reduction;Feature extraction;Breast cancer;Mammography;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10551125,IEEE Conferences,,,,,,
Classification of Microglial cells using Deep learning techniques,P. Chitra; Y. B. Vedha; S. J. R. Samuel; R. Krishnaprasanna; S. Rohan; C. S. Nandha Kishore,2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT),11-Jun-24,2024,"Microglia are specialized immune cells found only in the Central Nervous System (CNS), with 1.5 trillion numbers of them identified across the spinal cord and brain. These cells display various characteristics depending on the signals they receive, ranging from beneficial to harmful, as evidenced by their movement and physical changes. Numerous neurological illnesses, including Parkinsonâ€™s and Alzheimerâ€™s disease, have been connected to changes in the inflammatory condition. Understanding the significance of microglial cells in disease progression and examining the feasibility of novel therapy methods in such situations. In this paper, a study is conducted on the microscopy image of microglial cells. Initially, the image is preprocessed using image processing algorithms, and the number of cells in the image is also automatically counted using the image processing algorithm. The preprocessed image is classified using Deep Learning Neural Network (DNN) to classify whether the cell is in a normal or activated state. The classifiers such as Resnet50, VGG16, VGG 19 and Xception are applied to the preprocessed image. The classifierâ€™s performance is assessed using evaluation parameters. The results are compared to choose the model with the highest accuracy. For this data the accuracy of VGG16 net achieved is 96% which is higher than other model considered.",Deep learning;Image segmentation;Thresholding (Imaging);Spinal cord;Soft sensors;Microscopy;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10551020,IEEE Conferences,,,,,,
VCCINet: A Multi-model Deep Learning Approach for Melanoma Skin Cancer Detection,K. Srivastava; A. Istwal; J. Kaur; P. Gambhir,2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT),11-Jun-24,2024,"Recent years have witnessed an increase in skin cancer cases globally, with 2â€“3 million non-melanoma skin cancers and 132,000 melanoma skin cancer cases occurring annually. Early detection is crucial for effective treatment, and deep learning techniques offer a promising avenue for diagnosis. However, several existing individual deep learning models used for skin cancer detection, such as Convolutional Neural Networks (CNN), Visual Geometry Group-based Neural Networks (VGG16), Capsule Networks (CapsNet), and InceptionV3 can often deliver suboptimal results on multiple testing parameters, affecting the modelâ€™s overall performance. Hence, this paper proposes a multi-model approach called VCCINet, which combines four deep-learning techniques (CNN, Feature Extracted VGG16, CapsNet, and InceptionV3) by ensemble learning, to help improve the performance of the individual models. It also introduces a further extension on the VCCINet model, called FT-VCCINet, by implementing feature extraction (on VGG16) and fine-tuning (on CapsNet and Inceptionv3) before using ensemble learning on the models and achieving a better result than all the individual models and VCCINet. The accuracy of VCCINet and FTVCCINet came out to be 93.18% and 93.02% respectively, which is significantly higher compared to individual model accuracies. Furthermore, the multi-model approach showed exponential improvements in other parameters such as Precision, Recall, F1Score, and Specificity.",Deep learning;Visualization;Computational modeling;Computer architecture;Melanoma;Predictive models;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10551176,IEEE Conferences,,,,,,
Lung Opacity Classification in Chest X-Ray Images with a DenseNet201 Transfer Learning-Based Pre-Trained Convolutional Neural Network Model,M. Singla; K. S. Gill; K. Rajput; V. Singh,"2024 International Conference on Communication, Computing and Internet of Things (IC3IoT)",11-Jun-24,2024,"Using transfer learning, this study presents a new method for classifying lung opacity in chest X-ray pictures; the method centers on Keras and the DenseNet201 model. This paper examines the RSNA Pneumonia Detection Challenge dataset and its extensive preprocessing procedures, such as dataset overview, splitting, and balancing. Importantly, Keras has been modified to be DICOM compatible, which solves problems with processing chest X-ray pictures. The generated model is more resilient thanks to the deployment of basic data augmentation techniques, which increase the dataset. The study describes the pretrained DenseNet201 model in great detail, including the extra layers that were added to improve the modelâ€™s performance on the lung opacity classification task. A comprehensive examination of a model includes looking at its architecture, parameters, and performance indicators including F1-score, recall, and precision. Insights into test-time augmentation tactics are presented in the studyâ€™s conclusion, with an emphasis on the significance of making two predictions for each image to increase reliability. A 79% success rate in classifying lung opacity in chest X-ray pictures using a combination of transfer learning and Keras is a major step forward for medical imaging as a whole.",Pneumonia;Medical conditions;Computational modeling;Transfer learning;Lung;Telecommunication computing;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550393,IEEE Conferences,,,,,,
Revolutionizing Retinal Health: Transfer Learning for Early Detection of Pathologies in Limited OCT Image Datasets,K. Mittal; K. S. Gill; D. Upadhyay; S. Devliyal,"2024 International Conference on Communication, Computing and Internet of Things (IC3IoT)",11-Jun-24,2024,"The study effort focuses on the urgent need to diagnose retinal disorders at an early stage using revolutionary methods in medical image processing. The work demonstrates the effectiveness of transfer learning in the medical field by using a dataset of fewer than 1,000 retinal OCT pictures, despite the large number of about 30 million OCT scans performed each year. The dataset has been carefully compiled using a hierarchical grading method that involves undergraduate students, medical students, ophthalmologists, and senior retinal specialists. It consists of four categories: DME, CNV, NORMAL, and DRUSEN. The approach used involves the utilisation of a pre-trained ResNet18 Convolutional Neural Network. The main emphasis is on adjusting the final layer to precisely address the objective of classifying retinal disease. The study achieves an impressive level of precision, reaching 94%, highlighting the promise of transfer learning as a formidable method for effectively and accurately detecting retinal abnormalities at an early stage. This work not only makes a valuable contribution to the area of medical imaging, but also serves as a complete reference for newcomers in the subject who are interested in implementing Convolutional Neural Networks and transfer learning.",Pathology;Optical coherence tomography;Transfer learning;Propulsion;Retina;Telecommunication computing;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550199,IEEE Conferences,,,,,,
Implementation of Convolutional Neural Network Malarial Cells Detection,K. Venkatesan; S. B. Rahayu; M. Muthulakshmi; V. P. Velkur,"2024 International Conference on Communication, Computing and Internet of Things (IC3IoT)",11-Jun-24,2024,"This paper proposes a Convolutional Neural Network (CNN) approach to analyze and detect the malarial parasite-infected blood smear cells. Malaria is a fatal illness solely transmits through the bites of infected female mosquitoes of Anopheles.. Recent studies show that in 2020, there were 241 million cases of malaria worldwide, which resulted in the death of nearly 6,27,000 people. The diagnostic process must be automated to avoid human participation during the automated diagnosis because a delayed or inaccurate diagnosis causes most of these deaths. To enhance diagnostic reliability, deep-learning technologies and CNN, such as medical image processing techniques, are employed to assess parasitemia in microscopic blood slides. In this research, we propose a supervised learning-based Visual Geometry Group (VGG-19) that performs accurate classification to detect malaria-infected cells. The dataset comprises 27,560 images of segmented blood cells, equally divided into parasitized (infected) and uninfected cells, which were utilized for VGG-19 architecture. The first step is to define the image processing methods that can be used to analyze the dataset for training the model. The next stage discusses the techniques for training deep neural networks and the data augmentation techniques used to increase the size of the dataset and enhance the modelâ€™s performance. Finally, the accuracy of classification outcomes is compared from deep CNN using the same datasets for the testing, training and validating phases. Our trained model uses blood smear samples to predict the presence of malarial-infected cells and achieves a 97% accuracy rate.",Training;Analytical models;Visualization;Malaria;Computational modeling;Cells (biology);Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550234,IEEE Conferences,,,,,,
MindSight: Revolutionizing Brain Tumor Diagnosis with Deep Learning,R. Gandham; K. R. Manambakam; N. Nannapaneni; M. K. Enduri; K. Hajarathaiah; S. Anamalamudi,2024 IEEE 13th International Conference on Communication Systems and Network Technologies (CSNT),11-Jun-24,2024,"Brain tumors, characterized by abnormal cell growth, pose a substantial health challenge with non-cancerous (benign) and cancerous (malignant) categories. India witnesses the diagnosis of approximately 40,000 fresh instances of brain tumors annually. The rarity and diversity of tumor types make predicting survival rates challenging. Efficient identification of cerebral abnormalities is essential for the timely and effective management of neurological conditions. Exploring the application of deep learning, this study investigates brain tumor detection using a curated dataset of Magnetic Resonance Images (MRI). Utilizing this dataset, brain tumor detection is advanced through the application of diverse models, including EfficientNetB3, ResNet50, MobileNetV3, and VGG16. The study prioritizes dataset preprocessing, emphasizing data augmentation. Diverse brain tumor images contribute to model training, incorporating transfer learning from pre-trained models on extensive datasets for discerning intricate patterns in medical images. Efficiency evaluation considers computational resources, training time, and complexity. Quantitative metrics F1 score, accuracy, recall, and precision are employed to gauge model performance in classifying the tumor and non-tumor regions. In the conducted study, VGG16 demonstrated the best performance compared to all other models.",Training;Deep learning;Measurement;Computational modeling;Magnetic resonance imaging;Transfer learning;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545790,IEEE Conferences,,,,,,
SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis,Z. Xu; J. Rittscher; S. Ali,IEEE Transactions on Medical Imaging,,2024,"Data-driven methods have shown tremendous progress in medical image analysis. In this context, deep learning-based supervised methods are widely popular. However, they require a large amount of training data and face issues in generalisability to unseen datasets that hinder clinical translation. Endoscopic imaging data is characterised by large inter- and intra-patient variability that makes these models more challenging to learn representative features for downstream tasks. Thus, despite the publicly available datasets and datasets that can be generated within hospitals, most supervised models still underperform. While self-supervised learning has addressed this problem to some extent in natural scene data, there is a considerable performance gap in the medical image domain. In this paper, we propose to explore patch-level instance-group discrimination and penalisation of inter-class variation using additive angular margin within the cosine similarity metrics. Our novel approach enables models to learn to cluster similar representations, thereby improving their ability to provide better separation between different classes. Our results demonstrate significant improvement on all metrics over the state-of-the-art (SOTA) methods on the test set from the same and diverse datasets. We evaluated our approach for classification, detection, and segmentation. SSL-CPCD attains notable Top 1 accuracy of 79.77% in ulcerative colitis classification, an 88.62% mean average precision (mAP) for detection, and an 82.32% dice similarity coefficient for segmentation tasks. These represent improvements of over 4%, 2%, and 3%, respectively, compared to the baseline architectures. We demonstrate that our method generalises better than all SOTA methods to unseen datasets, reporting over 7% improvement.",Task analysis;Lesions;Training;Image segmentation;Image analysis;Diseases;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552251,IEEE Early Access Articles,,,,,,
Brain Tumor Detection Using integrated approach of FCM & CNN,B. S. Negi; R. Chauhan; R. Rawat; Y. Chanti; G. Kaur,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"The diagnosis of brain tumors is a major issue in medical imaging, having the potential to have a large influence on patient outcomes. This abstract provides an overview of the obstacles, most recent breakthroughs, and ongoing research in brain tumor detection. Early diagnosis requires medical imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI). Previously, radiologists were evaluated manually, but with the emergence of deep learning algorithms, particularly Convolutional Neural Networks (CNNs), the field has experienced a revolution. When paired with large and diverse datasets, these AI-based algorithms have demonstrated promising results in automating the diagnosis of brain tumors, boosting accuracy, and reducing the workload for medical workers. However, there are still challenges, such as the need for larger datasets and the interpretability of AI models, even though CNN attained a very outstanding accuracy of 97.87% in our study. The fundamental purpose of this research is to distinguish between normal and abnormal pixels using statistical and texture-based criteria.",Deep learning;Magnetic resonance imaging;Computed tomography;Brain modeling;Boosting;Convolutional neural networks;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543848,IEEE Conferences,,,,,,
Study on Detecting the Teeth and Classifying the Teeth Structure using Machine Learning and CNN,A. N; A. B; V. P. S,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"A dental professional deciphers several dental diagnostic modalities in order to detect anomalies, changes in tooth structure, or tooth-related issues. Dentists assess several dental X-ray diagnostic methods in order to detect problems with anomalies in teeth or structural alterations to teeth. In automated dental identification, distinguishing teeth into four categories (molars, premolars, canines, and incisors) has emerged as a difficult task. To find missing teeth and wide spaces between teeth, one must first understand the fundamentals of tooth anatomy before concentrating on tooth identification and tooth groups. A set of teeth from both the mandibles is shown in the dentistry panoramic imagery. Finding anomalies in developmental stages is hard; the same goes for classifying cavities, dental gaps, broken teeth, and data patterns that don't match expected behavior. The deep convolutional neural network is one of the most effective tools for resolving complex problems in a wide range of domains, including diagnosis in medicine, the illustration recognition, and classification. A dentistry panoramic image is made up of the teeth in the upper and lower jaws together. Dental professionals play a crucial role in the process of automatically classifying dental panoramic images into distinct tooth types, among which are canines, incisors, premolars, and molars. This is a challenging task. This study reviews the ways based on CNN, discusses the problems associated with classifying teeth and anomalies, and tabulates the benefits and drawbacks of each method. Regarding the process of numbering and categorizing, we can use the CNN technique to find anomalies and get the best level of accuracy.",Visualization;Biological system modeling;Salivary glands;Teeth;Machine learning;Loss measurement;Dentistry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543395,IEEE Conferences,,,,,,
Identifying Oral Carcinoma from Histopathological Image using Unsupervised Nuclear Segmentation,R. Shukla; B. Ajwani; S. Sharma; D. Das,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"Oral Cancer, a worldwide health concern, highlights the urgent need for accurate and swift detection and cure. Current diagnosing strategies primarily involve pathologists for analyzing tissue biopsy samples, a method that is time-consuming and heavily driven by pathologistsâ€™ experience. To address these drawbacks, this study proposes a novel technique that incorporates machine vision for cancer detection, aiming to enhance diagnostic accuracy. Given the intricate nature of histopathological images, we adopt an unsupervised approach for cancer detection, in contrast to traditional deep learning or supervised approaches. The nucleus in a cancerous tissue biopsy image is identified as the region of interest (ROI), due to its key characteristics and form. We extract the ROI using K-means clustering augmented with a thresholding technique and apply a novel classification method for the final stage of cancer detection. Our proposed model achieved an accuracy of approximately 97.28%, with a closely following validation accuracy of roughly 96.34% making it more efficient and reliable at cancer detection. These results underscore the effectiveness of our two-stage process starting with image segmentation followed by CNN-based binary classification for accurately detecting cancer cells. They reveal improved speed and precision in identifying cancerous tissues, thus offering a promising pathway for enhancing the efficacy and efficiency of oral cancer detection.",Deep learning;Image segmentation;Biological system modeling;Biopsy;Object segmentation;Cancer detection;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543340,IEEE Conferences,,,,,,
Transfer Learning Paradigm: A Comparative Analysis of VGG16 and EfficientNet in Childhood Leukemia Detection,R. Rajora; H. Gupta; M. Kumar; R. Rawat; K. R. Chythanya; S. Bansal,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"Childhood leukemia, a formidable health challenge, demands innovative strategies for timely detection and intervention. This study leverages the formidable capabilities of cutting-edge deep learning models, VGG16 and EfficientNetB3, to intricately classify a comprehensive dataset comprising 15,135 cell images from 118 patients. Resultantly, VGG16 achieves a commendable classification accuracy of 77%, while the EfficientNetB3 model excels with an exceptional 91% accuracy. Beyond classification proficiency, this research underscores the urgency of early detection in childhood leukemia, shedding light on the transformative potential of deep learning models in enhancing diagnostic capabilities. The findings not only pave the way for refined classification methodologies but also illuminate promising avenues for timely, personalized, and targeted therapeutic interventions. This holistic approach holds promise for significantly improving outcomes and quality of life for young leukemia patients, emphasizing the indispensable role of technology in propelling advancements in pediatric oncology.",Deep learning;Pediatrics;Analytical models;Adaptation models;Microprocessors;Transfer learning;Computer architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544231,IEEE Conferences,,,,,,
Advancing Renal Health: Unleashing the Power of CNNs in Multi-Class Classification for Precise Kidney Condition Diagnosis,R. Rajora; H. Gupta; S. Malhotra; S. Devliyal; G. Sunil,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"This research paper focuses on the application of Convolutional Neural Networks (CNNs) for the multi-class classification of kidney conditions, including Cyst, Normal, Stone, and Tumor. Chronic kidney diseases pose a significant health threat, necessitating timely and accurate detection for effective intervention. The study employs a dataset comprising 12,446 images across the specified classes, and the proposed CNN methodology demonstrates exceptional performance. The model achieved an impressive accuracy of 99.92%, emphasizing its robust ability to distinguish between different kidney conditions. Precision, recall, and F1-score metrics further validate the model's effectiveness, with precision and recall exceeding 99.8%. The findings underscore the critical role of CNNs in early kidney disease detection, showcasing the potential to enhance diagnostic capabilities and contribute to improved patient outcomes in the realm of renal health.",Measurement;Neural networks;Market research;Trajectory;Pattern recognition;Convolutional neural networks;Kidney,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544018,IEEE Conferences,,,,,,
Deep Learning Perspective for Preliminary Detection and Classification of Ovarian Cancer,P. S; L. A. Kumar; H. Phaniraj,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"Histopathological images are analyzed by a deep convolutional neural network in this study for the purpose of detecting and classifying the four subtypes of ovarian cancer (DCNN). Ovarian cancer has the lowest 5-year survival rate of any gynecologic cancer but is also the fifth most frequent and the most aggressive kind of this disease. Ovarian epithelial carcinoma may manifest in four primary forms: serous, mucinous, endometroid, and clear cell. Computers are increasingly being used to analyze medical images for clues to the presence of illnesses including cancer, brain tumors, seizures, and Alzheimer's. In this research, we present and implement an enhanced DCNN-based architecture for distinguishing between healthy and cancerous cells. If cancerous, it might be classified as a subtype. The authors created an extra 24,742 histopathological photos from The Cancer Genome Atlas (TCGA-OV) set of 500 images available to the public. When more images were used during training, the KK-Net classification model's accuracy rose from 75% to 91%. Receiver Operating Characteristics: The performance of this model was examined using the area under the curve analysis. It was shown that the AUC-ROC curve had an average accuracy of 95%. We also evaluate the proposed model's performance to the state-of-the-art using four additional networks (GoogleNet, VGG-19, VGG-16, and AlexNet). The newly formed distinctive design may serve as a benchmark for predicting and diagnosing the illness, allowing pathologists to discover ovarian cancer in its early stages.",Training;Deep learning;Histopathology;Microprocessors;Genomics;Computer architecture;Receivers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543469,IEEE Conferences,,,,,,
Survey Paper on Machine Learning and Deep Learning Driven Applications using Bayesian Techniques,A. Bagayatkar; B. Ivin,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"In todayâ€™s data-driven world, Bayesian methods, including Naive Bayes and Bayesian Networks, have become essential tools for solving complex data analysis and classification challenges. These techniques, grounded in Bayesian probability theory, excel in categorizing data based on observed features, making them valuable in various fields like medicine and image classification. Naive Bayes, though seemingly simple, proves effective in applications like spam filtering and sentiment analysis by calculating probabilities using Bayesâ€™ theorem. Bayesian Networks, more complex graphical models, capture intricate relationships among variables, finding uses in healthcare and engineering. These methods have been showcased in research papers tackling real-world issues, from image classification to medical diagnostics, highlighting their potential to shape the future of data analysis and machine learning in our data-rich era.",Surveys;Sentiment analysis;Data analysis;Uncertainty;Shape;Text categorization;Bayes methods,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543602,IEEE Conferences,,,,,,
Detecting Brain Neuronal Movements Using Histogram of Oriented Gradients,K. S. Vani; M. Komali; G. Harshini; S. V. V. Kollu,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"In recent years, artificial intelligence has been applied to problems in medical imaging, and in particular to medical imaging of the central nervous system. Many AI & ML innovations include image quality and automatic classification of disease. In this, the movement of neurons in our brain is detected. And if there are any deviations from their usual behavior, this suggests that the person has a brain disorder. These deviations are observed by mapping with normal human brain images. The Histogram of Oriented Gradients (HOG) technique is used to convert the MR image to a graph, which is then compared to a normal brain graph. This algorithm is also used in computer vision and image processing for object detection. And Support Vector Machine (SVM), a Binary classification algorithm is used for classifying the images.",Support vector machines;Image quality;Histograms;Brain;Technological innovation;Feature extraction;Vectors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543866,IEEE Conferences,,,,,,
Machine Learning in Medical Diagnosis of Cancer,R. Raj; Jyoti; A. Singh; K. Kumar,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"This study compares machine learning (ML)-based medical diagnosis techniques with conventional medical diagnosis techniques. The first section of the study gives background information on conventional medical diagnosis techniques and the difficultiesâ€”such as poor accuracy, inefficiency, and high costâ€”that they encounter. The use of ML technology for medical diagnostics is then explained, along with some of its possible advantages, including improved accessibility, efficiency, and accuracy. The process for creating and implementing an ML-based medical diagnosis system is then described in the study, along with data collection and analysis methods. The analysis's findings demonstrate that ML-based medical diagnostic systems are superior to conventional techniques in a number of ways, including increased accuracy, quicker diagnosis times, and reduced costs.The study also compares the system's performance with conventional medical diagnosis techniques and assesses its effectiveness based on a number of criteria. According to the study, ML-based medical diagnosis systems provide a number of important benefits over conventional techniques, including improved early-stage cancer detection and more precise prognosis prediction. In conclusion, the research findings and their consequences are discussed, future directions for research are suggested, and the study's limitations are emphasized. Overall, this work offers insightful information on the possibilities of machine learning for medical diagnostics and establishes a foundation for future investigations.",System performance;Machine learning;Data collection;Oncology;Cancer detection;Data models;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544354,IEEE Conferences,,,,,,
Fracture Detection in Radiology with Resnet-152 & CNN,S. Chauhan; R. Chauhan; R. Rawat; S. Aluvala; R. Singh,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"In the field of medicine, fracture identification in medical imaging is a crucial task that has significant effects on patient diagnosis and therapy. Deep learning methods have demonstrated amazing promise in improving fracture diagnosis efficiency and accuracy in recent years. This survey paper offers a thorough examination of the most recent developments, difficulties, and breakthroughs in deep learning-based fracture identification. We explore the key concepts of deep learning and how it applies to many types of medical imaging, including MRIs, CT scans, and X-rays. This survey covers widely used deep learning concepts including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs) and more recent developments like DenseNet and ResNet.Our goal is to give researchers, medical professionals, and policymakers a thorough grasp of the state of deep learning-based fracture diagnosis by analyzing a wide spectrum of recent publications. The authors highlight areas of prospective future research as well as how these developments might affect clinical practice. Will also discuss about the benefits deep learning brings to this field. In conclusion, the authors will discuss the future directions of this technology.",Deep learning;Surveys;Recurrent neural networks;Reviews;Computed tomography;X-rays;Radiology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543297,IEEE Conferences,,,,,,
Impact of a Novel Convolution Neural Network on Chest X-Ray Images for Pneumonia Detection,R. Rajora; H. Gupta; R. Chauhan; H. Singh Pokhariya; Y. Chanti,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"Pneumonia is still considered as a major worldwide healthcare hazard which needs immediate treatment and timely and precise identification. The research paper shows the method for the precise identification of the disease with the use of Convolutional Neural Network (CNN) on the Chest X-ray(CXR) images with the help of Batch normalisation, padding and data augmentation. The research uses a comprehensive dataset of 3450 CXR images. The experiments were conducted on over 200 epochs to ensure accuracy. Batch normalization and padding contributed to the stability and better interpretability of the CNN. Data Augmentation including Colour jitter made the training set more diverse for better generalization. The classification parameters like precision, recall ,F1 score and accuracy were used for the estimation of the modelâ€™s efficacy. The model showed a 96.02% accuracy reflecting its reliability in classification. This study gives medical professionals a dependable and effective tool that improves the state-of-the-art in pneumonia diagnosis.",Training;Pneumonia;Neural networks;Medical services;Data augmentation;Stability analysis;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543278,IEEE Conferences,,,,,,
Fusion of GoogleNet and Capsule Neural Network models for improving bone tumor classification,S. K. S; G. L. J. Salvin,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10-Jun-24,2024,"Combining the strengths of GoogleNet and Capsule Neural Network (CNN) models represents a novel approach to enhance the categorization of bone cancers, a critical aspect for accurate diagnosis and effective therapy planning. Current classification techniques often encounter challenges in achieving both resilience and high accuracy. In this study, we address these issues by leveraging the distinct features extracted from bone tumor images using the GoogleNet model. These features are subsequently fed into the Capsule CNN model, known for its proficiency in capturing complex spatial correlations within data. The proposed fusion strategy aims to improve precision and resilience in bone tumor categorization by synergistically utilizing these two models. The efficacy of this approach is evaluated on a publicly available bone tumor dataset, revealing a notable accuracy of 96.7%. Comparative analysis against state-of-the-art techniques underscores the superior performance of the integrated model. In conclusion, the amalgamation of GoogleNet and Capsule CNN models presents a promising avenue for advancing bone tumor classification, potentially leading to more accurate diagnoses and effective treatment plans for individuals with these conditions.",Neural networks;Medical treatment;Bones;Feature extraction;Spatial databases;Data models;Planning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544090,IEEE Conferences,,,,,,
Skin Cancer Classification using CNN and Transfer Learning (TL),S. Patel; M. Pandey; R. D,2024 International Conference on Inventive Computation Technologies (ICICT),07-Jun-24,2024,"Skin cancer is a predominant and possibly lethal condition that distresses people across the world. Primary detection and precise finding are crucial for leveraging efficacious treatment and enhanced patient consequences. This research study explores the application off Artificial Intelligence (AI) resolutions for the primary finding and analysis of skin cancer. The study leverages a dataset of dermatological images and employs various ML methods, including Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), and decision trees, to analyse the existence of skin cancer with a higher degree of accuracy. By utilizing a combination of image processing and feature extraction, the AI model demonstrates higher performance in classifying skin lesions into malignant or benign categories. The proposed model has achieved an accuracy rate of 98.52%, making the AI-based system a promising tool for dermatologists and healthcare professionals.",Support vector machines;Technological innovation;Dermatology;Transfer learning;Collaboration;Medical services;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544854,IEEE Conferences,,,,,,
Deep Learning-based Classification of Lung CT Scan for Accurate Cancer Diagnosis,D. C. Sujatha; T. R. V. Lakshmi; S. G; U. Surendar; R. M; R. Maranan,2024 International Conference on Inventive Computation Technologies (ICICT),07-Jun-24,2024,"This paper investigates the suitability of advanced deep learning models for precise diagnosis of lung cancer from MRI images. Recurrent neural networks (RNN), K-Nearest Neighbors (KNN), ResNet50, and convolutional neural networks (CNN) were all carefully evaluated to determine their unique contributions. The CNN showed off its good performance and capacity to recognize intricate patterns in lung images, achieving an accuracy of 92.3%. KNN demonstrated competitive results, demonstrating the adaptability of non-parametric methods for medical image classification. Remarkably, ResNet50 fared extremely well, exhibiting a remarkable accuracy of 94.8% and verifying the value of deep residual networks in differentiating between intricate features. RNNs gave the analysis a temporal dimension and contributed to its 89.5% accuracy. Information from confusion matrices containing comprehensive classification results was useful in refining the model. Spatial representations of expected cancer cell locations showed the effectiveness of the models by giving doctors visual cues for targeted interventions. Comparisons with the literature show that the results are in line with recent developments in deep learning for medical image analysis. Because of its comprehensive assessment of different deep learning architectures, which provides fresh perspectives that advance the field of lung cancer detection technologies, this work is an invaluable resource for future research.",Deep learning;Visualization;Recurrent neural networks;Magnetic resonance imaging;Lung cancer;Lung;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544580,IEEE Conferences,,,,,,
Predictive Modeling of Lung Cancer using Support Vector Machine,R. Arunkumar; K. Gowtham; R. G. Pavin; E. Kamalesh,2024 International Conference on Inventive Computation Technologies (ICICT),07-Jun-24,2024,"The segmentation of lung cancer regions in medical images is a crucial part of computer-aided diagnosis systems. Describe a technique in this paper that achieves this goal by employing the Support Vector Machine (SVM) algorithm. Training an SVM on a split dataset involves using feature vectors generated from the retrieved features. The testing set's regions are subsequently classified using the trained SVM, and the segmentation outcomes are then improved by using post-processing techniques. The performance of the method is evaluated using metrics including dice coefficient, precision, recall, and F1 score. The proposed SVM-based algorithm serves as a fundamental tool for lung cancer segmentation, contributing to the ongoing advancements in computer-aided diagnosis and medical image analysis.",Support vector machines;Measurement;Image segmentation;Computational modeling;Lung cancer;Predictive models;Prediction algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544595,IEEE Conferences,,,,,,
Skeletal Fragility Detection using Deep Learning,K. R. K. Reddy; V. S. Reddy; V. S. Sathvik; N. Muthukumaran; S. M. Subhani,2024 International Conference on Inventive Computation Technologies (ICICT),07-Jun-24,2024,"In the field of diagnosing health there has been a shift, in focus from simply identifying bone fractures to a broader evaluation of skeletal fragility. Skeletal fragility involves not detecting fractures but also assessing the health and resilience of the skeletal system, which is crucial for preventing future fractures. While an automated network (DNN) model has achieved impressive accuracy of 92.44% in distinguishing between healthy and fractured bones iti s important to consider additional factors when measuring skeletal fragility. These supplementary factors include bone density, composition, and comprehensive markers of bone health like vitamin D levels. To create a framework for assessing skeletal fragility and not just bone fractures it is necessary to expand the modelâ€™s scope by incorporating these essential additional factors and indicators. This holistic approach has the potential to provide a nuanced perspective, on bone health and the likelihood of future fractures. Ultimately this can significantly improve care by facilitating measures.",Deep learning;Image processing;Noise;Surgery;Manuals;Bones;Data augmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544871,IEEE Conferences,,,,,,
A Systematic Review of Deep Learning Techniques for Breast Cancer Detection,D. Deepika; P. Kanmani,2024 10th International Conference on Communication and Signal Processing (ICCSP),06-Jun-24,2024,"Breast cancer is an extremely serious disease that predominantly affects women. Nevertheless, the likelihood of mortality decreases significantly when the condition is detected and managed promptly during its early stages. To save the life of a female patient, it is crucial to locate the areas where cancer has spread. Genetic mutation associated with ageing and ignorance contribute to this condition. The resulting tumor might be either benign or malignant. The precise breast cancer detection and categorization pose a significant problem in the domain of medical imaging, primarily because of the intricate characteristics of breast tissues. Deep learning approaches have demonstrated considerable success in several domains, specifically in the realm of medical imaging, owing to their capability for autonomous feature extraction. In this work, many different deep learning models that are used in the detection of breast cancer are analyzed, and an evaluation of how well these models perform is presented.",Deep learning;Analytical models;Systematics;Reviews;Mortality rate;Neural networks;Signal processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543472,IEEE Conferences,,,,,,
Uncovering the Experimental Secrets of a Novel Brain Tumor Identification Scheme with Elevated Learning and Image Classification Principles,M. Gomathi; A. Ashwin Ram; U. Monesh; M. Kamalakannan; S. Marshal Subash; V. Niteesh Ram,2024 10th International Conference on Communication and Signal Processing (ICCSP),06-Jun-24,2024,"Masses or clusters of aberrant cells can grow into brain tumours. They could cause problems for the patient if they spread to other tissues. Brain tumours may be reliably detected using magnetic resonance imaging (MRI), the principal imaging method. An abundance of training data and advancements in model construction have allowed for faster development of Deep Learning approaches for computer vision applications, which in turn provide better approximations in a supervised scenario. The fundamental reason for this expansion has been the demand for these techniques. Improving the accuracy of MRI-based brain tumour identification and classification using deep learning approaches has demonstrated encouraging results. Brain tumours can be better treated and patients have a better chance of survival if diagnosed early. By comparing it to the tried-and-true Convolutional Neural Network (CNN), a more conventional learning scheme, this research introduces Elevated Learning based Image Classification Strategy (ELICS), a new approach to image processing-assisted brain tumour detection. Medical professionals execute the crucial yet time-consuming process of detecting brain tumours using Magnetic Resonance (MR) pictures. For the purposes of diagnosis, planning, and therapy, it is crucial to have this information about brain tumours. Brain tumour identification can pinpoint the exact position and size of a tumour in the brain, even in MRI scans that have been contaminated by noise. This systemâ€™s primary function is to identify brain tumours in magnetic resonance imaging (MRI) scans by calculating their areas after extracting them using a thresholding approach.",Deep learning;Image segmentation;Thresholding (Imaging);Magnetic resonance imaging;Noise;Training data;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544250,IEEE Conferences,,,,,,
Enhanced Brain Tumor Detection Using Resnet-50,A. Ramanagiri; M. Mukunthan; G. Balamurugan,2024 10th International Conference on Communication and Signal Processing (ICCSP),06-Jun-24,2024,"Brain tumors are major threat to human health and life. It is better for early detection and treatment the tumors to enhance survival rates. Traditional methods of detecting and categorizing brain tumor, such as manual segmentation and feature extraction, they are both time-consuming and prone to inaccuracies. In this context, Resnet50 enhanced detecting of brain tumor has emerged as a promising alternative for brain tumor classification. Deep learning models have the capacity to autonomously acquire pertinent features from medical images, like MRI scans, and assign them to various categories. This project introduces a unique methodology to enhance brain tumor detection. utilizing ResNet-50 as a feature extractor to capture intricate spatial hierarchies within brain magnetic resonance imaging (MRI) scans. These extracted features are then fed into a customized CNN tailored for precise tumor classification, and then assessed its performance on a separate test dataset. The findings demonstrate that the fine-tuned ResNet50 model achieved exceptional accuracy in classifying brain tumors, with a remarkable accuracy rate of $90.04 \%$ on the test dataset. These results strongly suggest that deep learning models, particularly the combination of CNNs and ResNets, hold great potential for the development of precise and efficient brain tumor classification systems. this project introduces a robust and innovative solution for brain tumor detection through a hybrid ResNet-50 and CNN model. The potential clinical impact includes faster and more accurate diagnoses, promising improved outcomes This advancement could significantly enhance the early detection and treatment of brain tumors, ultimately leading to improved outcomes for patients. The abstract can be tailored to the specific content of the report, including more detailed information about the research methods and outcomes, and potential applications for clinical use.",Deep learning;Training;Magnetic resonance imaging;Manuals;Signal processing;Feature extraction;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543742,IEEE Conferences,,,,,,
Deep Learning Algorithms for Enhanced Precision in Automated Medical Imaging Diagnoses to Improve Healthcare Outcomes,P. Neelaveni; K. Seethalakshmi; N. V. Rao; M. Prabhu; K. V. Kanimozhi; B. Arunagiri,2024 10th International Conference on Communication and Signal Processing (ICCSP),06-Jun-24,2024,"Over the last few years of when deep learning systems have been introduced into medical imaging procedures they are uncorrupted. They should be commended as the best ways of revolutionizing diagnosis processes and improving healthcare outcomes. This report focuses on detailed examination of deep learning for application to automated medical diagnostic, which is one of the most cutting-edge technology at present. Diverse input was the is collected by different kinds of modalities like X-ray, CT scan, MRI, and ultrasound. Also, this input modality is preprocessed to train and validate the proposed deep learning model. The network's architecture was appositely chosen, and the training strategy worked sense prep, which was made to improve the achieved success, included to the method of tuning and cross-validity was used. The qualitative analysis of accuracy, sensitivity and specificity was very good, which is a proof for a better performance of the novel method compared to the baseline ones. Furthermore, the qualitative data analysis and interpretability led to fundamental discussions on the model's performance and the decision-making procedure. Moreover, the model is of good generalizability, maintaining high accuracy on different datasets of validation group, therefore, it is predictive for clinical uses. Overall, this paper confirms wide application of deep learning methods in the field of machine learning-based medical diagnosis and highlights need of further research and development in the area of high tech ones which at present are highly dynamic by nature.",Deep learning;Training;Analytical models;Technological innovation;Machine learning algorithms;Ultrasonic imaging;Heuristic algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543493,IEEE Conferences,,,,,,
Retinal Fundus Image Synthesis using Image Segmentation and DCGAN,A. Kumar; S. Juliet; M. M. Anishin Raj,2024 10th International Conference on Communication and Signal Processing (ICCSP),06-Jun-24,2024,"Training high-quality deep learning models in the ophthalmic domain requires many good-quality labeled training images. Unlike natural images, which are readily available, there is a scarcity of good quality labeled medical images. This work, address the problem for the ophthalmic domain by proposing a methodology to generate good-quality synthetic fundus images using available real images. This work utilizes image segmentation to separate important components of a fundus image, such as the vessel network and optic disc, then combine them with the extracted background to form low-quality fundus images. These images, along with the real image set, are used to train a Deep Convolutional Generative Adversarial Network (DCGAN). The resulting GAN model will be capable of generating high-quality fundus images. The synthetic images generated are of good quality and, at the same time, different from the training set used initially. These images could be employed in tandem with both the preliminary synthetic image and the authentic image to train a classification model. Moreover, they can provide a foundation for transfer learning in the classification of eye diseases.",Training;Image segmentation;Convolution;Transfer learning;Optical fiber networks;Generative adversarial networks;Eye diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544132,IEEE Conferences,,,,,,
Diabetic Retinopathy Detection Using CNN With Res-LSTMDN,R. Gayathri; S. Karthikeyan; T. KausheekRaj; S. Keerthana,2024 10th International Conference on Communication and Signal Processing (ICCSP),06-Jun-24,2024,"For diabetic patients, early diagnosis and treatment of diabetic retinopathy (DR) are essential since they can avert irreversible visual loss. The ways we currently find DR aren't very accurate or fast, so we need better ways to do it automatically. CNNs, a type of machine learning algorithm, have been shown to work well for analyzing medical images and could be a good option for detecting DR. Among CNN architectures, ResNet-152 has gained reputation for its depth and effectiveness in image classification tasks. However, to further enhance its performance in DR detection, this study does a modification to the final layer of ResNet-152. Specifically, the final layer is replaced with Long-Short Term Memory (LSTM) and Deep Neural Network (DNN) components, resulting in an ensemble approach termed Res-LSTMDN. This novel architecture aims to improve the hierarchical feature learning of ResNet-152 while integrating the sequential memory capabilities of LSTM and the flexibility of DNN. Res-LSTMDN uses advanced technology to study eye images, specifically those from people with diabetes. By finding unique patterns in these images, this technique aims to improve the accuracy of detecting diabetes-related eye problems. This will allow for early intervention, potentially preventing vision loss. As part of the ongoing efforts to develop advanced computer tools for diabetes diagnosis, Res-LSTMDN has the potential to significantly impact global diabetes management strategies.",Representation learning;Diabetic retinopathy;Visualization;Machine learning algorithms;Memory management;Signal processing;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543841,IEEE Conferences,,,,,,
Parkinsonâ€™s Disease Detection: VGG-ResNet Hybrid Approach,S. Anantha Sivaprakasam; S. Senthil Pandi; S. Varsha; M. Vishnu; J. Sharath,2024 10th International Conference on Communication and Signal Processing (ICCSP),06-Jun-24,2024,"This study integrates the capabilities of VGG-16 and ResNet-50 convolutional neural networks to provide a novel method for Parkinsonâ€™s Disease (PD) detection. The approach uses a methodical preparation strategy and adapts the last layers of the networks for binary classification, all while leveraging a carefully annotated dataset. The combined model shows encouraging accuracy, highlighting the advantages of using both architectures in concert. The robustness of the suggested methodology is supported by thorough assessments that include cross-validation, precision, recall, and F1-score measures. The field of medical image analysis gains from this research, which lays the groundwork for automated diagnostic tools that can help medical professionals identify Parkinsonâ€™s disease early.",Training;Deep learning;Analytical models;Image analysis;Feature extraction;Trajectory;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543805,IEEE Conferences,,,,,,
Precision Brain Tumor Detection Using Integrated Batch Normalization,N. S. Balaji; M. Hemachandran; R. Jansi,2024 10th International Conference on Communication and Signal Processing (ICCSP),06-Jun-24,2024,"The proposed research introduces a new approach for brain tumor detection, leveraging a sophisticated computational model. A neural network is developed to accurately categorize brain tumor images into four distinct types. A significant innovation within the methodology is the integration of batch normalization after each layer of the neural network. This technique enhances the modelâ€™s learning process, facilitating effective data processing and minimizing errors. Thus, the proposed research presents a robust and accurate solution for brain tumor detection, with potential implications for enhancing patient care and treatment outcomes. The system achieves an accuracy rate of 97.47%, indicating its capability to correctly identify tumor types in the majority of cases. Further, the proposed model exhibits a low loss rate of 0.1193, showcasing its precision in making these classifications.",Technological innovation;Computational modeling;Signal processing;Brain modeling;Data processing;Data models;Biological neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544028,IEEE Conferences,,,,,,
Predicting Diabetic Retinopathy Severity with Deep Learning: A Survey of Fundus Image Analysis Technique,A. B. Sornil; C. Sheeja Herobin Rani; I. R. Sheeba,2024 10th International Conference on Communication and Signal Processing (ICCSP),06-Jun-24,2024,"Diabetic retinopathy (DR) is a significant complication of diabetes mellitus, impacting vision due to retinal abnormalities. Early detection and precise severity assessment are crucial for effective management. Leveraging deep learning techniques and image preprocessing methods, this paper proposes a comprehensive approach to DR classification. Utilizing publicly available datasets like EyePACS, Messidor-2, APTOS, and DDR, preprocessing steps including Gaussian blurring and data augmentation are employed to enhance image quality and address class imbalance. Wavelet decomposition is used for feature extraction to capture multi-resolution information from fundus images. Transfer learning with ResNet variants, coupled with regularization techniques, aids in model generalization. A modified ResNet50 architecture is introduced, featuring custom fully connected layers and additional convolutional layers for improved feature extraction. The model aims to classify retinal diseases into four severity levels: normal, mild, moderate, and severe proliferative. The survey aspect delves into preprocessing methodsâ€™ effectiveness in improving CNN performance for medical image analysis, specifically in DR detection. The applicability of transfer learning in medical imaging tasks, particularly in DR, is also explored. This study contributes to advancing medical image analysis for DR diagnosis and classification, addressing the critical need for efficient detection and management of this debilitating condition.",Deep learning;Surveys;Diabetic retinopathy;Transfer learning;Signal processing algorithms;Feature extraction;Retina,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543945,IEEE Conferences,,,,,,
Skin cancer classification using Progressive Growing of Generative Adversarial Network,A. Patil; N. Kour; P. Vora; A. D. Joshi; S. N. Ghotkar,2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS),05-Jun-24,2024,"Skin cancer detection has become a complex task for medical professionals worldwide. Early diagnosis of carcinogens is important for treating skin cancer and reducing mortality rates, but it still remains a daunting task The models available for effective diagnosis of patients provide less accuracy due to data imbalance. Presently, Generative Adversarial Networks are being utilized but the issue of variations within and between different categories of images is still prevalent. This problem arises due to the shortage of data. This study proposes the use of Progressive Growing of Generative Adversarial Network that aims at producing realistic synthetic images using PAD-UFES 20 dataset that consists of non-dermoscopic images. It is further integrated with classifiers based on convolutional networks for better results. This way model over-fitting and data imbalance is prevented, and diagnosis accuracy is enhanced. This study further aims at producing images with better resolution as well as increasing the classification accuracy between cancerous and non-cancerous skin lesions.",Image resolution;Mortality rate;Generative adversarial networks;Skin;Data models;Lesions;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544308,IEEE Conferences,,,,,,
Self-Supervised Learning for Melanoma Skin Cancer Diagnosis: Unravelling Representations in Dermoscopic Images,M. Packiamary; A. Muthukumaravel,2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS),05-Jun-24,2024,"The study provides an innovative system for identifying melanoma skin cancer using Dermoscopic images and self-supervised learning (SSL). Existing systems for melanoma diagnoses frequently rely on supervised learning, which requires large labelled datasets for training and might be constrained by data availability and bias. The proposed system solves these difficulties by utilizing SSL techniques, allowing the model to extract meaningful representations from input without needing explicit labels. The methodology includes data collection, preparation, and creation of a self-supervised learning system based on SimCLR and the ResNet-50 architecture. The results show that the proposed system outperforms existing systems across multiple datasets and skin types, with high accuracy (0.92), precision (0.93), recall (0.93), and F1 score (0.94). Furthermore, the approach improves interpretability and diagnostic accuracy, demonstrating its potential as a valuable tool in melanoma detection across various clinical situations. The research emphasizes the importance of SSL in tackling issues in medical image classification, particularly in dermatology, and provides a viable route for increasing melanoma detection speed and reliability.",Training;System performance;Supervised learning;Melanoma;Self-supervised learning;Skin;Real-time systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543421,IEEE Conferences,,,,,,
Explainability of Brain Tumor Classification Based on Region,P. Narayankar; V. P. Baligar,2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS),05-Jun-24,2024,"Medical image analysis plays a crucial role in modern healthcare, aiding clinicians in diagnosing and treating various medical conditions. With the advent of Artificial Intelligence (AI) and Machine Learning (ML), there has been a surge in the development of AI-powered algorithms for medical image analysis. Explainable Artificial Intelligence (XAI) techniques aim to provide interpretable and transparent insights into the decision-making process of AI models, enhancing their usability and trustworthiness in healthcare applications. We review the state-of-the-art XAI methods, including feature visualisation, attention mechanisms, and rule-based systems, and their application to medical image analysis. XAI techniques can pave the way for safer and more effective AI-driven medical solutions, ultimately benefiting healthcare providers and patients. As the healthcare industry continues to embrace AI, integrating XAI into medical image analysis is poised to revolutionize how diseases are detected, diagnosed, and treated. In our work, we are using the deep learning model to classify and the Explainable AI model, which explains the prediction of the model for Brain tumour disease using MRI images. We have used CNN for brain tumour disease classification. Out of 7043 images, we have taken 5722 for training and 1321 for testing and validating the disease. The model achieves 80% of accuracy. Explainable AI models like LIME, SHAP, Integrated Gradients, and Grad-CAM are used to interpret a modelâ€™s predictions on regions of interest inan image.",Image analysis;Explainable AI;Decision making;Predictive models;Brain modeling;Convolutional neural networks;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544289,IEEE Conferences,,,,,,
AI Models of Pulmonary Sarcoidosis Detection,Y. Shcherban; S. Kyrylo; A. Chupryna,"2024 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)",05-Jun-24,2024,"Sarcoidosis, a multifaceted inflammatory disorder, often involves the $\prime$ lungs, presenting challenges in diagnosis and management. Computed tomography (CT) imaging is pivotal in assessing pulmonary sarcoidosis, yet interpretation can be subjective and variable. This study explores the application of artificial intelligence (AI) models, including Convolutional Neural Networks (CNNs), Residual Networks (ResNets), Recurrent Neural Networks (RNNs), and the Ultralytics model, in automating the detection and classification of pulmonary sarcoidosis lesions on CT scans. Additionally, the research provides a detailed comparative analysis of these AI models, elucidating their strengths and limitations in pulmonary sarcoidosis detection.",Analytical models;Recurrent neural networks;Computed tomography;Computational modeling;Lung;Object detection;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542583,IEEE Conferences,,,,,,
Deep Learning for Automated Egg Maturation Prediction of Atlantic Salmon Using Ultrasound Imaging,Y. Yari; I. NÃ¦ve; P. Helge Bergtun; A. Hammerdal; S. -E. MÃ¥sÃ¸y; M. Marien Voormolen; L. LÃ¸vstakken,IEEE Access,11-Jun-24,2024,"The Atlantic salmon maturation process has been studied for decades to increase the quantity and quality of the production in farming facilities. An important topic in this context is the salmon egg maturation process. Ultrasound imaging is considered an effective tool for monitoring the egg development stage of salmon, but manual inspection is time-consuming and dependent on operator experience. We propose a method for automated monitoring of the egg maturation stage in salmon using deep learning, providing complimentary decisions on egg morphology. A segmentation network was developed to solve the challenge of separating and measuring individual eggs in the ovary. The segmentation part was combined with a classification network to determine the maturation stage of the eggs. Our model was able to segment eggs and classify their development stage with over 88% accuracy, outperforming established methods designed for similar tasks. A real-time application was developed which provided an estimation of size and maturity stage while scanning. The egg state estimation showed potential for replacing manual evaluations and can enable fully automatic evaluation of maturation in Atlantic salmon.",Image segmentation;Ultrasonic imaging;Fish;Freshwater;Monitoring;Watersheds;Deep learning;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10547256,IEEE Journals,,,,,,
Enhancing Parkinsonâ€™s Disease Diagnosis Through Stacking Ensemble-Based Machine Learning Approach,R. M. Al-Tam; F. A. Hashim; S. Maqsood; L. Abualigah; R. M. Alwhaibi,IEEE Access,10-Jun-24,2024,"Parkinsonâ€™s disease is a progressive neurological condition that affects motor abilities. Common symptoms include tremors, muscle stiffness, and difficulty with coordinated movements. A variety of efforts are underway to address these issues and improve diagnostic precision in Parkinsonâ€™s disease. This paper employs well-known machine-learning techniques to improve diagnostic accuracy. A variety of individual and ensemble AI models have been proposed, including Random Forest, Decision Tree, Logistic Regression, Gradient Boosting, Support Vector Machine, Stacking, and Bagging Ensemble classifiers. Three scenarios are applied to two standard benchmark datasets. The best performance is achieved when the Stacking Ensemble classifier is utilized, where the Support Vector Machine and Gradient Boosting are engaged for extracting features and Logistic Regression for classifying Parkinsonâ€™s disease. The Stacking Ensemble classifier reaches 94.87% accuracy and 90.00% AUC for the first dataset, while for the second dataset, 96.18% accuracy and 96.27% AUC are recorded. The final results demonstrate the importance of the suggested framework, which can help to improve the overall diagnosis outcomes.",Diseases;Stacking;Motors;Medical diagnostic imaging;Support vector machines;Image analysis;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546910,IEEE Journals,,,,,,
Explainable Multi-Modal Deep Learning With Cross-Modal Attention for Diagnosis of Dyssynergic Defecation Using Abdominal X-Ray Images and Symptom Questionnaire,S. Sangnark; P. Rattanachaisit; T. Patcharatrakul; P. Vateekul,IEEE Access,06-Jun-24,2024,"Dyssynergic defecation (DD) is a type of functional constipation that requires a specialized test for diagnosis. However, these tests are only accessible in tertiary care because they require devices that are not available elsewhere. In this work, we present explainable multi-modal deep learning models that can pre-screen patients with DD, using affordable data accessible in small hospitals i.e. abdominal X-ray images and symptom questionnaires; the output classifies whether DD is present or not. To enhance the modelâ€™s performance, we apply cross-modal attention to help the model find meaningful interactions between the two modalities. A convolution block attention module (CBAM) is added to obtain more important semantic and spatial features from the images. Masking augmentation is implemented to ignore irrelevant backgrounds in images. Both explainable AI techniques like gradient-weighted class activation mapping (Grad-CAM) and deep shapley additive explanations (DeepSHAP) are also used to explain the important parts of images and the symptom data for each patient. In our experiments, all models are run on 3 patient-based bootstraps. Our model is compared with single-modal models and human experts. Results demonstrate that our multi-modal model outperforms the single-modal model and achieves the highest in terms of sensitivity, specificity, F1, and accuracy (87.37%, 77.01%, 82.17%, and 82.27%), respectively. In addition, our model outperforms human experts, which shows its ability to assist human experts in diagnosing DD. This model is a novel clinical tool that combines symptom and image data for a more accurate diagnosis of DD.",Deep learning;X-ray imaging;Data models;Biomedical imaging;Diseases;Medical diagnostic imaging;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10547034,IEEE Journals,,,,,,
Improving Endoscopic Image Analysis: Attention Mechanism Integration in Grid Search Fine-Tuned Transfer Learning Model for Multi-Class Gastrointestinal Disease Classification,M. A. Elmagzoub; S. Kaur; S. Gupta; A. Rajab; K. D. Rajab; M. S. A. Reshan; H. Alshahrani; A. Shaikh,IEEE Access,11-Jun-24,2024,"Due to a continuous change in peopleâ€™s lifestyle and dietary habits, gastrointestinal diseases are on the increase, with dietary changes being a major contributor to a variety of bowel problems. Around two million people around the world die due to gastrointestinal (GI) diseases. Endoscopy is a medical imaging technology helpful in diagnosing gastrointestinal diseases like polyps and esophagitis. Its manual diagnosis is time-consuming; hence, computer-aided techniques are now widely used for accurate and fast GI disease diagnosis. In this paper, the Kvasir dataset of 4000 endoscopic images, comprising 500 images of each of the eight gastrointestinal tract disease classes have been classified using seven grid search fine-tuned transfer learning models. The fine-tuned transfer learning models employed in this paper are ResNet101, InceptionV3, InceptionResNetV2, Xception, DenseNet121, MobileNetV2, and ResNet50. The grid search algorithm has been used to determine the architectural and fine-tuning hyperparameters. The fine-tuned ResNet101 model performed the best, with a learning rate 0.001 and a batch size of 32 for the SGD optimizer at 40 epochs. These hyperparameters were optimized through grid search along with new set of layers added to the model. The newly added layers include one flatten layer, two dropout layers and five dense layers optimized using grid search. The grid search fine-tuned ResNet101 model obtained an accuracy of 0.90, a precision of 0.92, a recall of 0.92, and an f1-score of 0.91. Further, the grid search fine-tuned ResNet101 model was integrated with an attention mechanism to enhance performance by focusing on essential image features, notably in medical imaging where some regions may contain vital diagnostic information. The proposed grid search fine-tuned and attention mechanism integrated ResNet101 model achieved an accuracy of 0.935, precision of 0.93, recall of 0.94 and an f1-score of 0.93.",Computational modeling;Gastrointestinal tract;Endoscopes;Cancer;Transfer learning;Optimization;Residual neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545650,IEEE Journals,,,,,,
Adversarial Learning for MRI Reconstruction and Classification of Cognitively Impaired Individuals,X. Zhou; A. R. Balachandra; M. F. Romano; S. Peter Chin; R. Au; V. B. Kolachalama,IEEE Access,17-Jun-24,2024,"Game theory-inspired deep learning using a generative adversarial network provides an environment to competitively interact and accomplish a goal. In the context of medical imaging, most work has focused on achieving single tasks such as improving image resolution, segmenting images, and correcting motion artifacts. We developed a dual-objective adversarial learning framework that simultaneously 1) reconstructs higher quality brain magnetic resonance images (MRIs) that 2) retain disease-specific imaging features critical for predicting progression from mild cognitive impairment (MCI) to Alzheimerâ€™s disease (AD). We obtained 3-Tesla, T1-weighted brain MRIs of participants from the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI, N=342) and the National Alzheimerâ€™s Coordinating Center (NACC, N =190) datasets. We simulated MRIs with missing data by removing 50% of sagittal slices from the original scans (i.e., diced scans). The generator was trained to reconstruct brain MRIs using the diced scans as input. We introduced a classifier into the GAN architecture to discriminate between stable (i.e., sMCI) and progressive MCI (i.e., pMCI) based on the generated images to facilitate encoding of disease-related information during reconstruction. The framework was trained using ADNI data and externally validated on NACC data. In the NACC cohort, generated images had better image quality than the diced scans (Structural similarity (SSIM) index: $0.553 \pm 0.116$ versus $0.348 \pm 0.108$ ). Furthermore, a classifier utilizing the generated images distinguished pMCI from sMCI more accurately than with the diced scans (F1-score: $0.634 \pm 0.019$ versus $0.573 \pm 0.028$ ). Competitive deep learning has potential to facilitate disease-oriented image reconstruction in those at risk of developing Alzheimerâ€™s disease.",Magnetic resonance imaging;Image reconstruction;Biomedical imaging;Generative adversarial networks;Alzheimer's disease;Generators;Adversarial machine learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546982,IEEE Journals,,,,,,
A Transfer Learning Framework for Lung Cancer Classification Using EfficientV2-L: Generalizability Assessment,A. Bouamrane; M. Derdour; A. Alksas; A. El-Baz,2024 6th International Conference on Pattern Analysis and Intelligent Systems (PAIS),03-Jun-24,2024,"Lung cancer remains the deadliest cancer type worldwide, necessitating improved early detection and diagnostic methods to reduce the high mortality rate. Artificial intelligence, particularly deep learning, has shown remarkable success in analyzing medical images and classifying tumors. Despite their success, the majority of these systems have a lack of generalizability. In this work, we provide a novel generalizable method by employing transfer learning with the EfficientNetV2-L model using the LIDCI-IDRI dataset, with further validation using the IQ-OTH/NCCD dataset. The model was pre-trained on the massive ImageNet database, which enabled it to leverage its learning of numerous and valuable features. Our results display significant success, with the model achieving exceptional accuracy rates of 99.31% and a loss of 2.89% during the validation phases on the LIDCI-IDRI dataset. Furthermore, when tested on the external validation dataset, the model demonstrated a commendable accuracy of 96.46% and a loss of 12.3%. Notably, our approach yielded excellent discrimination of 100% using Area under the ROC Curve, F1-score, and specificity metrics, indicating its generalizability, robustness, and potential clinical utility. Additionally, it provides an effective solution to the issue of false negatives and false positives with 0 FP per scan. Looking ahead, future research could explore additional refinements to our methodology and investigate its application in diverse clinical settings, ultimately advancing the fight against this deadly disease.",Deep learning;Mortality rate;Transfer learning;Lung cancer;Lung;Robustness;Pattern analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541243,IEEE Conferences,,,,,,
An Automatic Segmentation and Disease Recognition Algorithm for Gastrointestinal Images Based on Deep Learning,Y. Zhang,"2024 International Conference on Electrical Drives, Power Electronics & Engineering (EDPEE)",########,2024,"Medical images are an important basis for doctors to diagnose diseases and formulate treatment plans. Among them, gastrointestinal imaging is particularly crucial in clinical diagnosis because it can intuitively reflect the structure and functional status of the human digestive system. However, traditional medical image processing methods often rely on the experience and skills of doctors, resulting in strong subjectivity and low efficiency. Based on this, this article proposes a deep learning (DL) based automatic segmentation and disease recognition algorithm for gastrointestinal images. Using DL model for automatic segmentation of gastrointestinal images and automatic recognition of diseases. During the training process, a large amount of gastrointestinal image data with segmentation labels is used. By optimizing the loss function and network parameters, the model can accurately segment the gastrointestinal structure. And by combining the morphological and texture features of gastrointestinal imaging, as well as the clinical information of patients, automatic classification and recognition of gastrointestinal diseases can be achieved by training image data with disease labels. The experimental results show that the algorithm proposed in this article exhibits high accuracy and stability in automatic segmentation and disease recognition tasks of gastrointestinal images.",Training;Deep learning;Image segmentation;Image recognition;Gastrointestinal tract;Stability analysis;Clinical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10539661,IEEE Conferences,,,,,,
Unraveling Gender Fairness Analysis in Deep Learning Prediction of Alzheimerâ€™s Disease,S. K. Hammonds; T. EftestÃ¸l; K. Oppedal; A. Fernandez-Quilez,2024 4th International Conference on Applied Artificial Intelligence (ICAPAI),########,2024,"Early prediction of Alzheimerâ€™s Disease (AD) is widely addressed within dementia research, where the use of deep learning (DL) has received increased popularity across disciplines within healthcare, particularly medical imaging. While unintentional bias in data collection and study design in the field of DL has been a highly debated topic, systematic analysis of the performance of DL models for female and male sub-groups is rarely considered during development. Recently, some studies have shed light on the importance of analyzing classification results for uneven performance with respect to protected characteristics like gender and ethnicity. However, the results are inconsistent and heterogeneous, especially for gender fairness analysis in DL classification of AD. In our work, we seek to present a standardized analysis of gender fairness in the AD classification scenario and discuss the lack of a benchmark to evaluate skewed performance with respect to gender for DL and AD studies. For that purpose, we train two deep neural networks to classify between cognitively normal (CN) and AD subjects from magnetic resonance imaging (MRI) and analyze the classification ability for male and female sub-groups separately. Both networks achieve high classification performance at testing time with an area under the receiver operating characteristics (ROC) curve (AUC) of 1) 0.97Â±0.05 and 2) 0.96Â±0.03. Contrary to other studies, our sub-group analyses do not show significant differences in performance between male and female subgroups.",Measurement;Deep learning;Three-dimensional displays;Systematics;Magnetic resonance imaging;Sociology;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541140,IEEE Conferences,,,,,,
Hybrid CNN Model for Pulmonary Disease Detection,M. S. Abirami; R. Katneni; S. D. Reddy K,2024 2nd International Conference on Networking and Communications (ICNWC),########,2024,"This research proposes a hybrid convolutional neural network (CNN) model for detecting various pulmonary diseases using a substantial dataset of Lung CT-Scan images. The hybrid architecture integrates ResNet, DenseNet-121, and InceptionV3 to harness diverse feature extraction capabilities, targeting the identification of diseases like AdenoCarcinoma, Large Cell Carcinoma, Squamous Cell Carcinoma, COVID-19, and Normal cases. The goal is to enhance accuracy and sensitivity in pulmonary disease identification for early diagnosis and intervention.The hybrid CNN model undergoes training on the extensive dataset, utilizing transfer learning techniques to leverage pre-trained weights from ResNet, DenseNet-121, and InceptionV3. The training process fine-tunes the model, enabling it to capture intricate patterns indicative of various pulmonary diseases present in Lung CT-Scan images, with a specific focus on distinguishing between different disease categories.Evaluation on an independent test dataset demonstrates the model's efficacy, exhibiting improved performance compared to individual CNN models. The hybrid CNN model achieves an average accuracy of 98.61% with a loss of 0.0971 in training, and an average accuracy of 81.70% with a loss of 0.4649 in validation. In the test phase, the model attains an accuracy of 84.40% with a loss of 0.4387. Preliminary results suggest that the hybrid CNN model provides enhanced accuracy and sensitivity in pulmonary disease detection, particularly in the classification of AdenoCarcinoma, Large Cell Carcinoma, Squamous Cell Carcinoma, COVID-19, and Normal cases. The amalgamation of various CNN architectures enhances the model's ability to recognize both subtle and prominent patterns associated with different pulmonary conditions. This research contributes significantly to advancing automated diagnostic tools for pulmonary diseases, aiming to facilitate early detection and improve overall healthcare outcomes.",Training;COVID-19;Squamous cell carcinoma;Sensitivity;Pulmonary diseases;Microprocessors;Computed tomography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537498,IEEE Conferences,,,,,,
Deep Learning Technique to detect Brain tumor disease using YOLO v8,G. Vineela; G. H. Vardhan; C. Kesava Rao; T. Geetamma; D. Drnivasa Rao,2024 2nd International Conference on Networking and Communications (ICNWC),########,2024,"Brain tumor identification is a vital step in medical imaging that identifies abnormalities in the brain. The abstract provides an overview of the techniques and procedures used in brain tumor identification using several imaging facilities such as MRI, CT scans, and PET scans. It emphasizes the importance of automated detection approaches enabled by advances in machine learning and deep learning algorithms. These techniques include preprocessing processes, feature extraction, and classification strategies for reliably distinguishing tumor areas from healthy brain tissues. Noise, diversity in tumor form, and computing complexity are highlighted, as well as continuing research attempts to improve detection accuracy and efficiency. The abstract emphasizes the need of robust and effective brain tumor detection technologies in clinical practice to assist early diagnosis, treatment planning, and monitoring of patients with brain diseases. Brain tumor identification is a critical component of neuroimaging for early diagnosis and therapy planning. Furthermore, the YOLOv8 architecture has been used to brain tumor identification, proving its ability to effectively localize and identify lesions inside medical pictures. Using YOLOv8, we made significant advances in real-time detection capabilities and model with accuracy of 96.4% allowing for more efficient diagnosis and treatment planning for patients with brain tumors. The abstract also investigates the potential of radiogenomics, which combines radiological and genomic data to uncover the underlying molecular features of brain. Finally, the abstract highlights the translational implications of brain tumor detection technologies, emphasizing their importance in improving patient outcomes and expanding our understanding of brain tumor biology.",Deep learning;YOLO;Neuroimaging;Noise;Real-time systems;Planning;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537552,IEEE Conferences,,,,,,
Enhancing Early Detection of Diabetic Retinopathy Through the Integration of Deep Learning Models and Explainable Artificial Intelligence,K. Ahnaf Alavee; M. Hasan; A. Hasnayen Zillanee; M. Mostakim; J. Uddin; E. Silva Alvarado; I. de la Torre Diez; I. Ashraf; M. Abdus Samad,IEEE Access,########,2024,"Humans can carry various diseases, some of which are poorly understood and lack comprehensive solutions. Such a disease can exists in human eye that can affect one or both eyes is diabetic retinopathy (DR) which can impair function, vision, and eventually result in permanent blindness. It is one of those complex complexities. Therefore, early detection of DR can significantly reduce the risk of vision impairment by appropriate treatment and necessary precautions. The primary aim of this study is to leverage cutting-edge models trained on diverse image datasets and propose a CNN model that demonstrates comparable performance. Specifically, we employ transfer learning models such as DenseNet121, Xception, Resnet50, VGG16, VGG19, and InceptionV3, and machine learning models such as SVM, and neural network models like (RNN) for binary and multi-class classification. It has been shown that the proposed approach of multi-label classification with softmax functions and categorical cross-entropy works more effectively, yielding perfect accuracy, precision, and recall values. In particular, Xception achieved an impressive 82% accuracy among all the transfer learning models, setting a new benchmark for the dataset used. However, our proposed CNN model shows superior performance, achieving an accuracy of 95.27% on this dataset, surpassing the state-of-the-art Xception model. Moreover, for single-label (binary classifications), our proposed model achieved perfect accuracy as well. Through exploration of these advances, our objective is to provide a comprehensive overview of the leading methods for the early detection of DR. The aim is to discuss the challenges associated with these methods and highlight potential enhancements. In essence, this paper provides a high-level perspective on the integration of deep learning techniques and machine learning models, coupled with explainable artificial intelligence (XAI) and gradient-weighted class activation mapping (Grad-CAM). We present insights into their respective accuracy and the challenges they face. We anticipate that these insights will prove valuable to researchers and practitioners in the field. Our ambition is that this in-depth study and comparison of models will inform and inspire future research endeavors, ultimately leading to enhanced disease detection in medical imaging and thereby assisting healthcare professionals.",Retina;Support vector machines;Diabetic retinopathy;Biological system modeling;Diseases;Deep learning;Computational modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10539012,IEEE Journals,,,,,,
Pneumonia Classification using deep learning: a comparative study,M. L. Tiar; N. Terki; J. J. Dominguez-Jimenez,2024 8th International Conference on Image and Signal Processing and their Applications (ISPA),########,2024,"Pneumonia is an infectious disease of the lungs, caused by viruses, bacteria or fungi. Pneumonia is distinguished by acute inflammation of the lung tissue, causing the consolidation of the terminal bronchioles and alveoli. According to the WHO (the World Health Organization), this disease causes about 4 million deaths. Among the methods of diagnosing pneumonia uses a chest X-ray. This is widely used to visualize pulmonary abnormalities. This work aims at the detection and characterization of pneumonia by the development of computer-assisted diagnosis systems (DAOC). We use deep learning algorithms and chest X-ray images from a pneumonia database to examine the classification of pneumonia. To improve the accuracy, we compare several deep learning models. This research contributes to meeting the growing demand for medical personnel by addressing the global incidence of chest disorders.",Deep learning;Visualization;Pneumonia;Lung;Signal processing algorithms;Medical services;Signal processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10536711,IEEE Conferences,,,,,,
Techniques and Applications of Image and Signal Processing : A Theoretical Approach,M. Aicha; A. E. Amina,2024 8th International Conference on Image and Signal Processing and their Applications (ISPA),########,2024,"This paper comprehensively overviews image and signal processing, including their fundamentals, advanced techniques, and applications. Image processing involves analyzing and manipulating digital images, while signal processing focuses on analyzing and interpreting signals in various domains. The fundamentals encompass digital signal representation, Fourier analysis, wavelet transforms, filtering, and noise removal. Advanced techniques, such as deep learning for image classification and object detection, are explored. Image and signal processing applications include computer vision, medical imaging, audio processing, and communications. This paper is a valuable resource for understanding image and signal processing principles and applications, fostering further research and development in these fields.",Wavelet transforms;Computer vision;Visualization;Image segmentation;Ethics;Image recognition;Signal processing algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10536860,IEEE Conferences,,,,,,
Application of Deep Transfer Learning in Medical Imaging for Thyroid Lesion Diagnostic Assistance,L. Chaouchi; D. Gaceb; F. Touazi; D. Djani; A. Yakoub,2024 8th International Conference on Image and Signal Processing and their Applications (ISPA),########,2024,"This academic work evaluates and compares the performance of various deep convolutional neural network (DCNN) architectures in classifying thyroid nodules into two categories, malignant and benign, using ultrasound images. The dataset comprises 269 cases of benign lesions and 526 cases of malignant lesions. Given the limited dataset size, we employ a progressive learning approach with three established CNN models: VGG-16, ResNet-50, and EfficientNet. Initially pretrained on ImageNet, these models undergo further fine-tuning using a radiographic image dataset related to a different medical condition but similar to our domain. Different levels and fine-tuning strategies are applied to these models. A supervised softmax classifier is used for classifying lesions as malignant or benign, with the exception of the VGG-16 model. For the VGG-16 model, two additional classifiers, Support Vector Machine (SVM) and Random Forest (RF), are evaluated. The results obtained demonstrate the possibility of easily transitioning from the classification of one disease to another, even with a limited number of images, by leveraging the knowledge already acquired from another extensive database.",Support vector machines;Training;Ultrasonic imaging;Transfer learning;Lesions;Convolutional neural networks;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10536856,IEEE Conferences,,,,,,
Cervical spine fracture detection using deep learning algorithm,M. N. Meadi; A. El MoumÃ©ne Zerari; H. Benbrahim,2024 8th International Conference on Image and Signal Processing and their Applications (ISPA),########,2024,"Artificial intelligence (AI) has revolutionized various scientific fields, including healthcare. In particular, deep learning (DL) algorithms have shown remarkable potential in medical imaging, enabling accurate disease detection and diagnosis. Among the prevalent injuries, cervical spine fractures require rapid identification for timely treatment. Recent advancements in AI and DL techniques have opened up new possibilities for automated recognition and classification of cervical spine fractures using medical imaging data. In this study, we investigate the capabilities of deep learning algorithms, specifically the convolutional neural network (CNN) architecture, for automated detection and classification of cervical spine fractures. The outcomes of our research have the potential to significantly enhance early detection and treatment of cervical spine fractures, leading to improved patient outcomes.",Deep learning;Neural networks;Signal processing algorithms;Medical services;Signal processing;Classification algorithms;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10536832,IEEE Conferences,,,,,,
Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Bag-Level Classifier is a Good Instance-Level Teacher,H. Wang; L. Luo; F. Wang; R. Tong; Y. -W. Chen; H. Hu; L. Lin; H. Chen,IEEE Transactions on Medical Imaging,,2024,"Multiple Instance Learning (MIL) has demonstrated promise in Whole Slide Image (WSI) classification. However, a major challenge persists due to the high computational cost associated with processing these gigapixel images. Existing methods generally adopt a two-stage approach, comprising a non-learnable feature embedding stage and a classifier training stage. Though it can greatly reduce memory consumption by using a fixed feature embedder pre-trained on other domains, such a scheme also results in a disparity between the two stages, leading to suboptimal classification accuracy. To address this issue, we propose that a bag-level classifier can be a good instance-level teacher. Based on this idea, we design Iteratively Coupled Multiple Instance Learning (ICMIL) to couple the embedder and the bag classifier at a low cost. ICMIL initially fixes the patch embedder to train the bag classifier, followed by fixing the bag classifier to fine-tune the patch embedder. The refined embedder can then generate better representations in return, leading to a more accurate classifier for the next iteration. To realize more flexible and more effective embedder fine-tuning, we also introduce a teacher-student framework to efficiently distill the category knowledge in the bag classifier to help the instance-level embedder fine-tuning. Intensive experiments were conducted on four distinct datasets to validate the effectiveness of ICMIL. The experimental results consistently demonstrated that our method significantly improves the performance of existing MIL backbones, achieving state-of-the-art results. The code and the organized datasets can be accessed by: https://github.com/Dootmaan/ICMIL/tree/confidence-based.",Training;Pipelines;Vectors;Task analysis;Semantics;Computer science;Iterative methods,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10538113,IEEE Early Access Articles,,,,,,
Automated Brain Tumor Detection and Classification Through Deep Learning Analysis of MRI Scans,A. Mary A; B. Khanna R; M. Malarvel,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),########,2024,"Deep learning, a prominent facet of AI, has rapidly reshaped the healthcare landscape by offering swift and accurate data analysis techniques. These methodologies aid medical professionals and caregivers worldwide in achieving precise diagnoses and administering follow-up treatments. The brain, serving as the central control unit of the human body is accountable for making decisions. and overseeing the functionality of all other organs and systems, both voluntary and involuntary. Brain tumors, characterized by abnormal cell growth in the brain, can lead to severe conditions like Cancer. Examination through Magnetic Resonance Imaging (MRI) scans are the primary the technique employed by medical professionals to identify brain tumors offers detailed insights into any atypical tissue growth within the brain. By applying machine learning (ML) and deep learning (DL) algorithms to MRI images accelerates and enhances the identification and prediction of brain tumors, empowering radiologists to make quick and well-informed decisions. Our proposed methodology entails training a Deep Neural Network (DNN) model on a dataset of MRI images to pinpoint tumor segments within them for further analysis. We utilize the Flask web framework to detect tumor presence. Convolutional Neural Networks (CNNs) are harnessed in our model to detect and classify brain tumors using advanced deep learning techniques. We evaluate our system's performance by comparing it with traditional methods, validating its effectiveness and accuracy through diverse performance metrics. The results exhibit promising potential for future healthcare applications, enabling more precise and timely diagnoses.",Deep learning;Training;Image segmentation;Magnetic resonance imaging;System performance;Medical services;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533189,IEEE Conferences,,,,,,
Tri-UnityNet: A Multifaceted Ensemble Model for Pneumonia Detection,C. R. Dhivyaa; K. Nithya; P. Shanmugapriya; R. Deebika; K. S. Kumar; R. Sudhakar,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),########,2024,"Pneumonia is an infection often caused by several viral infections and prediction of pneumonia requires expertise from radiotherapists, posing challenges, especially in remote areas. Developing an automatic pneumonia detection system is crucial to detect and treat pneumonia disease. Hence, an innovative approach is proposed to detect pneumonia by utilizing the ensemble of deep learning models, specifically VGG16, ResNet50, and InceptionV3. The ensemble method integrates predictions from these various architectures to enhance overall classification performance. Rather than starting the training of each model from the beginning, transfer learning techniques are utilized to fine-tune Convolutional Neural Network (CNN) models using a pre-existing knowledge base. This involves leveraging pre-trained models and adapting them specifically for the disease diagnose on Chest X-ray image dataset. The features are extracted from VGG16, ResNet50, and InceptionV3 models and the classes of disease are predicted by using softmax layers. Finally, ensemble learning is employed to fuse the predictions of multiple neural network architectures within Tri-UnityNet, enhancing its accuracy and robustness in pneumonia detection. By leveraging the complementary strengths of diverse models, the ensemble approach achieves superior performance, contributing to more reliable diagnostic outcomes.",Adaptation models;Pneumonia;Computational modeling;Predictive models;Feature extraction;Convolutional neural networks;Ensemble learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533190,IEEE Conferences,,,,,,
Skin Disease Classification using Transfer Learning with a Modified ResNet-50 Architecture,P. B. S; Y. C,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),########,2024,"This paper presents an advanced approach in skin disease classification using a modified ResNet-50 architecture, applied to a specific subset from the ISIC 2019 Dataset focusing on Benign Keratosis, Basal Cell Carcinoma, and Melanoma. The study integrates deep learning and medical imaging to enhance diagnostic accuracy in dermatology. It leverages transfer learning, adapting the well-established ResNet-50 model, and modifies it by incorporating global average pooling and dense layers, thus tailoring it for dermatological use. This method is inspired by significant strides in skin cancer detection using ResNet and innovative transfer learning techniques in skin disease classification. A notable aspect of this research is the evaluation of data augmentation's impact on model generalization and robustness. The findings of this study contribute to the growing field of deep learning in skin disease classification, providing a refined and effective tool for potential applications in dermatology.",Deep learning;Transfer learning;Dermatology;Computer architecture;Data augmentation;Skin;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533593,IEEE Conferences,,,,,,
Lung Cancer Classification Using CT Scan Images Through Deep Learning And CNN Based Model,N. K. Karthikeyan; S. S. Ali; V. S. R,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),########,2024,"Lung cancer stands as a formidable and prevalent threat, necessitating urgent attention to early diagnosis and precise treatment to mitigate its high fatality rates. In this context, the utilization of computed tomography (CT) scans, particularly in conjunction with advanced deep learning algorithms, emerges as a powerful strategy for effective lung cancer identification. This study introduces a specialized Convolutional Neural Network (CNN) framework meticulously designed for the early detection of lung cancer through the analysis of CT scan images. Through rigorous comparative analyses with alternative models, our research highlights the CNN's superior performance, marking a substantial improvement over conventional diagnostic technique. The results accentuate the efficacy of our proposed deep learning model, solidifying its position as a more robust and potent diagnostic tool compared to prevailing approaches for the early identification of lung cancer. Future research avenues may explore the integration of larger and more diverse datasets, ensuring the model's robustness and applicability across varied clinical scenarios, ultimately advancing the landscape of lung cancer diagnostics towards improved patient outcomes and healthcare practices.",Deep learning;Analytical models;Technological innovation;Solid modeling;Computed tomography;Mortality rate;Lung cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533528,IEEE Conferences,,,,,,
DeepRetina: A Deep Learning Approach for Diabetic Retinopathy Detection and Stage Classification,H. Nithishvikraman; S. G; A. J. D. D. I,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),########,2024,"Using color fundus imaging to identify diabetic retinopathy (DR) is a tough process that requires skilled doctors to comprehend the existence and significance of certain small characteristics. This effort is further complicated by a complex categorization system. This research aims to use convolution neural network (CNN) to consistently diagnose diabetic retinopathy and grade patients into five groups or stages. An automatically generated diagnostic may be provided by a data-enhanced CNN architecture network that can identify the complex components involved in the class task, as well as exudates, hemorrhages, and micro-aneurysms in the retina, without requiring human input. This work trained CNN using data that was made accessible to the public. When compared to other algorithms on the same dataset, it demonstrated an astounding performance. It was used to the global dataset and obtained a great accuracy of 97% on the validation image.",Deep learning;Diabetic retinopathy;Sensitivity;Image color analysis;Neural networks;Medical treatment;Retinal vessels,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533579,IEEE Conferences,,,,,,
Enhancing Diagnostic Accuracy in Lung Disease:A VIT Approach,E. Ajitha; S. Abishekkumaran; J. C. Deva,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),########,2024,"In the realm of medical diagnostics, where limited data availability poses a challenge, this project addresses the detection of pulmonary diseases, tuberculosis, Corona Virus using chest X-Radiation images from modest datasets containing fewer than a thousand samples. By harnessing the potential of deep Vision Transformer approach is employed for lung disease classification. A distinctive pipeline is established, encompassing image preceding classification. Comparative analysis with existing frameworks showcases the efficacy of this approach. Remarkably, the study reveals that pretrained models and even simpler classifiers like shallow neural networks can rival complex systems. The framework is also validated against publicly available lung datasets, positioning it as a competitive solution. This method achieves accuracy levels akin to state-of-the-art models while embracing the advantage of fewer trainable parameters. Importantly, the MobileViT based model showcases performance parity with superior solutions, effectively reconciling computational efficiency with diagnostic accuracy. This project pioneers an innovative pathway, transcending data constraints to empower accurate and accessible pulmonary disease detection through a resource-efficient framework.",COVID-19;Tuberculosis;Pulmonary diseases;Computational modeling;Pipelines;Neural networks;Lung,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533492,IEEE Conferences,,,,,,
Classification Of Brain Tumor Using Generative Adversarial Network With RES NET Discriminator,M. Umamaheswari; J. Sivadasan; R. K. Dwibedi; B. Senthilkumar; L. P. Rani; S. Oviya,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),########,2024,"The classification of brain tumors using deep learning techniques has emerged as a pivotal area of medical research and diagnostics. In this study, we present an innovative approach to brain tumor classification through the application of Generative Adversarial Networks (GAN). Specifically, we have devised a GAN model with a modified ResNet architecture in the generator and a DenseNet architecture in the discriminator. By improving the precision and effectiveness of brain tumor classification, this innovative design offers substantial breakthroughs in the field of medical imaging by utilizing the capabilities of generative and discriminative networks. The generator, based on a modified ResNet, is designed to create realistic and high-resolution brain tumor images. It learns to generate synthetic brain scans that mimic the characteristics of actual tumor images, thus contributing to data augmentation and diversification. This augmentation process is crucial for training deep learning models effectively, especially when the availability of medical images is limited. The discriminator, on the other hand, employs a DenseNet architecture to distinguish between real brain tumor images and the synthetic ones generated by the ResNet-based generator. The DenseNet's ability to capture intricate details and features in medical images ensures thatthe discriminator can effectively discern between genuine and synthetic data, contributing to the GAN's overall learning process. Our proposed GAN model is trained on a diverse and well-curated dataset of brain tumor images, enabling it to identify and classify various tumor types and their characteristics with remarkable accuracy. The generated synthetic images aid in improving the model's ability to generalize and adapt to new, unseen data, thereby enhancing its performance in brain tumor classification tasks.",Deep learning;Adaptation models;Generative adversarial networks;Brain modeling;Data augmentation;Generators;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533523,IEEE Conferences,,,,,,
A Comparative Study on Reinforcement Learning in Disease Prediction on Medical Data,N. V; P. S. Rajendran,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),########,2024,"Medical imaging has been used extensively in healthcare in recent years for a variety of purposes, including disease diagnosis, treatment planning, and tracking the course of an illness. These applications entail taking pictures of the afflicted organ using a variety of modalities. Image segmentation and classification is two important process performed in disease diagnostic application. Deep reinforcement learning approach applied in many gaming application. DRL provides the accurate result in many real world applications, so researchers pay attention in DRL in medical data that helps the physicians in treating the patient. This study focus on surveying the various applications such as Image registration, lesion localization, image segmentation, image classification and Landmark detection using deep reinforcement learning in healthcare. In addition, this study investigates on the various DRL algorithms (Qlearning, deep deterministic policy gradient (DDPG), Twin Delayed Deep Deterministic Policy Gradient (TDDDPG), Deep Q Network (DQN) and Soft Actor-Critic (SAC) in Alzheimer's disease prediction. Among the DRL methods the DDPG achieved the highest accuracy in detecting the Alzheimer's disease with 97%.",Location awareness;Surveys;Image segmentation;Image registration;Deep reinforcement learning;Prediction algorithms;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533525,IEEE Conferences,,,,,,
MRI Brain Tumor Segmentation and Classification using different deep learning models,D. R. Modi; S. K. AM,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),########,2024,"Brain tumors have significant challenges in diagnostic precision due to their complex morphological subtleties evident in MRI (Magnetic Resonance Imaging) scans. Clinical Decision Support Systems (CDSS) have become indispensable in this regard, with MRI brain tumor segmentation and classification at their forefront. This comprehensive survey deals with the profound impact of deep learning techniques on MRI brain tumor analysis, particularly within the context of CDSS. Emphasizing the models validated against the various Dataset, the exploration ranges from the intricate convolutional processes of CNNs (Convolutional Neural Networks) to the advanced self-attention mechanisms offered by recent Transformer-based methodologies. The significance of 3D datasets is underscored, given their ability to offer detailed and holistic representations of brain tumors. Metrics such as DSC (Dice Similarity Coefficient), JI (Jaccard Index), and sensitivity rates serve as pivotal benchmarks to evaluate the efficiency of these deep learning models. Moreover, this survey investigates the performance trade-offs inherent within these methodologies, pointing towards potential avenues for further enhancement and adaptability. By synthesizing this expansive body of knowledge, the goal is to harmonize cutting- edge technological strides with tangible clinical utility, thereby refining diagnostic accuracy and forging a brighter traj ectory for patient care in the realm of brain tumor studies.",Deep learning;Surveys;Decision support systems;Image segmentation;Three-dimensional displays;Sensitivity;Magnetic resonance imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533630,IEEE Conferences,,,,,,
An Effective Identification of Tuberculosis in Chest X-rays Using Convolutional Neural Network Model,S. Hossain; A. Islam; S. Lima; M. S. Ridoy; M. M. Rahman; S. Sharmin,2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT),########,2024,"Tuberculosis (TB) continues to be a major global health concern, contributing significantly to premature mortality rates around the globe. This research highlights the critical role that computational strategies play in enhancing diagnostic skills by showcasing notable gains in TB diagnosis accuracy through the use of an ensemble of heterogeneous CNN architectures. To detect TB in chest X-rays, the research that is being presented makes use of Convolutional Neural Network (CNN) models, namely DenseNet121 and ResNet50. Preprocessing techniques are used to improve the quality of the data before these models are trained on a dataset of chest X-ray pictures. The study illustrates the correctness and resilience of the suggested technique using a methodical procedure that includes processing datasets, choosing methods, testing, evaluating, and deploying them. This work is notable because it provides comprehensive explanations of model methodology, practical consequences, practicality, and requirement analysis, and it thoroughly examines numerous CNN architectures created specifically for tuberculosis detection. The study employs advanced deep learning algorithms and well-chosen datasets to attain high accuracy rates, as evidenced by the comparative analysis and visualization of training and validation metrics. This work not only shows the potential of CNNs in medical image processing, but it also significantly advances the ongoing battle against tuberculosis by facilitating accurate and timely diagnosis.",Training;Tuberculosis;Mortality rate;Computer architecture;Convolutional neural networks;X-ray imaging;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534374,IEEE Conferences,,,,,,
SwinMedNet: Leveraging Swin Transformer for Robust Diabetic Retinopathy Classification from the RetinaMNIST2D Dataset,M. M. Haque; S. Akter; A. F. Ashrafi,2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT),########,2024,"Diabetic retinopathy (DR) is a leading cause of blindness, making early detection crucial. This study investigates the performance of a Swin Transformer-based deep learning model to classify DR severity in fundus images. In this research, a Swin Transformer architecture was employed, known for its global context learning capabilities, for classifying fundus images into five levels of severity of DR: no apparent retinopathy, mild non-proliferative DR (NPDR), moderate NPDR, severe NPDR, neovascularization, and vitreous/preretinal hemorrhage (PDR). The model was trained and evaluated on a publicly available dataset of fundus images with corresponding DR labels. Performance metrics included accuracy and Area Under the ROC Curve (AUC) for each class. The Swin Transformer model achieved higher accuracy than state-of-the-art research works i.e. 56.8%. The AUC value, which was 83.4% also indicated strong discriminatory power for each class. Our model's performance surpassed previous studies utilizing different deep learning architectures, demonstrating the effectiveness of Swin Transformers for DR classification. This study demonstrates the potential of Swin Transformers for accurate and reliable DR classification in fundus images. This approach could contribute to automated DR screening systems, aiding in early diagnosis and timely intervention, potentially improving patient outcomes and preventing vision loss.",Measurement;Deep learning;Diabetic retinopathy;Image analysis;Blindness;Transformers;Communications technology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534544,IEEE Conferences,,,,,,
"Transforming Precision: A Comparative Analysis of Vision Transformers, CNNs, and Traditional ML for Knee Osteoarthritis Severity Diagnosis",T. S. Apon; M. Fahim-Ul-Islam; N. I. Rafin; J. Akter; M. G. R. Alam,2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT),########,2024,"Knee osteoarthritis is a degenerative joint disease that can cause severe pain and impairment. With increased prevalence, precise diagnosis by medical imaging analytics is crucial for appropriate illness management. This research investigates a comparative analysis between traditional machine learning techniques and new deep learning models for diagnosing knee osteoarthritis severity from X-ray pictures. This study does not introduce new architectural innovations but rather illuminates the robust applicability and comparative effectiveness of preexisting ViT models in a medical imaging context, specifically for knee osteoarthritis severity diagnosis. The insights garnered from this comparative analysis advocate for the integration of advanced ViT models in clinical diagnostic workflows, potentially revolutionizing the precision and reliability of knee osteoarthritis assessments. This study does not introduce new architectural innovations but rather illuminates the robust applicability and comparative effectiveness of pre-existing ViT models in a medical imaging context, specifically for knee osteoarthritis severity diagnosis. The insights garnered from this comparative analysis advocate for the integration of advanced ViT models in clinical diagnostic workflows, potentially revolutionizing the precision and reliability of knee osteoarthritis assessments. The study utilizes an osteoarthritis dataset from the Osteoarthritis Initiative (OAI) comprising images with 5 severity categories and uneven class distribution. While classic machine learning models like GaussianNB and KNN struggle in feature extraction, Convolutional Neural Networks such as Inception-V3 and VGG-19 achieve better accuracy between 55â€“65% by learning hierarchical visual patterns. However, Vision Transformer architectures like Da-VIT, GCViT and MaxViT emerge as indisputable champions, displaying 66.14% accuracy, 0.703 precision, 0.614 recall, and AUC exceeding 0.835 thanks to self-attention processes. This analysis strongly promotes the deployment of sophisticated vision transformers over CNNs and traditional ML for increased precision in knee osteoarthritis diagnosis using X-ray picture categorization.",Analytical models;Visualization;Technological innovation;Transformers;Feature extraction;Reliability;Osteoarthritis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534528,IEEE Conferences,,,,,,
A Multi-cancer Detection and Localization System Utilizing X-AI and Ensemble Technique using CNN,M. R. Karim; A. Rahman; R. Islam,2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT),########,2024,"Multi-cancer early detection (MCED) tests aim to identify multiple cancers at an early stage through the analysis of a single specimen, typically utilizing blood or liquid biopsy. However, the manual examination of blood samples under a microscope is time-consuming, biased, and relies on expert availability. Additionally, the integration of AI-based diagnosis into cancer detection faces challenges due to the unreliability of medical experts. Despite lacking FDA approval, MCED tests hold the potential to enhance cancer screening efficiency, reduce costs, improve patient survival, and automate medical treatment planning. In this research, we explored the efficacy of eight distinct CNN architectures (EfficientNet, MobileNetV3, DenseNet201, DenseNet121, VGG16, ResNet50, ResNet152, and VGG19). Subsequently, combining the top-five models into a stacking ensemble yielded improved results, achieving 98.78 % accuracy, 99% precision, and 99% sensitivity in detecting various cancers from digital histopathology images. The study also integrated grad-CAM visualization to enhance explainability in the cancer detection system, offering valuable insights into CNN-based cancer detection. The proposed model demonstrates precise early multi-cancer detection and monitoring, offering potential cost savings, fostering trust in AI among medical professionals, and streamlining autonomous diagnosis procedures.",Visualization;Costs;Histopathology;Stacking;Streaming media;Cancer detection;Artificial intelligence;Medical diagnostic imaging;Blood;Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534377,IEEE Conferences,,,,,,
Cataract Classification by Applying MobileNetV2 on Different Edge Detection Filters,S. I. Anik; M. S. Hasan; H. Jahan; M. M. Rahman; T. M. Shahriar Sazzad; J. A. Jahan,2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT),########,2024,"Advancements in image processing and deep learning have significantly impacted the diagnosis of ocular diseases. Specially, the utilization of edge detection filters is crucial for meticulously segmenting and analyzing vital retinal structures, facilitating early detection and accurate diagnosis of eye diseases. This study investigates the efficacy of edge detection filters in enhancing the quality of fundus images for cataract detection. To do so, we applied the pre-trained deep learning model MobileNetV2. The experiment has been conducted on a real dataset of fundus photographs which is collected from the Combined Military Hospital (CMH), Dhaka. To evaluate the impact of edge detection algorithms, diverse filters have been employed on the image dataset, and their corresponding Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) have been calculated. Our results indicated that the Prewitt filter notably improved the image quality, which, in turn, increased the accuracy of the deep learning model in detecting cataracts.",Deep learning;Cataracts;Image quality;Image segmentation;Filters;PSNR;Hospitals;Image edge detection;Retina;Eye diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534507,IEEE Conferences,,,,,,
Lightweight Deep Learning Model for Melanoma Classification in Dermoscopy Images for Smart Healthcare,P. N. Sree Charan Teja; T. B. Krishna; A. K. Reddy Poreddy; P. Kokil,2024 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET),########,2024,"Among various types of skin cancers, melanoma is the most aggressive and deadly. There is a notable growth in the implementation of deep learning (DL) methods to identify skin malignancies in dermoscopy images. This paper introduces a lightweight DL-based approach designed for seamless integration into low-memory devices within healthcare applications. The proposed method incorporates three lightweight convolutional neural network (CNN) models: MobileNet-v2, SqueezeNet, and GoogLeNet. Initially, test features are computed from fine-tuned deep CNN models. Subsequently, probability scores for each class are derived by training and testing a random forest classifier with features extracted from the models. Then, the proposed method uses an average ensemble voting technique on the probability scores to enhance the classification performance compared to the individual models. The proposed of lightweight CNN model demonstrated an accuracy of 85.19 % which is better than existing works.",Deep learning;Wireless communication;Adaptation models;Computational modeling;Melanoma;Medical services;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10532923,IEEE Conferences,,,,,,
Classification and Detection of Breast Cancer Review,A. K. Flayeh; A. Douik; S. Thajeal,2024 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET),########,2024,"The Cancer is the second-leading cause of death for women aged 20â€“59 worldwide and very few men. Compared to other cancers, breast cancer kills more people. According to Cancer.org, 13% of American women are at risk of having this cancer, and approximately 80% of them advance, which decreases recovery and treatment success. Deep learning methods for BC detection have been successful thanks to AI. This improves early diagnosis, increasing patient survival. This publication provides a detailed review of the deep learning-based BC diagnosis literature. It aims to help practitioners and researchers understand this domain's issues and trends. This article reviews looks at deep learning methods for breast cancer detection. Next, we examine and synthesize the latest AI-based breast cancer diagnostic studies using multiple breast DL modalities. We also provide a complete overview of breast-cancer imaging datasets, emphasizing their importance in AI-driven algorithms and deep learning model training. The investigation found that the CNN is the most widely used and accurate BC diagnosis model. Also, accuracy measures are the key way to evaluate such models. To provide a complete reference for breast cancer imaging researchers from the details of the researchers works. we can say that The performance of breast cancer detection is influenced by three factors: (1) the efficacy of the CAD system, (2) the characteristics of the population under analysis, and (3) the proficiency of the radiologists utilizing the system. CAD can assist in detecting microcalcifications, which may serve as potential indicators of",Deep learning;Solid modeling;Design automation;Reviews;Databases;Signal processing algorithms;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10532879,IEEE Conferences,,,,,,
Categorization of Brain Tumor on MR Images Using Deep Learning Models,M. N. Sree; R. Singamaneni; M. K. Sri,"2024 2nd International Conference on Device Intelligence, Computing and Communication Technologies (DICCT)",########,2024,"Convolution Neural Network (CNN) algorithms have demonstrated notable capability in effectively analyzing human body image datasets obtained from MRI or CT with sufficient efficiency to direct medical operations. Although clinicians and radiologists can adeptly detect the presence of tumors and general abnormalities in brain scans, differentiating between specific tumor types typically requires significant time and effort. This study introduces an approach employing CNN algorithms for precise brain tumor classification into four distinct classes, leveraging deep learning models such as ResNet-50 and InceptionV3.Prior to classification, the dataset undergoes preprocessing using Adaptive Median Filter (AMF) and Contrast Limited Adaptive Histogram Equalization (CLAHE) for denoising and contrast enhancement. The classification task involves categorizing images into four classes namely Glioma, Meningioma, Pituitary and Tumorless. The proposed methodology aims to streamline the analysis process by minimizing complexity while maximizing accuracy rates when compared to previous developments.",Deep learning;Histograms;Computational modeling;Magnetic resonance imaging;Noise reduction;Streaming media;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533087,IEEE Conferences,,,,,,
A Soft Computing Approach for Efficient Diagnosis of Otitis Media Infection by Mucosal Disease Early Detection and Referrals,A. K. Singh; A. S. Raghuvanshi; R. Mehta,"2024 2nd International Conference on Device Intelligence, Computing and Communication Technologies (DICCT)",########,2024,"Otitis Media (OM) represents a prevalent and potentially severe middle ear condition, primarily affecting children. Timely and accurate OM diagnosis is crucial for effective intervention. Often it happens due to mucosal diseases affected by mucosal membranes. This study explores the application of deep learning techniques to tympanic membrane (TM) images for precise OM detection. the dataset, collected and verified from AIIMS Raipur Hospital, comprises four distinct image categories: Chronic Otitis Media (COM), Earwax plug (EP), Myringosclerosis (MS), and normal Tympanic Membrane (TM). Each category encompasses 40 meticulously selected images for training and 40 for testing. Binary and multi-class classification 3-layer models were developed and rigorously assessed using standard performance metrics, including accuracy, sensitivity, specificity, precision, and Fl-score. The binary classification models consistently demonstrated impressive accuracy, surpassing 95%, in distinguishing various OM types from normal TMs. Notably, the COM vs. Normal model achieved an accuracy of 97.03%. Furthermore, the multi-class classification model exhibited an overall accuracy of 89.45% for categorizing TMs into COM, EP, MS, and normal classes.",Deep learning;Analytical models;Computational modeling;Ear;Media;Medical diagnosis;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10532803,IEEE Conferences,,,,,,
Covid-19 Detection Using Stacked Ensemble Learning Approach,R. Kondaparthi; A. Gorantla; K. R. Reddy; K. Vinoop; G. Suryanarayana,"2024 1st International Conference on Cognitive, Green and Ubiquitous Computing (IC-CGU)",########,2024,"This paper presents a novel methodology that incorporates deep learning models, specifically EfficientNet B2, EfficientNet B3, and Fine-Tuned EfficientNet B3, to analyze chest X-ray images with dimensions $260\times 260$ pixels for EfficientNet B2, $300\times 300$ pixels for EfficientNet B3, and $300\times 300$ pixels for Fine-Tuned EfficientNet B3. The primary focus of this study is the classification of these images into COVID-19 affected or normal cases using three distinct datasets specifically designed to cater to the specific input requirements of each model architecture. The study commenced with extensive data pre-processing, which involved collecting chest X-ray images and subsequently dividing them into training and validation sets for model training and evaluation, respectively. The images were then fed into their respective models, and the training process involved computing training accuracy, validation accuracy, training loss, and validation loss metrics. However, the achieved accuracies fell short of expected levels. Nevertheless, this study represents a pioneering effort to employ larger datasets in this research domain, thereby emphasizing the importance of conducting experiments on extensive datasets. The innovative utilization of large datasets lays the foundation for future research in this area, demonstrating the potential of refined model architectures or alternative approaches to effectively leverage larger datasets. This research contributes to the advancement of the scientific community's knowledge of deep learning applications in medical imaging, particularly in the context of COVID-19 detection using chest X-ray images. The significance of this study transcends its immediate outcomes and lies in its contribution to opening the door for future initiatives that aim to improve the model performance by leveraging extensive datasets.",Training;COVID-19;Deep learning;Measurement;Analytical models;Computational modeling;Computer architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530844,IEEE Conferences,,,,,,
Utilizing Advanced Deep Learning for Accurate Multilabel Classification of Ocular Diseases,M. K. Mayee; M. H. Khanam,"2024 1st International Conference on Cognitive, Green and Ubiquitous Computing (IC-CGU)",########,2024,"This study examined the potential of two cutting-edge computer programmes, ResNet50 and Inception V3, to assist in the diagnosis of various eye disorders. To create a wider variety of ocular images for training, they employed a unique technique. Four different eye conditions were depicted in these images: normal, diabetic, cataract- and glaucoma-afflicted. To ensure that there were adequate images of each kind, applied data preprocessing with GAN model and also using a technique known as transfer learning, these programmes were trained to identify eye disorders after initially being trained on a large collection of photographs (ImageNet). This study used two models on the dataset where it has divided into 80% train and 20% test, and then modifying the learning process as necessary. This model reached the accuracy of 89%, which held true for many output parameters such as recall and precision. Additionally, the deep learning techniques performed well on testing using different new data, proving their possibility for dependability in practical settings and then 90% accuracy rate gives positive impact that they can be helpful resources for ophthalmologists.",Deep learning;Training;Transfer learning;Data preprocessing;Ubiquitous computing;Data models;Diabetes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530831,IEEE Conferences,,,,,,
CBCD: Comprehensive Analysis for Bone Cancer Diagnosis through MRI Imaging,P. N. Srinivasu; V. S. Sravya; V. D. Tagore; V. Vinoothna; Y. Mokshada,"2024 1st International Conference on Cognitive, Green and Ubiquitous Computing (IC-CGU)",########,2024,"Bone cancer is an epidemic of the skeletal system that is impacted by a complex interaction of genetic variables and inherited susceptibility. It is critical to develop a readily accessible detection and classification method for early detection of this cancer. As a consequence, a computerized procedure for distinguishing between cancerous and healthy bones is necessary. This paper is dedicated to developing a system that uses multiple magnetic resonance imaging (MRI) scans in digital imaging and communications (DICOM) format of cancerous and healthy bones that exhibit comparable structural traits from different individuals to identify and classify bone cancer using convolutional neural networks (CNN). The proposed approach can be broken down into three parts: MR image improvement, feature encoding, and categorizing. After preprocessing an image, the features are retrieved using discrete wavelet transformation (DWT). Finally, neural networks (NN) differentiate between normal and diseased bone. This method hybridizes one of the most efficient CNN architectures, Visual Geometry Group (VGG) 19. An effective strategy for cancer detection is proposed based on spatial fuzzy C-means (SFCM) clustering. A rigorous performance evaluation showed that the proposed CBCD method is extremely successful in detecting bone cancer, with an amazing accuracy rate of 92.9%.",Geometry;Performance evaluation;Visualization;Image coding;Magnetic resonance imaging;Bones;Ubiquitous computing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530830,IEEE Conferences,,,,,,
Mpox Classifier: A Deep Transfer Learning Model for Monkeypox Disease Detection and Classification,A. K. Nayak; S. K. Bisoyi; A. Banerjee; D. Mahanta; A. Swain,"2024 1st International Conference on Cognitive, Green and Ubiquitous Computing (IC-CGU)",########,2024,"As humans recover from COVID-19, there is a new pandemic threat posed by the monkeypox virus. A daily report of new instances of monkeypox is made, but it is not as deadly or highly contagious as COVID-19. There's a good chance of a worldwide epidemic if nothing is done. Medical imaging is starting to show potential for using deep learning (DL) techniques to identify a patient's ailments. Because a picture has been utilized to learn more about the disease, the human skin infected with the monkeypox virus and the affected skin area may be used to identify the disease early. But as of right now, there isn't a reliable, publicly accessible MonkeyPox database that can be used for training and testing DL models. Furthermore, a suggested and assessed CNN model was the Mpox Classifier, which is an improved DenseNet-201 deep transfer learning model. Based on the initial and enhanced datasets, this research proposes a deep convolutional neural network with 94 and 99.1 percentage accuracy rates, respectively, for properly identifying monkeypox sickness.",Training;COVID-19;Databases;Transfer learning;Ubiquitous computing;Skin;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530778,IEEE Conferences,,,,,,
Multiple Instance Learning in Medical Images: A Systematic Review,D. Barbosa; M. Ferreira; G. B. Junior; M. Salgado; A. Cunha,IEEE Access,06-Jun-24,2024,"This article presents a systematic review of Multiple Instance Learning (MIL) applied to image classification, specifically highlighting its applications in medical imaging. Motivated by the need for a comprehensive and up-to-date analysis due to the scarcity of recent reviews, this study uses defined selection criteria to systematically assess the quality and synthesize data from relevant studies. Focusing on MIL, a subfield of machine learning that deals with learning from sets of instances or â€œbagsâ€, this review is crucial for medical diagnosis, where accurate lesion detection is a challenge. The review details the methodologies, advances and practical implementations of MIL, emphasizing the attention-grabbing and transformative mechanisms that improve the analysis of medical images. Challenges such as the need for extensive annotated datasets and significant computational resources are discussed. In addition, the review covers three main topics: the characterization of MIL algorithms in various imaging domains, a detailed evaluation of performance metrics, and a critical analysis of data structures and computational resources. Despite these challenges, MIL offers a promising direction for research with significant implications for medical diagnostics, highlighting the importance of continued exploration and improvement in this area.",Reviews;Biomedical imaging;Systematics;Medical diagnostic imaging;Image classification;Lesions;Search problems;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10535495,IEEE Journals,,,,,,
Detection of Respiratory Diseases from Auscultated Sounds Using VGG16 with Data Augmentation,I. K. Binti Roslan; F. Ehara,2024 2nd International Conference on Computer Graphics and Image Processing (CGIP),########,2024,"Lung disease, ranking third globally for causes of death with over 3 million annual fatalities according to the 2015 World Health Organization (WHO), underscores the significance of lung sound features in respiratory illness diagnosis. Auscultation, a subjective early detection method, led to the development of a Computer-Aided Diagnosis (CAD) system employing the VGG16 model for medical imaging complexity. Data limitations necessitated augmentation, enhancing VGG16's performance compared to non-augment results. Respiratory Disease Detection (RDD) task was introduced. Short-Time Fourier Transform (STFT) facilitated audio feature extraction, while VGG16, using transfer learning and fine-tuning, proved effective on a Kaggle-sourced dataset. Augmentation techniques, including pitch shifting, time stretching, and horizontal flipping, addressed class imbalance. The study introduces innovative data augmentation techniques to overcome the challenge of limited training data, demonstrating the effectiveness of augmentation in enhancing the VGG16 model's performance.",Deep learning;Solid modeling;Design automation;Pulmonary diseases;Computational modeling;Transfer learning;Training data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533392,IEEE Conferences,,,,,,
Deep Learning-based Skin Cancer Diagnosis using Dermoscopy Images,V. Ravi,2024 International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS),########,2024,"Over the past few years, deep learning-based models have been state-of-the-art for various classification problems in medical imaging. Transfer learning models in deep learning are successfully employed for skin cancer diagnosis. Model scaling is an important parameter in deep learning-based transfer learning models to achieve better classification accuracy. The current work employs the EfficientNet-based model that proposes an efficient model scaling by considering the accuracy of the classification models. Various types of EfficientNet models are finetuned and the penultimate layer features of the models are extracted. The features of the EfficientNet modelsâ€™ dimension are large, and by using the concept of principal component analysis the more significant features are extracted. Feature fusion of the important features is done and classification is performed using the stacked ensemble model. The stacked ensemble model is a two-stage classification model. The stacked ensemble models use kernel-based support vector machine and gradient boosting classifier for the prediction of labels for the dermoscopic images, and logistic regression-based model for classification. The proposed model performance is evaluated for Skin Cancer Detection (SCD) and Skin Cancer Classification (SCC) using the ISIC archive and HAM10000 skin disease dermoscopy datasets. In various test cases, the EfficientNet-based transfer learning approach has shown improved performance of accuracy compared to the other models with an accuracy of 98% in SCD using the ISIC archive dataset and 99% accuracy for SCC using the HAM10000 dataset. The EfficientNet-based finetuned model can serve as an early-stage diagnosis for SCD and SCC in the healthcare environment. The tool enables dermatologists and healthcare professionals to provide proper treatment for patients affected by skin cancer.",Support vector machines;Transfer learning;Medical services;Predictive models;Feature extraction;Skin;Intelligent systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533983,IEEE Conferences,,,,,,
Alzheimerâ€™s Disease Diagnosis and Categorization using Deep Learning,M. Sangeetha; H. Gunasekaran; A. T; N. J; U. M. P; M. D. R,2024 International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS),########,2024,"Alzheimer's disease (AD) is a significant global health concern, accounting for the sixth highest cause of mortality worldwide. Early diagnosis is critical to improving patient care and treatment outcomes. This study aims to classify Alzheimer's disease (AD) using Magnetic Resonance Imaging (MRI) brain images, allowing for prompt intervention and optimal disease treatment. The dataset consists of 6400 MRI pictures classified into four categories: very mildly demented, mildly demented, moderately demented, and non-demented, with an 80-10-10 split for training, validation, and testing. Initially, the utilization of VGG16 architecture results with an accuracy of 87%. To improve accuracy and reduce training time, the GoogleNet (Inception V3) Convolutional Neural Network (CNN) architecture was used. This method resulted with an accuracy of 97.62%.",Training;Deep learning;Magnetic resonance imaging;Planning;Convolutional neural networks;Medical diagnosis;Alzheimer's disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533928,IEEE Conferences,,,,,,
A Unified Deep Learning Approach for Integrating Retinal Image Diagnosis and Surgical Tool Classification,J. J; A. J; S. J. D; B. R V,2024 International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS),########,2024,"Early and accurate diagnosis by using retinal image processing is critical for enabling optimized patient care. Existing techniques for the diagnosis in medical image processing often face limitations. This research study leverages the comparative analysis of advanced Convolutional Neural Networks (CNNs), including AlexNet, InceptionV3, Xception, and EfficientNetB3; out of which EfficientNetB3 gives the comparatively higher accuracy of 84%. This research work presents a novel deep learning-based approach for detecting diseases along with the integration of Lenscraft AI. Lenscraft AI helps to segment surgical masks and identify the surgical tools used in surgical recordings using YOLOv8. The developed tool and tracking method offers a higher precision value of 0.8863 and efficiency in identifying the surgical instruments and potentially improving surgical technique for the trainee surgeons and patient safety.",Image recognition;Instruments;Face recognition;Neural networks;Surgery;Retina;Safety,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533920,IEEE Conferences,,,,,,
Maxillary Sinus Disease Detection and Analysis Approaches in Deep Learning: Survey,R. Vimala; D. M. D. Preethi,2024 International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS),########,2024,"The intend of this literature survey is to lessen the problems faced by dentists in the field of maxillary sinus diagnosis in image processing and to serve as a valuable reference to the literature related application. The odontogenic diseases may be diagnosed with atypical symptoms or it might mimic other conditions. This can create difficulty to disembark an accurate diagnosis. Sinusitis or temporomandibular joint disorder possibly is a symptom that resemble an odontogenic infection. Some odontogenic diseases may have overlapping symptoms, making it difficult to differentiate between them when based solely on clinical presentation. For instance, both a periapical abscess and a periodontal abscess can cause localized pain, swelling, and sensitivity. Diagnosing maxillary sinus issues through digital imaging, such as panoramic dental Xray, Cone Beam Computed Tomography (CBCT) and Computed Tomography (CT) scans, can be challenging due to the complex anatomy and the potential for overlapping structures. Radiologists utilize assorted computerized methods for maxillary sinus disease detection. CT scan analysis uses algorithms for segmentation and feature extraction, aiding machine learning algorithms in pattern recognition. CBCT provides detailed three-dimensional images, enabling comprehensive assessments of maxillary sinus anatomy and pathology. MRI utilizes signal intensity variations and texture analysis to identify potential diseases. Moreover, the integration of ultrasound, analysis of endoscopic video, and reporting of automated systems utilizing techniques of deep learning such as Convolutional Neural Networks and Recurrent Neural Networks, enhances precise detection by combining information from various imaging modalities. Interpreting dental radiographs can be complex, and certain conditions may not be clearly visible or may appear differently on different imaging modalities. It requires expertise and experience to accurately interpret radiographic findings and correlate them with clinical symptoms. Prompt and accurate diagnosis is crucial for effective treatment and management of odontogenic and maxillary sinus diseases. Early applications in image processing uses specific, well defined task containing edge detection, image denoising, image segmentation and basic image enhancement. Deep learning solves applications in a wide range of tasks including object detection, image generation, and image classification and complex tasks and provide absolute result. In medical imaging, deep learning approaches can be applied to locate, analysis and diagnose the maxillary sinus related disease.",Deep learning;Surveys;Pathology;Machine learning algorithms;Computed tomography;Teeth;Dentistry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534006,IEEE Conferences,,,,,,
Classification of Lung Disease with Recommendation using Deep Learning,M. R. Santhoosh; V. Praveen; A. M. Riyaz; M. Selvam Narendiran; R. V. Vibhinarayanan; B. Prabha,2024 International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS),########,2024,"The early diagnosis of lung diseases, including pneumonia, tuberculosis (TB), asthma, fibrosis, and COVID-19 is critical for effective treatment and management. With the emergence of COVID-19, distinguishing between pneumonia caused by the virus and other respiratory illnesses has become increasingly challenging. Medical imaging, particularly chest CT scans, plays a pivotal role in accurate diagnosis. In recent years, machine learning and image processing techniques have been employed to aid in the classification of lung diseases using various medical images such as X-rays, CT scans, and histopathology slides. Transfer learning, a powerful technique for addressing complex cognitive tasks, has shown promise in this domain. This study aims to utilize an Xception Model, a supervised neural network, to detect COVID-19 using CT scan images. Additionally, this project employs a Multi-class Xception model to differentiate between various lung diseases, including pneumonia, TB, influenza, cancer, and COVID-19. The methodology involves training the models on a dataset consisting of annotated medical images and evaluating their performance in disease classification. The results show that the proposed method improves accuracy in disease classification and provides valuable diagnostic insights for the diseases under study. The utilization of transfer learning with the Xception architecture enhances the efficacy of the models in accurately identifying lung diseases, including distinguishing COVID-19 from pneumonia and other respiratory conditions. Overall, this study contributes to the advancement of AI-based systems for aiding medical professionals in the early and accurate diagnosis of lung diseases.",COVID-19;Training;Pneumonia;Tuberculosis;Computed tomography;Transfer learning;Lung cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533947,IEEE Conferences,,,,,,
Oral Cancer Analysis for Early Detection using Deep Learning,K. M. Shaheer; E. B. Edwin; S. Kirubakaran; T. Mathu; V. Ebenezer; M. R. Thanka,2024 International Conference on Cognitive Robotics and Intelligent Systems (ICC - ROBINS),########,2024,"Worldwide, oral cancer ranks high in terms of death and disability, making it a major concern in public health. In order to effectively treat this condition, a prompt and accurate diagnosis is crucial. Deep learning algorithms used to the analysis of medical images have shown encouraging outcomes in recent years. Several clinical sources provided the pictures used to train and evaluate EfficientNet and CNNs for oral cancer. The goal of training the CNNs was to automatically detect and categorize various stages and forms of oral cancer from images. The EfficientNet method for designing and scaling convolutional neural networks reliably scales the depth, breadth, and resolution dimensions using a compound coefficient. The success of the CNN and EfficientNet models was assessed using standard assessment criteria, including AUC-ROC, sensitivity, specificity, and accuracy. The findings demonstrated that the EfficientNet model is very sensitive and accurate in identifying and categorizing oral cancer, suggesting that it might be a valuable resource for accurate cancer diagnosis.",Deep learning;Training;Sensitivity;Materials reliability;Reliability engineering;Convolutional neural networks;Public healthcare,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533923,IEEE Conferences,,,,,,
Toward the Development of an Oral-diagnosis Framework: A Case Study of Teeth Segmentation and Numbering in Bitewing Radiographs via YOLO Models,C. Deepho; V. Khlaisuwan; C. Pengchai; W. Intarachana; P. Rakchuai; K. Kajhan; A. Takhom,2024 IEEE International Conference on Cybernetics and Innovations (ICCI),########,2024,"This study presents an oral-diagnosis framework integrating the YOLOv8 model for precise tooth localization in dental imaging. The dental segmentation and numbering in the right-side bitewing radiographic images were evaluated through comparison of the YOLOv5 and YOLOv8 models, employing confidence thresholds. The dataset comprised 800 training images and 152 testing images, with the YOLOv8 architecture deployed in three variants. Precision, recall, F1-score, and mean average precision (mAP) were evaluated for both models. YOLOv8 demonstrated superior performance over YOLOv5 in precision (0.913 vs. 0.897), F1-score (0.931 vs. 0.920), and mAP (0.96 vs. 0.954). Variations in model dimensions were observed among YOLOv8 S, M, and L variants, with marginal mAP improvements in specific classes. In conclusion, while YOLOv8 did not enhance dental segmentation and numbering tasks across varying architecture sizes, it consistently outperformed YOLOv5, exhibiting superior segmentation and detection abilities.",YOLO;Radiography;Training;Location awareness;Technological innovation;Image segmentation;Teeth,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10532585,IEEE Conferences,,,,,,
Multi-Disease Diagnosis Using Medical Images,A. G. K; A. R; D. R; E. Mohanraj,2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA),########,2024,"This work presents a novel approach for disease diagnosis using medical imaging modalitiesâ€”MRI, X-ray, and CT scansâ€”to classify heart, brain, and lung disorders. Leveraging ""Deep learning"" techniques, particularly ""Convolutional Neural Networks"" (CNNs), this work focuses on accurately identifying anomalies within these organs. A diverse dataset is prepared, encompassing varied cases of cardiovascular, neurological, and pulmonary conditions, enabling the modelâ€™s training and validation. The methodology involves collecting images, employing transfer learning, and implementing data augmentation techniques to enhance the modelâ€™s robustness. Evaluation metrics including accuracy, sensitivity, specificity, and AUC demonstrate the modelâ€™s efficacy in differentiating between disease categories and healthy instances. This research contributes to advancing healthcare by providing a tool for precise and timely diagnosis, potentially improving patient outcomes across these critical medical domains.",Deep learning;Computed tomography;Transfer learning;Lung;Data models;Robustness;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531493,IEEE Conferences,,,,,,
Harnessing Deep Learning for Precise Diagnosis of 6 types of Cancers,M. F. B; D. Brindha; J. S; A. B; M. V,2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA),########,2024,"The use of deep learning in cancer detection has the potential to lead to more precise and timely diagnosis. In order to identify cancer, this study presents a deep learning-based picture categorization strategy. There are three steps to the process: gathering relevant data, training a model, and predicting a picture. Acquiring and preprocessing a set of high-resolution cancer photos for uniformity in dimensions, file type, and color space constitutes the data preparation step. The dataset size may be increased and the model's generalization ability can be enhanced by using data augmentation approaches. Model training involves applying a suitable optimizer and loss function to a deep learning model that has already been pre-trained, such as MobileNet or ResNet, using the supplemented dataset. Overfitting is prevented by constantly checking the model's performance. The model's generalizability is measured by comparing it to data from a validation set. Usersâ€™ own photographs are used in the image prediction step after being preprocessed to meet the input format requirements of the trained model. From the provided photos, the model determines which sort of cancer is most likely present. The user receives brief and unambiguous feedback regarding the precision of their forecasts. Python is utilized in conjunction with a deep learning and image processing framework, such as TensorFlow or Keras, during the system development process. The UI is created using graphical user interface libraries and web development frameworks. For every type of cancer, the accuracy, precision, recall, and Fl-score of the system are compared to industry standards. This strategy's main objective is to offer a reliable and userfriendly tool for cancer classification.",Deep learning;Training;Lung cancer;Medical services;Predictive models;Data models;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531311,IEEE Conferences,,,,,,
Application of Deep Learning in Detection of various Hepatic Disease Classification Using H & E Stained Liver Tissue Biopsy,N. K R; P. T S,2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA),########,2024,"The human liver is a vital organ that detoxifies the body and produces biochemicals that are important for body growth. Liver health is very important for a long, healthy life and the liver is prone to many ailments. Hepatic Diseases are ailments that affect the functioning of the human liver. These diseases include infections, fibrosis, ballooning or enlarging of the liver, inflammation, and others. Early diagnosis of hepatic diseases is very important and is tricky due to common symptoms. Machine Learning and Deep Learning have exhibited great potential in medical imaging and can be used to diagnose many diseases. This paper explores the application of leading deep learning models like MobileNet, Xception, and DenseNet121 that have exhibited exceptional performance on other computer vision tasks. The paper also explores the benefits of ensemble learning by building an ensemble model with the mentioned models. All models were able to perform well on a dataset of 3000 liver tissue biopsy images with 3 different classes. The average accuracy and Area Under ROC Curve scores were evaluated along with several other metrics. The average accuracy and AUROC Curve scores of MobileNet were 95.4% and 96.56% respectively while those of Xception were 95% and 96.24% respectively. DenseNet121 attained an average accuracy and AUROC Curve score of 93.74% and 95.32% while those of an ensemble model with MobileNet, DenseNet121, and Xception were 99.48% and 99.6% respectively.",Deep learning;Training;Biological system modeling;Computational modeling;Biopsy;Liver;Receivers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531423,IEEE Conferences,,,,,,
Unmasking Pneumothorax: Deep Learning Strategies for Precise Classification and Segmentation,S. KR; H. N. S; H. J,2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA),########,2024,"Deep learning has become an essential tool in modern healthcare to aid in diagnostic steps, particularly in life-threatening diseases like pneumothorax. Pneumothorax is a condition in which air leaks between the lungs and chest walls (Collapsed lung). This research addresses limitations and offers a thorough analysis of various methods for diagnosing pneumothorax. Using the U-Net architecture with flexible backbones, such as ResNet34, the studies produce notable Dice coefficients, with a maximum efficiency of 0.8574. Strategies such as UNet++ significantly improve segmentation accuracy by combining data from different semantic levels and integrating change maps. The significance of AI pneumothorax identification is highlighted by in-image annotations, yielding a remarkable AUROC value of 0.877. This research presents an end-to-end framework that consists of a two-step procedure that includes segmentation and classification. The CheXNet, a 121-layer DenseNet model is used in the classification module and UNet++ technique is used in the segmentation phase. Chest radiograph images are transformed into detailed outputs by this integrated method, which also highlights the exact regions impacted and indicates the occurrence of pneumothorax. The results provide a sophisticated and precise way for diagnosing pneumothorax, which advances the use of deep learning in healthcare applications.",Deep learning;Image segmentation;Adaptation models;Annotations;Scalability;Soft sensors;Stacking,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531304,IEEE Conferences,,,,,,
The Resnet-50 Revolution: Leveraging Transfer Learning for Malaria Diagnosis,S. Anslam Sibi; S. Sivamohan; S. Prabhakaran; S. B. Albin,2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA),########,2024,"This research is a novel methodology for malaria diagnosis that leverages the ResNet-50 deep learning architecture and transfer learning. This innovative approach aims to tackle the prevalent global health challenge of malaria, particularly in regions with limited resources. The study showcases the modelâ€™s exceptional accuracy and generalization capabilities in identifying malaria from microscopic blood smear images, even when using a relatively small dataset. The methodology encompasses crucial steps such as dataset preprocessing, fine-tuning, and comprehensive performance evaluation. Results reveal that the proposed approach outperforms traditional methods and other deep learning models, demonstrating its robustness in handling variations in image quality and staining techniques. This breakthrough underscores the potential of advanced machine learning in proactive healthcare interventions, offering an affordable and precise solution for malaria diagnosis. The research holds promise for transformative impacts on disease management in resource-constrained settings and contributes to global healthcare efforts.",Performance evaluation;Image quality;Deep learning;Malaria;Microscopy;Transfer learning;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531528,IEEE Conferences,,,,,,
Local Cross-View Transformers and Global Representation Collaborating for Mammogram Classification,W. Wu; Q. Rong; Z. Lu,IEEE Access,########,2024,"When analyzing screening mammography images, radiologists compare multiple views of the same breast to help improve the detection rate of lesions and reduce the incidence of false-positive results. Therefore, to make the deep learning-based mammography computer-aided detection/diagnosis (CAD) system meet the radiologistsâ€™ requirements for accuracy and generality, the construction of deep learning models needs to mimic manual analysis and consider the correlation between different views of the same breast. In this paper, we propose the Local Cross-View Transformers and Global Representation Collaborating for Mammogram Classification (LCVT-GR) model. The model uses different view images to train in an end-to-end manner. In this model, the global and local representations of mammogram images are analyzed in parallel using the global-local parallel analysis method. To validate the effectiveness of our method, we conducted comparison experiments and ablation experiments on two publicly available datasets, Mini-DDSM and CMMD. The results of the comparison experiments show that our method achieves better results compared with existing advanced methods, with greater improvements in both AUC-ROC and AUC-PR assessment metrics. The results of the ablation experiments show that our model architecture is scientific and effective and achieves a good trade-off between computational cost and model performance.",Feature extraction;Transformers;Lesions;Cancer;Breast;Training;Mammography;Deep learning;Biomedical imaging;Radiology;Classification algorithms;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533237,IEEE Journals,,,,,,
FastLeakyResNet-CIR: A Novel Deep Learning Framework for Breast Cancer Detection and Classification,R. Zeng; B. Qu; W. Liu; J. Li; H. Li; P. Bing; S. Duan; L. Zhu,IEEE Access,########,2024,"Breast cancer is a type of disease that primarily affects the breast tissue, and it is crucial to achieve early diagnosis for successful treatment and recovery. In recent years, the residual network (ResNet) has gained significant attention in the detection of breast cancer using medical images. In this paper, we propose an efficient and robust deep learning framework called FastLeakyResNet-CIR, an improved ResNet architecture, for breast cancer detection and classification. The FastLeakyResNet-CIR achieves an impressive accuracy of 98.94% when evaluated on a dataset of 7909 microscopic images of breast tumor tissue from BreakHis dataset, which outperforms the state-of-the-art methods, e.g. ResNet18, ResNet50, InceptionV3 and VGG16. The experiment results further highlight the potential of FastLeakyResNet-CIR for accurate and rapid diagnosis of breast cancer, thus facilitating effective medical treatment for patients.",Breast cancer;Training;Data models;Lesions;Transforms;Robustness;Convolutional neural networks;Image classification;Biomedical image processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531258,IEEE Journals,,,,,,
A Multimodal Transfer Learning Approach Using PubMedCLIP for Medical Image Classification,H. N. Dao; T. Nguyen; C. Mugisha; I. Paik,IEEE Access,########,2024,"Medical image data often face the problem of data scarcity and costly annotation processes. To overcome this, our study introduces a novel transfer learning method for medical image classification. We present a multimodal learning framework that incorporates the pre-trained PubMedCLIP model and multimodal feature fusion. Prompts of different complexities are combined with images as inputs to the proposed model. Our findings demonstrate that this approach significantly enhances image classification tasks while reducing the burden of annotation costs. Our study underscores the potential of PubMedCLIP in revolutionizing medical image analysis through its prompt-based approach and showcases the value of multi-modality for training robust models in healthcare. Code is available at:https://github.com/HongJapan/MTL_prompt_medical.git.",Biomedical imaging;Feature extraction;Image classification;Task analysis;Training;Transfer learning;Multimodal sensors;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531719,IEEE Journals,,,,,,
Novel Approach for Detecting Brain Neoplasm Using Deep Learning Technique,S. Sridevi; K. Hemalatha; G. Mariammal; V. V,"2024 Third International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS)",########,2024,"One of the most grinding and significant provoca- tions in the domain of handling images related to medicine is the segmentation of brain neoplasms due to physical categorization with the assist of humans might provoke inappropriate prognosis also an evaluation. Also whenever you've got an abundance of information required being processed, it is a challenging job. sorted through. The extraction of neoplasm regions from pictures becomes difficult since brain neoplasms have a wide variety of appearances and resemble normal tissues. proposed a technique for extracting brain neoplasms from 2D MRIs of the brain using the CNN clustering algorithm, which was then used to train conventional classifiers and a convolutional neural network. For the purpose of this work, a live collection with diverse Neoplasm dimensions, places shapes, and intensity levels of images was used. Sci-kit-Learn was used to implement Support Vector Machine (SVM) and Random Forest, two traditional classifiers, in the traditional classifier section. Then, due to it performing better than the conventional ones, went we now turn to Convolutional Neural Networks (CNN), that are constructed by using Tensorflow along with Keras. The accuracy rate achieved by CNN was 88.0%, which is impressive.",Support vector machines;Shape;Neural networks;Signal processing algorithms;Signal processing;Convolutional neural networks;Prognostics and health management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10527578,IEEE Conferences,,,,,,
Varicose Vein Detection and Real-Time Integration using Faster R-CNN Algorithm,T. Sriranjani; K. P. Senthil Kumar; G. Thiruksha,"2024 Third International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS)",########,2024,"Varicose veins are a prevalent vascular disorder affecting a significant portion of the population. This study aims to develop a user-friendly and cost-effective screening tool for early diagnosis and monitoring of varicose veins. A custom deep learning model was trained on a meticulously curated dataset of varicose vein images, categorizing them as â€œNormalâ€ and â€œVaricose.â€ The model's effectiveness was validated using a comprehensive dataset divided into training, testing, and validation. The system also extends to realtime varicose vein detection through laptop cameras, providing instant visual feedback for timely intervention. This real-time capability is complemented by long-term monitoring, making it a valuable tool for both clinical and home-based use. The proposed solution aligns with modern healthcare requirements, enabling early intervention in varicose vein management and contributing to improved healthcare outcomes.",Training;Visualization;Sociology;Signal processing algorithms;Medical services;Real-time systems;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10527750,IEEE Conferences,,,,,,
Blockchain with Sand Cat Swarm Optimized Semantic Attentive Mobilenetv3 Model for Smart IoHT System,M. Nalluri; C. B. Mupparaju; A. S. Rongali; A. Bhuvanesh,"2024 Third International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS)",########,2024,"Nowadays, the healthcare (HC) system based on blockchain (BC) assisted internet of things (IoT) has grown a lot in which the medical services are readily accessible to each user via internet sources. However, the HC IoT systems are highly vulnerable to security threats to render the patient's personal health data. Hence, this study put forth innovative optimized Deep learning (DL) model for BC-assisted secure IoT enabled HC system. The developed study undergoes three major stages namely, medical image encryption, optimal key selection, decryption, and disease classification. Initially, the medical image is encrypted and followed by this, optimal key selection process is performed using a novel sand cat swarm optimizer (SCSO). The encrypted medical data is then stored in the BC and decrypted for further process. Finally, semantic attentive mobileNetV3 (SAtt-MNetV3) model is introduced to extract the features and to classify the decrypted health data to the patients. The simulation is carried out on the Python platform and a publicly available ISIC-2017 database is considered for training process. Various existing studies are also compared with the developed model by computing several performance measures like accuracy, computation time, peak to signal noise ratio (PSNR), recall, sensitivity and specificity metrics. The developed methodology obtained the overall accuracy of $99.2{{\% }}$, recall of $98.9\%$, specificity of $99.5\%$, PSNR of $60.7{{\ dB}}$ and computation time of $0.13{{\ s}}$. The simulation performances outperform well compared to conventional studies.",Training;Databases;Computational modeling;Semantics;Medical services;Sensitivity and specificity;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10527504,IEEE Conferences,,,,,,
Coronary Artery Disease Classification With Different Lesion Degree Ranges Based on Deep Learning,A. JimÃ©nez-Partinen; K. Thurnhofer-Hemsi; J. RodrÃ­guez-CapitÃ¡n; A. I. Molina-Ramos; E. J. Palomo,IEEE Access,########,2024,"Invasive Coronary Angiography (ICA) images are considered the gold standard for assessing the state of the coronary arteries. Deep learning classification methods are widely used and well-developed in different areas where medical imaging evaluation has an essential impact due to the development of computer-aided diagnosis systems that can support physicians in their clinical procedures. In this paper, a new performance analysis of deep learning methods for binary ICA classification with different lesion degrees is reported. To reach this goal, an annotated dataset of ICA images containing the ground truth, the location of lesions, and seven possible severity degrees ranging between 0% and 100% was employed. The ICA images were divided into â€œlesionâ€ or â€œnon-lesionâ€ patches. We aim to study how binary classification performance is affected by the different lesion degrees considered in the positive class. Therefore, five Convolutional Neural Network architectures â€“ DenseNet-201, MobileNet-V2, NasNet-Mobile, ResNet-18, and ResNet-50 â€“ were trained with different input images where different lesion degree ranges were gradually incorporated until considering the seven lesion degrees. Besides, four types of experiments with and without data augmentation were designed, whose F-measure and Area Under Curve (AUC) were computed. Reported results achieved an F-measure and AUC of 92.7% and 98.1%, respectively. However, lesion classification is highly affected by the degree of the lesion intended to be classified, with 15% less accuracy when < 99% lesion patches are present.",Lesions;Training;Deep learning;Feature extraction;Solid modeling;Data augmentation;Convolutional neural networks;Biomedical imaging;Angiocardiography;Coronary arteriosclerosis;Arteries;Computer architecture;Computer aided diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530628,IEEE Journals,,,,,,
Deep Learning based Brain Tumor Classification for MR Images using ResNet50,Ã–. M. KÃ¶kÃ§am; A. Boyaci; M. E. Ã‡olak,2024 12th International Symposium on Digital Forensics and Security (ISDFS),########,2024,"Brain tumors are abnormal cell growths that occur in various parts of the brain, and the accurate classification of these tumors plays a critical role in determining treatment methods. Classification and diagnosis of brain tumors based on artificial intelligence and deep learning models have been made possible due to advances in medical image processing technologies. For instance, such technologies facilitate quick detection of tumors by health experts thereby improving early diagnosis rate and hence enhancing treatment outcomes. Importantly, however, one considers improvement or breakthroughs in artificial intelligence-based classification systems as a crucial step forward towards brain tumor diagnosis as well as its treatment strategy. In this study we use Google Research's ImageNet-21k pre-trained ResNet50 model for classifying brain tumor cases. This model is a deep learning-based image classification tool and is capable of learning from large data sets. The model is specifically trained for high-resolution image recognition and is designed to achieve high accuracy rates. The results showed a classification accuracy of 99.9 percent, which is an exceptional accuracy rate for this model. This accuracy rate indicates that the model is highly effective in detecting and classifying brain tumors. High accuracy rates allow for early diagnosis of patients' diseases and, as it is often said in medicine, â€œEarly Diagnosis Saves Livesâ€. This result is one of the many proofs of how effective artificial intelligence and deep learning are in medical image analysis.",Deep learning;Image recognition;Brain modeling;Internet;Security;Artificial intelligence;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10527322,IEEE Conferences,,,,,,
Comparative Analysis of Machine Learning and Optimization Techniques in Chest X-ray Image Analysis for Medical Diagnosis,T. Dhiman; P. Kumar,"2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",########,2024,"Nowadays, pneumonia, lung cancer, and COVID-19 are the main causes of death, particularly in developing nations. Even if far less advanced instruments and therapies are used to identify and cure illness. The detection of these diseases remains a main alarm in developing nations. This paper offers multiple disease detection by using optimized machine learning methods and a heuristic algorithm. Various complications and characteristics are also described for the disease. The proposed methodology positively described various machine learning methods such as KNN, support vector machine (SVM), Logistic Regression (LR), Random Forest (RF), and RBM, etc. It also includes adaptive and altruistic PSO (AAPSO) and heuristic red fox optimization (HRFO) methods for the classification and optimization process of pneumonia, Covid-19, and lung cancer. The overall analysis of this methodology, the adaptive and altruistic PSO method reached better performance as compared to other methods",COVID-19;Support vector machines;Radio frequency;Pneumonia;Lung cancer;Predictive models;Reliability engineering,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522102,IEEE Conferences,,,,,,
Enhancing Lung Cancer Diagnosis Through Convolutional Neural Networks: A Comprehensive Study on Image-Based Detection and Classification,Y. Aggarwal; N. Mittal,"2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",########,2024,"This study investigates how well image-based detection and classification of lung cancer may be achieved with Convolutional Neural Networks (CNNs). The goal of the project is to apply deep learning techniques to improve the efficiency and accuracy of lung cancer diagnosis while addressing the shortcomings of current diagnostic methods. With the help of a well selected dataset of lung pictures, used a methodical approach that included data collecting, preprocessing, and the creation of a CNN model specifically designed for this purpose. The experimental results show that the suggested model performs better than previous methods, with a noticeable increase in accuracy, sensitivity, and specificity. Our results highlight CNNs' potential to transform the diagnosis of lung cancer by offering a thorough grasp of their capabilities for image-based detection and classification. The findings of this study have ramifications for the larger field of medical imaging and open the door to quicker and more precise lung cancer diagnosis, which will lead to better patient outcomes.",Training;Lung cancer;Lung;Transforms;Sensitivity and specificity;Market research;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522127,IEEE Conferences,,,,,,
Using Swin Transformer and SIFT Algorithm to Detect Bone Abnormalities,S. Makwane; A. Kiran; S. Bhardwaj,"2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",########,2024,"This research paper investigates the combination of cutting-edge deep learning architectures, specifically the Swin Transformer, with conventional feature extraction methods, such as the Scale-Invariant Feature Transform (SIFT) algorithm, for the identification and classification of fractures. We seek to improve the accuracy and robustness of fracture identification from medical pictures by integrating characteristics acquired from SIFT with the capabilities of Swin Transformer during a 16- week period, we show the efficacy of our hybrid technique through thorough experimentation and evaluation, providing insights into its performance in comparison to traditional methods. Our research advances fracture diagnosis techniques, which may lead to more precise and effective clinical decision-making in hospital settings.",Hospitals;Decision making;Deep architecture;Transforms;Transformers;Feature extraction;Market research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522295,IEEE Conferences,,,,,,
Exploring Deep Learning for Efficient Data Management in Brain Tumor Classification: An Overview,V. Verma; A. Aggarwal,"2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",########,2024,"The effectiveness of brain tumor diagnosis with artificial intelligence (AI) based systems depends on effective data management (DM). The research on medical image analysis is increasingly focused on employing deep learning (DL) to detect and classify brain tumors. Presently, medical images are being evaluated and tumors are being classified using DL. Within DL a popular method, Convolutional Neural Networks (CNNs), is employed with great accuracy to detect brain tumors in medical images. The creation of these models involved utilizing extensive datasets of brain MRI scans, resulting in their exceptional accuracy in detecting the presence of tumors. The models have undergone testing and training utilizing several publicly accessible data, for example Kaggle, the Brain Tumor Segmentation (BraTS) dataset, etc dataset. Scientists have devised multiple techniques for data management (DM) involving machine learning (ML) to preprocess the data, augment it, and optimize the models in order to enhance their accuracy even further. In general, the topic of utilizing DM with the help of ML and DL in identifying brain tumors is quickly expanding. It holds great potential for improving patient outcomes. DL techniques have significantly improved the classification and identification of brain cancers using diverse datasets.",Training;Deep learning;Magnetic resonance imaging;Brain modeling;Data models;Convolutional neural networks;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522149,IEEE Conferences,,,,,,
NLP-Powered Healthcare Insights: A Comparative Analysis for Multi-Labeling Classification With MIMIC-CXR Dataset,E. Erberk Uslu; E. Sezer; Z. Anil Guven,IEEE Access,########,2024,"The digitization of the healthcare industry has led to a growing number of applications that use machine learning and image processing techniques to improve the diagnostic process. These applications utilize a variety of medical data, including laboratory results, clinical findings, MRI scans, tomographic images, and radiological images. In addition, free-text healthcare documentation, such as well-structured discharge summaries, contains valuable information. Natural Language Processing encompasses the development of automated systems for generating health reports. This process involves using domain-specific knowledge and prior knowledge to extract relevant information from medical records. This article investigates the use of natural language processing techniques for chest X-ray classification. A total of 14 distinct impressions derived from chest radiography findings from the MIMIC-CXR dataset were used in a multi-label classification procedure. Six distinct language models derived from the BERT language model, along with three distinct classification algorithms, were employed to evaluate the effectiveness of the models and the dataset for multi-label categorization. The experimental results showed a successful prediction rate of 80.47% for 14 distinct impressions within the dataset.",Natural language processing;Medical services;Radiology;MIMICs;Feature extraction;Medical diagnostic imaging;Data mining;Labeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529271,IEEE Journals,,,,,,
LEGAN: Addressing Intraclass Imbalance in GAN-Based Medical Image Augmentation for Improved Imbalanced Data Classification,H. Ding; N. Huang; Y. Wu; X. Cui,IEEE Transactions on Instrumentation and Measurement,########,2024,"Currently, the medical image classification is challenged by performance degradation due to imbalanced data. Balancing the data through sample augmentation proves to be an effective solution. However, traditional data augmentation methods and simple linear interpolation fall short in generating more diverse new samples, thereby limiting the enhancement of results with augmented data. Although generative adversarial networks (GANs) models have the potential to generate more diverse samples, current GAN models struggle to effectively address the issue of intraclass mode collapse. In this article, we propose a GAN model structure named LEGAN, based on local outlier factor (LOF) and information entropy, to address this problem. The LEGAN model focuses on resolving mode collapse caused by intraclass imbalances. First, LOF is used to detect sparse and dense sample points in intraclass imbalance, and affine transformations (ATs) are performed on sparse sample points to enhance the diversity of sample data and features. Then, we train LEGAN jointly using the augmented sparse samples and dense samples to effectively learn the sample distribution in sparse regions, thereby generating more diverse sparse samples. Second, we propose a decentralization constraint based on information entropy. This method measures the diversity of generated samples using information entropy during the training process and provides feedback to the generator, encouraging it to optimize towards better diversity. We conducted extensive experiments on three medical datasets, namely, BloodMNIST, OrgancMNIST, and PathMNIST, demonstrating that LEGAN can achieve more diverse intraclass sample generation. The quality of the generated images and the classification performance are both significantly improved.",Generative adversarial networks;Training;Biomedical imaging;Generators;Data models;Information entropy;Deep learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529541,IEEE Journals,,,,,,
EfficientSwin: A Hybrid Model for Blood Cell Classification with Saliency Maps Visualization,T. Patel; H. El-Sayed; M. K. Sarker,2024 35th Conference of Open Innovations Association (FRUCT),########,2024,"Blood cell (BC) classification holds significant importance in medical diagnostics as it enables the identification and differentiation of various types of BCs, which is crucial for detecting specific infections, disorders, or conditions, and guiding appropriate treatment decisions. Accurate BC classification simplifies the evaluation of immune system performance and the diagnosis of various ailments such as infections, leukemia, and other hematological disorders. Deep learning algorithms perform excellently in the automated identification and differentiation of various types of BCs. One of the advanced deep learning models, EfficientNet has shown remarkable performance with limited datasets, another model Swin Transformerâ€™s capability to capture intricate patterns and features makes it more accurate, albeit with limitations due to its large number of parameters. However, medical image datasets are often limited, necessitating a solution that balances accuracy and efficiency. To address this, we propose a novel hybrid model, by combining the strengths of these two models. We first fine-tuned the Swin Transformer on a blood cell dataset comprising wihite blood cells, red blood cells and platelets, achieving promising outcomes. Subsequently, our hybrid model, EfficientSwin, outperformed the standalone Swin Transformer, achieving an impressive 98.14% accuracy in BCs classification. Furthermore, we compared our approach with previous research on white blood cell datasets, showcasing the superiority of EfficientSwin in accurately classifying blood cells. We also employed saliency maps for a visual representation of our classification results, further illustrating the efficacy of our approach.",Deep learning;White blood cells;Visualization;Technological innovation;Red blood cells;Transformers;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516424,IEEE Conferences,,,,,,
Cervical Spine Fracture Detection and Classification Using Two-Stage Deep Learning Methodology,M. Yaseen; M. Ali; S. Ali; A. Hussain; M. -I. Joo; H. -C. Kim,IEEE Access,########,2024,"Cervical spine fractures are a medical emergency that can cause permanent paralysis and even death. Traditional fracture detection techniques, such as manual radiography image interpretation, are time-consuming and prone to human error. Deep learning algorithms have shown promising results in various medical imaging applications i.e., disease diagnosis, including fracture detection of bones. In this study, we propose a two-stage approach for detecting cervical spine fractures. The first stage employs a convolutional neural network (CNN) model to determine the presence or absence of a fracture in the cervical spine, using a dataset of cervical spine Computed Tomography (CT) scan images as well as Grad-CAM for enhanced visualization and interpretation. In the second stage, our focus shifts to specific vertebrae within the cervical spine. To accomplish this task, we trained and evaluated the performance of the YOLOv5 and YOLOv8 models with 9170 images consisting of seven vertebrae. The detection results of both YOLO versions are compared and evaluated. The precision, recall, mAP50, and mAP50-90 were 0.900, 0.890, 0.935, 0.872, respectively. The results of this research demonstrate the potential of deep learning-based approaches for cervical spine fracture detection. By automating the detection process, these algorithms can assist radiologists and healthcare professionals in making accurate and timely diagnoses, leading to improved patient outcomes.",Deep learning;Feature extraction;Computed tomography;Convolutional neural networks;Biomedical imaging;YOLO;Bones;Spine;Computer vision;Detection algorithms;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522658,IEEE Journals,,,,,,
GC2: Generalizable Continual Classification of Medical Images,N. Bayasi; G. Hamarneh; R. Garbi,IEEE Transactions on Medical Imaging,,2024,"Deep learning models have achieved remarkable success in medical image classification. These models are typically trained once on the available annotated images and thus lack the ability of continually learning new tasks (i.e., new classes or data distributions) due to the problem of catastrophic forgetting. Recently, there has been more interest in designing continual learning methods to learn different tasks presented sequentially over time while preserving previously acquired knowledge. However, these methods focus mainly on preventing catastrophic forgetting and are tested under a closed-world assumption; i.e., assuming the test data is drawn from the same distribution as the training data. In this work, we advance the state-of-the-art in continual learning by proposing GC2 for medical image classification, which learns a sequence of tasks while simultaneously enhancing its out-of-distribution robustness. To alleviate forgetting, GC2 employs a gradual culpability-based network pruning to identify an optimal subnetwork for each task. To improve generalization, GC2 incorporates adversarial image augmentation and knowledge distillation approaches for learning generalized and robust representations for each subnetwork. Our extensive experiments on multiple benchmarks in a task-agnostic inference demonstrate that GC2 significantly outperforms baselines and other continual learning methods in reducing forgetting and enhancing generalization. Our code is publicly available at the following link: https://github.com/ nourhanb/TMI2024-GC2.",Task analysis;Biomedical imaging;Training;Data models;Knowledge engineering;Image classification;Adaptation models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522818,IEEE Early Access Articles,,,,,,
DMA-HPCNet: Dual Multi-Level Attention Hybrid Pyramid Convolution Neural Network for Alzheimerâ€™s Disease Classification,S. Mu; S. Shan; L. Li; S. Jing; R. Li; C. Zheng; X. Cui,IEEE Transactions on Neural Systems and Rehabilitation Engineering,########,2024,"Computer-aided diagnosis (CAD) plays a crucial role in the clinical application of Alzheimerâ€™s disease (AD). In particular, convolutional neural network (CNN)-based methods are highly sensitive to subtle changes caused by brain atrophy in medical images (e.g., magnetic resonance imaging, MRI). Due to computational resource constraints, most CAD methods focus on quantitative features in specific regions, neglecting the holistic nature of the images, which poses a challenge for a comprehensive understanding of pathological changes in AD. To address this issue, we propose a lightweight dual multi-level hybrid pyramid convolutional neural network (DMA-HPCNet) to aid clinical diagnosis of AD. Specifically, we introduced ResNet as the backbone network and modularly extended the hybrid pyramid convolution (HPC) block and the dual multi-level attention (DMA) module. Among them, the HPC block is designed to enhance the acquisition of information at different scales, and the DMA module is proposed to sequentially extract different local and global representations from the channel and spatial domains. Our proposed DMA-HPCNet method was evaluated on baseline MRI slices of 443 subjects from the ADNI dataset. Experimental results show that our proposed DMA-HPCNet model performs efficiently in AD-related classification tasks with low computational cost.",Feature extraction;Convolution;Magnetic resonance imaging;Brain modeling;Computational modeling;Atrophy;Alzheimer's disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10526304,IEEE Journals,,,,,,
Deep Learning Technique for Detecting and Analysing Ischemic Stroke using MRI Images,N. Ayesha; H. S. Sheshadri; V. Shridhar,2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT),########,2024,"The quantitative examination of images taken from cerebrum MRI assumes a significant part in the process of diagnosis and medication for the strokes. The deep learning and convolutional neural networks with learning capacity supply a powerful mechanism for detection of lesions. To concentrate on the properties of stroke injuries and complete detection activities automatically, we have gathered from various medical means, the data of MRI images of brain, which are belong to different patients affected by ischemic strokes. The classifications of deep learning-based networks used for detecting the objects such as SSD, RCNN-ResNet101, RCNN-VGG16 and YOLOV3 are applied to carry out programmed detection of lesions by achieving the best accuracy compared to existing schemes on Diffused Weight, Flair and T1 modalities of MRI datasets. The technique is developed to extract deep features during encoding stage and features are minimized using layers connected fully. And the significant features which are handcrafted including LBP and GLCM are joined along with the deep features. In order to increase the dimension of feature vector to maximum, the concatenation of feature is implemented by combining deep features and handcrafted features. And this vector is then used to train and test performance of classifiers. In order to classify various modalities of brain images in to normal or stroked, the binary classification is applied in proposed work. In the beginning, SoftMax is used as default for performing the classification. In continuation, the classifiers including SVM, KNN, DT, and RF are utilized. The KNN records accuracy of 97.60% and SVM records accuracy of 98.60%. Depending on selected classifier, performance of the classifiers is evaluated individually and the best outcome is taken in to consideration for confirming the performance of the technique.",Support vector machines;Deep learning;YOLO;Magnetic resonance imaging;Stroke (medical condition);Feature extraction;Vectors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516241,IEEE Conferences,,,,,,
Liver Tumor Segmentation and Classification Using Deep Learning Methods,S. Narasimhulu; C. D. V. S. Rao,2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT),########,2024,"One of the most common causes of death due to cancer is liver cancer. After several types of research, automatically segmenting and classifying liver tumors remains a challenging task due to the deformable shape in CT, MERI, and ultrasound images and the low tissue contrast between surrounding organs. A liver tumor must be detected early to plan a treatment strategy, confirm the diagnosis with accuracy, and acquire a thorough knowledge of the tumor to determine its seriousness. In recent years, many researchers identified various methods for accurate prediction by utilizing segmentation and classification approaches. This research represents the methodologies like 3Dimensitional Deep Convolutional Neural Network (3D DCNN), Adding Inception Module-Unet (AIM-Unet), Simple Linear Iterative Clustering-based Deep Graph Network (SLIC-DGN), OPTimized Guided Contrast Enhancement (OPTGCE), Gradational modular network (GraMNet), Hyper-parameter tuned Improved Deep Neural Network (HI-DNN), Hybrid ResUNet, Unified learning multi-task model network (ULM-net) are utilized for liver tumor disease segmentation and classification analysis. This survey investigates the benefits, drawbacks, and relations between segmentation and classification-based prediction techniques.",Surveys;Solid modeling;Ultrasonic imaging;Three-dimensional displays;Shape;Liver;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10515854,IEEE Conferences,,,,,,
Revolutionizing Blood Cell Analysis: The Emergence of EfficientNetB7-Enhanced Deep Learning Models,S. Pothineni,2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT),########,2024,"The rapid evolution of machine learning (ML) in medical diagnostics has prompted a surge in the development of models capable of interpreting complex biological data with unprecedented precision. In the quest to leverage computational intelligence for enhancing diagnostic accuracy, this study explores the application of deep learning (DL) in the nuanced field of hematology. This article describes a better Convolutional Neural Network (CNN) model that is based on the EfficientNetB7 architecture and has been carefully tuned to sort blood cells into different groups. Our model addresses common challenges such as data scarcity, class imbalance, and the need for computational efficiency. We adopt an innovative fine-tuning strategy that adjusts the model parameters to the intricacies of the blood cell images while simultaneously employing weighted loss functions to tackle class imbalance effectively. Through extensive experimentation and evaluation, the proposed model achieves a remarkable 99% accuracy on the test set, outperforming existing models and setting a new standard in medical image analysis. The study's results indicate that our model can significantly enhance the accuracy and speed of blood cell classification, offering substantial potential for clinical application. Future work will look into the model's integration with live diagnostic systems and expansion to multi-modal medical data analysis.",Deep learning;Analytical models;Image analysis;Computational modeling;Cells (biology);Data models;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10515544,IEEE Conferences,,,,,,
Brain Tumor Detection and Classification using Deep Learning Techniques,A. Arora; A. Kumar; N. Singh; B. D. Parameshachari,2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT),########,2024,"Background: Brain tumor localization and segmentation from magnetic resonance imaging (MRI) are challenging yet crucial tasks in medical analysis. Recent approaches have utilized multiple MRI modalities, such as T1, T1c, T2, and FLAIR, to capture unique tumor characteristics. Despite promising results on datasets like BRATS 2018, many existing methods suffer from complexity and overfitting issues, requiring extensive training and testing time. To address these challenges, we propose a two-step approach for flexible and efficient brain tumor segmentation. Firstly, we introduce a preprocessing method that focuses on a small region of interest within each image slice, reducing computation time and mitigating overfitting. Secondly, we present a Cascade Convolutional Neural Network (C-CNN) designed to extract both local and global features through separate routes. Additionally, we introduce a novel Distance-Wise Attention (DWA) mechanism to enhance segmentation accuracy by considering the spatial relationship between the tumor center and surrounding brain tissue. Our proposed method achieves competitive results on the BRATS 2018 dataset, with mean whole tumor, enhancing tumor, and tumor core dice scores of 0.9203, 0.9113, and 0.8726, respectively. Quantitative and qualitative assessments further validate the effectiveness of the proposed model. The proposed approach offers a flexible and effective solution for brain tumor segmentation, demonstrating improvements over state-of-the-art models. By incorporating preprocessing techniques, a specialized network architecture, and the DWA mechanism, our method addresses key challenges in MRI-based tumor analysis, paving the way for more accurate and efficient medical image processing.",Deep learning;Training;Image segmentation;Magnetic resonance imaging;Computational modeling;Feature extraction;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10515320,IEEE Conferences,,,,,,
A Hybrid Deep Learning Approach for Early Detection and Classification of Lung Cancer Using the Pelican Optimization Algorithm,S. Kusuma; S. G. Krishnan; K. Samreen; M. V. Ramana; G. S. Prasad,2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT),########,2024,"Lung cancer is a global health challenge that requires advanced diagnostic tools for early detection and classification. This study proposes a hybrid deep learning model combining Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) for accurate and timely identification of lung cancer in medical imaging data The model uses Convolutional Neural Networks (CNN) for spatial features extraction and Recurrent Neural Networks (RNN) for temporal understanding. It achieves an accuracy of 97.3%, demonstrating its efficacy in identifying subtle lung cancer patterns. The Pelican Optimization Algorithm is employed to fine-tune the parameters of the hybrid model, enhancing its overall performance. The comprehensive experimental evaluations show that the hybrid CNN-RNN model, enriched by the Pelican Optimization Algorithm, outperforms existing approaches in early detection and classification of lung cancer. The combined strengths of spatial and temporal feature extraction, coupled with the optimization capabilities of the Pelican algorithm, make the model a promising tool for clinicians and healthcare professionals in the fight against lung cancer.",Deep learning;Adaptation models;Recurrent neural networks;Lung cancer;Lung;Feature extraction;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10515355,IEEE Conferences,,,,,,
Kidney Tumor Detection Using Computed Tomography Scan Images Through CNN,P. T. Akkasaligar; S. Pattar; A. Uppin; K. Kori; A. Kulakarni,2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT),########,2024,"The research in the current decade addresses a critical concern in the field of medical imaging, specifically focusing on the identification of kidney tumors within Computed Tomography (CT) scan images. The complexity of this task is highlighted by subtle variations in tumor appearance and the intricate nature of the dataset. To tackle these challenges, a robust solution employing Convolutional Neural Networks (CNN) is proposed. The study emphasizes the immense potential of DL in the realm of medical image classification, providing a powerful tool for enhancing diagnostic accuracy in the detection of kidney tumors. The CNN achieves a test accuracy of 94.99% and the pretrained ResNet50 model showcases even higher accuracy of 96.56%. The integration of these models into healthcare systems holds promise for enhancing diagnostic accuracy and reliability. The demonstrated success of the proposed approach emphasizes its practical applicability in clinical settings. The efficacy of kidney tumor detection methodology is offered in real-world healthcare scenarios.",Training;Computed tomography;Computational modeling;Medical services;Convolutional neural networks;Kidney;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516254,IEEE Conferences,,,,,,
A Multilabel Dermatologic Classification and Interpretability Study for Small Samples and Extremely Unbalanced Classes,J. Tang; L. Jiang; F. Zheng; X. Zhang; H. Guo; B. Chen,2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT),########,2024,"Early diagnosis of melanoma is important for patient care, but dermatologists are struggling to keep up with the increasing demand for skin disease care. Computer-aided diagnostic techniques can be used to assist physicians in diagnosis using non-invasive dermoscopy. In this thesis, deep learning techniques are utilized to improve the speed of melanoma diagnosis, alleviate the low accuracy of dermatology datasets due to class imbalance, and combine with a 7-point checklist for melanoma to propose an end-to-end, multi-task, interpretable deep learning diagnostic model. The model consists of three parts: class balancing module, course learning fusion module, and diagnostic decoder, which makes scores for the 7 features of melanoma, and combines the scores and image features to make the final diagnosis, with an average accuracy as high as 74.43%. The experimental results show that the model effectively improves the interpretability of deep learning while ensuring accuracy, and analyzes its advantages in practical applications.",Deep learning;Computational modeling;Melanoma;Medical services;Feature extraction;Multitasking;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10515674,IEEE Conferences,,,,,,
GAN Inversion for Data Augmentation to Improve Colonoscopy Lesion Classification,M. V. Golhar; T. L. Bobrow; S. Ngamruengphong; N. J. Durr,IEEE Journal of Biomedical and Health Informatics,,2024,"A major challenge in applying deep learning to medical imaging is the paucity of annotated data. This study explores the use of synthetic images for data augmentation to address the challenge of limited annotated data in colonoscopy lesion classification. We demonstrate that synthetic colonoscopy images generated by Generative Adversarial Network (GAN) inversion can be used as training data to improve polyp classification performance by deep learning models. We invert pairs of images with the same label to a semantically rich and disentangled latent space and manipulate latent representations to produce new synthetic images. These synthetic images maintain the same label as the input pairs. We perform image modality translation (style transfer) between white light and narrow-band imaging (NBI). We also generate realistic synthetic lesion images by interpolating between original training images to increase the variety of lesion shapes in the training dataset. Our experiments show that GAN inversion can produce multiple colonoscopy data augmentations that improve the downstream polyp classification performance by 2.7% in F1-score and 4.9% in sensitivity over other methods, including state-of-the-art data augmentation. Testing on unseen out-of-domain data also showcased an improvement of 2.9% in F1-score and 2.7% in sensitivity. This approach outperforms other colonoscopy data augmentation techniques and does not require re-training multiple generative models. It also effectively uses information from diverse public datasets, even those not specifically designed for the targeted downstream task, resulting in strong domain generalizability. Project code and model: https://github.com/DurrLab/GAN-Inversion.",Colonoscopy;Codes;Generative adversarial networks;Training;Lesions;Imaging;Data augmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10521726,IEEE Early Access Articles,,,,,,
AI-Powered Alzheimer's Diagnosis from Brain MRI Images Using CNNs,P. Shourie; V. Anand; D. Upadhyay; V. Singh; S. Singh,2024 3rd International Conference for Innovation in Technology (INOCON),########,2024,"Alzheimer's disease is a slowly progressing neurological condition that predominantly impacts cognitive skills, it is essential to diagnose the disease early and accurately in order to provide appropriate treatment and patient care. In the realm of medical image analysis, recent developments in deep learning techniques, in particular Convolutional Neural Networks (CNNs), have demonstrated a great deal of promise. This research outlines an innovative strategy for the early diagnosis of Alzheimer's disease that makes use of CNNs. This research makes use of a database of brain magnetic resonance imaging (MRI) scans. This database contains images from people who have been diagnosed with Alzheimer's disease as well as healthy individuals. This research demonstrates that CNNs have the potential to be an effective diagnostic tool for Alzheimer's disease when used in conjunction with MRI images. The application of techniques from deep learning in medical imaging holds the potential to improve the accuracy and effectiveness of Alzheimer's disease diagnosis, which would ultimately be to the advantage of persons who are at risk and would make it easier to begin early intervention and treatment.",Deep learning;Technological innovation;Image analysis;Databases;Magnetic resonance imaging;Data models;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10511648,IEEE Conferences,,,,,,
Brain Tumour Disease Detection in MRI Images using Xception Model,G. Kaur; N. Sharma; D. Upadhyay; M. Singh; R. Gupta,2024 3rd International Conference for Innovation in Technology (INOCON),########,2024,"The main aim of this study is to examine the utilization of the Xception transfer learning model for the crucial task of detecting brain tumor diseases in medical images, with a specific focus on improving mortality rates. This study highlights the urgent requirement for automated and precise techniques for detecting brain tumors in medical imaging. The present study proposed a novel Xception model architecture to classify brain tumor illness images into two categories. The study included a dataset consisting of 253 photos. The model completed training for a total of 40 epochs, employing a batch size of 8 and utilizing the default learning rate. The model training procedure involves optimizing it using a combination of Adam and Adamax optimizers. The findings illustrate the model's competence, achieving a noteworthy level of accuracy at 96%. The results of this discoveries are of considerable importance, as they demonstrate the potential of the Xception model in furthering the field of medical diagnostics. The attainment of a high level of accuracy implies enhanced capabilities in the early diagnosis and prompt intervention of brain tumor disorders, potentially leading to significant consequences for patient outcomes and healthcare practices.",Training;Technological innovation;Magnetic resonance imaging;Transfer learning;Brain modeling;Medical diagnosis;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10511504,IEEE Conferences,,,,,,
Automatic White Blood Cells Classification using Optimized Convulutional Neural Network,P. P. Mahale; R. K. Deshmukh; P. Pise; M. S. Gardi,2024 3rd International Conference for Innovation in Technology (INOCON),########,2024,"Methods based on deep learning are widely used nowadays for data analysis. These procedures involve low time investment and simple tools. In this research, WBC classification was suggested using module weighted optimized CNN. Two transfer learning modules and deformable convolutional (DC) layers characterize our technique, which we use to improve resilience. For WBC classification on low-resolution and noisy data sets, the suggested Optimized CNN (OCNN) demonstrated the best results because to its precise feature extraction and improved network weights. It has the potential to be employed in clinical settings as an alternate technique. White blood cells (WBCs) are immune system cells that may be found throughout the body in the blood, lymph, and other tissues. There is a current upward trend in the prevalence of WBC-related blood disorders such leukemia and lymphoma. The process of analyzing leukocytes in microscopic blood images is the major concern of this research. In this paper, we work on WBCs segmentation from microscopic blood images.",White blood cells;Technological innovation;Microscopy;Transfer learning;Neural networks;Market research;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10511689,IEEE Conferences,,,,,,
Natural Language Processing for Sentiment Analysis with Deep Learning,S. Sruthi; V. Trinath; V. Jayanth; V. P. Balaji; T. Singh; A. Mandal,2024 3rd International Conference for Innovation in Technology (INOCON),########,2024,"In order to identify and analyse the thoughts, feelings, and sentiments indicated in textual data, the field of sentiment analysis is crucial. Deep learning models have become effective tools for sentiment analysis thanks to their capacity to recognise semantic links and understand complicated patterns. In-depth analysis of the use of deep learning algorithms for sentiment analysis tasks is presented in this research report. We investigate different deep learning architectures, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformer-based models, to assess how well they are able to capture nuances in sentiment and contextual information. On benchmark datasets, numerous trials are run, and the models are assessed using important performance indicators as accuracy, precision, recall, and F1 score. applications that are in this field. we also investigate methods like pre- trained word embeddings and transfer learning. The study also investigates the effects of regularisation and hyperparameter tuning methods on the accuracy and applicability of models. In addition, we discuss issues with sentiment analysis's class imbalance, model interpretability, and data pre-processing. The findings show that deep learning models regularly perform better than conventional machine learning techniques, attaining cutting-edge performance in sentiment classification tasks. The results of this study open the door for more advanced applications in this field by giving important new insights into the capabilities and constraints of deep learning models for sentiment analysis.",Deep learning;Sentiment analysis;Analytical models;Technological innovation;Recurrent neural networks;Transfer learning;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10511769,IEEE Conferences,,,,,,
Deep Augmented Metric Learning Network for Prostate Cancer Classification in Ultrasound Images,X. Lu; Y. Guo; S. Zhang; Y. yuan; C. -C. Wang; Z. Shen; S. Liu,IEEE Journal of Biomedical and Health Informatics,,2024,"Prostate cancer screening often relies on cost-intensive MRIs and invasive needle biopsies. Transrectal ultrasound imaging, as a more affordable and non-invasive alternative, faces the challenge of high inter-class similarity and intra-class variability between benign and malignant prostate cancers. This complexity requires more stringent differentiation of subtle features for accurate auxiliary diagnosis. In response, we introduce the novel Deep Augmented Metric Learning (DAML) network, specifically tailored for ultrasound-based prostate cancer classification. The DAML network represents a significant innovation in the metric learning space, introducing the Semantic Differences Mining Strategy (SDMS) to effectively discern and represent subtle differences in prostate ultrasound images, thereby enhancing tumor classification accuracy. Additionally, the DAML network strategically addresses class variability and limited sample sizes by combining the Linear Interpolation Augmentation Strategy (LIAS) and Permutation-Aided Reconstruction Loss (PARL). This approach enriches feature representation and introduces variability with straightforward structures, mirroring the efficacy of advanced sample generation techniques. We carried out comprehensive empirical assessments of the DAML model by testing its key components against a range of models, ensuring its effectiveness. Our results demonstrate the enhanced performance of the DAML model, achieving classification accuracies of 0.857 and 0.888 for benign and malignant cancers, respectively, underscoring its effectiveness in prostate cancer classification via medical imaging.",Prostate cancer;Microstrip;Measurement;Ultrasonic imaging;Magnetic resonance imaging;Deep learning;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10518097,IEEE Early Access Articles,,,,,,
Expert Knowledge Driven Human-AI Collaboration for Medical Imaging: a Study on Epileptic Seizure Onset Zone Identification,P. Kamboj; A. Banerjee; S. K. S. Gupta,IEEE Transactions on Artificial Intelligence,,2024,"Supervised artificial intelligence (AI) techniques are good at learning class specific characteristic properties despite variance across samples. However, for rare class classification, challenges arise due to class imbalance. To the contrary, knowledge based techniques encode class specific information without class labels but face difficulties in parsing knowledge which is often vague, uncertain, and result in high intra-class variability. This manuscript presents a human-AI collaboration methodology to integrate AI with expert knowledge for rare class classification, mitigating class imbalance and intra-class variability effects. We present a formal framework for expert knowledge representation using logical connectives of atomic propositions, a rule refinement strategy to derive class specific machine checkable formulas, and a rule implementation strategy that extracts explainable partitions of rare class expert rules for its recognition. A knowledge-AI integration strategy is presented that uses entropy imbalance gain and Gini index to quantify class imbalance and intra-class variability, and orchestrates supervised AI and expert knowledge machines to effectively identify rare class through human-AI collaboration with reduced human effort. We apply proposed integration framework to develop DeepXSOZ that identifies seizure onset zones (SOZ) in focal epilepsy patients from resting state functional magnetic resonance imaging. DeepXSOZâ€™s performance is validated on multi-center datasets against anatomical MRI based manual SOZ identification, and Engel outcomes after surgical SOZ alteration. This human-AI collaboration demonstrates increased F1 score compared to state-of-the-art â€œAI-onlyâ€ techniques, minimal data leakage effect with statistically similar performance across multicenter datasets without fine tuning, consistent results across age and gender, and reduced manual effort.",Location awareness;Artificial intelligence;Noise;Manuals;Collaboration;Surgery;Sorting,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10518119,IEEE Early Access Articles,,,,,,
"A Comprehensive Review on COVID-19 Detection Based on Cough Sounds, Symptoms, CXR, and CT Images",C. Mahanty; S. Gopal Krishna Patro; S. Rathor; V. Rachapudi; J. Mohanty; K. Muzammil; S. Islam; W. Ahmad Khan,IEEE Access,########,2024,"The worldwide spread of the coronavirus illness has led to the requirement of creating machine-based technologies to identify the diseases. The worldwide pandemic caused by new coronaviruses has resulted in a significant loss of life and necessitates the development of several affordable diagnostic methods to detect the presence of COVID-19 infection. Thankfully, the current era of advanced technology, including transfer learning (TL) approaches, has improved several areas of human health and enabled the identification of chronic and communicable diseases. There is a need for thorough investigation in order to combat the transmission of this alarming virus via the use of evidence-based intelligence models and implementation of preventive measures. The present systematic review focuses on the examination of TL and fuzzy ensemble techniques that have been described in the literature pertaining to strategies for detecting COVID-19. Multiple studies have used cough sounds, CT scans, X-ray images, and symptoms information to identify cases of COVID-19. The application of DL/ML, TL, fuzzy ensemble, and fuzzy inference approaches for COVID-19 identification is discussed in this paper.",COVID-19;Computed tomography;Convolutional neural networks;Residual neural networks;Noise measurement;X-ray imaging;Mel frequency cepstral coefficient;Acoustics;Ensemble learning;Predictive models;Detection algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10517993,IEEE Journals,,,,,,
CNN - Based Medical Image Classification Models for the Identification of Pneumonia and Malaria,N. Behera; S. Das; A. P. Singh; A. K. Swain; M. Rout,"2024 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)",30-Apr-24,2024,"Across the globe, deep learning methodologies are rapidly transforming the medical field. Among the rapidly expanding domains, medical image categorization stands out as a crucial area for developing effective intelligent systems. Consequently, our research employs various CNN variations to establish a robust and reliable model for medical image categorization. Within this study, we utilize two distinct medical imaging datasets: one comprising malaria cell images and the other featuring chest X-ray images for pneumonia diagnosis. To enhance the model's accuracy, we fine-tune its performance by adjusting parameters such as layer count and activation functions. This approach empowers researchers to identify optimal CNN parameters for image classification and observe how model behavior evolves with changing image types. The presented models undergo validation using precision metrics, including F -score, specificity, and accuracy. Notably, in the malaria and pneumonia datasets, our model achieves accuracy rates of 96 % and 95%, respectively.",Pneumonia;Malaria;Gray-scale;Image categorization;Convolutional neural networks;Reliability;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10507970,IEEE Conferences,,,,,,
A Dual-branch Model for Early Detection of Alzheimerâ€™s Disease Using Resting-State fMRI,Y. Wang; W. Li,"2024 IEEE 7th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)",25-Apr-24,2024,"Alzheimerâ€™s disease (AD) is the most prevalent form of dementia, and early diagnosis is crucial for delaying and treating AD. Resting-state functional magnetic resonance imaging (rs-fMRI), a widely used medical imaging technique, offers rich temporal and spatial data, which has led researchers to explore various feature extraction methods based on rs-fMRI images for AD identification. However, the related work still suffers from insufficient utilization of temporal and spatial information which leads to unsatisfactory early diagnosis. In this study, we propose a dual-branch fusion model to extract spatial-temporal features from rs-fMRI images. Our proposed model can extract temporal features at different levels. We developed a Class Activation Sequence (CAS) branch, which is a structure that emphasizes the function of each temporal node throughout the whole time series. Additionally, we created a time-domain local branch for local feature extraction. Further, we designed a fusion module for the model to describe temporal contextual relationships and fuse features at various levels. We tested the performance of the model on the ADNI dataset, and the experimental results show that compared with other algorithms, the dual-branch fusion model achieves higher classification accuracy on several classification tasks including early diagnosis, which proves the advantage of the dual-branch fusion model in temporal and spatial feature extraction for rs-fMRI images, and our work also provides a foundation for the temporal domain characterization of rs-fMRI images.",Fuses;Time series analysis;Functional magnetic resonance imaging;Feature extraction;Spatial databases;Alzheimer's disease;Time-domain analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503940,IEEE Conferences,,,,,,
U-CCNet: Brain Tumor MRI Image Segmentation Model with Broader Global Context Semantic Information Abstraction,X. Han; J. Liu; J. Zhao,"2024 IEEE 7th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)",25-Apr-24,2024,"Since brain tumor detection can help in the diagnosis of diseases such as Alzheimerâ€™s disease, segmentation and classification of brain tumors are crucial for diagnosis and treatment. Although radiologists can find the tumor area using magnetic resonance imaging (MRI), it is more time-consuming. The high-precision segmentation results provided by deep learning models can assist radiologists in better-identifying brain tumors. In this paper, we propose a U-Net-based approach to detect brain tumors in MRI. In this paper, the cross-attention mechanism is added to the U-Net benchmark architecture and trained and tested on the LGG Segmentation dataset (TCGA-LGG) dataset. The test results show that the proposed model performs well in brain tumor MRI segmentation tasks. Finally, the proposed U-CCNet model is compared with other studies. Experimental results show that this study is superior to other models in the Dice similarity coefficient and IoU index. The proposed method is tested on the LGG brain MRI segmentation dataset. There is a significant improvement in segmentation accuracy in terms of dice score and mIoU, reducing the use of computationally expensive 3D networks.",Image segmentation;Three-dimensional displays;Magnetic resonance imaging;Computational modeling;Semantics;Computer architecture;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503985,IEEE Conferences,,,,,,
Contrastive Graph Pooling for Explainable Classification of Brain Networks,J. Xu; Q. Bian; X. Li; A. Zhang; Y. Ke; M. Qiao; W. Zhang; W. K. J. Sim; B. GulyÃ¡s,IEEE Transactions on Medical Imaging,,2024,"Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinsonâ€™s, Alzheimerâ€™s, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literature, and disclose direct and interesting insights. Our contributions underscore the potential of ContrastPool for advancing the understanding of brain networks and neurodegenerative conditions. The source code is available at https://github.com/AngusMonroe/ContrastPool.",Functional magnetic resonance imaging;Feature extraction;Task analysis;Data mining;Alzheimer's disease;Message passing;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10508252,IEEE Early Access Articles,,,,,,
Empowering Healthcare through Privacy-Preserving MRI Analysis,A. Amin; K. Hasan; S. Zein-Sabatto; D. Chimba; L. Hong; I. Ahmed; T. Islam,SoutheastCon 2024,24-Apr-24,2024,"In the healthcare domain, Magnetic Resonance Imaging (MRI) assumes a pivotal role, as it employs Artificial Intelligence (AI) and Machine Learning (ML) methodologies to extract invaluable insights from imaging data. Nonetheless, the imperative need for patient privacy poses significant challenges when collecting data from diverse healthcare sources. Conse-quently, the Deep Learning (DL) communities occasionally face difficulties detecting rare features. In this research endeavor, we introduce the Ensemble-Based Federated Learning (EBFL) Framework, an innovative solution tailored to address this challenge. The EBFL framework deviates from the conventional approach by emphasizing model features over sharing sensitive patient data. This unique methodology fosters a collaborative and privacy-conscious environment for healthcare institutions, empowering them to harness the capabilities of a centralized server for model refinement while upholding the utmost data privacy standards. Conversely, a robust ensemble architecture boasts potent feature extraction capabilities, distinguishing itself from a single DL model. This quality makes it remarkably dependable for MRI analysis. By harnessing our ground-breaking EBFL methodology, we have achieved remarkable precision in the classification of brain tumors, including glioma, meningioma, pituitary, and non-tumor instances, attaining a precision rate of 94 % for the Global model and an impressive 96% for the Ensemble model. Our models underwent rigorous evaluation using conventional performance metrics such as Accuracy, Precision, Recall, and Fl Score. Integrating DL within the Federated Learning (FL) framework has yielded a methodology that offers precise and dependable diagnostics for detecting brain tumors.",Data privacy;Federated learning;Magnetic resonance imaging;Computational modeling;System performance;Medical services;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500144,IEEE Conferences,,,,,,
The Deep Learning Model For The Examination Of Brain Tumor,P. K. Sahu; K. Jain,2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),24-Apr-24,2024,"Radiologists can locate the tumor location using the experimental medical imaging technique known as magnetic reasoning imaging (MRI). However, manually testing the MRI pictures is time-consuming and requires experience. The development of Computer-assisted Diagnosis (CAD), and deep learning in particular, now enable radiologists to diagnose brain tumors more accurately. Deep learning techniques can be developed to produce accurate classification results without requiring manual feature extraction. The research article suggests the deep learning model for classifying multi-class (pituitary, glioma, meningioma, and no-tumor) and binary-class (tumor, no-tumor) brain tumor datasets. Two datasets were used to test the proposed model: Dataset-1 contains 3264 photos, while Dataset-2 contains 3000 images. The model consists of three convolution blocks whose kernel size is 10, 8, and 6, respectively. The model has three max-pooling blocks. The model also contains one flattened block and two fully connected blocks. Finally, the model included softmax and sigmoid activation functions for the multi-class and binary-class brain tumor image classification. The created deep learning model achieved 94.22% classification accuracy with multi-class brain tumor dataset-1. The same model gained 97.78% classification accuracy for binary-class brain tumor dataset-2. The created deep learning model performs similarly to VGG19 and works well with a smaller dataset than CNN.",Deep learning;Solid modeling;Technological innovation;Computational modeling;Magnetic resonance imaging;Manuals;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10502750,IEEE Conferences,,,,,,
CogniVisio: Fusion Image Classifier,Z. Mundargi; A. Shinde; S. Bahirat; S. Das; O. Deokar; D. Shetiya,2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),24-Apr-24,2024,"CogniVisio is an innovative project that leverages state-of-the-art technologies such as Google Colab, Python, TensorFlow, Convolutional Neural Network (CNN) Inception for developing a robust image classification system. The project addresses the challenge of enhancing image classification accuracy by employing a fusion approach that combines the strengths of image generation and the powerful feature extraction capabilities of CNN Inception. With a dataset comprising 8732 images, our methodology achieves an impressive accuracy of 86.86%, showcasing the effectiveness of the proposed fusion image classifier. The utilization of cloud-based computing through Google Colab provides scalability and accessibility, making CogniVisio a versatile solution for image classification task.",Technological innovation;Scalability;Transfer learning;Robustness;Internet;Convolutional neural networks;Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10502548,IEEE Conferences,,,,,,
Brain MRI Image Classification using Transfer-Learning based Ensemble Approach,K. M. Sadman Sakib; M. Irtiza Masroor; M. A. Al Arman; D. Sarker; M. Hasan,2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),24-Apr-24,2024,"Brain tumor categorization is one of the most vital tasks in the world of medical imaging. There are many resemblances between the images of a patient with normal tissues and that of a patient with a tumor, which makes matters difficult and prone to error resulting in detrimental effects. Furthermore, a brain tumor's classification normally involves a lot of data that takes an extended period of time, which hinders down the treatment for doctors. For such reasons, computer-assisted techniques offer a quick and effective solution. This paper aims to detect brain tumors on a small dataset collected from an online source using few pre-trained deep learning algorithms which is known as transfer learning to achieve a greater result and in addition prevent overfitting. Pre-trained models for classifying brain tumors included InceptionV3, VGG16, DenseNet121, ResNet50, and MobileNetV3 Small. These models were assessed based on a range of metrics, such as F1-score, accuracy, precision, and recall. MobileNetV3 Small achieved the greatest accuracy, precision and F1-score of 0.9167, 0.9400 and 0.9307 respectively among all the models that were implemented, whereas RestNet50 achieved the highest recall of 0.9804. In order to attain an even better result, an ensemble, or collection, of these models, was employed in the identification of brain tumors. This led to improvements in nearly every evaluation metric, including accuracy, precision and F1-score, which resulted in values of 0.9405, 0.9419, and 0.9412, respectively.",Measurement;Deep learning;Transfer learning;Brain modeling;Reliability;Convolutional neural networks;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503556,IEEE Conferences,,,,,,
Computer-Aided Diagnostic System for Brain Tumor Classification using Explainable AI,S. T. Padmapriya; M. S. G. Devi,2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),24-Apr-24,2024,"The field of computer-aided diagnosis (CAD) of brain tumors has been transformed by developments in medical imaging and artificial intelligence. The accuracy and interpretability of brain tumor classification are improved in this research using Explainable AI (XAI) techniques. A timely and correct diagnosis of brain tumors is essential for the best possible care and treatment of the patient. In contrast, traditional machine learning models often lack transparency and interpretability, making it difficult for clinicians to rely on their judgment. This study uses a Grad-Cam algorithm to create an understandable and interpretable CAD system for classifying brain tumors. Our approach not only achieves high classification accuracy, but also provides physicians with insights into the decision-making process, improving their understanding and confidence in the systemâ€™s recommendations. We evaluate our model using a large and diverse dataset and compare it to modern deep learning models and traditional CAD systems. The results show that our CAD system with XAI extensions not only achieves improved accuracy but also provides useful insights into the decision-making process. Using this method, doctors may be able to diagnose patients more accurately.",Deep learning;Solid modeling;Technological innovation;Machine learning algorithms;Design automation;Explainable AI;Decision making,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10502596,IEEE Conferences,,,,,,
AI-Powered Dermatology: Achieving Dermatologist-Grade Skin Cancer Classification,P. Kaushik; Y. Chopra; A. Kajla; M. Poonia; A. Khan; D. Yadav,2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),24-Apr-24,2024,"In the realm of dermatology, the accurate diagnosis of skin cancer has long been a challenging endeavor. This paper introduces a cutting-edge solution for achieving dermatologist-grade skin cancer classification through the power of artificial intelligence (AI). Departing from traditional methods that necessitate laborious manual feature extraction and domain-specific preprocessing, our system adopts a deep neural network architecture, specifically Google Net Inception v3 CNN, fine-tuned using a vast and diverse clinical image dataset. Dataset comprises 135,550 images meticulously organized within a structured taxonomy encompassing 2,055 distinct disease categories. To unlock the full potential of fine-grained classification, An innovative algorithm is introduced to facilitate precise identification of various skin diseases. This research underscores the transformative potential of AI-powered dermatology in the realm of early skin cancer detection. By achieving dermatologist-level accuracy, this approach has the capacity to significantly impact public health outcomes, particularly in regions where skin cancer is a prevalent concern.",Deep learning;Visualization;Technological innovation;Dermatology;Artificial neural networks;Classification algorithms;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10502664,IEEE Conferences,,,,,,
Context Aware CNN approach to denoise Salt and Pepper Images,R. Maheshwari; K. Pathak; A. K. Kamal,2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),24-Apr-24,2024,"Salt and pepper noise is a type of impulse noise that affects digital images. It appears as bright or dark pixels that are randomly distributed throughout the image. Salt and pepper noise can be caused by a variety of factors, such as sensor defects, transmission errors, and software errors. Denoising salt and pepper noise is important because it can significantly improve the quality of an image, making it easier to interpret and process. It can also improve the performance of image processing tasks such as edge detection, segmentation, and classification. There are a number of methods for denoising salt and pepper noise, including median filtering, adaptive median filtering, morphological filtering, and deep learning-based methods. The following paper discusses some of the latest methods which are used to denoise SAP affected images and identifies some of the shortcomings of the current methods. This forms the basis of the research which has been subsequently carried out to design a new method of denoising SAP affected images using deep learning. This paper proposes a new model to denoise SAP affected images using custom layers from the Tensorflow library along with Convolutional Neural Networks, specifically the DnCNN model.",Training;Technological innovation;Filtering;Noise;Noise reduction;Software;Libraries,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10502512,IEEE Conferences,,,,,,
Automated Brain Tumor Diagnosis and Localization Using CNN and U-Net 3D: A Radiomics Approach for Enhanced Medical Imaging Analysis,B. S. Sharvani; G. JayaLakshmi; K. Bhargavi,2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),24-Apr-24,2024,"Brain tumors require precise analysis and delineation for treatment. This paper presents an automatic method of identifying brain tumors and segmentation utilizing the U-Net architecture and CNN. The method uses deep learning to identify probable tumor spots in brain MRI data. The CNN-based detection model locates aberrant tissue areas, and the following U-Net segmentation network refines the results by providing pixel-level tumor boundaries. The advantages of this approach include its capacity to handle complex tumor shapes, adjust to image fluctuations, and reduce manual labor. It assists healthcare providers in making correct and timely treatment decisions. The framework's usefulness is demonstrated by experimental findings on various datasets. It beats previous approaches by providing excellent accuracy in tumor detection and precise segmentation, making it a crucial medical tool.",Deep learning;Location awareness;Image segmentation;Three-dimensional displays;Magnetic resonance imaging;Medical services;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10502501,IEEE Conferences,,,,,,
Deep Learning Based Thoracic Disease Detection and Localization in Chest Radiographs,H. Pant; M. C. Lohani; A. K. Bhatt,2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),24-Apr-24,2024,"In the area of medical imaging analysis, deep learning has demonstrated significant promise, notably in the detection and treatment of thoracic disorders. With a focus on COVID-19, lung opacity, and viral pneumonia, this research study attempts to examine the working and use of deep learning approaches for the classification, segmentation, and localization of thoracic illnesses. In order to accurately classify, segment, and localize diseases, the study uses appropriate dataset of posteoranterior chest radiographs and provides a deep learning-based architecture that blends convolutional neural networks (CNNs) with U-Net. The findings show how deep learning may be used to help medical practitioners identify and analyze thoracic disorders. The results of classification and segmentation demonstrate an average validation accuracy of 98.5%.",Location awareness;Deep learning;Radiography;Image segmentation;Technological innovation;Pneumonia;Lung,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503014,IEEE Conferences,,,,,,
Comprehensive CNN Model for Brain Tumour Identification and Classification using MRI Images,C. R. Prasad; K. Srividya; K. Jahnavi; T. Srivarsha; S. Kollem; S. Yelabaka,"2024 IEEE International Conference for Women in Innovation, Technology & Entrepreneurship (ICWITE)",23-Apr-24,2024,"Brain tumours are critical malignancies that develop as a result of aberrant cell division. Typically, tumour classification involves a biopsy, which is conducted after the final brain operation. Technological advances have facilitated the utilization of medical imaging by physicians to diagnose a wide range of symptoms within the domain of medicine. In this project, we propose the Comprehensive CNN method for the detection and classification of brain tumours. For experimentation, we used the SARTAJ, Br35H, and Figshare datasets. This proposed model outperforms in terms of accuracy, recall, F1 score, and precision as compared to other traditional methods. This research contributes to the ongoing efforts to enhance the capabilities of medical imaging and paves the way for more accurate and efficient brain tumor analysis.",Training;Machine learning algorithms;Magnetic resonance imaging;Entrepreneurship;Medical services;Predictive models;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10502486,IEEE Conferences,,,,,,
Enhancing Brain Tumor Detection: A Comparative Study of Machine Learning Models,M. Chand; G. Mathur; S. K. Jain,"2024 IEEE International Conference for Women in Innovation, Technology & Entrepreneurship (ICWITE)",23-Apr-24,2024,"Modern methods for the early diagnosis and categorization of brain tumors have been developed as a result of the dogged quest for novel approaches in medical imaging and diagnostics. Among these methods, machine learning stands out as a game-changer that might completely alter our strategy for detecting brain tumors. In this work, we set out to investigate the capabilities of state-of-the-art machine learning models in detecting brain tumors. With an outstanding SVClassifier Score of 0.9570714421558011 and a Dice Score of 0.767267409441446, the SVC model stands out in our thorough study. With a Dice Score of 0.7800347169351646, CatBoost likewise demonstrates promise. Insights into the accuracy of brain tumor identification and the possibility of enhanced patient care are offered by the study, which validates these models on test data, reinforcing their potential to improve medical imaging and diagnostics. Using state-of-the-art machine learning techniques, this project aims to create and assess a system for detecting brain tumors. Here, we zero down on testing several models to see which ones do the best job of identifying tumor zones in brain images. Machine learning techniques such as RandomForest Classifier, MLPClassifier, C-Support Vector Classification (SVC), and the 2018-introduced CatBoost model are all part of the models being evaluated. We ran an examination on BRATS 2018 multi-modal challenge data to further confirm that these models worked. Using a more refined machine learning approach, this stage entailed testing and evaluating the system for segmenting and classifying brain tumors. The findings of this extensive investigation have important consequences for the development of better diagnostic imaging tools. They provide important information about how machine learning models may be used to identify and categorize brain tumors, which helps with the continuous attempts to improve patient treatment and results.",Training;Image segmentation;Static VAr compensators;Machine learning;Brain modeling;Data models;Vectors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10502714,IEEE Conferences,,,,,,
Perceptually Imperceptible Backdoor Attacks Using High-Frequency Information in Deep Learning Models,Q. Ma; J. Qin; Y. Cao; J. Ren,"2024 4th International Conference on Neural Networks, Information and Communication Engineering (NNICE)",22-Apr-24,2024,"Backdoor attacks, which involve injecting malicious triggers into training data, pose a significant threat to the security of deep learning models across various domains. This paper introduces a trigger injection method that capitalises on the model's high-frequency feature learning ability. The method uses imperceptible high-frequency features to conceal the backdoor during training and accumulates high-frequency components as triggers for a covert clean-label backdoor attack in the inference phase. This approach ensures high concealment while preserving image quality and perception. It highlights the broader significance of understanding human visual systems for enhancing the reliability of deep learning models in diverse applications.",Deep learning;Training;Analytical models;Adaptation models;Training data;Visual systems;Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498764,IEEE Conferences,,,,,,
Explainable Hybrid CNN Swin-Transformer Network for Tuberculosis Diagnosis in Chest X-Rays of Sri Lankan Patients,T. Jegatheeswaran; L. Kumaralingam; S. Selvarajah; N. Ratnarajah,2024 4th International Conference on Advanced Research in Computing (ICARC),22-Apr-24,2024,"Tuberculosis remains a formidable global health challenge, requiring advanced and interpretable methodologies. Conventional diagnostic approaches for tuberculosis often suffer from outdated techniques and unnecessary features, impairing their reliability, especially in developing countries like Sri Lanka. This study introduces an explainable Hybrid Convolutional Neural Network (CNN) Swin-Transformer (Swin-T) tailored for Tuberculosis detection in chest X-ray images by leveraging the power of pre-trained CNN and Swin-T.A dataset comprising 270 ethically sourced chest X-ray images, including 171 healthy subjects and 99 Pulmonary Tuberculosis subjects, was meticulously curated from Trincomalee General Hospital, Sri Lanka. The proposed network showcased exceptional performance, yielding a precision of 82.14%, a recall of 92.00%, a specificity of 82.76%, and an accuracy of 92.22%. Notably, employing the gradient-based class activation map (Grad-CAM) technique, the model elucidated Tuberculosis-indicative regions in the chest X-ray images, offering transparency in its diagnostic decisions. These findings underscore the potential of the explainable hybrid CNN Swin-T Network as a powerful and interpretable tool for early Tuberculosis diagnosis. By highlighting crucial regions indicative of the disease, this model aids clinicians in expedited and accurate diagnosis, contributing to improved disease management and better patient outcomes.",Location awareness;Ethics;Visualization;Tuberculosis;Neural networks;Lung;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499756,IEEE Conferences,,,,,,
Few-Shot Lung Cancer Classification Using Prototypical Networks,N. H. Amarasinghe; T. D. Ambegoda,2024 4th International Conference on Advanced Research in Computing (ICARC),22-Apr-24,2024,"Lung cancer, a leading cause of mortality globally, demands early and accurate detection to improve patient out-comes. Current diagnostic methods, primarily relying on CT scans, face challenges in lung cancer subtype identification, particularly due to the scarcity of extensive medical image datasets for each subtype. This study addresses this limitation by leveraging prototypical networks, an innovative few-shot learning approach, which excels in scenarios with limited data. The proposed method capitalizes on a small number of samples per category, integrating a pre-trained model for feature extraction from lung CT scans. We rigorously evaluated the model's performance, focusing on its accuracy relative to the sample size per category. Remarkably, the method achieved a 98% accuracy rate after 15 epochs, showcasing its efficacy. This research not only confirms the feasibility of using prototypical networks for lung cancer subtype classification but also opens new avenues for applying few-shot learning techniques in medical imaging. Our findings hold significant potential for enhancing lung cancer diagnostics, thereby contributing to improved patient care and survival rates.",Computed tomography;Lung cancer;Lung;Focusing;Feature extraction;Medical diagnostic imaging;Faces,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499732,IEEE Conferences,,,,,,
A Systematic Literature Review of XAI-based Approaches on Brain Disease Detection using Brain MRI Images,S. Madapatha; P. Fernando,2024 4th International Conference on Advanced Research in Computing (ICARC),22-Apr-24,2024,"Brain Magnetic Resonance Imaging (MRI) analysis is a widely used medical procedure for the early diagnosis of various brain diseases. Accurate pathology identification during the brain MRI analysis procedure is crucial as misdiagnoses or missed findings can greatly affect a patient's treatment and long-term prediction. With the recent advancement of Artificial Intelligence (AI) in the medical field, researchers have approached various techniques to detect brain diseases using AI. Although AI models exhibit high accuracy, they suffer from a lack of transparency and interpretability, paving the way for the development of eXplainable Artificial Intelligence (XAI) methods in brain disease diagnosis. Image segmentation, machine learning, deep learning and XAI are important for assisting the diagnostic procedure. In this paper, a comprehensive overview of various existing techniques in brain disease detection using MRI is presented, starting with image segmentation techniques, followed by classification techniques, and finally, XAI techniques. In conclusion, the paper identifies a critical need for further research on XAI integration to advance brain disease detection.",Image segmentation;Three-dimensional displays;Explainable AI;Reviews;Magnetic resonance imaging;Computer architecture;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499752,IEEE Conferences,,,,,,
A Content Based Recommender Algorithm for Clustering Brain MRI Scan Images leveraging Deep Neural Networks,R. R. Subramanian; M. Sravani; M. Babitha; M. Neeraja; M. R. Sri,2024 4th International Conference on Data Engineering and Communication Systems (ICDECS),22-Apr-24,2024,"The accurate interpretation of medical imaging, such as MRI and CT scans, is crucial for the difficult medical procedures of diagnosing and treating brain tumors. Due to the exponential growth of medical image collections, healthcare professionals have trouble choosing relevant reference cases throughout the diagnosis procedure. The Brain Tumor Image Recommendation System (BTIRS), which is described in this abstract, is a novel technique that attempts to provide tailored help to medical practitioners by proposing suitable images of brain tumors from enormous databases. BTIRS uses cutting-edge machine learning methods, semantic analysis, and user input to provide efficient image suggestion. This method automatically recommends a set of relevant brain tumor images based on the input scan, the patientâ€™s medical background, and the clinical setting",Machine learning algorithms;Databases;Magnetic resonance imaging;Semantics;Medical services;Machine learning;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503254,IEEE Conferences,,,,,,
IAOMDR: Improved Aquila Optimized Hybrid deep learning Model for Classification of Diabetic Retinopathy,B. Saju; S. Barwal; V. Asha; S. Kumar; N. Tressa; N. T,2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE),18-Apr-24,2024,"One of the scariest illnesses that causes irreversible blindness is Diabetic Retinopathy (DR). As a result, early exposure to Diabetic Retinopathy can help to preserve vision. The study proposes a hybrid model to classify Diabetic Retinopathy images. Initially, pre-processing of the photographs includes resizing them. Grey scale translation and noise removing using a combination of Weiner filter and Median filter. Further, images are enhanced using Gaussian Mixture based histogram equalization. Features of the images are extracted using Dense Convolutional neural network. An optimized ResNet101 model with improved Aquila optimization is used for classification. Performance of the models is as opposed to other studies in literature. The proposed model has acquired F1 score of 98.98%, accuracy of 99.2%, sensitivity of 98.9%, specificity of 99.1%, and precision of 99%.",Diabetic retinopathy;Histograms;Sensitivity;Noise;Blindness;Feature extraction;Market research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493665,IEEE Conferences,,,,,,
Brain Tumor Diagnosis Using Deep Learning: A Literature Review,S. Bhuvaneswari; J. Thomas; S. Nithish; S. Prithvi,2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE),18-Apr-24,2024,"Brain tumor diagnosis is a critical task in the field of medical imaging, with the potential to significantly impact patient outcomes and treatment planning. The use of deep learning has been more well-known within the last ten years as a potential method for improving and automating the detection and categorization of brain tumors. The goal of this literature review is to present a thorough overview of the application of deep learning to the diagnosis of brain tumors.",Deep learning;Bibliographies;Network architecture;Market research;Planning;Task analysis;Information technology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493599,IEEE Conferences,,,,,,
Cholangiocarcinoma Classification using MedisawHSI: A Breakthrough in Medical Imaging,H. Namburu; V. N. Munipalli; M. Vanga; M. Pasam; S. Sikhakolli; S. Chinnadurai,2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE),18-Apr-24,2024,"Liver bile-duct cancer is also called as cholangio- carcinoma that stands a significant global health hazard, due of its low 5-year survival rate that is about (2-24%). So Precise and prompt diagnoses is vital in order to improve patient diagnosis and increase survival rates. Hyperspectral imaging (HSI) offers a promising avenue for improving liver cancer diagnosis due to its ability to capture detailed continuous spectral plus spatial information that is beyond the visible range of the human eye. Classifying cholangiocarcinoma through HSI is complex because of its high dimensionality. To solve this,a network called as MedisawHSI is introduced in this article. Inspired from Jigsaw HSI that demonstrates superior performance compared to other Neural Networks. In this article we present Medisaw-based clas- sification involves dividing the hyperspectral image into smaller non - overlapping patches, which are then classified individually based on their spectral characteristics. Results demonstrate that we have achieved better results in comparison with the literature. This will help the surgeons in image - guided surgery, ultimately reducing the burden of liver cancer on global healthcare systems.",Liver cancer;Neural networks;Surgery;Liver;Reliability engineering;Market research;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493579,IEEE Conferences,,,,,,
Comparative Analysis of Deep Learning and Machine Learning for Detection and Classification of Brain Tumors at Multiple Stages,B. Ramprakash; S. S. Hari; D. N. Kumar; C. Santhiya,2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE),18-Apr-24,2024,"The brain acts as the body's command central location. As time goes on, more and more brain illnesses are pinpointed. Because of this diversity, improving current methods of diagnosing or detecting brain illnesses is a constant focus of study. The effectiveness of treatment for disorders of the brain can be greatly improved by catching them early. Recent years have seen the widespread adoption of AI across the scientific community, and without a doubt, this has had a profound impact on neurology. Brain disease diagnosis and prognosis have both benefited from AI's incorporation into the medical industry. Cancer of the brain is a major killer in the world today. Because of these characteristics, there are many ways to check for brain cancer. Prompt diagnosis is essential for effective treatment of brain tumors. One such technique is magnetic resonance imaging. In contrast, state-of-the-art approaches have been employed to address several classification-related challenges in medical imaging in recent years. These include deep learning, neural networks, and machine learning. To differentiate between glioma, meningioma, pituitary, and no tumour in patients with brain cancer, the SVM classifier in machine learning (ML) was utilized in this study. The information in this study comes from people's contrast-enhanced MRI images. This study provides a comparison between the suggested model and competing models to prove the superiority of our method. Both the raw data and the data after it had been cleaned and supplemented were evaluated. In this research, we explore the possibility of using deep learning and other advanced machine learning approaches to spot brain abnormalities. To be noteworthy concerns with machine learning/deep learning-based methodologies for identifying brain illnesses are discussed, and the most noteworthy findings from the publications we analyzed are presented. The ultimate purpose of the research is to determine the best method for diagnosing various forms of brain diseases.",Deep learning;Support vector machines;Neurology;Magnetic resonance imaging;Brain cancer;Brain modeling;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493717,IEEE Conferences,,,,,,
Automatic Kidney Stone Detection using XResNet 152 Model for CT Images,S. M. Manudhas; R. Manoranjitham,2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE),18-Apr-24,2024,"This study provides an automatic kidney stone identification using computed tomography (CT) scan images and cutting-edge deep learning techniques. The identification and categorization of diverse kidney disorders, such as stones and normal structures, are the main goals of this study. Our research intends to transform the diagnosis process by offering an accurate and effective automated system for renal health evaluation using the resilience of the XResNet152 architecture. The methodology includes a well-selected dataset with a variety of kidney diseases, which makes thorough model training, validation and assessment possible. The model used augmentation approaches and preprocessing processes specific to medical imaging data to improve its capacity to identify complex patterns and features that correspond to various kidney abnormalities. The model demonstrated remarkable precision in categorization, demonstrating encouraging outcomes in discerning and differentiating various renal ailments. The model demonstrated remarkable accuracy in distinguishing between stones and normal structures, and it demonstrated resilience in intricate medical imaging situations. In this study, we present a thorough explanation of our approach, dataset preparation, model architecture, and in-depth performance analysis. We critically assess the advantages and disadvantages of our methodology and suggest directions for further development. By advancing the field of medical image analysis, this research opens the door to more advanced precision medicine and better healthcare diagnostics.",Training;Computed tomography;Precision medicine;Computer architecture;Transforms;Medical services;Performance analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493554,IEEE Conferences,,,,,,
Rosacea Classification Using Deep Stacking Multi-Layer Model (DSMLM),A. S. N; T. R. Saravanan; V. M; D. N; K. Uma; M. P,2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE),18-Apr-24,2024,"Rosacea, a widespread and complex skin disorder affecting most of the world population, is divided into various subtypes with unique clinical features the complexity of these subtypes makes accurate classification difficult. A unique technique called the Deep Stacking Multi-Layer Model (DSMLM) has been proposed as a solution for this complex classification. To extract microelements from clinical images and patient history data, DSMLM uses a multilayered neural network architecture. Using the stacking ensemble strategy to combine predictions from different base models makes it possible to improve the classification performance. The effectiveness of the DSMLM is evaluated using a large dataset of different rose cases, and it shows remarkable accuracy in distinguishing between different rose subtypes",Soft sensors;Stacking;Sociology;Neural networks;Predictive models;Skin;Real-time systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493644,IEEE Conferences,,,,,,
Evaluation of ResNet Architectureâ€™s Performance for Early Brain Infarction Detection,S. Bajaj; M. Bala; MohitAngurala,2024 11th International Conference on Computing for Sustainable Global Development (INDIACom),18-Apr-24,2024,"The main objective of this research, is to compare the performance of ResNet models when deployed on the images of Brain CT scans. In this research three ResNet models are selected with the goal of identifying the most effective one for detecting and classifying cerebral infarcts. These models demonstrated exceptional results, highlighting the promising capabilities of deep learning in the field of medical imaging. The remarkable accuracy, specificity, recall, and sensitivity exhibited by these models generate enthusiasm for the potential application of deep learning in real-world clinical settings. Beyond merely identifying infarcts, these findings suggest that leveraging deep learning could contribute to refining model performance in practical, clinical scenarios.",Deep learning;Analytical models;Sensitivity;Computational modeling;Computed tomography;Refining;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498838,IEEE Conferences,,,,,,
Grey Wolf Optimizer and Deep Neural Network based Feature Selection and Classification in Medical Image Analysis,K. Chandrashekhar; P. Leela; K. R. Madhavi; J. Avanija; N. Tangudu,2024 11th International Conference on Computing for Sustainable Global Development (INDIACom),18-Apr-24,2024,"Accurate and efficient medical image analysis is required for modern healthcare applications, facilitating timely disease diagnosis and treatment. Considering the importance of optimization algorithms and deep learning methodologies, this work presents a novel approach, Grey Wolf Optimizer-Assisted Deep Neural Networks (GWO-DNN), for feature selection and classification in medical image analysis. The proposed framework utilizes the power of the Grey Wolf Optimizer to intelligently select the most relevant features from complex medical images. The selected features are then integrated into Deep Neural Networks, to create accurate models for medical image classification tasks. This integrated approach is designed to improve diagnostic accuracy and enhance patient care. The experimentation and evaluation, reveal that the proposed GWO-DNN based model for image analysis is more efficient than the existing methods.",Pathology;Image analysis;Scalability;Artificial neural networks;Radiology;Feature extraction;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498958,IEEE Conferences,,,,,,
Technical Review of Breast Cancer Screening and Detection using Artificial Intelligence and Radiomics,A. Singh; S. Kaur; D. Singh; G. Singh,2024 11th International Conference on Computing for Sustainable Global Development (INDIACom),18-Apr-24,2024,"Breast cancer is a lethal disease. Cancer occurs because of the unwanted growth of cells. Globally breast cancer (BC) is increasing rapidly. BC is the most widespread tumor and is among the foremost reason of cancer-related deaths in females. More alarming is that it is being increasingly diagnosed at a younger age in India compared to the west. Restrictions of the existing imaging modalities aggravated researchers to design new methods for early breast cancer detection. New artificial intelligence techniques and machine learning models with radiomics revolutionize the medical field and are becoming accepted assistive tools in the early diagnosis and prognosis of BC. These techniques have been giving admirable results with superior efficiency in healthcare industry applications over the past decade. This paper gives detailed insight of workflow of radiomics, and the role of Artificial intelligence (AI), Machine learning (ML), and Deep learning (DL) for treatment of BC. Various ML techniques used for early detection of BC are discussed.",Deep learning;Reviews;Design methodology;Industry applications;Medical services;Breast cancer;Prognostics and health management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498427,IEEE Conferences,,,,,,
Alzheimerâ€™s Disease Detection using Deep Learning Algorithm,J. F. Banu; R. Kingsly Stephen; N. Aditya; L. C. Dhanush Raaghav,2024 11th International Conference on Computing for Sustainable Global Development (INDIACom),18-Apr-24,2024,"Alzheimerâ€™s disease is a neurological disorder. Research on early detection and classification using machine learning techniques has become essential in recent years. This paper focuses on developing a Convolutional Neural Network for early-stage Alzheimerâ€™s disease detection from brain MRI scans. Aim to create a highly accurate and robust model with sensitivity and specificity for identifying Alzheimerâ€™s, mild cognitive impairment, and healthy individuals. The research utilizes the OASIS Alzheimerâ€™s image dataset as the input data set. CNN-based preprocessing, feature extraction, extensive training, fine-tuning, data augmentation, and cross-validation on these datasets ensure the modelâ€™s reliability. The proposed prediction model achieves the precision values for each class ranging from 0.209 for mild demented to 0.255 for moderate demented.",Training;Deep learning;Systematics;Magnetic resonance imaging;Sensitivity and specificity;Brain modeling;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498963,IEEE Conferences,,,,,,
Utilizing Advanced Deep Learning Techniques for Brain Tumor Classification,P. Sreedevi; K. P. Rani; M. Anand; A. Madhavi; A. Balaram; A. Kiran,2024 11th International Conference on Computing for Sustainable Global Development (INDIACom),18-Apr-24,2024,"Brain tumors, both benign and malignant, pose a substantial hazard to human health, and needing accurate and prompt diagnosis for successful treatment. The goal of this work is to improve brain tumor detection accuracy and effectiveness of categorization through the implementation of an advanced technique. The proposed method leverages the ResNet50 deep neural network, renowned for its proficiency with extensive datasets. By utilizing the pre-trained ResNet50 model from the ImageNet dataset and employing the Kaggle dataset for testing, we conducted comprehensive data preparation, including partitioning into distinct training and validation sets. Over 20,000 categories and over 14 million records make up the ImageNet collection or classes. This vast dataset has been instrumental in training various deep learning models, including ResNet50, to recognize and classify objects accurately. In our suggested method, comprehensive data preparation, including partitioning into distinct training and validation sets, was conducted. Data augmentation techniques were employed to enhance model robustness, resulting in impressive accuracy levels of up to 98% in brain tumor categorization. Our research sets the stage for more precise clinical diagnoses and improved treatment strategies in the realm of brain tumor classification, thereby potentially saving lives.",Deep learning;Training;Computational modeling;Brain modeling;Data augmentation;Data models;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498650,IEEE Conferences,,,,,,
Comparative Analysis of Diverse Architectures for Accurate Blood Cancer Cell Classification,G. K. M. Muttakin; F. Yesmin; S. S. Alam; M. S. Ashraf; V. Akuthota; M. A. Quader,"2024 International Conference on Computer, Electrical & Communication Engineering (ICCECE)",18-Apr-24,2024,"Blood cancer cell diagnosis is crucial in medical diagnostics. It demands accurate classification of blood cell images. Proper classification of blood cancer cells is fundamental for accurately diagnosing the specific type and subtype of blood cancer as well as efficient treatment planning. It provides specific information, helping healthcare professionals predict the prognosis, survival rates, and the potential for disease recurrence. On this basis, deep learning models have demonstrated remarkable performance. In this paper, we have introduced a comprehensive research into blood cancer cell classification, employing a diverse dataset encompassing various blood cell types. We explore the effectiveness of VGG19 with batch normalization, Vision Transformer (ViT), Ensemble Adversarial Inception-ResNetV2, DenseNet201, and ResNeXt50 architectures in this challenging task. In order to enhance the model performance, we integrate essential data preprocessing techniques, including resizing, cropping, and normalization. Additionally, novel data augmentation strategies, such as random cropping, and flipping are introduced to augment the training dataset and improve model generalization. Remarkably, VGG19 with batch normalization has shown tremendous success by achieving 99.86% accuracy. Moreover, the DenseNet201 model has performed brilliantly by achieving an accuracy of 98.55%. The ResNeXt50 model shows excellent performance with an accuracy of 98.31%. The Vision Transformer (ViT) achieves a solid accuracy of 98.91%. Lastly, Ensemble Adversarial Inception-ResNet V2 also performed no-tably achieving 96.38%. In this context, VGG19 with batch normalization is able to show more excellent performance than other models by achieving the highest accuracy.",Training;Microprocessors;Computer architecture;Transformers;Solids;Data models;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10497341,IEEE Conferences,,,,,,
Brain Tumor Identification Using a Deep Learning Approach,P. Sarkar; A. Saha; T. Pal,"2024 International Conference on Computer, Electrical & Communication Engineering (ICCECE)",18-Apr-24,2024,"The health of people is seriously threatened by brain tumours, and the prognosis of patients is greatly enhanced by early identification. Convolutional Neural Networks (CNNs) have demonstrated impressive performance in a range of computer vision applications, such as the processing of medical images. This work proposes an effective brain tumor detection system using the VGG16 (Visual Geometry Group 16-layer) CNN architecture. A dataset of brain magnetic resonance imaging (MRI) scans is used by the proposed system to identify characteristics of the existence of tumors. Utilizing pre-trained weights on ImageNet, transfer learning is used to improve the model's performance. Our results found VGG16 model is a good option for precisely identifying brain tumors from MRI images. As a dependable instrument for early diagnosis, the proposed system has promise for improving patient care and neuroimaging results.",Neuroimaging;Visualization;Magnetic resonance imaging;Transfer learning;Brain cancer;Brain modeling;Real-time systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10497396,IEEE Conferences,,,,,,
Deep Learning-Based Optimization of Dolphin Echolocation for Skin Lesion Image Classification,A. N; A. P,2024 International Conference on Integrated Circuits and Communication Systems (ICICACS),18-Apr-24,2024,"The healthcare industry has benefited greatly from the recent developments in Machine Learning (ML), especially deep learning (DL). This motivates the study of how to best employ AI, and deep learning (DL) algorithms in particular, to identify skin lesions in dermoscopic pictures. To do this, the model employs an Inventive Dolphin Echolocation Optimization Deep Learning driven Skin Lesion Detection and Classification (IDEODL-SLDC) technique to analyse these images and improve upon a previously developed Dolphin Echolocation Optimizer (IDEO). In addition, the SqueezeNet method can be used to obtain feature vectors. The dermoscopy images are also classified using IDEO and a deep wavelet neural network (DWNN) approach. The IDEODL-SLDC method is improved on the reference dataset after undergoing extensive testing. The experimental results demonstrated that the IDEODL-SLDC model exhibited superior performance compared to alternative approaches across different dimensions of model classifier classes.",Deep learning;Neural networks;Medical services;Skin;Vectors;Lesions;Integrated circuit modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498765,IEEE Conferences,,,,,,
Brain Tumor Classification Using MobileNet,M. P. Kumar; D. Hasmitha; B. Usha; B. Jyothsna; D. Sravya,2024 International Conference on Integrated Circuits and Communication Systems (ICICACS),18-Apr-24,2024,"One of the deadliest diseases is a brain tumor, which develops when brain tissue inside the skull grows suddenly and uncontrollably. Brain tumors are a significant health concern worldwide, necessitating timely and accurate diagnosis for effective treatment. Due to advancements in deep learning techniques to automatically identify brain cancers using MRI data. Here the MobileNet model is used because of its efficiency and effectiveness, it is a lightweight deep learning model architecture, which has made it a viable contender for medical image processing tasks. The presented methodology involves three main phases: Pre-processing, Feature extraction, and lastly detection of the tumor. Here MobileNet is used to automatically extract discrimination features from preprocessed images. Subsequently, the model was trained using the dataset, tested, and compared with other deep learning methods. The results showed that the MobileNet model accurately detects cancers, allowing for timely treatment to prevent any physical aftereffects like paralysis and other issues.",Deep learning;Training;Magnetic resonance imaging;Brain cancer;Brain modeling;Feature extraction;Paralysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499117,IEEE Conferences,,,,,,
Deep Learning for Automated Classification of Kidney Lesions in Chronic Kidney Disease Patients,B. S. Lakshmi; V. Anand R; H. Omotunde; M. Birundadevi; M. S; M. G. Brahmam,2024 International Conference on Integrated Circuits and Communication Systems (ICICACS),18-Apr-24,2024,"Deep Learning was utilized to autonomously classify renal abnormalities, which improved the diagnostic accuracy for individuals with chronic kidney disease (CKD), according to this study. During training on a meticulously selected dataset, the proposed model outperformed the most recent approaches. By utilizing medical imaging dataâ€” specifically quantitative and qualitative dataâ€”to differentiate between discrete kidney abnormalities, the model demonstrated exceptional levels of accuracy, precision, and recall. Furthermore, apart from effectively resolving ethical considerations pertaining to data privacy, this innovative methodology has the potential to fundamentally transform the existing landscape of chronic kidney disease (CKD) diagnosis and treatment. The results demonstrate that the model effectively manages intricate renal disorders, signifying a noteworthy progression in the field of medical image processing. This research paves the way for additional fruitful lines of inquiry, which ought to enhance nephrology diagnostics and patient care.",Deep learning;Training;Ethics;Transforms;Chronic kidney disease;Lesions;Integrated circuit modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498835,IEEE Conferences,,,,,,
Combined Deep Learning Approach for Colorectal Tumor Classification in Endoscopy Images,A. Santhoshi; A. Muthukumaravel,2024 International Conference on Integrated Circuits and Communication Systems (ICICACS),18-Apr-24,2024,"Colorectal cancer detection in endoscopic images is essential. The Proposed methodology classifies colorectal tumors deeply and reliably by exploiting each method's capabilities. Individual approaches have limitations; thus, the proposed system employs ensemble methods to analyze endoscopic images thoroughly. The classification performance is improved with feature-level and decision-level fusion. Transfer learning from large medical image datasets is used better to adapt the proposed models to colorectal tumor detection. Using the Area Under Curve statistic, the tests show that the proposed system is highly efficient in all metrics compared to the existing systems. The diagnosis accuracy is better than single methods as well. The research advances colorectal cancer detection and illuminates classification technique synergy. The work improves medical image analysis reliability and precision. From 0.88 to 0.95, the proposed method's AUC score, which assesses tumor type recognition, increased. It detects colorectal tumors with 0.90 sensitivity. Specificity was 0.89 for the proposed method. The new method went from 0.90 to 0.96 in accuracy.",Integrated circuits;Sensitivity;Image analysis;Transfer learning;Real-time systems;Reliability;Ensemble learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498840,IEEE Conferences,,,,,,
Dynamic Channel Attention for Enhanced Spatial Feature Extraction in Medical Image Analysis using Advanced Attention Capsule Network,G. Maheswari; S. Gopalakrishnan,2024 International Conference on Integrated Circuits and Communication Systems (ICICACS),18-Apr-24,2024,"Medical image analysis is essential in healthcare, guiding diagnosis, treatment, and monitoring. This study presents AACNet (Advanced Attention Capsule Network), a deep learning framework addressing the complexity of diverse medical images. AACNet incorporates a multi-feature extractor with SPP layer, multi-level capsule network, and dynamic channel attention modules. Trained on curated datasets, including chest X-rays and CT scans, augmented for enhanced generalization, AACNet achieves 92.43% accuracy on X-rays and 94.64% on CT scans, surpassing other models in multiple metrics. The model's interpretability, utilizing dynamic channel attention, underscores its capacity to emphasize crucial spatial features. The innovative integration of dynamic channel attention and capsule networks makes AACNet a pivotal solution for medical image analysis. The research findings underscore the model's adaptability, effectiveness, and interpretability. AACNet emerges as a pivotal solution for medical image analysis, exhibiting consistent superior performance and potential for real-world clinical applications.",Measurement;Deep learning;Adaptation models;Image analysis;Computed tomography;X-rays;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498708,IEEE Conferences,,,,,,
Improved Butterfly Optimization Algorithm for Automated Breast Cancer Detection and Classification using Deep Learning,B. Patil; V. P; M. Al-Farouni; B. Sathyavani; P. K. Pareek,2024 International Conference on Integrated Circuits and Communication Systems (ICICACS),18-Apr-24,2024,"Breast cancer is a worldwide health issue affecting women, and research is primarily done in medical images for early diagnosis and detection. The problem statement is to train model strategies during pre-processing with the absence of identification and specific consideration for development, leading to a biased model. The MIAS dataset is used in mammography scans with the three classes of malignant, normal, and benign. The proposed method goes with the feature extraction process with the vector, the Improved Butterfly Optimization algorithm (IBOA) for feature selection, deep adaptive spatial-based feature fusion, and finally the feature vector selection process. The numerical validation of the proposed method results in an accuracy of 99.98%, a sensitivity of 98.79%, a specificity of 99.50%, and an F1-score of 98.90%. Comparing the existing methods, like Improved Multi-fractal Dimension (M-FD), Improved Marine Predator Algorithm (IMPA-ResNet50), and Infinite Genetic Algorithm (IFSGA-DNN), can resolve the problems overcome by the proposed method.",Deep learning;Integrated circuits;Sensitivity;Feature extraction;Vectors;Breast cancer;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499033,IEEE Conferences,,,,,,
Dual Attention Convolutional AutoEncoder for Diagnosis of Alzheimerâ€™s Disorder in Patients Using Neuroimaging and MRI Features,S. Al-Otaibi; M. Mujahid; A. R. Khan; H. Nobanee; J. Alyami; T. Saba,IEEE Access,30-Apr-24,2024,"Alzheimerâ€™s disease is a neurodegenerative disease causing memory loss and brain protein accumulation. Early diagnosis is crucial for clinical trials and patient care. Magnetic resonance imaging (MRI) methods have improved diagnosis and prognosis, but doctors need to interpret images proficiently. Deep learning technology has shown potential in detecting Alzheimerâ€™s disease, but the disease progresses slower in early phases. A new dual-attention convolutional autoencoder model is presented, offering improved detection abilities and potential for real-time use in Alzheimerâ€™s disease diagnosis. The study utilized two datasets: the first ADNI dataset, which includes three classes (MCI, CN, and AD), and the second Alzheimerâ€™s Disease Neuroimaging Dataset, which includes two distinct classes (AD and MCI). We analyze the effectiveness of our proposed model by evaluating key performance metrics such as accuracy, precision, sensitivity, specificity, F1 score, and AUC score. In addition, we utilize cross-validation and mean absolute error to validate our model while also fine-tuning the parameters. Based on experimental data, the proposed model accurately detected Alzheimerâ€™s disease with an accuracy of 0.9902 Â± 0.0139. Based on the results, the proposed model demonstrates excellent performance compared to the existing methods described in the literature. The proposed mode achieves precision, sensitivity, and specificity of 0.9882 Â± 0.0587, 0.9898 Â± 0.0865, 0.9912 Â± 0.0872 respectively. The model achieved an AUC score of 0.9992 for MCI and 0.9919 for AD class. Furthermore, the proposed method can enhance the affordability of Alzheimerâ€™s disease diagnostics and increase the rate of early AD detection by facilitating remote healthcare.",Alzheimer's disease;Magnetic resonance imaging;Feature extraction;Brain modeling;Deep learning;Convolutional neural networks;Neuroimaging;Medical services;Neurological diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10504280,IEEE Journals,,,,,,
Deep Transfer Learning Based Diagnosis of Multiple Neurodegenerative Disorders,R. K. Mahendran; R. Aishwarya; R. Abinayapriya; P. Kumar,2024 International Conference on Emerging Smart Computing and Informatics (ESCI),17-Apr-24,2024,"Neurodegenerative disorders challenge healthcare systems globally, impacting millions of lives and diminishing quality of life. Machine learning, particularly deep learning, offers promise in improving disease classification. Using a two-step methodology, This study looks into classifying three distinct neurodegenerative diseases using transfer learning: cerebral ataxia, Alzheimer's disease, and Parkinson's disease. First, we use neural network architectures that have already been trained, including VGG16 and ResNet101, which were first trained on large image datasets. This enables us to extract high-level features that capture important disease patterns and characteristics from medical images. By adapting these pre-trained networks to the unique attributes of each disease, we enhance the model's ability to discriminate between them effectively. We use optimization techniques like SGD, Rmsprop, and Adam algorithm to get the optimized results, we employ a range of evaluation metrics, including accuracy and loss. These metrics provide a comprehensive understanding of the model's competence in accurately classifying patients with diverse neurodegenerative diseases. To make our model more accessible and user-friendly, we deploy it through the Flask web framework. Our research findings conclusively demonstrate that the amalgamation of transfer learning, Flask deployment, and stringent evaluation metrics substantially enhances the accuracy of neurodegenerative disease classification when compared to traditional diagnostic methods. This novel strategy promises to transform the identification and management of neurodegenerative illnesses, resulting in more prompt and efficient patient interventions.",Measurement;Parkinson's disease;Transfer learning;Neural networks;Transforms;Medical services;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10497320,IEEE Conferences,,,,,,
Neuro-Cognitive Pattern Recognition: Advancements in Memory-Related Disorders Identification,V. J. D. V; A. N; M. Faheem; K. K; S. K. J.D.T; M. Muthulakshmi,2024 International Conference on Emerging Smart Computing and Informatics (ESCI),17-Apr-24,2024,"The application of machine learning in medical imaging and diagnostics opens up new pathways for disease identification and classification. Our research investigates the use of various machine learning models for Alzheimer's disease detection and classification using MRI image data, including Support Vector Machines (SVM) with different kernels, Linear Discriminant Analysis (LDA), and the advanced convolutional neural network, EfficientNetB0. The comparative study is an important component of this research, concentrating not only on the models' performance but also on the theoretical assumptions that underpin their efficacy. This investigation sheds light on why certain models outperform others, notably in the context of Alzheimer's disease classification. The research emphasizes the robust performance of linear models such as SVM and LDA, as well as the complexities of convolutional neural networks. Notably, our findings show a tremendous success with a 98.8 % accuracy rate, highlighting the potential of machine learning in improving diagnostic processes. This study contributes to the expanding field of machine learning in medical diagnostics by providing in-depth analysis and theoretical understanding that guide model selection and optimization in high-stakes healthcare applications.",Support vector machines;Analytical models;Magnetic resonance imaging;Machine learning;Data models;Linear discriminant analysis;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10497280,IEEE Conferences,,,,,,
Applications of Deep Learning Models on the Medical Images of Osteonecrosis of the Femoral Head (ONFH): A Comprehensive Review,J. Wang; Y. Zheng; J. Luo; T. Tin-Yan Lee; P. Li; Y. -Q. Zhang; J. C. -W. Cheung; D. W. -C. Wong; M. Ni,IEEE Access,26-Apr-24,2024,"Deep learning models have demonstrated promising results in the early and accurate diagnosis of osteonecrosis of the femoral head (ONFH), enabling early detection and informed surgical decision-making. The objective of this review is to summarize the applications of deep learning models on the medical images of ONFH. English papers were searched from CINAHL via EBSCOhost, Embase, IEEE XploreÂ® Digital Library, PubMed, Scopus, and Web of Science. Sixteen studies (n =16) were eligible for data synthesis. Among these, five studies (n =5) focusing on radiographs, ten studies (n =10) focusing on magnetic resonance imaging, and one study (n =1) focusing on computed tomographic images. The applications of these studies included identifying ONFH from normal or other hip pathologies, classifying severity, segmenting, and detecting femoral head and necrotic regions, predicting signs and symptoms of ONFH, and predicting potential ONFH after fracture fixation. Generally, the models demonstrated good to excellent classification performance and excellent discriminatory power; and generally comparable to that of experienced physicians and superior to that of less experienced physicians. However, the external validity of these studies demonstrated only moderate, as evidenced by the performance on the external testing set and might be attributed to the relatively small data size used during model training. we observed a shift from CNN-based models to U-Net-based models (i.e., with encoder-decoder architecture). In addition to streamlining the segmentation, detection, and classification procedures, future studies will explore multimodal attention, self-supervised learning, explainable models, and data augmentation through generative models.",Hip;Feature extraction;Magnetic heads;Head;Deep learning;Medical diagnostic imaging;Artificial intelligence;Machine learning;Osteoporosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500822,IEEE Journals,,,,,,
Deep Location Soft-Embedding-Based Network with Regional Scoring for Mammogram Classification,B. Han; L. Sun; C. Li; Z. Yu; W. Jiang; W. Liu; D. Tao; B. Liu,IEEE Transactions on Medical Imaging,,2024,"Early detection and treatment of breast cancer can significantly reduce patient mortality, and mammogram is an effective method for early screening. Computer-aided diagnosis (CAD) of mammography based on deep learning can assist radiologists in making more objective and accurate judgments. However, existing methods often depend on datasets with manual segmentation annotations. In addition, due to the large image sizes and small lesion proportions, many methods that do not use region of interest (ROI) mostly rely on multi-scale and multi-feature fusion models. These shortcomings increase the labor, money, and computational overhead of applying the model. Therefore, a deep location soft-embedding-based network with regional scoring (DLSEN-RS) is proposed. DLSEN-RS is an end-to-end mammography image classification method containing only one feature extractor and relies on positional embedding (PE) and aggregation pooling (AP) modules to locate lesion areas without bounding boxes, transfer learning, or multi-stage training. In particular, the introduced PE and AP modules exhibit versatility across various CNN models and improve the modelâ€™s tumor localization and diagnostic accuracy for mammography images. Experiments are conducted on published INbreast and CBIS-DDSM datasets, and compared to previous state-of-the-art mammographic image classification methods, DLSEN-RS performed satisfactorily.",Feature extraction;Lesions;Mammography;Solid modeling;Image segmentation;Convolutional neural networks;Training,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500847,IEEE Early Access Articles,,,,,,
VB-SOLO: Single-Stage Instance Segmentation of Overlapping Epithelial Cells,L. Li; W. Chen; J. Qi,IEEE Access,17-Apr-24,2024,"The instance segmentation of overlapping cells in smear images of epithelial cells is challenging due to the significant overlap and adhesion between the cellsâ€™ translucent cytoplasm. In this paper, an improved single-stage instance segmentation network called VoVNet-BiFPN-SOLO (VB-SOLO) is proposed to address this problem. The model takes SOLOv2 model as its main frame. Firstly, the backbone network uses Efficient Channel Attention (ECA) to optimize the VoVNetv2 network to increase the information interaction across channels and enhance the extraction of cell instance features. Secondly, the bi-directional feature pyramid network (BiFPN) is introduced to connect with the new backbone. BiFPN can achieve the weighted fusion of features with different resolutions from bottom to top and keep more shallow semantic information in the network. Finally, the Convolutional Block Attention Module (CBAM) is added to the mask branch to improve cell segmentation results in feature maps. Experimental results on the publicly available datasets CISD and Cx22 demonstrate the effectiveness of the VB-SOLO model, achieving a DCP of 0.966 and 0.940 and a FNRO of 0.055 and 0.03. Compared to the original SOLOv2 algorithm, the proposed method achieved improvements in DCP of 1.3% and 1.1% respectively. Additionally, comparative tests with multiple instance segmentation networks have shown that the proposed improved network can achieve a better balance between segmentation accuracy and efficiency. The experimental results demonstrate the effectiveness of the proposed network improvements and the potential of single-stage instance segmentation networks in overlapping cell image segmentation.",Instance segmentation;Feature extraction;Semantic segmentation;Classification algorithms;Convolutional neural networks;Deep learning;Biomedical imaging;Cervical cancer;Image segmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496694,IEEE Journals,,,,,,
Malaria and Pneumonia Disease Prediction Using Deep Learning,V. Rai; M. Singhal; B. Dwivedy; P. Jain; Ritika,2024 2nd International Conference on Disruptive Technologies (ICDT),11-Apr-24,2024,"Early identification, intervention, and efficient treatment planning all rely heavily on disease prediction. This study examines the methods for disease prediction based on deep learning, concentrating on pneumonia and malaria in particular. Pneumonia and malaria are two common, sometimes fatal infections, especially in underdeveloped areas. Traditional diagnostic techniques for these disorders can be time-consuming and frequently need for specialized knowledge. Therefore, it is necessary to investigate automated and reliable illness prediction methods. This study's goal is to assess how well deep learning algorithms can foresee malaria and pneumonia from medical imaging data. The study makes use of a broad dataset that includes chest X-ray pictures for the identification of pneumonia and blood smear images with malaria infection. To improve the dataset's quality and variety, preprocessing procedures including picture normalization and augmentation are used. The results of this research demonstrate the potential of deep learning algorithms for malaria and pneumonia prediction, which advances the field of medical diagnostics. These automated prediction algorithms could help medical personnel diagnose patients quickly, allowing for quick treatment to start and better patient outcomes.",Deep learning;Training;Pneumonia;Malaria;Transfer learning;Predictive models;Prediction algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489408,IEEE Conferences,,,,,,
Breast Cancer Detection Using Convolutional Neural Network,S. S. Saniganti; S. R. Gaddam; S. R. Eppa; S. K. Rout; B. K. Sethi; B. Kethireddy,2024 2nd International Conference on Disruptive Technologies (ICDT),11-Apr-24,2024,"This research endeavors by combining transfer learning techniques with Convolutional Neural Networks (CNNs), this study aims to improve the categorization of breast cancer. The primary objective is to improve diagnostic precision in detecting malignancies from mammographic images, ultimately impacting clinical decision-making and patient care positively. The study employs a robust methodology utilizing a diverse dataset for training and validation. Transfer learning optimizes CNNs' efficiency, fine-tuning the architecture to adapt to breast cancer detection nuances. Rigorous training-validation cycles refine the model, ensuring generalizability across diverse datasets. The automated system minimizes subjective variability, contributing to a more objective diagnostic process. Scalability is achieved by designing the model to handle large volumes of mammographic images, a critical feature for widespread implementation. The integration of CNNs and transfer learning yields promising results, demonstrating a substantial improvement in accuracy compared to existing methods. Automation significantly reduces diagnosis time, while introduced objectivity minimizes result variability. The property of scalability shows promise for broad use since it works well in managing massive amounts of images. These outcomes highlight the viability and effectiveness of the suggested strategy in improving the diagnosis of breast cancer. In conclusion, the developed model, combining CNNs and transfer learning, represents a significant advancement with the potential to revolutionize clinical decision-making and patient care, offering a more accurate, efficient, and widely applicable approach to breast cancer diagnosis.",Training;Adaptation models;Automation;Scalability;Transfer learning;Decision making;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489574,IEEE Conferences,,,,,,
An Automatic Robust Deep Learning and Feature Fusion-based Classification Method for Early Diagnosis of Oral Cancer Using Lip and Tongue Images,K. Dwivedi; K. Patel; J. P. Pandey; P. Garg,2024 2nd International Conference on Disruptive Technologies (ICDT),11-Apr-24,2024,"Oral cancer is becoming a more challenging issue globally as it is 5thmost common cancer. Alcohol, betel nut and tobacco are responsible for more than 95% of oral cancer cases. Early diagnosis of oral cancer can improve survival prospects. Artificial intelligence is becoming more popular in medical diagnosis systems. This study aims to define the ability of AI models to analyze and identify early stages of oral cancer. Different deep-learning models were employed to develop an automated fusion-based network to recognize oral cancer. The proposed method is implemented on a publicly available dataset having tongue and lip images for the diagnosis of oral cancer. The data augmentation is applied in the dataset to avoid the problem of data unbalancing. The hyperparameters are optimally selected specifically for the considered dataset to provide higher accuracy and analyze the effectiveness of the proposed model. The evaluated performance of the proposed fusion-based model was compared with other state-of-the-art deep learning models which show that the proposed model outperforms all other models by achieving an overall accuracy of 94.62 %. The effectiveness of the proposed model can help in the medical diagnosis system for the detection and classification of oral cancer at an early stage.",Deep learning;Analytical models;Tongue;Lips;Feature extraction;Data augmentation;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489266,IEEE Conferences,,,,,,
Brain Tumor Detection Using ADARN Optimizer,R. Kumar; A. Dwivedi; S. Joshi; V. C. Tripathi,2024 2nd International Conference on Disruptive Technologies (ICDT),11-Apr-24,2024,"The use of MRI scans to identify and categorise brain tumors is the main focus of this study. The goal is to develop a precise and reliable method for early identification and accurate categorisation of brain tumours, including pituitary neoplasms, meningiomas, and gliomas. Deep learning methods were utilized to analyze MRI image databases, resulting in a 96% identification accuracy for brain tumors and a 98% categorization accuracy for the three types. This research demonstrates the potential of deep learning techniques for accurate brain tumor identification and categorization. The early identification and precise categorization of brain tumors can assist medical professionals in making informed decisions about the best treatment options for patients, leading to better outcomes and survival rates. The article provides an extensive review of current methods for identifying and classifying brain tumors using MRI data. It emphasizes the critical importance of this field of study in improving patient outcomes and reducing unnecessary treatments.",Deep learning;Ethics;Magnetic resonance imaging;Transfer learning;Brain modeling;Data models;Neoplasms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489239,IEEE Conferences,,,,,,
Brain Cancer Detection With MRI and ML,P. Gupta; S. Srivastava; R. Gupta; S. Kumar,2024 2nd International Conference on Disruptive Technologies (ICDT),11-Apr-24,2024,"Brain based cancer is a devastating and potentially fatal disease that demands efficient & timely diagnosis. [MRI] magnetic resonance-imaging has come as a very important utility for early detection and evaluation of tumors located inside the brain. In past few years, machine learning techniques (MLT) has played pivotal role in enhancing the diagnostic accuracy of brain cancer detection from MRI scans. This paper presents a deep review of many papers focused on integration of MRI and machine learning for brain cancer detection. Our primary objective is to evaluate the advancements and identify gaps within these studies to determine the most effective ML algorithm for building an optimal brain cancer detection model. Through a systematic analysis of the selected research papers, we assess the methodologies, datasets, and outcomes of various ML approaches, while considering their strengths and limitations. By comparing and contrasting the methodologies and results of the surveyed papers, we aim to make informed recommendations on the choice of an ML algorithm for brain cancer detection. Our review seeks to contribute to the ongoing efforts to improve early-stage detection and management of brain tumors, emphasizing pivotal role of machine learning in enhancing the medical techniques for imaging.",Training;Machine learning algorithms;Reviews;Magnetic resonance imaging;Brain cancer;Machine learning;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489290,IEEE Conferences,,,,,,
Deep Learning-Based Self-Supervised Transfer Learning for Medical Image Classification,M. Z. Shaikh; S. Singh; N. Varshney; B. K. Saraswat,2024 2nd International Conference on Disruptive Technologies (ICDT),11-Apr-24,2024,"Self-supervised switch getting to know is a practical approach for the scientific image type. It includes using unlabeled statistics (from non-clinical images) to teach models to categorize medical photos with low attempts and excessive accuracy. This approach has been utilized in numerous duties, including clinical photo segmentation, computer-aided prognosis, and disease category. To summarize the research studies carried out in this discipline, a complete survey of self-supervised switch learning for scientific image type was performed in 2020. The survey evaluations numerous transfer mastering techniques often used in healthcare, which include area adaptation, multitasking studying, and zero/one-shot mastering. It additionally gives an in-intensity analysis of the modern-day challenges and capability solutions.",Surveys;Image segmentation;Transfer learning;Switches;Medical services;Multitasking;Prognostics and health management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488985,IEEE Conferences,,,,,,
Polyp Detection Using U-Net Neural Network Based Algorithm,B. K. Rai; D. Singh; A. Shukla,2024 2nd International Conference on Disruptive Technologies (ICDT),11-Apr-24,2024,"Polyp is an earlier stage of cancer development in gastro-intestinal tract. Despite the fact that numerous techniques for automatic segmentation and detection of polyps have been developed, it still remains an open problem. Examining color images captured by the board camera in endoscopy is contemplated the most reliable method for detectingdeveloping polyps. To automate this process, in this paper we present a modified U-Net Neural Network based algorithm. This algorithm is a binary classifier which labels the pixels as part of the polyp or not. This model can analyze the video of GItract frame by frame and produce the collection of frames with possibility of presence of polyp thus reducing the time required to process the GI-tract. Detecting, localizing, and segmenting polyps using Kvasir-SEG, an open-access dataset of colonoscopy images, we benchmark various modern day approaches in this work, assessing bothmethod speed and accuracy. This is attributed to outstanding performance of image classification compared to preceding techniques. The automatically identifying polyps serves to aid gastroenterologists during colonoscopies. While existing literature contains publications addressing the challenge of polyp detection, many of these systems remain confined toresearch settings and lack implementation for clinical use. Hence, we present the inaugural fully open-source automated polyp detection system, which not only outperforms the best- performing system documented in the literature but is also prepared for immediate clinical application.",Training;Image segmentation;Biological system modeling;Neural networks;Stacking;Colonoscopy;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488975,IEEE Conferences,,,,,,
Utilizing YOLOv5x for the Detection and Classification of Brain Tumors,M. Kumar; U. Pilania; T. Bhayana; S. Thakur,2024 2nd International Conference on Disruptive Technologies (ICDT),11-Apr-24,2024,"Development in image processing and Deep Learning (DL) techniques have many applications in the medical field. In this work, YOLOv5x (You Only Look Once) has been used for brain tumor detection and classification in Magnetic Resonance Imaging (MRI) images. YOLOv5x is well recognized for its accuracy and speed in real-time object detection. The proposed work comprises training the YOLOv5x model on the Brats and Roboflow dataset of brain MRI images. It is trained to identify and classify types of brain tumors. With the proper training process, the accuracy and speed of detecting irregularities in several tumor cases can be improved. Performance metrics like precision, recall, mAP (Mean Average Precision), and F1-Score are utilized to measure the robustness and reliability of the model. For validation of the proposed work, simulation results are computed using two datasets. The results of the work hold significant potential for timely recognition and diagnosis of brain tumors.",Training;Measurement;YOLO;Magnetic resonance imaging;Computational modeling;Simulation;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489507,IEEE Conferences,,,,,,
A Comprehensive Analysis of Implementing Convolutional Neural Networks (CNN) and InceptionV3 for Early-Lung Cancer Detection,H. R; S. T; V. S,2024 5th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI),11-Apr-24,2024,"This work focuses on the categorization and comparison of early-stage lung cell pictures that are malignant and non-cancerous utilizing Convolutional Neural Networks (CNNs) and Transfer Learning with the InceptionV3 model. Images of lung cells from four different categoriesâ€” adenocarcinoma, big cell carcinoma, normal, and squamous cell carcinomaâ€”make up the dataset used in this study. This study's main focus is on accurately classifying lung cell pictures into carcinogenic and non-cancerous groups. This is an important step towards the early diagnosis and detection of lung disorders. To improve the features and lower noise, the dataset underwent intensive preprocessing, which included adaptive histogram equalization, noise reduction using Gaussian blur, and histogram equalization. Two deep learning models were used for image classification: The InceptionV3 model and a modified CNN architecture. The models' performance was assessed on distinct validation and test sets after they had been trained on a labelled dataset. The models were evaluated using criteria such F1 score, area under the ROC curve (AUC), accuracy, precision, and recall. The findings show that while both models performed well in terms of accuracy, precision, recall, AUC, and F1 score, the InceptionV3 model performed better. Comprehensive insights into the algorithms' capacity to accurately categorize photos of malignant and non-cancerous lung cells are offered by the confusion matrix and classification report. By showcasing the efficacy of deep learning models in the classification of lung cell pictures, this study advances the field of medical image analysis. The results imply that transfer learning, especially in medical image analysis, can greatly improve classification job accuracy by using pre-trained models such as InceptionV3. According to the data, InceptionV3 has a testing accuracy of 0.9755 compared to CNN's 0.8955, making it a better fit for classifying lung tumors than classic CNN.",Deep learning;Adaptation models;Histograms;Analytical models;Microprocessors;Transfer learning;Lung,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493987,IEEE Conferences,,,,,,
A Comprehensive Systematic Review of YOLO for Medical Object Detection (2018 to 2023),M. G. Ragab; S. J. Abdulkadir; A. Muneer; A. Alqushaibi; E. H. Sumiea; R. Qureshi; S. M. Al-Selwi; H. Alhussian,IEEE Access,26-Apr-24,2024,"YOLO (You Only Look Once) is an extensively utilized object detection algorithm that has found applications in various medical object detection tasks. This has been accompanied by the emergence of numerous novel variants in recent years, such as YOLOv7 and YOLOv8. This study encompasses a systematic exploration of the PubMed database to identify peer-reviewed articles published between 2018 and 2023. The search procedure found 124 relevant studies that employed YOLO for diverse tasks including lesion detection, skin lesion classification, retinal abnormality identification, cardiac abnormality detection, brain tumor segmentation, and personal protective equipment detection. The findings demonstrated the effectiveness of YOLO in outperforming alternative existing methods for these tasks. However, the review also unveiled certain limitations, such as well-balanced and annotated datasets, and the high computational demands. To conclude, the review highlights the identified research gaps and proposes future directions for leveraging the potential of YOLO for medical object detection.",YOLO;Real-time systems;Medical services;Feature extraction;Reviews;Artificial intelligence;Deep learning;Object detection;Biomedical imaging;Surgery,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494845,IEEE Journals,,,,,,
LatentDR: Improving Model Generalization Through Sample-Aware Latent Degradation and Restoration,R. Liu; S. Khose; J. Xiao; L. Sathidevi; K. Ramnath; Z. Kira; E. L. Dyer,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),09-Apr-24,2024,"Despite significant advances in deep learning, models often struggle to generalize well to new, unseen domains, especially when training data is limited. To address this challenge, we propose a novel approach for distribution-aware latent augmentation that leverages the relationships across samples to guide the augmentation procedure. Our approach first degrades the samples stochastically in the latent space, mapping them to augmented labels, and then restores the samples from their corrupted versions during training. This process confuses the classifier in the degradation step and restores the overall class distribution of the original samples, promoting diverse intra-class/cross-domain variability. We extensively evaluate our approach on a diverse set of datasets and tasks, including domain generalization benchmarks and medical imaging datasets with strong domain shift, where we show our approach achieves significant improvements over existing methods for latent space augmentation. We further show that our method can be flexibly adapted to long-tail recognition tasks, demonstrating its versatility in building more generalizable models. https://github.com/nerdslab/LatentDR.",Degradation;Deep learning;Training;Adaptation models;Training data;Space mapping;Self-supervised learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483935,IEEE Conferences,,,,,,
Learning Quality Labels for Robust Image Classification,X. Wang; Z. Xu; D. Yang; L. Tam; H. Roth; D. Xu,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),09-Apr-24,2024,"Supervised learning paradigms largely benefit from the tremendous amount of annotated data. However, the quality of the annotations often varies among labelers. Multi-observer studies have been conducted to examine the annotation variances (by labeling the same data multiple times) to see how it affects critical applications like medical image analysis. In this paper, we demonstrate how multiple sets of annotations (either hand-labeled or algorithm-generated) can be utilized together and mutually benefit the learning of classification tasks. A scheme of learning-to-vote is introduced to sample quality label sets for each data entry on-the-fly during the training. Specifically, a label-sampling module is designed to achieve refined labels (weighted sum of attended ones) that benefit the model learning the most through additional back-propagations. We apply the learning-to-vote scheme on the classification task of a synthetic noisy CIFAR-10 to prove the concept and then demonstrate superior results (3-5% increase on average in multiple disease classification AUCs) on the chest x-ray images from a hospital-scale dataset (MIMIC-CXR) and hand-labeled dataset (OpenI) in comparison to regular training paradigms.",Training;Annotations;Supervised learning;MIMICs;Noise measurement;Labeling;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483956,IEEE Conferences,,,,,,
Are Natural Domain Foundation Models Useful for Medical Image Classification?,J. P. Huix; A. R. Ganeshan; J. F. Haslum; M. SÃ¶derberg; C. Matsoukas; K. Smith,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),09-Apr-24,2024,"The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely Sam, Seem, Dinov2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. Dinov2 consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.",Training;Adaptation models;Computer vision;Computational modeling;Transfer learning;Medical services;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483777,IEEE Conferences,,,,,,
GazeGNN: A Gaze-Guided Graph Neural Network for Chest X-ray Classification,B. Wang; H. Pan; A. Aboah; Z. Zhang; E. Keles; D. Torigian; B. Turkbey; E. Krupinski; J. Udupa; U. Bagci,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),09-Apr-24,2024,"Eye tracking research is important in computer vision because it can help us understand how humans interact with the visual world. Specifically for high-risk applications, such as in medical imaging, eye tracking can help us to comprehend how radiologists and other medical professionals search, analyze, and interpret images for diagnostic and clinical purposes. Hence, the application of eye tracking techniques in disease classification has become increasingly popular in recent years. Contemporary works usually transform gaze information collected by eye tracking devices into visual attention maps (VAMs) to supervise the learning process. However, this is a time-consuming preprocessing step, which stops us from applying eye tracking to radiologistsâ€™ daily work. To solve this problem, we propose a novel gaze-guided graph neural network (GNN), GazeGNN, to leverage raw eye-gaze data without being converted into VAMs. In GazeGNN, to directly integrate eye gaze into image classification, we create a unified representation graph that models both images and gaze pattern information. With this benefit, we develop a real-time, real-world, end-to-end disease classification algorithm for the first time in the literature. This achievement demonstrates the practicality and feasibility of integrating real-time eye tracking techniques into the daily work of radiologists. To our best knowledge, GazeGNN is the first work that adopts GNN to integrate image and eye-gaze data. Our experiments on the public chest X-ray dataset show that our proposed method exhibits the best classification performance compared to existing methods. The code is available at https://github.com/ukaukaaaa/GazeGNN.",Training;Visualization;Computer vision;Gaze tracking;Transforms;Graph neural networks;Real-time systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484310,IEEE Conferences,,,,,,
PHG-Net: Persistent Homology Guided Medical Image Classification*,Y. Peng; H. Wang; M. Sonka; D. Z. Chen,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),09-Apr-24,2024,"Modern deep neural networks have achieved great successes in medical image analysis. However, the features captured by convolutional neural networks (CNNs) or Transformers tend to be optimized for pixel intensities and neglect key anatomical structures such as connected components and loops. In this paper, we propose a persistent homology guided approach (PHG-Net) that explores topological features of objects for medical image classification. For an input image, we first compute its cubical persistence diagram and extract topological features into a vector representation using a small neural network (called the PH module). The extracted topological features are then incorporated into the feature map generated by CNN or Transformer for feature fusion. The PH module is lightweight and capable of integrating topological features into any CNN or Transformer architectures in an end-to-end fashion. We evaluate our PHG-Net on three public datasets and demonstrate its considerable improvements on the target classification tasks over state-of-the-art methods.",Costs;Network topology;Feature extraction;Transformers;Vectors;Topology;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484262,IEEE Conferences,,,,,,
Domain Generalization with Correlated Style Uncertainty,Z. Zhang; B. Wang; D. Jha; U. Demir; U. Bagci,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),09-Apr-24,2024,"Domain generalization (DG) approaches intend to extract domain invariant features that can lead to a more robust deep learning model. In this regard, style augmentation is a strong DG method taking advantage of instance-specific feature statistics containing informative style characteristics to synthetic novel domains. While it is one of the state-of-the-art methods, prior works on style augmentation have either disregarded the interdependence amongst distinct feature channels or have solely constrained style augmentation to linear interpolation. To address these research gaps, in this work, we introduce a novel augmentation approach, named Correlated Style Uncertainty (CSU), surpassing the limitations of linear interpolation in style statistic space and simultaneously preserving vital correlation information. Our methodâ€™s efficacy is established through extensive experimentation on diverse cross-domain computer vision and medical imaging classification tasks: PACS, Office-Home, and Camelyon17 datasets, and the Duke-Market1501 instance retrieval task. The results showcase a remarkable improvement margin over existing state-of-the-art techniques. The source code is available https://github.com/freshman97/CSU.",Deep learning;Interpolation;Computer vision;Uncertainty;Correlation;Computational modeling;Source coding,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484431,IEEE Conferences,,,,,,
Prototype Learning for Explainable Brain Age Prediction,L. S. Hesse; N. K. Dinsdale; A. I. L. Namburete,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),09-Apr-24,2024,"The lack of explainability of deep learning models limits the adoption of such models in clinical practice. Prototype-based models can provide inherent explainable predictions, but these have predominantly been designed for classification tasks, despite many important tasks in medical imaging being continuous regression problems. Therefore, in this work, we present ExPeRT: an explainable prototype-based model specifically designed for regression tasks. Our proposed model makes a sample prediction from the distances to a set of learned prototypes in latent space, using a weighted mean of prototype labels. The distances in latent space are regularized to be relative to label differences, and each of the prototypes can be visualized as a sample from the training set. The image-level distances are further constructed from patch-level distances, in which the patches of both images are structurally matched using optimal transport. This thus provides an example-based explanation with patch-level detail at inference time. We demonstrate our proposed model for brain age prediction on two imaging datasets: adult MR and fetal ultrasound. Our approach achieved state-of-the-art prediction performance while providing insight into the modelâ€™s reasoning process.",Training;Measurement;Visualization;Ultrasonic imaging;Magnetic resonance imaging;Prototypes;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484288,IEEE Conferences,,,,,,
Generalizing to Unseen Domains in Diabetic Retinopathy Classification,C. J. Galappaththige; G. Kuruppu; M. H. Khan,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),09-Apr-24,2024,"Diabetic retinopathy (DR) is caused by long-standing diabetes and is among the fifth leading cause for visual impairment. The prospects of early diagnosis and treatment could be helpful in curing the disease, however, the detection procedure is rather challenging and mostly tedious. Therefore, automated diabetic retinopathy classification using deep learning techniques has gained interest in the medical imaging community. Akin to several other real-world applications of deep learning, the typical assumption of i.i.d data is also violated in DR classification that relies on deep learning. Therefore, developing DR classification methods robust to unseen distributions is of great value. In this paper, we study the problem of generalizing a model to unseen distributions or domains (a.k.a domain generalization) in DR classification. To this end, we propose a simple and effective domain generalization (DG) approach that achieves self-distillation in vision transformers (ViT) via a novel prediction softening mechanism. This prediction softening is an adaptive convex combination of one-hot labels with the modelâ€™s own knowledge. We perform extensive experiments on challenging open-source DR classification datasets under both multi-source and more challenging single-source DG settings with three different ViT backbones to establish the efficacy and applicability of our approach against competing methods. For the first time, we report the performance of several state-of-the-art domain generalization (DG) methods on open-source DR classification datasets after conducting thorough experiments. Finally, our method is also capable of delivering improved calibration performance than other methods, showing its suitability for safety-critical applications, including health-care. We hope that our contributions would instigate more DG research across the medical imaging community. Code is available at github.com/Chumsy0725/SPSD-ViT.",Deep learning;Diabetic retinopathy;Adaptation models;Visual impairment;Predictive models;Transformers;Calibration,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483961,IEEE Conferences,,,,,,
Innovative Diagnosis of Dental Diseases Using YOLO V8 Deep Learning Model,M. Razaghi; H. E. Komleh; F. Dehghani; Z. Shahidi,2024 13th Iranian/3rd International Machine Vision and Image Processing Conference (MVIP),09-Apr-24,2024,"The diagnosis and identification of dental problems pose significant challenges. Traditionally, dental disease diagnosis was a manual and time-consuming process, requiring dentists to meticulously examine and evaluate the condition. The integration of artificial intelligence (AI) represents a transformative approach to aid in medical imaging diagnostics. Specifically, leveraging AI for diagnosing dental issues entails the automatic localization of lesions. In this study, the Yolo V8 deep learning model is employed to develop an innovative method for the detection and categorization of common dental problems. The primary objective of this approach is to establish a comprehensive database comprising two distinct categories of dental X-ray images: BiteWing X-ray Images and Orthopantomography X-ray (OPG). These categories aim to facilitate the diagnosis and classification of various dental diseases. The results of the experiments showed that the best performance in training YOLOv8m was achieved with mAP of 71.6%, recall of 90%, and precision of 90%.",Deep learning;YOLO;Training;Databases;Data models;Dentistry;Artificial intelligence;Task analysis;X-ray imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10491172,IEEE Conferences,,,,,,
Dual-Channel Prototype Network for Few-Shot Pathology Image Classification,H. Quan; X. Li; D. Hu; T. Nan; X. Cui,IEEE Journal of Biomedical and Health Informatics,02-Jul-24,2024,"In the field of pathology, the scarcity of certain diseases and the difficulty of annotating images hinder the development of large, high-quality datasets, which in turn affects the advancement of deep learning-assisted diagnostics. Few-shot learning has demonstrated unique advantages in modeling tasks with limited data, yet explorations of this method in the field of pathology remain in the early stages. To address this issue, we present a dual-channel prototype network (DCPN), a novel few-shot learning approach for efficiently classifying pathology images with limited data. The DCPN leverages self-supervised learning to extend the pyramid vision transformer (PVT) to few-shot classification tasks and combines it with a convolutional neural network to construct a dual-channel network for extracting multi-scale, high-precision pathological features, thereby substantially enhancing the generalizability of prototype representations. Additionally, we design a soft voting classifier based on multi-scale features to further augment the discriminative power of the model in complex pathology image classification tasks. We constructed three few-shot classification tasks with varying degrees of domain shift using three publicly available pathological datasetsâ€”CRCTP, NCTCRC, and LC25000â€”to emulate real-world clinical scenarios. The results demonstrated that the DCPN outperformed the prototypical network across all metrics, achieving the highest accuracies in same-domain tasksâ€”70.86% for 1-shot, 82.57% for 5-shot, and 85.2% for 10-shot setupsâ€”corresponding to improvements of 5.51%, 5.72%, and 6.81%, respectively, over the prototypical network. Notably, in the same-domain 10-shot setting, the accuracy of the DCPN (85.2%) surpassed that of the PVT-based supervised learning model (85.15%), confirming its potential to diagnose rare diseases within few-shot learning frameworks.",Task analysis;Pathology;Prototypes;Adaptation models;Metalearning;Transformers;Measurement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494883,IEEE Journals,,,,,,
Research on Single-Object Tracking Algorithm for Thyroid Nodules Based on NanoTrack,X. Li; W. Liu; Y. Qu; S. Zhang,"2024 IEEE 3rd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)",08-Apr-24,2024,"With the rapid development of technology in the medical industry, hospitals have begun to introduce AI medical devices for auxiliary diagnosis. Among these, ultrasound AI medical imaging analysis technology has started to play a very important auxiliary role in the daily case diagnosis of doctors. This paper will use deep learning neural network technology to build a single-object tracking deep learning neural network, applied in the field of ultrasound imaging, to achieve single-object tracking of thyroid nodules. The algorithm will use three networks in a cascade. In the first frame of the video, the backbone network will extract deep features of the selected area of interest as the standard template. Starting from the second frame, the areas of interest in the image and the expanded regions of the image are sent into two different scale base networks for feature extraction. Finally, the features extracted by the two different base networks are fused and sent into a cascade network for classification and bounding box regression. This method locates the target by searching for the area that best matches the target template in each frame. It is a lightweight, high-speed tracking network. Experiments are deployed on the embedded device Jetson Nano, with a processing speed of up to 625 fps per second.",Deep learning;Ultrasonic imaging;Target tracking;Neural networks;Feature extraction;Inference algorithms;Nanoscale devices,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10485907,IEEE Conferences,,,,,,
Deep Reinforcement Learning in Healthcare and Bio-Medical Applications,P. Thakur; N. S. Talwandi,"2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)",08-Apr-24,2024,"Deep reinforcement learning (DRL), which learns a set of behaviors that maximize the projected reward, combines the representational power of deep neural networks with the reinforcement learning paradigm. DRL holds great promise for the future of healthcare and medicine, according to recent studies.An overview of the research on DRL in medical imaging is provided in this article. We start with a comprehensive DRL course that covers both the most recent model-based and model-free approaches. The tasks covered in the next section of this article are loosely divided into three main categories: (i) parametric medical image analysis tasks like landmark detection,object/lesion detection, registration, and view plane localization; optimization tasks like hyperparameter tuning, augmentation strategy selection, and neural architecture search; and (iii) other applications like surgical gesture segmentation, person tracking, and perso The study finishes with thoughts of potential future directions.",Location awareness;Image segmentation;Image analysis;Surgery;Medical services;Deep reinforcement learning;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486549,IEEE Conferences,,,,,,
Adversarial Attack and Defense Mechanisms in Medical Imaging: A Comprehensive Review,M. Surekha; A. K. Sagar; V. Khemchandani,"2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)",08-Apr-24,2024,"Medical imaging is essential in modern healthcare, allowing for accurate diagnoses and treatment planning. The practice of machine learning algorithms into clinical imaging uses has significantly enhanced diagnostic abilities. In contrast, as technology has forward-looking, new problems have emerged, main among them existence adversarial assaults. This complete research study investigates the background information of adversarial attacks and its rising defense measures on medical image model. Adversarial assaults involve intelligent alterations of input data to change machine learning algorithms, possibly important to misdiagnosis or inaccurate clinical diagnosis. This paper thoroughly analyses many adversarial assault practices precisely tailored for medical imaging based on division, classification and rebuilding of clinical image with different defensive tactic. Also, the research study provides the better path for collect rising counter measures on adversarial attacks posed by threats. These protective measures contain different types of proposals. For example, adversarial practices, input pre-processing and algorithm understandable strategies.",Threat modeling;Machine learning algorithms;Reviews;Computer architecture;Switches;Reliability engineering;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486235,IEEE Conferences,,,,,,
Empirical Analysis on Breast Cancer Datasets with Machine Learning,A. Singh; K. S. Kaswan,"2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)",08-Apr-24,2024,"Breast cancer is a global health problem that requires early detection and precise diagnosis for better patient outcomes. Recent advances in machine learning and medical imaging, along with large datasets, have transformed breast cancer research. This in-depth examination digs into these datasets and their various applications in detection and diagnosis. While existing research focus on breast cancer, a thorough examination of available datasets is still required. This review tries to close the gap by thoroughly investigating datasets such as WDBC, BreakHis, Mammography, Ultrasound, and Thermography. It goes beyond technical specifics to investigate their diverse applications, ranging from feature-based classification to image-based diagnostics. The major goal is to better understand how databases help with breast cancer detection and diagnosis. This study demonstrates that each dataset has a distinct and important role.",Ultrasonic imaging;Databases;Reviews;Histopathology;Merging;Machine learning;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486606,IEEE Conferences,,,,,,
Revolutionizing Brain Tumor Diagnosis: Harnessing Convolutional Neural Networks for Enhanced Prediction and Classification,A. K. Singh; A. Mishra,"2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)",08-Apr-24,2024,"Diagnosing brain tumors correctly and on time is the only way of effectively planning suitable treatment. Advances made recently in the field of medical imaging and artificial intelligence are leading to better methods of diagnosis. This paper is going to explore how Convolutional Neural Networks (CNNs) can be used to predict and classify tumors in the brain. The challenges of diagnosing brain tumors, the importance of CNNs in medical image processing, a detailed survey about recent studies and techniques used in this area would also be covered. Moreover, we propose an innovative CNN architecture designed for brain tumor detection by using modern developments in deep learning.That said, a two stage analysis methodology was followed with a total focus towards prevalent challenges within Image Restoration and Image Enhancement. As such, this study introduces new techniques that address these issues effectively.This study proposes an approach utilizing the power of Convolutional Neural Networks (CNNs). The novel CNN classification technique is applied, leveraging the Python and TensorFlow environment.The findings underscore the potential of this approach in revolutionizing brain tumor detection. By providing detailed insights into the strengths and limitations of the proposed model, this research contributes significantly to the field.The paper concludes by emphasizing the transformative impact of this research, opening avenues for further exploration and innovation in this critical domain.",Surveys;Technological innovation;Computational modeling;Computer architecture;Communications technology;Planning;Image restoration,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486347,IEEE Conferences,,,,,,
A Novel DL Structure for Brain Tumor Identification Using MRI Images,S. Kollem; P. Harika; J. Vignesh; P. Sairam; A. Ramakanth; S. Peddakrishna; S. Samala; C. R. Prasad,"2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)",08-Apr-24,2024,"The multimodal MRI scans described in this article are used to categorize brain tumors based on their location and size. Brain tumors need to be categorized in order to assess the tumors and choose the appropriate course of treatment for each class. Many different imaging methods are used to detect brain tumors. However, because MRI does not use ionizing radiation and generates better images, it is commonly used. Using deep learning (DL), a branch of machine learning has recently demonstrated impressive results, particularly in segmentation and classifiable tasks. This paper proposes a convolutional neural network-based deep learning model (DL) that uses transfer learning and EfficientNet to classify various kinds of brain cancers using publically accessible datasets. The first divides cancers into three categories: glioma, meningioma, and pituitary tumor. Compared to conventional deep learning techniques, the suggested approach produces superior results. The Python platform can be used to complete the task.",Deep learning;Image segmentation;Sensitivity;Ionizing radiation;Magnetic resonance imaging;Transfer learning;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486662,IEEE Conferences,,,,,,
Improving Brain Tumor Detection and Classification Using Intelligent Methods,B. S. Liya; S. Saswath; R. Sumanth; B. V. Sivapriyadarsan,"2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)",08-Apr-24,2024,"This study concerns major advances in the field of neuro-oncology by developing a highly intelligent system capable of classifying and distinguishing brain tumors. This investigation is highly persuasive in its conclusions that reveal the supreme performance of a system. The system has the capability to attain an accuracy score of 92.5%, sensitivity rate as high at 89.7% specificity and precision up by 94.2%. F1 Score is recorded a whooping influential result with value pegged from side bracket range of about above points nine for one four while AUC-ROC present figure earned rating in around Significantly, the latter intelligent system extends beyond classical methods to achieve a higher level of accuracy while maintaining compatibility with neuro-oncology. In this research, it is omitted the significance and depth of intervention which has an intelligent system that will do on a field around medical imaging braintumor diagnosis. More importantly, the results suggest that new intelligent schemas go beyond a simple addition to existing initiatives and highlight radically transforming medical imaging approaches. The research demonstrates that using advanced technologies not only enhances the accuracy of diagnoses but also increases greatly on effectiveness in performing neurological diagnostic procedures. The prominence that is attributed to technological innovation in this case, highlights its centrality at establishing better patient outcomes within the field of neurology. The provided outcome not only proves that the proposed system is able to identify a benign brain tumor, but also calls for adopting other intelligent approaches in medical imaging because they may contribute positively towards neuro-oncology diagnostics.",Technological innovation;Neurology;Sensitivity;Image processing;Decision making;Receivers;Intelligent systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486806,IEEE Conferences,,,,,,
Machine-to-Machine Transfer Function in Deep Learning-Based Quantitative Ultrasound,U. Soylu; M. L. Oelze,"IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control",10-Jun-24,2024,"A transfer function approach was recently demonstrated to mitigate data mismatches at the acquisition level for a single ultrasound scanner in deep learning (DL)-based quantitative ultrasound (QUS). As a natural progression, we further investigate the transfer function approach and introduce a machine-to-machine (M2M) transfer function, which possesses the ability to mitigate data mismatches at a machine level. This ability opens the door to unprecedented opportunities for reducing DL model development costs, enabling the combination of data from multiple sources or scanners, or facilitating the transfer of DL models between machines. We tested the proposed method utilizing a SonixOne machine and a Verasonics machine with an L9-4 array and an L11-5 array. We conducted two types of acquisitions to obtain calibration data: stable and free-hand, using two different calibration phantoms. Without the proposed method, the mean classification accuracy when applying a model on data acquired from one system to data acquired from another system was 50%, and the mean average area under the receiver operator characteristic (ROC) curve (AUC) was 0.405. With the proposed method, mean accuracy increased to 99%, and the AUC rose to the 0.999. Additional observations include the choice of the calibration phantom led to statistically significant changes in the performance of the proposed method. Moreover, robust implementation inspired by Wiener filtering provided an effective method for transferring the domain from one machine to another machine, and it can succeed using just a single calibration view. Lastly, the proposed method proved effective when a different transducer was used in the test machine.",Transfer functions;Calibration;Ultrasonic imaging;Imaging;Machine-to-machine communications;Training;Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10491402,IEEE Journals,,,,,,
A Novel Transfer Learning Framework for Multimodal Skin Lesion Analysis,S. Remya; T. Anjali; V. Sugumaran,IEEE Access,15-Apr-24,2024,"Skin lesion classification is a pivotal process in dermatology, enabling the early detection and precise diagnosis of skin diseases, leading to improved patient outcomes. Deep learning has shown great potential for this task by leveraging its ability to learn complex patterns from images. However, diagnostic accuracy is compromised by exclusive reliance on single-modality images. This research work proposes an innovative framework that unifies a Vision Transformer model with transfer learning, channel attention mechanism, and ROI for the accurate detection of skin conditions, including skin cancer. The proposed approach blends computer vision and machine-learning techniques, leveraging a comprehensive dataset comprised of macroscopic dermoscopic images, appended with patient metadata. Compared with conventional techniques, the proposed methodology exhibits significant improvements in various parameters, including sensitivity, specificity, and precision. Moreover, it demonstrates outstanding performance in real-world datasets, reinforcing its potential for clinical implementation. With a remarkable accuracy of 99%, the method outperforms existing approaches. Overall, this investigation underscores the transformative impact of deep learning and multimodal data analysis in the dermoscopic domain, projecting substantial headway into the field of skin lesion analytic diagnosis.",Skin;Lesions;Transfer learning;Deep learning;Dermatology;Task analysis;Skin cancer;Classification algorithms;Data analysis;Machine vision,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10491261,IEEE Journals,,,,,,
Detecting Colon and Lung Cancer through Deep Learning CNN Model,A. Kumar; L. Nelson; V. S. Venu,"2024 2nd International Conference on Computer, Communication and Control (IC4)",04-Apr-24,2024,"Lung and colon cancers pose substantial global health challenges, being recognised as prevalent and potentially life-threatening tumours. This research focuses on the development and evaluation of a DL model that utilises Convolutional Neural Networks to classify images of lung and colon cancer into five distinct categories. By making use of a dataset consisting of 25,000 medical images, the CNN model was subjected to a rigorous 40-epoch training process with a batch size of 40. The noteworthy outcome of this investigation is the remarkable achievement of a 99% accuracy rate by the CNN model, showcasing the transformative potential of DL in medical image analysis. The implications of such high accuracy extend to early diagnosis, therapy planning, and overall patient care in the medical field. This paper underscores the versatility of CNN architecture in healthcare applications, emphasising its significance in addressing complex medical classification tasks. The importance of automation and accuracy in clinical settings is highlighted, urging further research and validation of deep learning methodologies. The study demonstrates the effectiveness of CNN models in cancer classification, thereby facilitating the advancement of medical image processing systems that are more precise and effective. The end result will be advantageous for healthcare practitioners and patients equally.",Deep learning;Training;Lung cancer;Lung;Convolutional neural networks;Biomedical image processing;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486421,IEEE Conferences,,,,,,
Enhancing Kidney Disease Classification through Transfer Learning with VGG16,A. Kumar; L. Nelson; V. S. Venu,"2024 2nd International Conference on Computer, Communication and Control (IC4)",04-Apr-24,2024,"This work introduces a thorough methodology for classifying kidney disease with deep learning techniques, specifically highlighting the well-recognized VGG16 model. The dataset used in this study was obtained from Kaggle and consists of a wide range of medical images depicting normal instances of kidney cysts, tumours, and stones. Each image has been standardised to a resolution of 150x150 pixels. The dataset is systematically partitioned into training and validation sets and then subjected to thorough preprocessing, normalisation, and augmentation techniques in order to optimise the training of the model. The utilisation of the VGG16 model architecture is implemented, incorporating base and intermediate convolutional layers to facilitate the recognition and extraction of fundamental features, complemented by fully connected layers to enable decisive classification. In order to optimise the VGG16 model for the purpose of classifying kidney diseases, a customised classification layer is incorporated, hence improving its effectiveness in the field of medical image analysis. This work aims to enhance the VGG16 modelâ€™s accuracy to 98% through the adjustment of pre-existing weights, the establishment of non-trainable weights for the purpose of universal feature recognition, and the customisation of the model to effectively classify kidney conditions by accounting for their unique characteristics. The objective of this project is to create a highly accurate and dependable model for promptly detecting and categorising kidney problems. This research will make a substantial contribution to enhancing patient care in the fields of nephrology and urology.",Training;Deep learning;Nephrology;Medical services;Feature extraction;Kidney;Urology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486470,IEEE Conferences,,,,,,
"Evaluation of Deep Learning Models on Alzheimer's MRI Dataset: AD-VGG16, AD-Resnet50, and AD-2DCNN",L. M. Elnaghi; Y. M. Eltariny,2024 6th International Conference on Computing and Informatics (ICCI),02-Apr-24,2024,"Alzheimer's disease accounts for 60-70% of instances of dementia. It is a neurological illness that often begins slowly, progresses, and worsens over time. The most typical early symptom is trouble memorizing recent events. As the illness progresses, symptoms may include confusion, difficulty speaking, and difficulty doing daily tasks. Using MRI medical images, previous studies have considered the application of deep learning to Alzheimer's disease, but this study introduced a novel aspect by simultaneously evaluating AD-VGG16, AD-Resnet50, and AD-2DCNN. Synergistic evaluation of these models provides a unique perspective to improve diagnostic accuracy. We used various preprocessing techniques, including pixel normalization and data augmentation using a well-known Alzheimer's MRI dataset, such as the Alzheimer MRI Preprocessed Dataset from Kaggle, to evaluate the models' precision, area under the curve (AUC), accuracy, and recall. Our analysis reveals distinct performance levels among the models. AD-VGG16, AD-Resnet50, and the 2D CNN were evaluated with resulting accuracies of 93.45%, 79.43%, and 98.28% on the test set, respectively. This research discusses a comparative evaluation of three frequently used deep learning models for the classification of Alzheimer's disease.",Deep learning;Analytical models;Sensitivity;Magnetic resonance imaging;Data models;Pattern recognition;Alzheimer's disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10485046,IEEE Conferences,,,,,,
Optimizing Medical Image Analysis: Leveraging Efficient Hardware and AI Algorithms,S. Dolai; E. Mitra,2024 37th International Conference on VLSI Design and 2024 23rd International Conference on Embedded Systems (VLSID),02-Apr-24,2024,"The surging use of medical AI algorithms and their hardware integration is transforming healthcare by improving non-invasive medical analysis with early disease detection, advanced segmentation, and classification. However, realizing comprehensive and accurate medical analysis through efficient AI-based tools necessitates a fundamental requirement â€” extensive multimodal data for training deep learning models. Handling this extensive data volume demands significant hardware resources, including multi-node training, to address the substantial computational requirements essential for accelerating model development. Hence, the challenge is two-fold: Achieving high accuracy while upholding a computationally inexpensive solution. To navigate this challenge, we propose a novel and efficient solution: a lightweight predictive tool for medical image classification developed by combining a Radiomics-based Random Forest model with MobileViT transformer, tailored for mobile applications. This approach ensures enhanced accuracy and reproducibility along with hardware flexibility. Our proposed method is exemplified by its superior performance in the BraTS2021 challenge, surpassing current state-of-the-art models with the best AUROC of 0.64 and 0.63 on both public and private test datasets respectively. The success of our approach highlights the potential of hybrid models in diverse medical applications beyond image classification.",Training;Computational modeling;Medical services;Very large scale integration;Hardware;Data models;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483298,IEEE Conferences,,,,,,
Deep CNN-Based MRI Imaging for Brain Tumor Detection and Classification,P. V. Kale; A. B. Gadicha,"2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)",02-Apr-24,2024,"Depression and anxiety disorders are two psychological problems that can be significantly triggered by brain tumors. Detecting various disorders requires patients to receive assistance from healthcare image analytics. Accurate tumor identification facilitates the process of recovering from a brain tumor. Classifying the types of brain tumors is a crucial factor that relies on the doctor's competence and understanding. To assist doctors, a sophisticated method for identifying and categorizing brain tumors is required. This work is novel as it employs a deep CNN approach to classify various types of brain tumors. For a speedy and effective recovery, analysis and tumor categorization are crucial, and MRI brain image analytics via a Deep CNN is producing exceptional results in this area. The Deep CNN classifies the various tumor types and trains the data accordingly. The classification of brain tumors into four distinct primary subtypes, namely Glioma, Meningioma, Pituitary, and Healthy, has been achieved through a proposed approach. The proposed model outperforms the most recent approaches in Brain tumors were classified and categorized with an accuracy rate of 97.85%. This suggested solution holds the potential to provide valuable clinical aid to the healthcare industry.",Industries;Computer science;Electric potential;Magnetic resonance imaging;Anxiety disorders;Psychology;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482056,IEEE Conferences,,,,,,
DCGAN for Synthetic Data Augmentation of Cervical Cancer for Improved Cervical Cancer Classification,M. Ali; M. Ali; M. Javed,"2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)",02-Apr-24,2024,"Medical diagnosis and treatment are greatly aided by biomedical image analysis. Deep learning model training is difficult, nevertheless, due to the scarcity of labelled medical images. For creating synthetic biological images to supplement the training data, Generative Adversarial Networks (GANs) have shown good promise. In this research paper, we focus on the application of GANs for biomedical image augmentation. Specifically, we investigate and compare the performance of a prominent GAN architecture: Deep Convolutional GAN (DCGAN) is a variant of GAN specifically designed for image generation tasks. We assess the generated images based on their quality, diversity, and preservation of biomedical aspects using different evaluation metrics. We also used a classification process to compare the classifier on real and synthetic augmented data. Our test findings show that DCGAN is capable of producing realistic synthetic biomedical images. The results of this work advance knowledge of GANs for biomedical image enhancement and offer guidance on choosing the DCGAN designs for tasks requiring medical image analysis. Researchers and practitioners can increase the diversity and quantity of training data by utilizing GAN-based augmentation strategies, which will enhance the performance and generalization of deep learning models in biomedical applications.The primary objective of this study is to evaluate the effectiveness of DCGAN in generating synthetic microscopic biomedical images of cervical cancer that can augment the training data for deep learning classification models. We aim to assess the quality, diversity, and preservation of biomedical features in the generated images.The structure of this study is as follows: we begin by brief introduction of Cervical cancer, GAN in general and DCGAN. Section 2: lays down review of the related literature. Section 3: presents the methodology of the study. Section 4: presents the results and discussion from the study. Section 5: summaries the finding and concludes with prospective future applications.",Deep learning;Training;Biological system modeling;Training data;Generative adversarial networks;Data models;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482312,IEEE Conferences,,,,,,
Unleashing the Power of Deep Convolutional Neural Networks in a Stacked Ensemble Model for Precise Paediatric Pneumonia Diagnosis from Chest Radiographs,S. J. Gupta; R. Kumar; B. P. Singh; S. Bansal; H. Kaur; M. C. Pandey,"2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)",02-Apr-24,2024,"Pneumonia is a global health concern caused by various pathogens and its most prevalent impact is in Sub-Saharan Asia and South Asia. The Reports indicate that highest mortality is under the age of 5 years. The mortality specially in developing countries is attributed to delayed diagnosis and cost involved in conducting the tests. Chest X-ray has become the preferable diagnosis tool for pneumonia detection. But the doctor patient ratio in developing nations is also of concern. Automated diagnosis to detect the disease will aid in addressing these challenges. Deep learning based automated techniquesto detect pneumonia from Chest X-ray images have become an important advent in the field. However, the accuracy of these techniques is still in the exploration phase. We have proposed a stacked ensemble learning classifier using transfer learning to detect pneumonia. Our designed classified is based on DenseNet201, MobileNetV2 and InceptionResNetV2 pretrained classifiers. The accuracy of the pretrained individual model is 91%, 90% and 90% respectively but our designed stacked classifier is with 94%. The performance gain suggests its promising application in the field.",Pediatrics;Pathogens;Pneumonia;Asia;Transfer learning;Medical services;Performance gain,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482262,IEEE Conferences,,,,,,
"Multi-Class Classification of Brain Tumours: Leveraging VGG, InceptionV3, and DenseNet201 Transfer Learning",N. Fathima; P. Kumar,"2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)",02-Apr-24,2024,"In this study, brain tumour is classified on the Brain MRI(bt_dataset) dataset using Visual Geometry Group16(VGG), Inception V3 (Inv3), and DenseNet201(DN201) deep learning architectures. In this process, the models were developed for each architecture using Transfer learning, results were achieved, and the model with a minimum loss was introduced. The dataset used for the study is taken from the Kaggle website. The classification process was applied repeatedly on the bt_dataset, and the multi-class classification or prediction with the minimum error rate was aimed at the established models. In the study process, initially the bt_dataset was collected, prepared, pre-processed and visualized. Models were developed using the Jupiter Notebook editor via the Google Colaboratory. Then, the models were further examined and the most successful architecture was selected. As a result, according to the prediction/classification models, it was observed that the VGG architecture has achieved better results than the Inv3 model and the DN architecture.",Visualization;Protocols;Transfer learning;Computer architecture;Predictive models;Brain modeling;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482369,IEEE Conferences,,,,,,
Deep Radiomics: A Pictureâ€™s Worth a Thousand Words - A Review,S. Meraj; A. Shah; A. Ismail; T. M. Sembokb; S. Shadab; S. Aftab; S. Nageena; N. Khan; K. Nusratullah,2024 IEEE 1st Karachi Section Humanitarian Technology Conference (KHI-HTC),02-Apr-24,2024,"In recent decades, the prevalence of diseases has witnessed a notable surge. The increasing health challenges have led to a greater use of advanced diagnostic machines for detecting and understanding these disease conditions. The diagnostic modalities, such as X-rays, CT scans, MRI scans, and nuclear medicine imaging, generate a wealth of interpretative data through medical images. Deep Radiomics aims to improve the efficiency and accuracy of image interpretation by automating the process of feature extraction, allowing for more comprehensive insights for advancing personalized medicine and improving outcomes in medical imaging. Enhance disease detection, diagnosis, and subsequent treatment strategies.",Training;Nuclear medicine;Reviews;Precision medicine;X-rays;Planning;Surges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482359,IEEE Conferences,,,,,,
2D T1-Weighted CE MRI Images Classification with Hybrid Channel Attention Block enhanced VGG-16 and MobileNet,M. J. Fareed; C. Pluempitiwiriyawej; G. Murtaza,2024 IEEE 1st Karachi Section Humanitarian Technology Conference (KHI-HTC),02-Apr-24,2024,"Motivated by the imperative need for precise and swift classification of brain tumors depicted in MRI images, this study unveils a novel classification network tailored explicitly for this task. Existing lightweight methodologies, although explored extensively, have fallen short of achieving the accuracy benchmarks attained by the model. The approach underwent rigorous testing on a challenging 2D T1-weighted CE-MRI dataset housing three distinct types of brain tumors: Meningioma, Glioma, and Pituitary. Recognizing the susceptibility of models to overfitting during training, introduced was a normalized combined channel and spatial attention mechanism, strategically incorporated to serve as an effective regularizer. Comparative analysis against state-of-the-art methodologies on this dataset highlights a notable performance boost of 2.12 percentage points achieved by integrating this attention mechanism into a base network. Furthermore, the fusion of the model with the pre-trained VGG16 network and GoogleNet into an ensemble demonstrates superior accuracy; however, this amalgamation comes at the expense of execution speed.",Training;Technological innovation;Magnetic resonance imaging;Transfer learning;Brain modeling;Robustness;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482091,IEEE Conferences,,,,,,
Fairness Meets Cross-Domain Learning: A Benchmark of Models and Metrics,L. Iurada; S. Bucci; T. M. Hospedales; T. Tommasi,IEEE Access,05-Apr-24,2024,"Deep learning-based recognition systems are deployed at scale for real-world applications that inevitably involve our social life. Although of great support when making complex decisions, they might capture spurious data correlations and leverage sensitive attributes (e.g., age, gender, ethnicity). How to factor out this information while maintaining high performance is a problem with several open questions, many of which are shared with those of the domain adaptation and generalization literature which aims at avoiding visual domain biases. In this work, we propose an in-depth study of the relationship between cross-domain learning (CD) and model fairness, by experimentally evaluating 14 CD approaches together with 3 state-of-the-art fairness algorithms on 5 datasets of faces and medical images spanning several demographic groups. We consider attribute classification and landmark detection tasks: the latter is introduced here for the first time in the fairness literature, showing how keypoint localization may be affected by sensitive attribute biases. To assess the analyzed methods, we adopt widely used evaluation metrics while also presenting their limits with a detailed review. Moreover, we propose a new Harmonic Fairness (HF) score that can ease unfairness mitigation model comparisons. Overall, our work shows how CD approaches can outperform state-of-the-art fairness algorithms and defines a framework with dataset and metrics as well as a code suite to pave the way for a more systematic analysis of fairness problems in computer vision (Code available at: https://github.com/iurada/fairness_crossdomain).",Measurement;Task analysis;Hafnium;Biological system modeling;Benchmark testing;Visualization;Trust management;Face recognition;Detection algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487929,IEEE Journals,,,,,,
Enhancing Early Lung Cancer Detection using Deep Learning Techniques,H. Badaya; S. Mishra; S. K. Satapathy; S. -B. Cho; P. K. Mallick,2024 International Conference on Emerging Systems and Intelligent Computing (ESIC),01-Apr-24,2024,"Lung cancer is one of the most deadly and persistent diseases in the world today, making early identification of this disease crucial. This research focuses on the integration of cutting-edge deep learning technologies, namely Convolutional Neural Networks (CNNs), with modern medical insights to improve lung tumour detection using plain chest X-rays (CXR). Using a novel technique to transfer learning, we optimise our modelâ€™s effectiveness for better diagnostic results in the early identification of lung cancer. Our suggested model, which combines ResNet50, EfficientNetB0, and DenseNet121, achieves outstanding accuracy (97.41%) while minimising loss (0.0803), exceeding individual models. This unique model combines the characteristics of many architectures to provide improved feature extraction and sophisticated pattern identification in medical imaging. Beyond a literature review, our study adds a real answer that has the potential to revolutionise early detection, giving a powerful weapon for combatting this lethal illness.",Deep learning;Weapons;Transfer learning;Lung cancer;Computer architecture;Feature extraction;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481673,IEEE Conferences,,,,,,
A Transfer Learning and Feature Ranking-based Feature Extraction Approach for Enhancing Skin Lesion Classification,N. Mohanty; M. Pradhan; P. K. Mallick; A. R. Routray,2024 International Conference on Emerging Systems and Intelligent Computing (ESIC),01-Apr-24,2024,"This study investigates skin lesion classification through feature fusion, focusing on transfer learning-based extraction for improved model discernment. Utilizing VGG16, ResNet, and EfficientNet B0, the research ranks features using methods like model weights, RFE, correlation-based techniques, LASSO regression, gradient boosting, and variance thresholding. The approach aims to enhance diagnostic precision by combining diverse features essential for accurate skin lesion classification. Assessing HAM 10000 and BCN 20000 datasets, the study evaluates the impact of ranking on model performance. Results consistently demonstrate that ranked feature sets outperform initial sets across classifiers (KNN, SVM, CNN) for both datasets. Notably, CNN excels with ranked feature sets from RFE applied to fused features from transfer learning networks.",Support vector machines;Transfer learning;Diversity reception;Focusing;Feature extraction;Boosting;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481593,IEEE Conferences,,,,,,
Identification of Uterine Fibroids in Medical Pictures Employing Deep Neural Networks,A. Mohanty; P. Pattnayak; P. K. Mallick,2024 International Conference on Emerging Systems and Intelligent Computing (ESIC),01-Apr-24,2024,"Uterine fibroids, commonly known as leiomyomas, are prevalent non-cancerous pelvic tumors in fertile women. These growths, comprising muscle and fibrous tissue, vary in size. Often asymptomatic, fibroids can be challenging to detect through CT scans and ultrasounds if small in size. Due to its high specificity and sensitivity, as well as its affordability and greater accessibility compared to CT and MRI exams, ultrasonography is currently the initial form of imaging used for the clinical diagnosis of uterine fibroids. The primary issue is misinterpretation of big and subplasmic fibroids, as well as pelvic and adnexal tumours. In order to do this, an automated method that utilises deep learning classification modelsâ€” namely, VGG 16â€”for identifying the presence of fibroids in the uterus has been proposed in this study. Our approach has demonstrated a 97.5% accuracy rate which is the best model in predicting the uterine fibrosis class based on imaging data as compared with earlier studies.",Sensitivity;Computed tomography;Computational modeling;Artificial neural networks;Ultrasonography;Predictive models;Muscles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481672,IEEE Conferences,,,,,,
Thyroid Nodule Detection Using Deep Learning Strategies,S. Naganjaneyulu; R. S. Lakshmi; S. NagulMeeraBee; M. S. K. Reddy,2024 International Conference on Emerging Systems and Intelligent Computing (ESIC),01-Apr-24,2024,"Detection of thyroid nodule is an important part in medical imaging because they occur more often and can be of any form from benign to malignant. If we detect the thyroid nodules in the beginning stages itself we can provide better treatment for the patient. The proposed model helps in thyroid nodules detection by using Convolutional Neural Network(CNN) and classifies thyroid nodules into functional, malignant and benign categories. First step involves loading and preprocessing the pictures dataset. These pictures contain various types of thyroid images collected from various medical imaging methods like ultrasound scanning reports. The images are shrunked, grayscaled and labelled based on file keyword names similar to real world diagnosis classification of medical photographs. The dataset is divided mainly into the training set and the testing set so that model is developed using the training data can be checked for itâ€™s performance by extracting patterns from the testing data. During training, maintaining the consistency in pixel values between photos, data normalization keeps features of any image occupying the center stage. The CNN model is constructed using layers with an aim to identify complex patterns in image. Maxpooling layers are useful in extracting main features, where convolutional layers are used to find spatial patterns. Dense layers formed afterwards helps to develop the intricate representations which are required for a precise classification. After the model architecture is defined, the annotated photos are used to train the model architecture. By using the optimizer â€œAdamâ€ and the â€œCategorical cross entropy loss functionâ€, the model gets trained with the ten epochs which improve the models performance. It is very useful since it helps the doctors to quickly determine the precise type of thyroid nodules, therefore planning early diagnosis and therapy planning for the patient. This helps in speeding up the decision making by improving classification process, resulting prompt patient interventions. These type of automated methods also scale medical diagnostics which is helpful in providing services to large population round the globe by reaching out to the places with limited special healthcare.",Training;Ultrasonic imaging;Training data;Feature extraction;Planning;Convolutional neural networks;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481585,IEEE Conferences,,,,,,
Mixed Supervision of Histopathology Improves Prostate Cancer Classification From MRI,A. Rajagopal; A. C. Westphalen; N. Velarde; J. P. Simko; H. Nguyen; T. A. Hope; P. E. Z. Larson; K. Magudia,IEEE Transactions on Medical Imaging,01-Jul-24,2024,"Non-invasive prostate cancer classification from MRI has the potential to revolutionize patient care by providing early detection of clinically significant disease, but has thus far shown limited positive predictive value. To address this, we present a image-based deep learning method to predict clinically significant prostate cancer from screening MRI in patients that subsequently underwent biopsy with results ranging from benign pathology to the highest grade tumors. Specifically, we demonstrate that mixed supervision via diverse histopathological ground truth improves classification performance despite the cost of reduced concordance with image-based segmentation. Where prior approaches have utilized pathology results as ground truth derived from targeted biopsies and whole-mount prostatectomy to strongly supervise the localization of clinically significant cancer, our approach also utilizes weak supervision signals extracted from nontargeted systematic biopsies with regional localization to improve overall performance. Our key innovation is performing regression by distribution rather than simply by value, enabling use of additional pathology findings traditionally ignored by deep learning strategies. We evaluated our model on a dataset of 973 (testing ${n}={198}$ ) multi-parametric prostate MRI exams collected at UCSF from 2016â€“2019 followed by MRI/ultrasound fusion (targeted) biopsy and systematic (nontargeted) biopsy of the prostate gland, demonstrating that deep networks trained with mixed supervision of histopathology can feasibly exceed the performance of the Prostate Imaging-Reporting and Data System (PI-RADS) clinical standard for prostate MRI interpretation (71.6% vs 66.7% balanced accuracy and 0.724 vs 0.716 AUC).",Magnetic resonance imaging;Biopsy;Prostate cancer;Lesions;Glands;Systematics;Biological system modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483083,IEEE Journals,,,,,,
Distribution-Aware Loss for Lesions Detection Using White-Light Endoscopy in Colorectal Region,W. Sun; R. Zhao; K. Zhang; J. Gao; G. Qu,IEEE Access,02-Apr-24,2024,"Computer-aided diagnostic systems have evolved into critical tools for endoscopists in diagnosing and reducing missed diagnoses. However, due to the lower incidence of rare diseases in medical images compared to common diseases, there exists an imbalance in sample distribution for lesion classification. This imbalance results in reduced detection and classification accuracy. In the realm of deep learning, detection accuracy is influenced not only by the model but also by the choice of the loss function. This study introduces a novel solution to address the imbalance issue of colorectal lesions in white light endoscopy by proposing a loss function named Label Distribution and Scale Distribution Aware Loss (LSDA-Loss). Our innovative loss function resolves the category imbalance problem by considering sample distribution and employing Bayesian equations to quantify the degree of imbalance. Furthermore, we adopt proportional distribution to evaluate the complexity of categorizing each sample. Experimental results from three independent datasets demonstrate that: 1) the integration of the proposed loss function with three typical FPN models significantly enhances detection accuracy, achieving improvements of up to 94.56%. 2) Our loss function effectively balances detection accuracies across the three categories, surpassing the performance of the original loss function.",Lesions;Predictive models;Medical diagnostic imaging;Training;Endoscopes;Complexity theory;Task analysis;Colorectal cancer;Deep learning;Biomedical imaging;Endoscopes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10478917,IEEE Journals,,,,,,
Exploiting Geometric Features via Hierarchical Graph Pyramid Transformer for Cancer Diagnosis using Histopathological Images,M. Liu; Y. Liu; P. Xu; H. Cui; J. Ke; J. Ma,IEEE Transactions on Medical Imaging,,2024,"Cancer is widely recognized as the primary cause of mortality worldwide, and pathology analysis plays a pivotal role in achieving accurate cancer diagnosis. The intricate representation of features in histopathological images encompasses abundant information crucial for disease diagnosis, regarding cell appearance, tumor microenvironment, and geometric characteristics. However, recent deep learning methods have not adequately exploited geometric features for pathological image classification due to the absence of effective descriptors that can capture both cell distribution and gathering patterns, which often serve as potent indicators. In this paper, inspired by clinical practice, a Hierarchical Graph Pyramid Transformer (HGPT) is proposed to guide pathological image classification by effectively exploiting a geometric representation of tissue distribution which was ignored by existing state-of-the-art methods. First, a graph representation is constructed according to morphological feature of input pathological image and learn geometric representation through the proposed multi-head graph aggregator. Then, the image and its graph representation are feed into the transformer encoder layer to model long-range dependency. Finally, a locality feature enhancement block is designed to enhance the 2D local representation of feature embedding, which is not well explored in the existing vision transformers. An extensive experimental study is conducted on Kather-5K, MHIST, NCT-CRC-HE, and GasHisSDB for binary or multi-category classification of multiple cancer types. Results demonstrated that our method is capable of consistently reaching superior classification outcomes for histopathological images, which provide an effective diagnostic tool for malignant tumors in clinical practice.",Cancer;Transformers;Image classification;Histopathology;Computer architecture;Deep learning;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10479483,IEEE Early Access Articles,,,,,,
Automatic Classification of White Blood Cells Using a Semi-Supervised Convolutional Neural Network,H. Song; Z. Wang,IEEE Access,########,2024,"The correct classification of white blood cell subtypes is critical in the diagnosis of blood disease. However, the performance of classical computer vision-based classification methods is heavily dependent on the features that should be carefully designed by trial and error. The machine learning-based classifier outperforms the traditional classifiers but suffers from sample labeling, which is labor intensive and time consuming. This paper presents a semi-supervised convolutional neural network that can maintain a similarly high accuracy of classification as deep learning approaches with only 10% labeled data or less. A Visual Geometry Group (VGG) network model was pre-trained with a small amount of labeled data and then used to predict unlabeled data. After implementing entropy filtering and confidence filtering processes, high-quality pseudo label data were obtained and served as input for the final mean teacher model training. The proposed methodology was validated on a dataset of 9069 synthetic images that correspond to five different subtypes of white blood cells. The model yielded an overall average accuracy of 94.4% with only 500 labeled samples, which is slightly lower than that of the fully supervised model with 9069 labeled samples (97.9%) but much higher than that of the fully supervised model with 500 labeled samples (86.5%). With such results, the proposed model demonstrates promising prospects for developing clinically useful solutions that are able to detect white blood cells based on blood cell images.",White blood cells;Feature extraction;Deep learning;Training;Image segmentation;Data models;Convolutional neural networks;Classification algorithms;Biomedical imaging;Semisupervised learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10478005,IEEE Journals,,,,,,
Implementation of medical image retrieval algorithm based on preference learning model,Y. Chen; C. Luo; Z. He; Z. Shu,"2024 IEEE 4th International Conference on Power, Electronics and Computer Applications (ICPECA)",########,2024,"This paper utilizes the Stacked Denoising Auto Encoder (SDAE) model to capture the general edge features of images. Additionally, it employs the Convolutional Neural Networks model to extract fine edge features from medical images. By combining preference learning algorithms, the paper implements an image retrieval system and designs a medical image retrieval model termed SDAE+CNN+Preference Learning. In the design of the reconstruction residual function in the SDAE model, noise factors are introduced to ensure the accuracy of image feature extraction by accounting for the impact of noise. Adjustments have been made to the structure of the CNN model, specifically in the convolutional layers, pooling layers, and fully connected layers. These adjustments aim to further enhance the accuracy of image feature extraction and ensure the precision of image feature classification. By conducting learning training on key parameters, an optimal preference learning model is obtained. Following the rules for matching the category attributes of input images with those in the medical image database and optimizing the loss function of the preference learning model, the similarity of category attributes for images is ranked. Ultimately, images with higher category attribute similarity are listed as retrieval results. Relevant experiments indicate that the proposed SDAE+CNN image feature extraction module in this paper achieves high accuracy in extracting image features, effectively controlling the influence of image noise; The designed SDAE+CNN+Preference Learning medical image retrieval model achieves a retrieval similarity of 96%, a retrieval accuracy of 94%, and a retrieval recall rate of 95%, as per the conducted experiments.",Training;Support vector machines;Image edge detection;Computational modeling;Image retrieval;Noise reduction;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10471067,IEEE Conferences,,,,,,
Vision Transformer-based Model for Gastric Cancer Detection and Classification using Weakly Annotated Histopathological Images,T. P. Theodore Armand; S. Bhattacharjee; H. -J. Kim; A. Hussain; S. Ali; H. -K. Choi; H. -C. Kim,2024 26th International Conference on Advanced Communications Technology (ICACT),########,2024,"Gastric Cancer (GC) is the fifth most diagnosed cancer worldwide. An early diagnosis is a hope for patients suffering from GC. A biopsy is a procedure that helps detect abnormal and suspicious areas to determine whether cancer cells are in the stomach. Tissue samples collected through biopsy are stained using Hematoxylin and Eosin (H&E) and digitalized through scanning to produce a whole slide image (WSI) needed for further analysis. Recently, most prognostics have proven effective using artificial intelligence techniques combined with related computer aid detection systems. This research used a vision transformer to detect and classify gastric cancer from weakly annotated tissue images. After acquiring normal and cancer histopathological samples, we applied the vision transformer (ViT) model for binary classification. We generalized our approach by performing region-based prediction on unannotated tissue samples. The proposed approach will ease diagnosis and support pathologists in decision-making.",Learning systems;Stomach;Image segmentation;Computational modeling;Decision making;Biopsy;Parallel processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10471994,IEEE Conferences,,,,,,
Deep Learning Based Cervical Spine Bones Detection: A Case Study Using YOLO,M. Yaseen; M. Ali; S. Ali; A. Hussain; A. Athar; H. -C. Kim,2024 26th International Conference on Advanced Communications Technology (ICACT),########,2024,"Cervical spine bones detection plays a crucial role in various medical applications, such as diagnosis, surgical planning, and treatment assessment. Traditional methods for cervical spine bones detection often rely on manual identification and segmentation, which are time-consuming and prone to errors. In recent years, deep learning approaches have shown great potential in automating the detection process and achieving high accuracy. In this research paper, we propose a deep learning-based approach for detecting cervical spine bones. Our suggested approach employs the YOLOv5 architecture, a cutting-edge object identification system renowned for its effectiveness and precision. The model is trained to recognize and locate bones structures using computed tomography (CT) scans image of the cervical spine as inputs. We conduct extensive evaluations using the trained models on the cervical spine dataset. The mean average precision (mAP) scores achieved by our model are 93% at threshold (mAP _0.5) and 83% at thresholds ranging from (mAP _0.5:0.95), which demonstrate the effectiveness of our approach in accurately detecting and localizing cervical spine bones. Our deep learning-based method for detecting cervical spine bones with high mAP scores presented in this research paper has significant implications for medical applications. With accurate and reliable bones detection, medical professionals can enhance diagnosis, surgical planning, and treatment assessment processes. The achieved mAP scores showcase the performance and potential of our proposed method, contributing to the advancement of bone detection techniques in cervical spine imaging and facilitating collaboration between the medical imaging and deep learning communities.",YOLO;Deep learning;Computational modeling;Computed tomography;Surgery;Medical services;Computer architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10472013,IEEE Conferences,,,,,,
Optimising Corn Leaf Disease Classification with MobileNet and Oversampling,A. Kumar; L. Nelson; V. S. Venu,2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),########,2024,"This work addresses the challenge of crop disease identification in agriculture by introducing an improved deep learning model. The study employs extensive data augmentation to enhance the training dataset, promoting better model generalisation across diverse agricultural scenarios. The model architecture utilises MobileNetV2 as a base, prioritising efficiency and lightweight design. The model's training progress is monitored over 40 epochs with a batch size of 10, resulting in an impressive accuracy of 93.3% in crop disease classification. The work also explores alternative preprocessing steps and assesses the impact of various base models for feature extraction. The outcomes contribute to advancing agricultural technology, providing reliable tools for crop management. These proposed enhancements offer valuable insights for researchers and practitioners in precision agriculture, facilitating sustainable and efficient practices. This work represents a significant step forward in applying deep learning techniques to achieve accurate and robust crop disease identification, with potential applications in improving crop yield, reducing losses, and promoting sustainable agricultural practices.",Training;Deep learning;Measurement;Adaptation models;Transfer learning;Feature extraction;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467581,IEEE Conferences,,,,,,
Classification of Lung Cancer Images Using Optimized Hybrid Deep Learning Model,S. P. Sreeja; V. Asha; B. Saju; H. E. S; H. Achuth; K. Sengupta,2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),########,2024,"This research study provides a concise overview of lung diseases, encompassing their diverse nature and impact on the respiratory system. Lung diseases can result from various factors, including genetics, environmental influences, and lifestyle choices. Diagnostic methods and treatment options are discussed, emphasizing the importance of early detection and a multifaceted approach to managing these conditions. Preventive measures, such as vaccination and smoking cessation, are very much vital in reducing lung diseases. This study serves as a valuable resource for those seeking a comprehensive understanding of these conditions and their management. The proposed model demonstrates impressive outcome with an accuracy level of 99.07%, a sensitivity level of 98.40%, a specificity level of 99.09%, a precision level of 98.11 %, and an Fl score of 97.94%.",Sensitivity;Pneumonia;Lung cancer;Lung;Genetics;Vaccines;Pollution measurement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467283,IEEE Conferences,,,,,,
Improved AO based Classification Framework for Eye Diseases,B. Saju; V. Asha; J. C. Mathew; S. Sangma; S. Ghosh; S. K,2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),########,2024,"A sensory organ that reacts to visible light, the human eye is a component of the sensory nervous system. A new approach is needed since, although physical detection of the eye condition is still a possibility, it is challenging and time-consuming. The study recommends a hybrid paradigm to classify various eye illnesses. Prior to further processing, pictures are first reduced in size, made grayscale, and noise-free using a combination of the Weiner and Median filters. Histogram equalization based on a Gaussian Mixture is also used to enhance pictures. Picture features are recovered with the aid of a thick convolutional neural network. An improved Aquila optimized ResNet101 model is used for classification. The effectiveness of the models is contrasted with that of related studies in the literature. A F1 score of 98.18%, accuracy of 99.2%, sensitivity of 98.9%, specificity of 98.91%, and precision of 98% were attained by the suggested model.",Histograms;Sensitivity;Gray-scale;Nervous system;Eye diseases;Diabetes;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467996,IEEE Conferences,,,,,,
Hybrid VGG19 with Combined LSTM Deep Neural Network For Improved Brain Tumor Classification,A. Divya; N. V. Sri Sai; N. N. Uma Mahesh; N. Teja; M. V. Sri; D. S. Lakshmi,2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),########,2024,"Medical imaging, specifically the categorization of brain tumors, continues to be a crucial field of study in healthcare. Ensuring precision and accuracy in categorization is essential for enabling medical practitioners to make fast and accurate diagnoses. Despite the diligent efforts of computer vision researchers to improve categorization algorithms, persistent problems remain. This approach presents a novel method that combines the powerful features of VGG19, a widely-used pre-trained convolutional neural network, with the stateful information storing capability of Long Short-Term Memory networks. By using pre-trained models like VGG19, it becomes possible to extract complex characteristics from medical pictures, which forms a strong basis for classification tasks. Moreover, the inclusion of LSTM layers enables it to remember long term dependencies across epochs and also solves vanishing gradient issue. The suggested technique showcases significant improvements in accuracy as compared to conventional methodologies. This novel framework shows potential for enhancing the field of medical image analysis, hence improving the accuracy and reliability of diagnoses in the complex area of brain tumor categorization.",Computer architecture;Medical services;Brain modeling;Data models;Reliability;Task analysis;Long short term memory,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467798,IEEE Conferences,,,,,,
EfficientNet-Bl based Maize Plant Leaf Disease Classification Using Deep Learning,A. Kumar; L. Nelson; V. S. Venu,2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),########,2024,"This study focuses on the problem of class imbalance in the categorisation of maize plant leaf diseases. This study proposes a methodology that aims to enhance the performance of machine learning models while dealing with underrepresented classes. The approach combines random oversampling with the use of the EfficientNet-BI architecture. The initial step involves loading and processing the dataset, which consists of images depicting a range of leaf maladies in maize plants. Following this, the minority class is augmented via random oversampling, with a specific focus on occurrences of Gray Leaf S pot. The objective of this strategy is to rectify disparities and improve the model's capacity to distinguish between various disease types. The oversampled dataset is partitioned into several training and validation sets. The image classification work is performed using the EfficientNet-BI model, which has undergone pre-training on the ImageNet dataset. The training progress of the model is observed for 40 epochs, showing a remarkable accuracy of 0.95 and a minimal loss of 0.20. The final trained model is evaluated on the validation set, and the results, including a confusion matrix and a detailed classification report, showcase the effectiveness of the proposed approach in addressing class imbalance and achieving robust performance in maize plant leaf disease classification. This work contributes to the broader field of agricultural image analysis, offering insights into strategies for enhancing model generalisation and accuracy in the presence of imbalanced datasets.",Training;Deep learning;Measurement;Transfer learning;Refining;Loading;Internet of Things,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467408,IEEE Conferences,,,,,,
Automated Anomaly Detection in Pregnancy: AI-Driven Ultrasound Analysis,K. P. Revathi; I. Mahesh; K. Makitha Sree; N. Deepthi,2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),########,2024,"This research explores the application of AI-driven techniques for automated anomaly detection in ultrasound scans during pregnancy. Leveraging a substantial dataset of labeled ultrasound images, this study delves into robust data collection, preprocessing methodologies, meticulous model selection, and comprehensive model training techniques. The combination of U-Net for image segmentation and ResNet for image classification was selectively chosen, followed by an in-depth analysis of their adaptability to medical imaging tasks. The study concludes by proposing a hybrid approach using both segmentation and classification for more accurate anomaly detection in ultrasound images. This study delves into a comprehensive survey of AI methodologies applied to fetal ultrasonography during the second trimester. By leveraging curated datasets and innovative AI techniques, this research navigates the evolving landscape of automated anomaly detection, aiming to enhance diagnostic capabilities and bridge accessibility gaps in prenatal care.",Pregnancy;Training;Ethics;Image segmentation;Adaptation models;Ultrasonic imaging;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467631,IEEE Conferences,,,,,,
Supervised Semantic Segmentation of Murine THz Spectroscopy Images with Imprecise Annotations,H. Liu; N. Vohra; K. Bailey; M. El-Shenawee; A. Nelson,2024 IEEE 18th International Conference on Semantic Computing (ICSC),########,2024,"Semantic Artificial Intelligence possesses attributes that are particularly beneficial for deep learning tasks in medical imaging. By infusing semantic context into the fundamental classification process, the richness of data in medical images can be enhanced, leading to a potential increase in the reliability of the outcomes. In this research, we explore the use of semantic AI for distinguishing different tissue types within breast tumors that have been excised and imaged using pulsed terahertz (THz) technology, which is a cutting-edge method in the imaging field. Prior work has demonstrated traditional data driven methodology for deep learning on THz images has been challenging due to lacking of precise pixel-to-pixel labels, which is caused by domain transformation and tissue changes during histopathology. This work seeks to address this limitation through a noisy label handling enabled semantic AI mechanism. Specifically, we introduce a three stage training pipeline using supervised deep learning networks, contrastive feature reduction, and metric learning. The combination of these contributions enables better cancerous tissue differentiation. The recall for cancer tissue regions is significantly improved to 90 % from 55 % using the proposed semantic segmentation network.",Deep learning;Training;Terahertz wave imaging;Spectroscopy;Semantic segmentation;Semantics;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10475624,IEEE Conferences,,,,,,
Robust Object Recognition with Genetic Algorithm and Composite Saliency Map,M. W. Ahmed; A. Jalal,2024 5th International Conference on Advancements in Computational Sciences (ICACS),########,2024,"Object detection and recognition have emerged as crucial research areas, driven by the dire need to address the challenges posed by identifying and recognizing objects in diverse domains. Accurate object identification and recognition is required for tasks such as autonomous navigation, video surveillance, precision agriculture, and medical imaging. We propose a method based on a unique combination of machine learning techniques for object recognition. The proposed approach starts by applying K-means segmentation to the input image to cluster similar regions and colors. Next, a composite saliency map is generated based on the K-means segmentation output. Subsequently, the technique of extracting objects from the saliency map is accomplished through the employment of the connected pixel object extraction method. Ultimately, the Genetic Algorithm is utilized to optimize decision tree classifier for the purpose of object recognition. The MSRC dataset was utilized to assess the proposed approach, resulting in an accuracy of 81.5%, accompanied by a precision of 83.3% and recall of 86.1%. The results depicts that the our approach is highly valuable for accurate object categorization, wherein the utilization of Genetic Algorithm significantly enhances the precision of the classification.",Deep learning;Image segmentation;Scalability;Object detection;Video surveillance;Planning;Object recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473285,IEEE Conferences,,,,,,
Optimized Hybrid Model Framework for Breast Cancer Classification,A. V. S. Swetha; M. Bala; K. Sharma,"2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",########,2024,"Breast diagnosis from pathology reports is a widely employed clinical method for diagnosing breast tumors. But it's challenging due to low contrast, high noise, and diverse appearances. Experienced professionals rely on factors like global context, local geometry, and intensity changes, acquired through years of clinical experience. To ease the burden on doctors, we propose integrating machine learning with diagnosis practice. However, effective and efficient breast tumor detection is crucial for automated disease diagnosis. In the last decade, various deep learning models have emerged for breast tumor detection. Traditional methods lack real-time responsiveness, accuracy, and scalability. To address these issues, we introduce an enhanced framework with hybrid model based on a multi-task cascaded convolutional network. This framework leverages a substantial dataset of clinically confirmed images with precise labels, employing a multi-task cascaded architecture. It involves two stages of deep convolutional networks designed to detect and recognize tumor affected area progressively, ensuring real-time operation and high accuracy. Additionally, we introduce an improved feature fusion-based augmentation method, enhancing diagnostic model performance. Our experiments demonstrate exceptional accuracy and time efficiency. Furthermore, the model's versatility is highlighted in the context of related diseases. Our proposed model showed a high accuracy of 97.69 for multi classification and 99.39 for binary classification. Also, our proposed framework when compared with many other hybrid model utilized less time (3.19 seconds per image) and resources making it efficient.",Pathology;Breast tumors;Scalability;Multitasking;Feature extraction;Breast cancer;Real-time systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463295,IEEE Conferences,,,,,,
Augmenting Clinical Decisions with Deep Learning Lung Cancer Image Abnormality Segmentation,K. Venkatraman; S. N. P. S. Reddy,"2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",########,2024,"Lung cancer is a major global cause of death, highlighting the critical need for quick and accurate detection methods. The exploration of computational alternatives arose from the standard way of manually processing CT images, which is time-consuming and error-prone. In this work, we combine the advantages of Support Vector Machine (SVM) and VGG16 classifiers to provide a novel method for enhancing lung cancer diagnosis. By analyzing the â€œIQ-OTH/NCCDâ€ dataset, our hybrid model-which combines the VGG 16 and SVM algorithms-performs admirably in differentiating between aggressive, benign, and normal lung cancer cases. This combination of traditional machine learning with deep learning tackles accuracy and efficiency issues, which is a promising development over current diagnostic methods. We conduct a comprehensive comparative analysis with prominent architectures to select the optimal model based on accuracy, efficiency, and resource requirements. In addition to introducing the VGG 16+SVM model, our research provides valuable insights into deep learning architectures, with the ultimate goal of advancing precise and efficient diagnosis of lung cancer, which is crucial in combating this global health challenge in the future.",Support vector machines;Deep learning;Analytical models;Neural networks;Lung cancer;Lung;Computer architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463381,IEEE Conferences,,,,,,
Alzheimer's Dementia and its Various Biomarker Details with Several Detection Methodologies - A Comprehensive Review,R. K. Singh; D. K. Choubey,"2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",########,2024,"Dementia is the most critical neurodegenerative disease that gradually destroys memory and other cognitive functions. Hence early detection is utmost necessary and to build an effective detection model, its required to understand its type, symptoms, stages & causes. This review work deep dives into most critical dementia type which is still incurable i.e., â€œAlzheimer'sâ€ and surveyed different biomarkers (CFS, AÎ²42, t-Tau, p-Tau, miRNAs, HNE, TNF, etc.) as well as diagnosis methodologies - APM, AI/ML, Deep learning & Fuzzy logic algorithms implementation based on medical images data generated using CTS, MRI, sMRI, fMRI, PET, Retinal photography (most cost effective) those were used in AD early detection. This review work summarizes various techniques mentioned in survey/articles/research papers and analyzed their accuracy, data sources, affordability, features used as well as issues. Based on this paper, new techniques may be designed. This study emphasis on the use of retinal images to build most affordable, accessible, and efficient future system on AD detection.",Photography;Fuzzy logic;Deep learning;Reviews;Soft sensors;Biomarkers;Functional magnetic resonance imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463269,IEEE Conferences,,,,,,
Evaluation of CNN Models for Alzheimer's Classification Using MRI Images,S. Sharma; H. Mittal,"2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",########,2024,"Alzheimer's Disease poses a significant global health burden as pervasive neurodegenerative affliction, underscoring the critical need for timely and accurate diagnosis to optimize patient care and implement effective therapeutic interventions. This research delves into the application of deep learning, a subset of artificial intelligence, as a potent tool for automating AD classification using medical imaging data, particularly MRI. Utilizing convolutional neural networks (CNNs), Inception Net, ResNet50, and VGG16, our study focuses on classifying distinct AD stages. By training these models on extensive medical imaging datasets, intricate patterns and features are extracted from MRI images, enabling robust discrimination between individuals at varying AD stages. Beyond technical exploration, this research underscores the significance of integrating cutting-edge technology, offering a promising avenue to improve early-stage AD detection precision, contributing to enhanced patient outcomes and more effective therapeutic strategies.",Training;Deep learning;Magnetic resonance imaging;Feature extraction;Convolutional neural networks;Alzheimer's disease;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463522,IEEE Conferences,,,,,,
Image Classification of Brain Tumors through Hybrid Learning,S. P. Sreeja; V. Asha; B. Saju; K. M. Suchitra Dwivedi; K. P; K. Bhavana,"2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",########,2024,"Brain tumor, the well-known deadly medical condition where treatment sometimes fails to show results. Various treatments and medical diagnosis shows ineffective as the chances of this condition appearing again are more than curing them in first attempt. The abnormal growth of cells inside or surrounding brain causes these brain tumors which may differ depending on the type of tumor in each individual. The study proposes a hybrid model to classify the brain images. First the images are pre-processed with the help of Image resizing, Gray scale conversion and noise removal with the help of combined Weiner filter and Median filter. Then the Gaussian Mixture based histogram equalization are used to enhance the images. A Dense Convolutional neural network is used for extracting the Features of the images. An optimized ResNet101 model with improved Aquila optimization is used for classifying the brain tumor images. Performance of the model is compared with other studies in literature. The proposed model has obtained an accuracy of 99.01%, along with the various performance metrics as sensitivity of 98.60%, specificity of 98.01%, precision of 98%, F1 score of 98.44%.",Sensitivity;Computational modeling;Magnetic resonance imaging;Brain modeling;Feature extraction;Object recognition;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10468939,IEEE Conferences,,,,,,
Detection of Skin cancer using Optimized Hybrid deep learning Model,J. C. Mathew; V. Asha; B. Saju; N. Tressa; P. Imagoudanavar; S. M. Mandave,"2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",########,2024,"This research focuses on Melanoma skin cancer, an often-occurring condition originating in melanocytes responsible for melanin production. The study employs a deep hybrid learning model to effectively discriminate and categorize skin cancers, utilizing a dataset comprising benign and malignant classes. To address the dataset's imbalance, particularly the scarcity of images depicting malignant lesions, augmentation techniques are applied. Additionally, Contrast Limited Adaptive Histogram Equalization (CLAHE) is utilized to enhance image clarity. Lesion segmentation is conducted using a neural network-based ensemble model, combining segmentation algorithms from Fully Convolutional Network (FCN), SegNet, and U-Net. This results in a binary representation of the skin and lesion, with lesions in white and the skin in black. Subsequently, these binary images undergo further classification using various pre-trained models such as Inception ResNet V2, Inception V3, ResNet 50, Densenet, and CNN. The best-performing pre-trained model undergoes fine-tuning for enhanced classification performance. To augment the classification model's effectiveness, a combination of deep learning (DL) and machine learning (ML) is applied. DL models are utilized to extract features, while classification is executed using Support Vector Machines (SVM). This computer-aided tool is designed to expedite disease diagnosis for medical professionals compared to conventional methods, resulting in a notable 4% improvement in the proposed method's performance.",Support vector machines;Image segmentation;Computational modeling;Melanoma;Production;Feature extraction;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10469400,IEEE Conferences,,,,,,
Advancing Breast Cancer Prediction and Early Detection with Advanced Deep Learning Models,P. Garg; P. Sharma; P. U. M,"2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",########,2024,"Breast cancer is a common disease that predominantly affects women worldwide, with the potential to be fatal. Histopathologists utilize various criteria to examine tissue samples under a microscope for diagnosing cancer through histopathological imaging. However, pathologists often disagree during the diagnostic process. To address this issue, Deep Neural Networks are employed for supervised classification, given the considerable time required for manual categorization of histological pictures. In our classification task, we utilized the Breast Histology dataset, comprising 241 training and 21 test pictures. Effective preprocessing of the dataset is crucial for successful categorization. Images were classified into four categories (Normal, Benign, Insitu carcinoma, and Invasive cancer) using transfer learning based on GoogleNet, AlexNet, and ResNet. Notably, our method achieved the highest accuracy, with ResNet reaching 97.11%. Ongoing research aims to enhance the technique's effectiveness and reduce reliance on human involvement. Furthermore, this promising approach can be adapted to automate additional medical imaging modalities, potentially advancing the automation of various aspects of medical diagnosis through improvements to the suggested framework.",Deep learning;Training;Histopathology;Microscopy;Transfer learning;Predictive models;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10469397,IEEE Conferences,,,,,,
OHMBC: Optimized Hybrid deep learning Model for Classification of Breast Cancer,B. Saju; V. Asha; N. Tressa; P. B. Kondikoppa; Pooja; P. K. V J,"2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",########,2024,"Men are much less likely than women to develop breast cancer. Breast lumps, bloody nipple discharge, and changes in the nipple's or breast's shape or texture are all indications of breast cancer. The study proposes a hybrid model to classify Diabetic retinopathy images. Initially, images are pre-processed which includes Image resizing, Gray scale conversion and noise removal using a combination of Weiner filter and Median filter. Further, images are enhanced using Gaussian Mixture based histogram equalization. Features of the images are extracted using Dense Convolutional neural network. An optimized ResNet101 model with improved Aquila optimization is used for classification. Performance of the models is compared with other studies in literature. The proposed model has obtained 96% accuracy, 98.1% sensitivity, 98.2% specificity, 98% precision, and 98.18% F1 Score.",Histograms;Diabetic retinopathy;Sensitivity;Shape;Computational modeling;Feature extraction;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10469242,IEEE Conferences,,,,,,
Biomedical Image Classification using Deep Reinforcement Learning,S. Parihar; A. Kukker; S. Dhar; V. Amitabh; V. Singh; V. Krishna,"2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",########,2024,"The amalgamated tool of two renowned tools deep learning and reinforcement learning is a powerful representation for deep neural networks that improves the basic reinforcement learning framework. With the use of deep learningâ€™s representational power, it learns from the agentâ€™s activities on how to increase the expected reward. Recent work has shown a lot of success of deep reinforcement learning in different domains such as video games, robotics, finance, medical and computer vision. In this project, various DRL models and methods for planning medical image analysis are discussed. This study covers the fundamentals of reinforcement learning. DRL algorithms can address the problems with limited and inconsistent annotated medical imaging data, which has been a major obstacle to the implementation of deep learning models in clinical settings. DRL algorithms support these models for the reward function, interactions between agents, and the environment. There has been an extensive amount of research being done in this area, and it has the potential to enhance the utilisation of deep learning in medical imaging.",Training;Video games;Medical conditions;Biological system modeling;Computational modeling;Refining;Deep reinforcement learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10468733,IEEE Conferences,,,,,,
"Attention U-Net Model for Accurate Thoracic Disease Detection, Segmentation and Localization",H. Pant; M. C. Lohani; A. K. Bhatt,"2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",########,2024,"Lung cancer, pneumonia, and other chest disorders including TB continue to be major global public health issues. For prompt and successful treatment, it is essential to identify these disorders early and accurately in medical imaging, particularly in chest X-rays. Deep learning approaches have produced encouraging outcomes in medical image processing applications in recent years. To enhance chest disease identification and segmentation, this research study presents a novel method that combines the strength of the U-Net layout with attention processes. The Attention U-Net model takes advantage of both feature extraction and attention processes to draw attention to key areas in the pictures, improving task performance for segmenting and identifying chest diseases. The suggested Attention U-Net model's design, implementation details, evaluation criteria, and experimental findings are presented in this work, along with the localization of the boundary regions of the chest disease. It has good generalizability to CXR images from unknown datasets with various patients.",Location awareness;Image segmentation;Pneumonia;Reviews;Task analysis;Public healthcare;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10469141,IEEE Conferences,,,,,,
LeafNet: Classification of Plant Leaf Diseases using IAO based Hybrid DL Framework,B. Saju; V. Asha; J. C. Mathew; S. N; S. SanganagoudaDambala; Shruti,"2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",########,2024,"Plant diseases can be found using an analysis of these observable symptoms. The traditional approach to disease diagnosis entails professionals visually evaluating plants. The study proposes a hybrid model to classify plant leaf diseases. Initially, images are pre-processed which includes Image resizing, Gray-scale conversion and noise removal using a combination of Weiner filter and Median filter. Further, images are enhanced using Gaussian Mixture based histogram equalization. Features of the images are extracted using Dense Convolutional neural network. An optimized ResNet101 model with improved Aquila optimization is used for classification. Performance of the models is compared with other studies in literature. The proposed model has obtained an an accuracy of 99.4%, sensitivity of 98.9%, specificity of 99.1%, precision of 99%, and an F1 score of 98.98%.",Plant diseases;Histograms;Sensitivity;Computational modeling;Gray-scale;Feature extraction;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10468870,IEEE Conferences,,,,,,
Leveraging the Capabilities of EfficientNetB3 to Classify Eye Diseases with Reliability,P. Shourie; V. Anand; D. Upadhyay; V. Singh; S. Gupta,"2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",########,2024,"The objective of this research is to examine the utilization of deep learning in the classification of ocular diseases and healthy lives, with a specific emphasis on the EfficientNetB3 architecture. By employing a convolutional neural network (CNN), it is possible to differentiate among various eye conditions, such as diabetic retinopathy, cataracts, and normal vision. Based on the ImageNet dataset pre-training of the EfficientNetB3 model, which functions as the foundation for feature extraction, custom layers are incorporated to refine and classify the model. The evaluation of the proposed model's performance is conducted on distinct test and validation sets. In addition to the recall and confusion matrix, other pertinent metrics such as precision and recall are utilized to evaluate the model's capability of accurately categorizing every eye disease. The findings offer valuable insights regarding the efficacy of the EfficientNetB3 architecture in classifying ocular diseases and its potential practical implementation in clinical settings. The results have the potential to facilitate the creation of dependable and effective instruments for timely identification and intervention in the field of ophthalmic healthcare.",Measurement;Cataracts;Diabetic retinopathy;Pathology;Instruments;Medical services;Propulsion,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10469017,IEEE Conferences,,,,,,
Leveraging Transfer Learning for Improved Medical Image Classification,D. Wadhwa; S. Kumar; A. Rengarajan,2024 International Conference on Optimization Computing and Wireless Communication (ICOCWC),########,2024,"Transfer studying is a powerful approach for enhancing the performance of systems, and getting to know models in new domains. The primary concept is transferring existing information from a source area to a goal area with special classes and contexts. Switch-gaining knowledge is usually used in many one-of-a-kind fields consisting of pc vision and natural language processing. This paper focuses on the utility of transfer in getting to know in medical image category. We discover how to leverage current models and strategies from the pc imaginative and prescient domain for progressed accuracy and performance in scientific picture class. Mainly, we gift a complete evaluation of the literature awareness on recent techniques for utilizing transfer to know to classify diverse forms of scientific images, including computed tomography, radiography, and chest X-rays scans, and magnetic resonance imaging scans. We also talk about the capability of new guidelines for the utility of switch learning in medical photo classes. We speak about aspects that want special attention when running with clinical photos, including the want for special preprocessing steps and the significance of incorporating area understanding. Finally, we speak about the capability of switch-gaining knowledge of for improving prognosis accuracy and efficiency in medical imaging.",Wireless communication;Training;Radiography;Transfer learning;Switches;Natural language processing;Prognostics and health management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10470652,IEEE Conferences,,,,,,
Advancing Support Vector Machines for Automated Medical Image Diagnosis,A. Agrawal; R. Gupta; N. R. S. Jebaraj,2024 International Conference on Optimization Computing and Wireless Communication (ICOCWC),########,2024,"aid vector machines (SVMs) have become increasingly famous as computerized clinical photograph diagnosis gear due to their high performance in numerous duties, including binary classification and object segmentation. Specifically, SVMs had been applied in several automated clinical picture diagnosis responsibilities, such as classifying and diagnosing illnesses and predicting effects for patients. To in addition enhance the performance of SVMs in automatic medical photo prognosis, diverse tendencies have been made in recent years. First, researchers have proposed advanced kernel features and penalty capabilities that better model the records and permit more efficient optimization of the SVM fashions. Second, researchers have proposed several techniques for decreasing the complexity of the SVM fashions, together with regularization and feature selection. ultimately, researchers have explored architectural modifications of SVMs for higher overall performance, including convolutional neural networks and deep learning fashions. Those advancements appreciably enhance the overall performance of SVMs in automatic medical photograph prognosis obligations, making them extra dependable in real-global packages.",Support vector machines;Performance evaluation;Wireless communication;Neural networks;Prediction algorithms;Feature extraction;Vectors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10470696,IEEE Conferences,,,,,,
Recurrent Neural Networks for Improved Medical Image Classification,U. K. Singh; K. R; P. Saraswat,2024 International Conference on Optimization Computing and Wireless Communication (ICOCWC),########,2024,"In recent years, scientific imagery has ended up with an increasing number of essential approaches for diagnosing and monitoring many sicknesses. As a result, scientific photo classification has become a crucial research area. Deep learning procedures have opened new avenues for the medical photo category, with current tendencies because of recurrent neural networks (RNNs). Recurrent neural networks are robust neural networks that could discover ways to version temporal or sequential systems. Using RNNs, researchers can train a deep community in a supervised fashion without the need for manual photo segmentation. It has been validated to improve performance in scientific image type, with examples in the skin lesion category and lung nodule classification. The latest paintings have additionally validated the usage of RNNs to find latent features in clinical imagery, including latent anatomical systems and covariate relationships between disorder states. This type of evaluation can be beneficial in developing greater correct classifiers for medical images, similar to presenting a higher know-how of the imaging records. In precis, recurrent neural networks (RNNs) display promise in improving the accuracy of medical image class obligations. RNNs are crucial to discovering new features and covariate relationships between disease states in medical pics. With ongoing advances, RNNs will offer powerful equipment for scientific imaging.",Wireless communication;Recurrent neural networks;Manuals;Spatial databases;Skin;Convolutional neural networks;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10470908,IEEE Conferences,,,,,,
Image Colorization System: An Implementation using the Caffe Deep Learning Framework,A. H N; A. P S; A. Ali; A. Surendran,"2024 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)",########,2024,"Image colorization produces a colored version of a grayscale picture using only a single input image. Given a greyscale picture as input, this study attempts to create a realistic color version of the image. This work presents a method that uses Convolutional Neural Networks to colorize a grayscale image. The proposed system takes a single grayscale image as the input and learns the map between black-and-white image and the colored picture. The proposed approach, which is implemented using Caffe framework, has been tested on grayscale images and has achieved good accuracy and speed. Color mapping using Caffe2 deep learning framework clearly proves to be better suited for human images compared with previous methodologies. This study also presents a novel system for automatically colorizing grayscale images, which has potential uses in applications such as digital painting, image restoration, medical imaging and image editing.",Deep learning;Training;Image color analysis;Semantics;Training data;Gray-scale;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467366,IEEE Conferences,,,,,,
Accurate Prediction of Bacterial Infections using Deep Learning-Based Approaches: A Recent Study and Progress,J. Singh; A. Oberoi; Y. Kumar,"2024 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)",########,2024,"Bacterial diseases cause a major threat to the health globally which necessitates to its accurate detection as well as diagnosis. There are various traditional methods like clinical assessments, laboratory techniques, microbiological tests etc which no doubt are effective but consumes a lot of time and relies completely on experienced professionals for validation. Hence, to address such challenges, deep learning models have been termed as game changers as they are capable of analysing the data rapidly, diagnosing the patient on time, and minimizing the human errors. This manuscript provides complete information related to the techniques which have been used for the detection and classification of bacterial diseases. It commences with a concise introduction to bacterial diseases and an exploration of conventional diagnostic methods. Subsequently, it delves into the transformative impact of deep learning models on disease detection and classification. The work of researchers in this domain is examined and compared by highlighting their limitations, which serve as the foundation for the proposed system. In essence, this manuscript offers a detailed examination of research papers that leverage such AI learning models for bacterial disease detection and classification, paving the way for more efficient and accurate healthcare practices.",Deep learning;Microorganisms;Neural networks;Medical services;Transforms;Games;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467600,IEEE Conferences,,,,,,
Exploring Convolution Neural Networks for Image Classification in Medical Imaging,V. P. V; P. Chattu; K. Sivasankari; D. T. Pisal; B. Renuka Sai; D. Suganthi,"2024 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)",########,2024,"Modern healthcare relies on medical imaging to diagnose and cure diseases. Convolution Neural Networks (CNNs) have excelled at image categorization, and their use in medical imaging could change illness diagnosis. This study examines CNN's medical picture categorization performance. This study used X-rays, CT scans, MRIs, and histopathology slides. Traditional CNNs, transfer learning models, and bespoke networks were constructed and trained. The training data included lung ailments, cancer detection, bone fractures, and neurological disorders. Our investigations showed that CNNs can extract complex characteristics from medical photos, improving classification accuracy. Transfer learning, where pre-trained models were fine-tuned on medical data, performed well with 94.8% accuracy. We used cutting-edge data augmentation and attention strategies to improve model generalization and interpretability. In addition to high classification accuracy, we studied model explainability using gradient-based methods and visualization to highlight medical picture regions of relevance that influenced model predictions. Building trust with medical practitioners and understanding deep learning model decision-making requires interpretability. In clinical settings, CNN deployment raises ethical and practical issues such as data privacy, model robustness, and regulatory compliance. According to our research, CNNs may increase medical picture classification accuracy and speed. X-rays had the best accuracy at 95.2%, followed by CT scans at 92.7% and MRIs at 94.1%. Histopathological slides exhibited 88.6% accuracy; however, this shows that CNNs can diagnose diseases from microscopic images.",Deep learning;Ethics;Magnetic resonance imaging;Transfer learning;X-rays;Predictive models;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467794,IEEE Conferences,,,,,,
Classification of Images Related to Kidney Cancer using Hybrid Deep Learning,V. Asha; S. P. Sreeja; A. Prasad; G. V. Kathari; K. A S; Kathyayini,"2024 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)",########,2024,"Kidney cancer is a very critical condition that require prompt diagnosis to successful treatment. In this study, it is proposed an enhanced hybrid deep learning (DL) model for kidney cancer image categorization. A research work is carried out in which it advances medical image analysis and shows the way Machine Learning (ML) can assist with kidney-related cancer detection. The study proposes a hybrid model to classify the images related to Kidney disease, where initially the images are pre-processed by Image resizing, Gray scale conversion and noise removal using a combination of Weiner filter and Median filter. Then the images are enhanced using Gaussian Mixture based histogram equalization followed by imagesâ€™ feature extraction using Dense Convolutional neural network. To improve classification precision, the study combines the IAO method with ResNet101. The suggested model beats current models with an amazing precision of 98.90% when assessed using a variety of effectiveness metrics.",Deep learning;Histograms;Sensitivity;Gray-scale;Image categorization;Feature extraction;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467958,IEEE Conferences,,,,,,
An Improved Transfer Learning-Based Model with Data Augmentation for Brain Tumor Detection,R. D. Prayogo; N. Hamid; H. Nambo,2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),########,2024,"Current advances in deep learning have brought various breakthroughs in processing medical data. However, dealing with a limited number of medical datasets remains a challenge in deep learning and often leads to overfitting. To solve this research gap, here we show a new approach to improve the performance of a transfer learning-based model for brain tumor detection from 253 brain magnetic resonance imaging (MRI) sample images. The concept of transfer learning has been applied using a pretrained Inception V3combined with data augmentation. Modified layers using dropout and regularization have been additionally utilized to deal with overfitting. The proposed method shows an increase in accuracy of 6.430%, a precision of 5.531 %, a recall of 10.545%, and an F1-score of 8.040% compared to the baseline method. We show that our proposed method has been able to effectively enhance performance and reduce overfitting, even with a small number of datasets. Moreover, our proposed method outperforms state-of-the-art brain tumor detection.",Training;Deep learning;Magnetic resonance imaging;Computational modeling;Transfer learning;Brain modeling;Data augmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463471,IEEE Conferences,,,,,,
A Comparative Study of Noisy Label Detection Techniques in a Thai Hospital's Chest X-Ray Database,S. Tatitaisakul; S. Wilainuch; I. Chamveha; W. Chaisangmongkon,2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),########,2024,"This paper addresses the problem of noisy labels in chest X-ray datasets, which significantly impact the training of deep neural network models. Noisy labels often occur due to errors in reports from experts or the use of algorithms to extract labels from medical reports written in natural language. To tackle this issue, we compared the effectiveness of O2U-net, a state-of-the-art noisy label detection method, and NVUM, a noise-resistant model training technique in identifying noisy samples. We contrasted these methods with a heuristic approach which uses a simple classification model to flag samples with large differences between predicted and actual labels as noisy. Our findings indicated that NVUM outperformed the other methods in identifying noisy labels, providing a promising solution to the challenge of noisy labels in medical image analysis.",Training;Schedules;Databases;Natural languages;Predictive models;Observers;Noise measurement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463316,IEEE Conferences,,,,,,
Leukemia Classification Using EfficientNetB5: A Deep Learning Approach,A. Alshoraihy; A. Ibrahim; H. H. B. Issa,2024 Conference of Young Researchers in Electrical and Electronic Engineering (ElCon),########,2024,"Leukemia is a critical disease that requires early and accurate diagnosis. Leukemia is a type of blood cancer mainly occurring when bone marrow builds extra white blood cells in the human body. This disease affects adults and is a common cancer type among children. This paper presents a deep-learning approach using EfficientNetB5 to classify Leukemia using The Cancer Imaging Archive (TCIA) with more than 10,000 images from 118 patients. The achieved confusion matrix will contribute to improving the research in diagnosing cancer.",White blood cells;Deep learning;Pediatrics;Imaging;Bones;Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10468284,IEEE Conferences,,,,,,
Using Teaching Learning-Based Optimization with Convolutional Neural Network to Detect Pneumonia Based On Chest X-Ray Images,R. A. Alenezi; S. A. Ludwig,"2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)",########,2024,"Chest X-ray imaging plays a vital role in the treatment of respiratory diseases such as pneumonia. Recent technological innovations have significantly improved the efficiency of the image analysis process especially Artificial and convolutional neural networks. However, there is a need to improve the accuracy. Thus, it is important to develop an automated, early diagnosis system that can deliver quick decisions and significantly lower diagnosis error. Recent advancements in emerging Artificial Intelligence approaches, particularly Deep Learning algorithms, have made the chest X-ray screening a viable option for early Pneumonia detection. Therefore, this paper focuses on using Teaching Learning Based Optimization (TLBO) with Convolutional Neural Network (CNN) to improve the accuracy of detecting pneumonia in Chest X-ray images. The research study was conducted on the chest X-ray images of the pneumonia data set and is compared with previous work. Results confirm that TLBO with CNN is a good choice for detecting pneumonia at the accuracy of 98.88% and is an improvement over benchmark studies.",Graphics;Histograms;Pneumonia;Sensitivity;Machine learning algorithms;Education;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467902,IEEE Conferences,,,,,,
A Few-shot custom CNN Model for Retinal Nerve Fibre Layer Thickness Measurement in OCT Images of Epilepsy,R. Muhammad; M. M. Boukar; S. Adeshina; S. Dane,"2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)",########,2024,"This study aims to assess the effectiveness of employing deep learning models for measuring retinal nerve fiber layer (RNFL) thickness in optical coherence tomography (OCT) scans of epilepsy patients. Conventional OCT scan segmentation methods typically rely on supervised learning, demanding substantial data for training and assuming fixed network weights post-training. To mitigate these challenges, we explore the applicability of few-shot learning (FSL) in CNN architectures, allowing dynamic fine-tuning of network weights with minimal additional data. Experimental results demonstrate enhanced segmentation accuracy, with the proposed Few-shot Custom CNN achieving a notable 91% accuracy, surpassing both the Custom CNN (86%) and the OCT machine data. This suggests the superiority of the few-shot Custom CNN model in segmentation performance compared to OCT scans.",Training;Optical fibers;Image segmentation;Adaptation models;Supervised learning;Epilepsy;Optical variables measurement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10468036,IEEE Conferences,,,,,,
"Advances in Feature Extraction for Brain Cancer Analysis: A Review of Traditional, Machine Learning, and Deep Learning Approaches",B. Almuhaya; B. Saha; M. A. Bazel,2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS),########,2024,"Brain cancer remains a formidable health challenge, necessitating continuous advancement in diagnostic and therapeutic strategies. Feature extraction techniques, encompassing traditional, machine learning, and deep learning-based methods, play a pivotal role in the early detection, prognosis, and treatment planning of brain tumors. This paper comprehensively analyses the state-of-the-art feature extraction methods employed in brain cancer research. The paper critically evaluates the strengths and weaknesses of various feature extraction approaches, including traditional handcrafted features, machine learning-based techniques, and advanced deep learning methods. Furthermore, we discuss the emerging trends and potential future directions in this dynamic field. By synthesizing the current literature, this review aims to provide researchers and clinicians with a valuable resource for understanding the role of feature extraction, spanning traditional, machine learning, and deep learning-based methods, in brain cancer analysis and promoting further advancements in this critical area of study.",Deep learning;Learning systems;Training;Reviews;Brain cancer;Feature extraction;Market research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459674,IEEE Conferences,,,,,,
Integrated Transfer Learning and Nature-Inspired Optimization for Enhanced Feature Extraction in Diabetic Retinopathy Image Analysis,R. G. Tiwari; A. Kumar,2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS),########,2024,"This research aims to detect diabetic retinopathy using optimized features extracted from deep learning model. Initially, several deep learning architectures are trained using retinal image dataset and the best model is determined. Regarding transfer learning approaches for diabetic retinopathy patients, SqueezeNet seems to be the best model. The proposed model in this research relies on a two-stage optimization process to enhance the features extracted by SqueezeNet. Deep features obtained by SqueezeNet are optimized using Particle Swarm Optimization (PSO) and the Crow Search Algorithm (CSA). Merging the results of the two optimization methods with a value-maximizing solution is essential for producing an accurate and resilient feature vector. The proposed hybrid model employs a variety of machine-learning algorithms to classify diabetic retinopathy and non-diabetic retinopathy cases. The experimental findings indicate that the suggested method is effective with correct classification accuracy of 96.8%.",Deep learning;Diabetic retinopathy;Heuristic algorithms;Transfer learning;Feature extraction;Retina;Vectors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459394,IEEE Conferences,,,,,,
Diabetic Eye Health: Deep Learning Classification,P. Dongre; S. Kedia; J. Banubakade; D. M. Kotambkar,2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS),########,2024,"In individuals around the world, Diabetic Retinopathy (DR) and Diabetic Macular Edema (DME) is the most prevalent consequence of diabetes and a major factor in visual loss. The Convolutional Neural Network (CNN) architecture shown in this research is intended to automatically identify Diabetic Macular Edema (DME) and Diabetic Retinopathy (DR) from retinal fundus images. After being trained on a sizable dataset made up of several classes, the CNN model used inception is capable of outperforming earlier methods by reliably diagnosing the presence and severity of specific diseases. Its ability to handle a wide range of image qualities and intricate pathological aspects makes it a solid instrument for improved patient outcomes and early intervention, which lessens the toll that Diabetic eye disease takes on society and healthcare systems. We give a thorough experimental assessment of our methodology on a benchmark dataset, illustrating its efficacy in precisely identifying various stages involves Diabetic Retinopathy and Diabetic Macular Edema. The obtained results demonstrate a good level of performance and highlight the potential of deep learning methods in diagnosis.",Deep learning;Diabetic retinopathy;Visualization;Pathology;Medical services;Solids;Retina,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459705,IEEE Conferences,,,,,,
A Deep Learning Based Image Processing Technique for Early Lung Cancer Prediction,N. Tasnim; K. R. Noor; M. Islam; M. N. Huda; I. H. Sarker,2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS),########,2024,"Lung cancer is the primary cause of cancer mor-tality all over the world due to the increase of tobacco consumption, and industrialization in developing nations. As the early-stage diagnosis can reduce the mortality rate significantly, early detection with the availability of high-tech Medical facilities is highly necessary. In this research, we used deep learning (DL) methods initially on patient's 1190 CT scan images from the Kaggle IQ-OTH lung cancer dataset, and after significant image preprocessing steps we found augmented images including normal, malignant, and benign cases to identify high-risk in-dividuals to detect lung cancer and also predict the malignancy and thus, taking early actions to prevent long-term consequences. A thorough performance comparison between several classifiers, including the conventional CNN, Resnet50, and InceptionV3, has been presented. Here, affine transformation, gaussian noise, and other rigorous image preprocessing techniques are used. The contribution obtained a 98% validation accuracy while reducing the model's complexity with the previous preprocessing stage. The comparison method shows that the suggested preprocessing method yields a higher F1 score value of 97%, validating our suggested methodology.",Deep learning;Measurement;Computed tomography;Lung cancer;Image preprocessing;Reliability;Sustainable development,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459494,IEEE Conferences,,,,,,
Deep Learning in Medical Image Diagnosis for COVID-19,S. R. Satti; J. S. K. Lankadasu; A. Sharma; S. Sharma; S. Gochhait,2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS),########,2024,"The global COVID-19 pandemic has resulted in significant loss of life and profoundly affected every aspect of human existence. A noteworthy area of study in this crisis is the use of deep learning (DL) models in medical imaging for the treatment of patients with COVID-19. This in-depth research delves into various methods of medical imaging, such as X-rays and computed tomography (CT) images, and their use in DL approaches for differentiating between COVID-19 and pneumonia. The paper outlines how DL techniques, including image localization, segmentation, registration, and classification, can aid in the detection of COVID-19. Recent evaluations have shown InstaCovNet-19 to have a remarkable classification accuracy of 99.80 percent when applied to an Xray dataset of 361 COVID-19 patients, 362 pneumonia patients, and 365 healthy individuals.",COVID-19;Deep learning;Location awareness;Pneumonia;Pandemics;Computed tomography;X-rays,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459430,IEEE Conferences,,,,,,
An Explainable Artificial Intelligence Integrated System for Automatic Detection of Dengue From Images of Blood Smears Using Transfer Learning,H. Mayrose; N. Sampathila; G. M. Bairy; T. Nayak; S. Belurkar; K. Saravu,IEEE Access,########,2024,"Dengue fever is a rapidly increasing mosquito-borne ailment spread by the virus DENV in the tropics and subtropics worldwide. It is a significant public health problem and accounts for many deaths globally. Implementing more effective methods that can more accurately detect dengue cases is challenging. The theme of this digital pathology-associated research is automatic dengue detection from peripheral blood smears (PBS) employing deep learning (DL) techniques. In recent years, DL has been significantly employed for automated computer-assisted diagnosis of various diseases from medical images. This paper explores pre-trained convolution neural networks (CNNs) for automatic dengue fever detection. Transfer learning (TL) is executed on three state-of-the-art CNNs â€“ ResNet50, MobileNetV3Small, and MobileNetV3Large, to customize the models for differentiating the dengue-infected blood smears from the healthy ones. The dataset used to design and test the models contains 100x magnified dengue-infected and healthy control digital microscopic PBS images. The models are validated with a 5-fold cross-validation framework and tested on unseen data. An explainable artificial intelligence (XAI) approach, Gradient-weighted Class Activation Mapping (GradCAM), is eventually applied to the models to allow visualization of the precise regions on the smears most instrumental in making the predictions. While all three transferred pre-trained CNN models performed well (above 98% overall classification accuracy), MobileNetV3Small is the recommended model for this classification problem due to its significantly less computationally demanding characteristics. Transferred pre-trained CNN based on MobileNetV3Small yielded Accuracy, Recall, Specificity, Precision, F1 Score, and Area Under the ROC Curve (AUC) of 0.982 Â± 0.011, 0.973 Â± 0.027, 0.99 Â± 0.013, 0.989 Â± 0.015, 0.981 Â± 0.012 and 0.982 Â± 0.012 respectively, averaged over the five folds on the unseen dataset. Promising results show that the developed models have the potential to provide high-quality support to haematologists by expertly performing tedious, repetitive, and time-consuming tasks in hospitals and remote/low-resource settings.",Viruses (medical);Predictive models;Microscopy;Medical diagnostic imaging;Feature extraction;Training;Blood,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10474015,IEEE Journals,,,,,,
An MIoT Framework of Consumer Technology for Medical Diseases Prediction,S. Pattanaik; C. Chakraborty; S. Behera; S. K. Majhi; S. K. Pani,IEEE Transactions on Consumer Electronics,29-Apr-24,2024,"The healthcare sector has evolved by integrating consumer technologies, IoT, and deep learning. IoT in healthcare includes connected-health, smart-health, and mobile-health, enabling devices to share information for better care. Deep learning, particularly in medical imaging, shows promise for future medical applications. A recent study proposed a hybrid model using Stacked BiLSTM with Resnet50 Model and Adaswarm optimizer to classify medical disorders from five image datasets collected from consumer devices. These datasets, including COVID-19, Pneumonia, Malaria, lung cancer, and Brain Tumor, were employed to train the model. The dataset collected by sensors are sent to the cloud for sorting through a gateway. In this IoT framework, more consumer electronic products like microcontrollers and sockets are used in consumer devices. The proposed meta-heuristic algorithm-based model achieved an impressive accuracy of 99% with an average loss of 0.019. Additionally, the study compared this model with existing prototypes across various classification measures, demonstrating its efficacy.",Predictive models;Diseases;Medical services;Consumer electronics;Medical diagnostic imaging;Residual neural networks;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473196,IEEE Journals,,,,,,
Balanced And Discriminative Contrastive Learning For Class-Imbalanced Medical Images,X. Li; Y. Fan; H. Zheng; J. Gao; X. Wei; M. Yu,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",########,2024,"The class imbalance problem, which is prevalent in medical image datasets, seriously affects the diagnostic effectiveness of deep learning-based network models. Recently, the method based on two-stage learning has produced promising results in solving class imbalance. In two-stage learning, the learning of unbiased classifiers has been well studied, but the representation of imbalanced data is still being explored. In this paper, we focus on the representation learning stage of class-imbalanced and propose a novel balanced and discriminative contrastive learning (BDCL) method. Compared with supervised contrastive learning, BDCL has two improvements: temperature dynamic learning, which balances the gradient contribution of negative samples from different classes; hard example prototypes, which can better learn the feature differences between tail classes. A variety of experimental results on the imbalanced medical image datasets show that BDCL enables the network models to learn representations with desirable balancedness and discriminativeness, effectively solving the class imbalance problem.",Representation learning;Training;Prototypes;Self-supervised learning;Tail;Signal processing;Acoustics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10447586,IEEE Conferences,,,,,,
Mixed Informed Transformer for Few-Shot Medical Image Segmentation,J. Li; Z. Wang; S. Zhu,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",########,2024,"With the rapid development of deep learning techniques trained on datasets that require a large amount of manual annotation, significant progress has been made in the field of medical image segmentation. However, the annotation of medical image analysis requires not only very high labor costs, but also specific expertise in the medical field. In order to address this limitation, few-shot medical image segmentation has become a very promising and popular research direction, aiming to learn novel classes from limited supervised medical data. We propose a novel framework called Mixed Informed Transformer (MIT) for few-shot medical image segmentation. Our MIT feasibly augments the representation capability of support prototype and query features. By incorporating augmented query features and support features, MIT effectively learns the foreground information of medical images and eliminates redundant background information. Experimental results obtained on three benchmark datasets are compared with state-of-the-art few-shot medical image segmentation methods, which demonstrates the effectiveness and competitiveness of our proposed approach.",Deep learning;Image segmentation;Image analysis;Annotations;Prototypes;Manuals;Signal processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448512,IEEE Conferences,,,,,,
NSGA-II-DL: Metaheuristic Optimal Feature Selection With Deep Learning Framework for HER2 Classification in Breast Cancer,T. A. Rashid; J. Majidpour; R. Thinakaran; M. Batumalay; D. A. Dewi; B. A. Hassan; H. Dadgar; H. Arabi,IEEE Access,########,2024,"Immunohistochemistry (IHC) slides are graded for breast cancer based on visual markers and morphological characteristics of stained membrane regions. The usage of whole slide images (WSIs) from histology in digital pathology algorithms for computer-assisted evaluations has increased recently. Human epidermal growth factor receptor 2 (HER2)-stained microscopic images are challenging, time-consuming, and error-prone to evaluate manually. This is due to different staining, overlapped regions, and huge, non-homogeneous slides. Additionally, the classification of HER2 images by the selection of fundamental features must be used to capture the difficult elements of the images, such as the irregular cell structure and the coloring of the tissue of the cells. To solve the above problems, a transfer learning model-based, trainable metaheuristic method for choosing the best features is suggested in this paper. Moreover, the suggested model is efficient in reducing model complexity and computational costs as well as avoiding overfitting. The four main components of the proposed cascaded design are: (1) converting WSIs to tiled images and enhancing contrast with fast local Laplacian filtering (FlLpF); (2) extracting features with a ResNet50 CNN technique based on transfer learning; (3) selecting the most informative features with the help of a non-dominated sorting genetic algorithm (NSGA-II) optimizer; and (4) using a support vector machine (SVM) to classify HER2 scores. Results from the HER2SC and HER2GAN datasets show that the suggested model is superior to other methods already in use, with 94.4% accuracy, 93.71% precision, 98.07% specificity, 93.83% sensitivity, and a 93.71% F1-score for the HER2SC dataset being achieved.",Transfer learning;Feature extraction;Computational modeling;Support vector machines;Microscopy;Metaheuristics;Sensitivity analysis;Visualization;Breast cancer;Electronic healthcare;Computer aided diagnosis;Pathology;Residual neural networks;Accuracy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463051,IEEE Journals,,,,,,
Cardiac Valve Event Timing in Echocardiography Using Deep Learning and Triplane Recordings,B. S. Fermann; J. Nyberg; E. W. Remme; J. F. Grue; H. Grue; R. HÃ¥land; L. Lovstakken; H. Dalen; B. Grenne; S. A. Aase; S. R. Snare; A. Ã˜stvik,IEEE Journal of Biomedical and Health Informatics,########,2024,"Cardiac valve event timing plays a crucial role when conducting clinical measurements using echocardiography. However, established automated approaches are limited by the need of external electrocardiogram sensors, and manual measurements often rely on timing from different cardiac cycles. Recent methods have applied deep learning to cardiac timing, but they have mainly been restricted to only detecting two key time points, namely end-diastole (ED) and end-systole (ES). In this work, we propose a deep learning approach that leverages triplane recordings to enhance detection of valve events in echocardiography. Our method demonstrates improved performance detecting six different events, including valve events conventionally associated with ED and ES. Of all events, we achieve an average absolute frame difference (aFD) of maximum 1.4 frames (29 ms) for start of diastasis, down to 0.6 frames (12 ms) for mitral valve opening when performing a ten-fold cross-validation with test splits on triplane data from 240 patients. On an external independent test consisting of apical long-axis data from 180 other patients, the worst performing event detection had an aFD of 1.8 (30 ms). The proposed approach has the potential to significantly impact clinical practice by enabling more accurate, rapid and comprehensive event detection, leading to improved clinical measurements.",Valves;Timing;Recording;Biomedical measurement;Long short term memory;Deep learning;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10460097,IEEE Journals,,,,,,
Pneumonia Detection Using Chest Radiographs With Novel EfficientNetV2L Model,M. Ali; M. Shahroz; U. Akram; M. F. Mushtaq; S. C. Altamiranda; S. A. Obregon; I. D. L. T. DÃ­ez; I. Ashraf,IEEE Access,########,2024,"Pneumonia is a potentially life-threatening infectious disease that is typically diagnosed through physical examinations and diagnostic imaging techniques such as chest X-rays, ultrasounds or lung biopsies. Accurate diagnosis is crucial as wrong diagnosis, inadequate treatment or lack of treatment can cause serious consequences for patients and may become fatal. The advancements in deep learning have significantly contributed to aiding medical experts in diagnosing pneumonia by assisting in their decision-making process. By leveraging deep learning models, healthcare professionals can enhance diagnostic accuracy and make informed treatment decisions for patients suspected of having pneumonia. In this study, six deep learning models including CNN, InceptionResNetV2, Xception, VGG16, ResNet50 and EfficientNetV2L are implemented and evaluated. The study also incorporates the Adam optimizer, which effectively adjusts the epoch for all the models. The models are trained on a dataset of 5856 chest X-ray images and show 87.78%, 88.94%, 90.7%, 91.66%, 87.98% and 94.02% accuracy for CNN, InceptionResNetV2, Xception, VGG16, ResNet50 and EfficientNetV2L, respectively. Notably, EfficientNetV2L demonstrates the highest accuracy and proves its robustness for pneumonia detection. These findings highlight the potential of deep learning models in accurately detecting and predicting pneumonia based on chest X-ray images, providing valuable support in clinical decision-making and improving patient treatment.",Pneumonia;Deep learning;Biomedical imaging;X-ray imaging;Lung;Computational modeling;Diseases;Data augmentation;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10458135,IEEE Journals,,,,,,
Medical SAM: A Glioma Segmentation Fine-tuning Method for SAM Using Brain MR Images,X. Shi; Y. Li; J. Cheng; J. Bai; G. Zhao; Y. -W. Chen,2024 IEEE International Conference on Consumer Electronics (ICCE),########,2024,"Based on the 2016 World Health Organization (WHO) Classification scheme for gliomas, accurate glioma segmentation serves as a fundamental basis for glioma diagnosis. In the realm of glioma diagnosis, brain MRI has emerged as an indispensable diagnostic tool due to its ability to provide comprehensive information. Over the past decade, there has been a notable surge in the utilization of machine learning techniques, particularly deep learning, for processing medical images. These deep learning methods, based on convolutional neural networks or transformers, proposed analogous architectures such as U-Net for precise segmentation of medical images, thereby significantly enhancing the accuracy of brain glioma segmentation. Thanks to the development of foundation models, models pre-trained with large-scale datasets have achieved better results on a variety of tasks. However, for medical images with small dataset sizes, deep learning methods struggle to achieve better results than on real-world image datasets. In this study, we proposed an adapter to effectively fine-tune the foundation model (SAM) for improved glioma segmentation using brain MR images. The effectiveness of the proposed method is validated via our private glioma data set from the First Affiliated Hospital of Zhengzhou University (FHZU) in Zhengzhou, China. The proposed method outperforms current state-of-the-art methods, achieving a Dice coefficient of 87.33% and a Hausdorff distance of 10.87 for accurate segmentation of the glioma region in glioma treatment, representing a significant 4% improvement in Dice coefficient.",Deep learning;Image segmentation;Adaptation models;Brain modeling;Data models;Task analysis;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10444252,IEEE Conferences,,,,,,
Autonomous Diagnosis System of Breast Cancer,M. Son; W. Jun; S. Lee,2024 IEEE International Conference on Consumer Electronics (ICCE),########,2024,"This paper focuses on a study of medical image techniques using deep learning, specifically addressing methods for diagnosing breast cancer. The research aims to enhance breast cancer classification and localization through image classification and segmentation techniques utilizing mammography, ultrasound, and histopathology images. Among various image classification and segmentation techniques, the study selects technology and loss functions optimized for medical imaging characteristics, along with proposing data augmentation methods. The research findings demonstrate that using filter-based techniques for data augmentation yields excellent performance in image classification using ResNet50. Additionally, for the segmentation of mammography and ultrasound images, the UNet architecture performs exceptionally well. Through the application of these techniques, the segmentation performance of mammography images improved by 33.3%, ultrasound image segmentation improved by 29.9%, and histopathology image classification accuracy increased by 22.8%. This research presents a contribution to deep learning-based medical image processing in the context of breast cancer diagnosis.",Image segmentation;Ultrasonic imaging;Histopathology;Data augmentation;Breast cancer;Medical diagnostic imaging;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10444471,IEEE Conferences,,,,,,
An Efficient and Robust Approach Using Inductive Transfer-Based Ensemble Deep Neural Networks for Kidney Stone Detection,J. Chaki; A. UÃ§ar,IEEE Access,########,2024,"Chronic kidney disorder is a global health problem involving the repercussions of impaired kidney function and kidney failure. A kidney stone is a kidney scenario that impairs kidney function. Because this disease is usually asymptomatic, early and quick detection of kidney problems is essential to avoid significant consequences. This study presents an automated detection of Computed Tomography (CT) kidney stone images using an inductive transfer-based ensemble Deep Neural Network (DNN). Three datasets are created for feature extraction from kidney CT images using pre-trained DNN models. After assembling several pre-trained DNNs, such as DarkNet19, InceptionV3, and ResNet101, the ensemble deep feature vector is created using feature concatenation. The Iterative ReliefF feature selection method is used to choose the most informative ensemble deep feature vectors, which are then fed into the K Nearest Neighbor classifier tuned using a Bayesian optimizer with a 10-fold cross-validation approach to detect kidney stones. The proposed strategy achieves 99.8% and 96.7% accuracy using the quality and noisy image datasets, which are superior to other DNN-based and traditional image detection approaches. This proposed automated approach can help urologists confirm their physical inspection of kidney stones, reducing the possibility of human mistakes.",Feature extraction;Computed tomography;Image recognition;Diseases;Vectors;Classification algorithms;Deep learning;Kidney stones;Ensemble learning;Chronic kidney disease;Artificial neural networks;Detection algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10445453,IEEE Journals,,,,,,
Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery,M. Xu; M. Islam; L. Bai; H. Ren,IEEE Transactions on Medical Imaging,03-Jun-24,2024,"Deep Neural Networks (DNNs) based semantic segmentation of the robotic instruments and tissues can enhance the precision of surgical activities in robot-assisted surgery. However, in biological learning, DNNs cannot learn incremental tasks over time and exhibit catastrophic forgetting, which refers to the sharp decline in performance on previously learned tasks after learning a new one. Specifically, when data scarcity is the issue, the model shows a rapid drop in performance on previously learned instruments after learning new data with new instruments. The problem becomes worse when it limits releasing the dataset of the old instruments for the old model due to privacy concerns and the unavailability of the data for the new or updated version of the instruments for the continual learning model. For this purpose, we develop a privacy-preserving synthetic continual semantic segmentation framework by blending and harmonizing (i) open-source old instruments foreground to the synthesized background without revealing real patient data in public and (ii) new instruments foreground to extensively augmented real background. To boost the balanced logit distillation from the old model to the continual learning model, we design overlapping class-aware temperature normalization (CAT) by controlling model learning utility. We also introduce multi-scale shifted-feature distillation (SD) to maintain long and short-range spatial relationships among the semantic objects where conventional short-range spatial features with limited information reduce the power of feature distillation. We demonstrate the effectiveness of our framework on the EndoVis 2017 and 2018 instrument segmentation dataset with a generalized continual learning setting. Code is available at https://github.com/XuMengyaAmy/Synthetic_CAT_SD.",Instruments;Task analysis;Surgery;Semantic segmentation;Robots;Data models;Semantics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10443356,IEEE Journals,,,,,,
Multi-Scale Feature Alignment for Continual Learning of Unlabeled Domains,K. Thandiackal; L. Piccinelli; R. Gupta; P. Pati; O. Goksel,IEEE Transactions on Medical Imaging,01-Jul-24,2024,"Methods for unsupervised domain adaptation (UDA) help to improve the performance of deep neural networks on unseen domains without any labeled data. Especially in medical disciplines such as histopathology, this is crucial since large datasets with detailed annotations are scarce. While the majority of existing UDA methods focus on the adaptation from a labeled source to a single unlabeled target domain, many real-world applications with a long life cycle involve more than one target domain. Thus, the ability to sequentially adapt to multiple target domains becomes essential. In settings where the data from previously seen domains cannot be stored, e.g., due to data protection regulations, the above becomes a challenging continual learning problem. To this end, we propose to use generative feature-driven image replay in conjunction with a dual-purpose discriminator that not only enables the generation of images with realistic features for replay, but also promotes feature alignment during domain adaptation. We evaluate our approach extensively on a sequence of three histopathological datasets for tissue-type classification, achieving state-of-the-art results. We present detailed ablation experiments studying our proposed method components and demonstrate a possible use-case of our continual UDA method for an unsupervised patch-based segmentation task given high-resolution tissue images. Our code is available at: https://github.com/histocartography/multi-scale-feature-alignment.",Feature extraction;Task analysis;Training;Generative adversarial networks;Adaptation models;Medical diagnostic imaging;Generators,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10443417,IEEE Journals,,,,,,
MFEM-CIN: A Lightweight Architecture Combining CNN and Transformer for the Classification of Pre-Cancerous Lesions of the Cervix,P. Chen; F. Liu; J. Zhang; B. Wang,IEEE Open Journal of Engineering in Medicine and Biology,########,2024,"Goal: Cervical cancer is one of the most common cancers in women worldwide, ranking among the top four. Unfortunately, it is also the fourth leading cause of cancer-related deaths among women, particularly in developing countries where incidence and mortality rates are higher compared to developed nations. Colposcopy can aid in the early detection of cervical lesions, but its effectiveness is limited in areas with limited medical resources and a lack of specialized physicians. Consequently, many cases are diagnosed at later stages, putting patients at significant risk. Methods: This paper proposes an automated colposcopic image analysis framework to address these challenges. The framework aims to reduce the labor costs associated with cervical precancer screening in undeserved regions and assist doctors in diagnosing patients. The core of the framework is the MFEM-CIN hybrid model, which combines Convolutional Neural Networks (CNN) and Transformer to aggregate the correlation between local and global features. This combined analysis of local and global information is scientifically useful in clinical diagnosis. In the model, MSFE and MSFF are utilized to extract and fuse multi-scale semantics. This preserves important shallow feature information and allows it to interact with the deep feature, enriching the semantics to some extent. Conclusions: The experimental results demonstrate an accuracy rate of 89.2% in identifying cervical intraepithelial neoplasia while maintaining a lightweight model. This performance exceeds the average accuracy achieved by professional physicians, indicating promising potential for practical application. Utilizing automated colposcopic image analysis and the MFEM-CIN model, this research offers a practical solution to reduce the burden on healthcare providers and improve the efficiency and accuracy of cervical cancer diagnosis in resource-constrained areas.",Biomedical engineering;Medical services;Visualization;Neoplasms;Lesions;Cervical cancer;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440497,IEEE Journals,,,,,,
A Detectability Analysis of Retinitis Pigmetosa Using Novel SE-ResNet Based Deep Learning Model and Color Fundus Images,R. Rashid; W. Aslam; A. Mehmood; D. L. R. Vargas; I. D. L. T. Diez; I. Ashraf,IEEE Access,########,2024,"Retinitis pigmentosa (RP) is a group of genetic retinal disorders characterized by progressive vision loss, culminating in blindness. Identifying pigment signs (PS) linked with RP is crucial for monitoring and possibly slowing the diseaseâ€™s degenerative course. However, the segmentation and detection of PS are challenging due to the difficulty of distinguishing between PS and blood vessels and the variability in size, shape, and color of PS. Recently, advances in deep learning techniques have shown impressive results in medical image analysis, especially in ophthalmology. This study presents an approach for classifying pigment marks in color fundus images of RP using a modified squeeze-and-excitation ResNet (SE-ResNet) architecture. This variant synergizes the efficiency of residual skip connections with the robust attention mechanism of the SE block to amplify feature representation. The SE-ResNet model was fine-tuned to determine the optimal layer configuration that balances performance metrics and computational costs. We trained the proposed model on the RIPS dataset, which comprises images from patients diagnosed at various RP stages. Experimental results confirm the efficacy of the proposed model in classifying different types of pigment signs associated with RP. The model yielded performance metrics, such as accuracy, sensitivity, specificity, and f-measure of 99.16%, 97.70%, 96.93%, 90.47%, 99.37%, 97.80%, 97.44%, and 90.60% on the testing set, based on GT1 & GT2 respectively. Given its performance, this model is an excellent candidate for integration into computer-aided diagnostic systems for RP, aiming to enhance patient care and vision-related healthcare services.",Retina;Image color analysis;Pigments;Image segmentation;Diseases;Deep learning;Visualization;Computational modeling;Visual impairment;Visual systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440277,IEEE Journals,,,,,,
Application of Semi-Supervised Learning in Image Classification: Research on Fusion of Labeled and Unlabeled Data,S. Li; P. Kou; M. Ma; H. Yang; S. Huang; Z. Yang,IEEE Access,########,2024,"Deep learning has attracted wide attention recently because of its excellent feature representation ability and end-to-end automatic learning method. Especially in clinical medical imaging diagnosis, the semi-supervised deep learning model is favored and widely used because it can make maximum use of a limited number of labeled data and combine it with a large number of unlabeled data to extract more information and knowledge from it. However, the scarcity of medical image data, the vast image size, and the instability of image quality directly affect the modelâ€™s robustness, generalization, and image classification performance. Therefore, this paper proposes a new semi-supervised learning model, which uses quadratic neurons instead of traditional neurons, aiming to use quadratic convolution instead of the conventional convolution layer to improve the feature extraction ability of the model. In addition, we introduce two Dropout layers and two fully connected layers at the end of the model to enhance the nonlinear fitting ability of the network. Experiments on two large medical public data sets - ISIC 2019 and Retinopathy OCT - show that our method can improve the modelâ€™s generalization performance and image classification accuracy.",Neurons;Convolutional neural networks;Medical diagnostic imaging;Image classification;Feature extraction;Training;Mathematical models;Semisupervised learning;Biomedical imaging;Quadratic programming,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440308,IEEE Journals,,,,,,
A Collaborative Self-Supervised Domain Adaptation for Low-Quality Medical Image Enhancement,Q. Hou; Y. Wang; P. Cao; S. Cheng; L. Lan; J. Yang; X. Liu; O. R. Zaiane,IEEE Transactions on Medical Imaging,01-Jul-24,2024,"Medical image analysis techniques have been employed in diagnosing and screening clinical diseases. However, both poor medical image quality and illumination style inconsistency increase uncertainty in clinical decision-making, potentially resulting in clinician misdiagnosis. The majority of current image enhancement methods primarily concentrate on enhancing medical image quality by leveraging high-quality reference images, which are challenging to collect in clinical applications. In this study, we address image quality enhancement within a fully self-supervised learning setting, wherein neither high-quality images nor paired images are required. To achieve this goal, we investigate the potential of self-supervised learning combined with domain adaptation to enhance the quality of medical images without the guidance of high-quality medical images. We design a Domain Adaptation Self-supervised Quality Enhancement framework, called DASQE. More specifically, we establish multiple domains at the patch level through a designed rule-based quality assessment scheme and style clustering. To achieve image quality enhancement and maintain style consistency, we formulate the image quality enhancement as a collaborative self-supervised domain adaptation task for disentangling the low-quality factors, medical image content, and illumination style characteristics by exploring intrinsic supervision in the low-quality medical images. Finally, we perform extensive experiments on six benchmark datasets of medical images, and the experimental results demonstrate that DASQE attains state-of-the-art performance. Furthermore, we explore the impact of the proposed method on various clinical tasks, such as retinal fundus vessel/lesion segmentation, nerve fiber segmentation, polyp segmentation, skin lesion segmentation, and disease classification. The results demonstrate that DASQE is advantageous for diverse downstream image analysis tasks.",Image quality;Lesions;Image enhancement;Lighting;Anatomical structure;Task analysis;Image segmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440106,IEEE Journals,,,,,,
Weakly Supervised Lesion Detection and Diagnosis for Breast Cancers With Partially Annotated Ultrasound Images,J. Wang; L. Qiao; S. Zhou; J. Zhou; J. Wang; J. Li; S. Ying; C. Chang; J. Shi,IEEE Transactions on Medical Imaging,01-Jul-24,2024,"Deep learning (DL) has proven highly effective for ultrasound-based computer-aided diagnosis (CAD) of breast cancers. In an automatic CAD system, lesion detection is critical for the following diagnosis. However, existing DL-based methods generally require voluminous manually-annotated region of interest (ROI) labels and class labels to train both the lesion detection and diagnosis models. In clinical practice, the ROI labels, i.e. ground truths, may not always be optimal for the classification task due to individual experience of sonologists, resulting in the issue of coarse annotation to limit the diagnosis performance of a CAD model. To address this issue, a novel Two-Stage Detection and Diagnosis Network (TSDDNet) is proposed based on weakly supervised learning to improve diagnostic accuracy of the ultrasound-based CAD for breast cancers. In particular, all the initial ROI-level labels are considered as coarse annotations before model training. In the first training stage, a candidate selection mechanism is then designed to refine manual ROIs in the fully annotated images and generate accurate pseudo-ROIs for the partially annotated images under the guidance of class labels. The training set is updated with more accurate ROI labels for the second training stage. A fusion network is developed to integrate detection network and classification network into a unified end-to-end framework as the final CAD model in the second training stage. A self-distillation strategy is designed on this model for joint optimization to further improves its diagnosis performance. The proposed TSDDNet is evaluated on three B-mode ultrasound datasets, and the experimental results indicate that it achieves the best performance on both lesion detection and diagnosis tasks, suggesting promising application potential.",Training;Solid modeling;Lesions;Annotations;Task analysis;Ultrasonic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10439278,IEEE Journals,,,,,,
Leveraging Advanced Visual Recognition Classifier For Pneumonia Prediction,M. Raval; J. Aobo; Y. Wan; H. Gohel,2024 IEEE 3rd International Conference on AI in Cybersecurity (ICAIC),########,2024,"Pneumonia prediction using chest X-ray images is a challenging task because of the complex image processing involved. The radiographic features of pneumonia, especially in the earlier stages, easily overlap with other lung conditions, which makes the differentiation even more challenging. Moreover, X-ray image quality varies due to equipment, patient condition, and techniques, particularly in rural areas with undertrained radiologists and medical experts. The use of Artificial Intelligence (AI) models in detecting pneumonia is a novel but crucial research field and rapid advancement in medical imaging technology and neural network models along with the availability of large de-identified public datasets has paved the way for this life-saving biomedical research. In this paper, we propose a unique comprehensive solution for predicting pneumonia using chest X-ray images. We utilize an enhanced VGGNet model tailored for the binary classification task. The modified VGG19 with a binary classifier provides a solid foundation for feature extraction and leverages the pretrained features and deep architecture to differentiate between normal and pneumonia-affected lung images. The use of transfer learning allows us to extend the pre-trained model on a diverse and large-scale dataset by further training it on limited-size medical imaging data for the crucial task of biomedical classification without the need for large, labeled training data or computational resources. The robust model displays high accuracy of 92% with a high recall of 96.4% and AUC of 97%. With high adaptability and efficient learning capacity from limited data. This implementation may serve as a powerful tool assisting medical professionals in diagnosing pneumonia by quickly analyzing X-ray images with the same consistency and accuracy. During crises such as pandemics where lung diseases might surge, such tools can aid in rapid screening and monitoring of public health.",Visualization;Pulmonary diseases;Biological system modeling;Task analysis;Artificial intelligence;X-ray imaging;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433800,IEEE Conferences,,,,,,
Scan-Specific Self-Supervised Bayesian Deep Non-Linear Inversion for Undersampled MRI Reconstruction,A. P. Leynes; N. Deveshwar; S. S. Nagarajan; P. E. Z. Larson,IEEE Transactions on Medical Imaging,05-Jun-24,2024,"Magnetic resonance imaging is subject to slow acquisition times due to the inherent limitations in data sampling. Recently, supervised deep learning has emerged as a promising technique for reconstructing sub-sampled MRI. However, supervised deep learning requires a large dataset of fully-sampled data. Although unsupervised or self-supervised deep learning methods have emerged to address the limitations of supervised deep learning approaches, they still require a database of images. In contrast, scan-specific deep learning methods learn and reconstruct using only the sub-sampled data from a single scan. Here, we introduce Scan-Specific Self-Supervised Bayesian Deep Non-Linear Inversion (DNLINV) that does not require an auto calibration scan region. DNLINV utilizes a Deep Image Prior-type generative modeling approach and relies on approximate Bayesian inference to regularize the deep convolutional neural network. We demonstrate our approach on several anatomies, contrasts, and sampling patterns and show improved performance over existing approaches in scan-specific calibrationless parallel imaging and compressed sensing.",Coils;Magnetic resonance imaging;Deep learning;Image reconstruction;Sensitivity;Imaging;Compressed sensing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431784,IEEE Journals,,,,,,
Toward Ground-Truth Optical Coherence Tomography via Three-Dimensional Unsupervised Deep Learning Processing and Data,G. Ni; R. Wu; F. Zheng; M. Li; S. Huang; X. Ge; L. Liu; Y. Liu,IEEE Transactions on Medical Imaging,03-Jun-24,2024,"Optical coherence tomography (OCT) can perform non-invasive high-resolution three-dimensional (3D) imaging and has been widely used in biomedical fields, while it is inevitably affected by coherence speckle noise which degrades OCT imaging performance and restricts its applications. Here we present a novel speckle-free OCT imaging strategy, named toward-ground-truth OCT ( ${t}$ GT-OCT), that utilizes unsupervised 3D deep-learning processing and leverages OCT 3D imaging features to achieve speckle-free OCT imaging. Specifically, our proposed ${t}$ GT-OCT utilizes an unsupervised 3D-convolution deep-learning network trained using random 3D volumetric data to distinguish and separate speckle from real structures in 3D imaging volumetric space; moreover, ${t}$ GT-OCT effectively further reduces speckle noise and reveals structures that would otherwise be obscured by speckle noise while preserving spatial resolution. Results derived from different samples demonstrated the high-quality speckle-free 3D imaging performance of ${t}$ GT-OCT and its advancement beyond the previous state-of-the-art. The code is available online: https://github.com/Voluntino/tGT-OCT.",Three-dimensional displays;Speckle;Imaging;Noise measurement;Deep learning;Spatial resolution;Training,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10423838,IEEE Journals,,,,,,
A Deep Learning Harmonization of Multi-Vendor MRI for Robust Intervertebral Disc Segmentation,C. Kim; S. -M. Park; S. Lee; D. Lee,IEEE Access,########,2024,"Magnetic resonance imaging (MRI) provides enhanced soft tissue contrast and high spatial resolution. However, the relationship between intensity values among soft tissues in MRI is inconsistent, even when obtained under the same conditions (e.g., vendors and acquisition protocols). This inconsistency hinders accurate medical image segmentation and disease classification. Therefore, we propose a framework to harmonize multi-vendor MRI using a novel radiomics approach for robust segmentation. The proposed model comprises a cycle-consistent adversarial network (CycleGAN)-based network and a segmentation network. The CycleGAN-based network harmonizes MRI with the support of a radiomics-based method (radiomic feature (RF) loss function newly designed for this study). The segmentation network encourages the CycleGAN-based network to enhance intervertebral disc (IVD) segmentation features using dice loss functions during harmonization. Furthermore, publicly available datasets and diverse MRI scans provided by a collaborating hospital were used to make our model more robust to MRI variability. The proposed model was evaluated for segmentation using the Dice coefficient, intersection-over-union (IoU), F1 score, precision, and recall. It outperformed other segmentation methods (Dice = 0.920, IoU = 0.853, F1 score = 0.920, precision = 0.940, and recall = 0.902), even on diverse test datasets with disease information. The harmonization performance was assessed using the relative error of the RF values between the target (standard) and harmonized data. It achieved the four best scores ( $\mathrm {\approx 0}$ ) among the five features in a relative error of RF compared to other harmonization methods (e.g., conventional histogram-based method and deep learning model).",Magnetic resonance imaging;Image segmentation;Feature extraction;Radiomics;Radio frequency;Computed tomography;Diseases;Harmonic analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10419023,IEEE Journals,,,,,,
Terahertz Data Extraction and Analysis Based on Deep Learning Techniques for Emerging Applications,M. Gezimati; G. Singh,IEEE Access,########,2024,"Following the recent progress in the development of Terahertz (THz) generation and detection, THz technology is being widely used to characterize test sample properties in various applications including nondestructive testing, security inspection and medical applications. In this paper, we have presented a broad review of the recent usage of artificial intelligence (AI) particularly, deep learning techniques in various THz sensing, imaging, and spectroscopic applications with emphasis on their implementation for medical imaging of cancerous cells. Initially, the fundamentals principles and techniques for THz generation and detection, imaging and spectroscopy are introduced. Subsequently, a brief overview of AI â€“ machine learning and deep learning techniques is summarized, and their performance is compared. Further, the usage of deep learning algorithms in various THz applications is reported, with focus on metamaterials design and classification, detection, reconstruction, segmentation, parameter extraction and denoising tasks. Moreover, we also report the metrics used to evaluate the performance of deep learning models and finally, the existing research challenges in the application of deep learning in THz cancer imaging applications are identified and possible solutions are suggested through emerging trends. With the continuous increase of acquired THz data â€“ sensing, spectral and imaging, artificial intelligence has emerged as a dominant paradigm for embedded data extraction, understanding, perception, decision making and analysis. Towards this end, the integration of state-of-the-art machine learning techniques such as deep learning with THz applications enable detailed computational and theoretical analysis for better validation and verification than modelling techniques that precede the era of machine learning. The study will facilitate the large-scale clinical applications of deep learning enabled THz imaging systems for the development of smart and connected next generation healthcare systems as well as provide a roadmap for future research direction.",Deep learning;Imaging;Machine learning;Sensors;Artificial intelligence;Task analysis;Support vector machines;Terahertz materials,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418122,IEEE Journals,,,,,,
Brain Structural Connectivity Guided Vision Transformers for Identification of Functional Connectivity Characteristics in Preterm Neonates,W. Mao; Y. Chen; Z. He; Z. Wang; Z. Xiao; Y. Sun; L. He; J. Zhou; W. Guo; C. Ma; L. Zhao; K. M. Kendrick; B. Zhou; B. Becker; T. Liu; T. Zhang; X. Jiang,IEEE Journal of Biomedical and Health Informatics,04-Apr-24,2024,"Preterm birth is the leading cause of death in children under five years old, and is associated with a wide sequence of complications in both short and long term. In view of rapid neurodevelopment during the neonatal period, preterm neonates may exhibit considerable functional alterations compared to term ones. However, the identified functional alterations in previous studies merely achieve moderate classification performance, while more accurate functional characteristics with satisfying discrimination ability for better diagnosis and therapeutic treatment is underexplored. To address this problem, we propose a novel brain structural connectivity (SC) guided Vision Transformer (SCG-ViT) to identify functional connectivity (FC) differences among three neonatal groups: preterm, preterm with early postnatal experience, and term. Particularly, inspired by the neuroscience-derived information, a novel patch token of SC/FC matrix is defined, and the SC matrix is then adopted as an effective mask into the ViT model to screen out input FC patch embeddings with weaker SC, and to focus on stronger ones for better classification and identification of FC differences among the three groups. The experimental results on multi-modal MRI data of 437 neonatal brains from publicly released Developing Human Connectome Project (dHCP) demonstrate that SCG-ViT achieves superior classification ability compared to baseline models, and successfully identifies holistically different FC patterns among the three groups. Moreover, these different FCs are significantly correlated with the differential gene expressions of the three groups. In summary, SCG-ViT provides a powerfully brain-guided pipeline of adopting large-scale and data-intensive deep learning models for medical imaging-based diagnosis.",Pediatrics;Magnetic resonance imaging;Brain modeling;Transformers;Gene expression;Task analysis;Neuroscience,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416360,IEEE Journals,,,,,,
Siamese Cooperative Learning for Unsupervised Image Reconstruction From Incomplete Measurements,Y. Quan; X. Qin; T. Pang; H. Ji,IEEE Transactions on Pattern Analysis and Machine Intelligence,05-Jun-24,2024,"Image reconstruction from incomplete measurements is one basic task in imaging. While supervised deep learning has emerged as a powerful tool for image reconstruction in recent years, its applicability is limited by its prerequisite on a large number of latent images for model training. To extend the application of deep learning to the imaging tasks where acquisition of latent images is challenging, this article proposes an unsupervised deep learning method that trains a deep model for image reconstruction with the access limited to measurement data. We develop a Siamese network whose twin sub-networks perform reconstruction cooperatively on a pair of complementary spaces: the null space of the measurement matrix and the range space of its pseudo inverse. The Siamese network is trained by a self-supervised loss with three terms: a data consistency loss over available measurements in the range space, a data consistency loss between intermediate results in the null space, and a mutual consistency loss on the predictions of the twin sub-networks in the full space. The proposed method is applied to four imaging tasks from different applications, and extensive experiments have shown its advantages over existing unsupervised solutions.",Image reconstruction;Training;Noise measurement;Loss measurement;Deep learning;Imaging;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415211,IEEE Journals,,,,,,
Advancing Breast Cancer Detection: Enhancing YOLOv5 Network for Accurate Classification in Mammogram Images,M. Anas; I. U. Haq; G. Husnain; S. A. F. Jaffery,IEEE Access,########,2024,"Recent advances in artificial intelligence (AI), notably deep learning, have sparked widespread curiosity with bioinformatics, particularly the challenges presented by medical imaging. Itâ€™s been really helpful in enabling the Computer Aided Diagnosis CAD system to provide precise outcomes. Nonetheless, it is still a difficult task to identify breast cancer in mammography images. The purpose of this effort is to lower False Positive Rate FPR and False Negative Rate FNR and increase Matthewsâ€™s correlation coefficient MCC value. Two highly tailored object detection models, YOLOv5 and Mask R-CNN, are utilized to get the job done. YOLOv5 is able to detect the mass and determine whether it is benign or malignant. However, YOLOV5â€™s limited real estate necessitates certain tweaks to the original model in order to get the desired effects. Tumor borders and size are both identified by Mask RCNN as it traverses breast parenchyma in search of malignancies. Stages of cancer are based on the magnitude of the patientsâ€™ tumours. This model employs YOLOv5+Mask RCNN and is trained on the INbreast, CBIS-DDSM, and BNS dataset. The proposed model is compared against the baseline version of YOLOv5 to determine how well it performs. The proposed method improves performance, with an FPR of 0.049%, a FNR of 0.029%, and a high MCC value of 92.02%. Based on the results of the studies, combining YOLOv5 with Mask RCNN improves accuracy by 0.06 percentage points compared to using either method alone. Furthermore, this effort may aid in determining the patientâ€™s prognosis and allowing clinicians to be more accurate and predictable in the diagnosing process at an early stage.",Cancer;Breast cancer;YOLO;Solid modeling;Mammography;Convolutional neural networks;Delta-sigma modulation;Deep learning;Recurrent neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10414093,IEEE Journals,,,,,,
Dense Contrastive-Based Federated Learning for Dense Prediction Tasks on Medical Images,Y. Yang; X. Liu; T. Gao; X. Xu; P. Zhang; G. Wang,IEEE Journal of Biomedical and Health Informatics,04-Apr-24,2024,"Deep learning (DL) models have achieved remarkable success in various domains. But training an accurate DL model requires large amounts of data, which can be challenging to obtain in medical settings due to privacy concerns. Recently, federated learning (FL) has emerged as a promising solution that shares local models instead of raw data. However, FL in medical settings faces challenges of client drift due to the data heterogeneity across dispersed institutions. Although there exist studies to address this challenge, they mainly focus on the classification tasks that learn global representation of an entire image. Few have been studied on the dense prediction tasks, such as object detection. In this study, we propose dense contrastive-based federated learning (DCFL) tailored for dense prediction tasks in FL settings. DCFL introduces dense contrastive learning to FL, which aligns the local optimization objectives towards the global objective by maximizing the agreement of representations between the global and local models. Moreover, to improve the performance of dense target prediction at each level, DCFL applies multi-scale contrastive representation by utilizing multi-scale representations with dense features in contrastive learning. We evaluated DCFL on a set of realistic datasets for pulmonary nodule detection. DCFL demonstrates an overall performance improvement compared with the other federated learning methods in heterogeneous settingsâ€“improving the mean average precision by 4.13% and testing recall by 6.07% in highly heterogeneous settings.",Task analysis;Predictive models;Training;Federated learning;Self-supervised learning;Prediction algorithms;Servers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10413546,IEEE Journals,,,,,,
Robust Stochastic Neural Ensemble Learning With Noisy Labels for Thoracic Disease Classification,H. Wang; J. He; H. Cui; B. Yuan; Y. Xia,IEEE Transactions on Medical Imaging,03-Jun-24,2024,"Chest radiography is the most common radiology examination for thoracic disease diagnosis, such as pneumonia. A tremendous number of chest X-rays prompt data-driven deep learning models in constructing computer-aided diagnosis systems for thoracic diseases. However, in realistic radiology practice, a deep learning-based model often suffers from performance degradation when trained on data with noisy labels possibly caused by different types of annotation biases. To this end, we present a novel stochastic neural ensemble learning (SNEL) framework for robust thoracic disease diagnosis using chest X-rays. The core idea of our method is to learn from noisy labels by constructing model ensembles and designing noise-robust loss functions. Specifically, we propose a fast neural ensemble method that collects parameters simultaneously across model instances and along optimization trajectories. Moreover, we propose a loss function that both optimizes a robust measure and characterizes a diversity measure of ensembles. We evaluated our proposed SNEL method on three publicly available hospital-scale chest X-ray datasets. The experimental results indicate that our method outperforms competing methods and demonstrate the effectiveness and robustness of our method in learning from noisy labels. Our code is available at https://github.com/hywang01/SNEL.",Noise measurement;Biomedical imaging;Training;X-ray imaging;Optimization;Stochastic processes;Ensemble learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10413624,IEEE Journals,,,,,,
A Very Large-Scale Integration (VLSI) Chip Design for Abnormal Heartbeat Detection Using a Data-Shifting Neural Network (DSNN),Y. -H. Chen; S. -W. Chen; H. -W. Jian; S. -Y. Lin; R. -S. Chen,IEEE Access,30-Jan-24,2024,"In this paper, we propose a data-shifting neural network (DSNN) for the detection of abnormal heartbeats. Our study aims to identify six types of electrocardiogram (ECG) signals using the deep learning network. In order to enhance the detection accuracy, the DSNN is devised by doubling the input signal using a data shifting scheme so that the amount of information for training may be adequately sufficient. Although the computational time doubles, the accuracy can be improved. When implemented using the Taiwan Semiconductor Manufacturing Company (TSMC) $0.18-\mu m$ complementary metal oxide semiconductor (CMOS) process, the proposed DSNN chip has an operating frequency at 20 MHz with chip area of $0.619 mm^{2}$ and maximum power dissipation $0.75 mW$ . As a result, the proposed DSNN can substantially increase detection accuracy for the task of ECG heartbeat classification. Results obtained after applying the proposed circuit to the ECG signals drawn from the MIT-BIH arrhythmia database showed that it achieved a detection rate of 97.17% with a small chip area, suggesting that it may be suitable for wearable or portable devices in healthcare.",Electrocardiography;Heart beat;Arrhythmia;Training data;Convolutional neural networks;Very large scale integration;Real-time systems;Very large scale integration;Heart rate,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411502,IEEE Journals,,,,,,
Enhancing the Quality and Authenticity of Synthetic Mammogram Images for Improved Breast Cancer Detection,D. Shah; M. A. U. Khan; M. Abrar; F. Amin; B. F. Alkhamees; H. AlSalman,IEEE Access,25-Jan-24,2024,"Breast cancer is widespread throughout the world and can be cured if diagnosed early. Mammography is an irreplaceable and critical technique in modern medicine that serves as a foundation for the detection of breast cancer. In medical imaging, the reliability of synthetic mammogram images is produced by deep convolutional generative adversarial networks (DCGAN). Human validation to assess the quality of synthetic images to examine and calculate the perceptual variations between synthetic images and their real-world counterparts is a difficult task. Thus, this research focused on improving the quality and authenticity of synthetic mammogram images. For this, we explored and identified a new research gap because radiologists consistently expressed much higher confidence levels in real mammogram images in their assessment process. This research highlights the key difference between synthetic and real mammograms by defining mean scores. The defined mean identifies a large gap, with real mammographic images receiving an average score of 0.73 and a synthetic score of 0.31. A statistical analysis was performed, which produced a T-statistic of -6.35, a p-value less than 0.001, and a 95% confidence interval ranging from -0.50 to -0.28. These results have a wide range of implications. It emphasizes the urgent need for further improvements in the generative model, improving the legitimacy and caliber of synthetic mammogram images. Our research highlights how crucial it is to incorporate synthetic images into clinical practice with caution and thought. Ethical considerations must encompass the potential consequences of relying on synthetic data in medical decision-making, along with concerns related to diagnostic accuracy and patient safety.",Medical diagnostic imaging;Mammography;Breast cancer;Training;Reliability;Generators;Data models;Computer aided diagnosis;Deep learning;Generative adversarial networks;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10401230,IEEE Journals,,,,,,
DeepDiabetic: An Identification System of Diabetic Eye Diseases Using Deep Neural Networks,A. Albelaihi; D. M. Ibrahim,IEEE Access,22-Jan-24,2024,"Deep Learning (DL) plays a successful and influential role in medical imaging diagnosis, image detection, and image classification. Diabetes is a significant public health concern, and diabetic eye disease will be the leading cause of vision loss across the globe. This research proposed a multi-classification deep learning model for diagnosing and identifying four different diabetic eye diseases: Diabetic Retinopathy (DR), Diabetic Macular Edema (DME), glaucoma, and cataract which we called the DeepDiabetic framework. The proposed models were assessed using 1228 images from six different available datasets (DIARETDB0, DIARETDB1, Messidor, HEI-MED, Ocular, and Retina). In addition to the original dataset, we measured the performance of the deep learning models according to two different geometric augmentation methods called online augmented and offline augmented. The present work considers five architecturesâ€™ performances: EfficientNetB0, VGG16, ResNet152V2, ResNet152V2 + Gated Recurrent Unit (GRU), and ResNet152V2 + Bidirectional GRU (Bi-GRU). A comprehensive analysis and evaluation of these deep learning architectures is provided using public fundus datasets with four classes (i.e., DR, DME, Glaucoma, and Cataract). To the best of our knowledge, no other deep learning models for choosing between these models for these specific diseases are found in the literature. The results of the experiments showed that the EfficientNetB0 model outperforms the other four proposed models. The EfficientNetB0 model achieved 0.9876 in accuracy, 0.9876 recall, 0.9876 precision, and 0.9977 AUC based on fundus images. Our EfficientNetB0 model achieves 98.76% accuracy, while the previous studies only achieved 88.33%, 89.54%, 97.23%, and 80.33% accuracy, respectively. When compared to the previous studies as Fast-RCNN, RCNN-LSTM, and InceptionResNet, our EfficientNetB0 model achieves much higher accuracy, recall, precision, and AUC. According to the outcomes, our proposed models, especially the EfficientNetB0 model, are significantly more accurate than the state-of-the-art models.",Diabetes;Eye diseases;Diseases;Deep learning;Feature extraction;Artificial neural networks;Cataracts;Retina,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10401038,IEEE Journals,,,,,,
Deep Learning-Based Classification of Gamma Photon Interaction in Room-Temperature Semiconductor Radiation Detectors,S. K. Chaudhuri; Q. Li; K. C. Mandal; J. Hu,IEEE Access,########,2024,"Photon counting radiation detectors have become an integral part of medical imaging modalities such as Positron Emission Tomography or Computed Tomography. One of the most promising detectors is the wide bandgap room temperature semiconductor detectors, which depends on the interaction gamma/x-ray photons with the detector material involves Compton scattering which leads to multiple interaction photon events (MIPEs) of a single photon. For semiconductor detectors like CdZnTeSe (CZTS), which have a high overlap of detected energies between Compton and photoelectric events, it is nearly impossible to distinguish between Compton scattered events from photoelectric events using conventional readout electronics or signal processing algorithms. Herein, we report a deep learning classifier CoPhNet that distinguishes between Compton scattering and photoelectric interactions of gamma/x-ray photons with CdZnTeSe (CZTS) semiconductor detectors. Our CoPhNet model was trained using simulated 662 keV samples to resemble actual CZTS detector pulses and validated using both simulated and experimental data. The model remarkably exhibited a 100% accuracy in predicting the type of interaction. These results demonstrated that our CoPhNet model can achieve high classification accuracy over the simulated test set. It also holds its performance robustness under operating parameter shifts such as Signal-Noise-Ratio (SNR) and incident energy. Our work thus show a positive direction for developing next-generation high energy gamma-rays detectors for better biomedical imaging.",Detectors;Photonics;Imaging;Gamma-rays;Semiconductor radiation detectors;Semiconductor detectors;Preamplifiers;Deep learning;Biomedical imaging;Radiation detectors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400458,IEEE Journals,,,,,,
ResX: Feature Extraction Block for Medical Image Segmentation,S. Qi; Z. Lee; J. Liu; M. Han; Y. Qin; Q. Du,IEEE Access,########,2024,"Accurate medical image segmentation is critical for various clinical applications, and convolutional neural networks (CNNs) have demonstrated promising results in this field. The performance of CNN models for segmenting specific organs or lesion areas from medical images heavily depends on the feature extraction ability of the backbone network. In this study, we aim to explore the deep features of the backbone network and propose a novel network that can accurately capture multi-scale image features for medical image segmentation. To achieve this goal, we built upon the widely used U-Net framework and evaluated the feature extraction performance of different backbone networks for medical images. Then, we introduced a novel backbone network called ResX block, which utilizes rectangular and dilated convolutions to capture multi-scale features.To validate our conclusions, we conducted experiments on four benchmark datasets, including Lits2017, 3Dircadb, LIDC, and LCTSC. Our results demonstrate that the proposed ResX block outperforms mainstream feature extraction blocks in terms of accuracy and robustness. Our study confirms the importance of accurate multi-scale feature extraction for improving the performance of CNNs in medical image segmentation. Furthermore, we have verified the potential of rectangular and dilated convolutions for capturing multi-scale features in medical images. Finally, we proposed a novel backbone network, the ResX block, which can be seamlessly integrated into any CNN used for medical image segmentation. Our study provides valuable insights for developing more accurate and efficient CNN models for medical image analysis.",Feature extraction;Image segmentation;Convolutional neural networks;Biomedical imaging;Medical diagnostic imaging;Computed tomography;Deep learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10399789,IEEE Journals,,,,,,
Artifact Reduction in 3D and 4D Cone-Beam Computed Tomography Images With Deep Learning: A Review,M. Amirian; D. Barco; I. Herzig; F. -P. Schilling,IEEE Access,22-Jan-24,2024,"Deep learning based approaches have been used to improve image quality in cone-beam computed tomography (CBCT), a medical imaging technique often used in applications such as image-guided radiation therapy, implant dentistry or orthopaedics. While deep learning methods have been applied to reduce various types of CBCT image artifacts arising from motion, metal objects, or low-dose acquisition, a comprehensive review summarizing the successes and shortcomings of these approaches, with a primary focus on the type of artifacts rather than the architecture of neural networks, is lacking in the literature. In this review, the data generation and simulation pipelines, as well as artifact reduction techniques are specifically investigated for each type of artifact. We provide an overview of deep learning techniques that have successfully been shown to reduce artifacts in 3D, as well as in time-resolved (4D) CBCT through the use of projection- and/or volume-domain optimizations, or by introducing neural networks directly within the CBCT reconstruction algorithms. Research gaps are identified to suggest avenues for future exploration. One of the key findings of this work is an observed trend towards the use of generative models including GANs and score-based or diffusion models, accompanied with the need for more diverse and open training datasets and simulations.",Computed tomography;Image reconstruction;Deep learning;Three-dimensional displays;Measurement;Training;Surveys,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398205,IEEE Journals,,,,,,
Deep Omni-Supervised Learning for Rib Fracture Detection From Chest Radiology Images,Z. Chai; L. Luo; H. Lin; P. -A. Heng; H. Chen,IEEE Transactions on Medical Imaging,########,2024,"Deep learning (DL)-based rib fracture detection has shown promise of playing an important role in preventing mortality and improving patient outcome. Normally, developing DL-based object detection models requires a huge amount of bounding box annotation. However, annotating medical data is time-consuming and expertise-demanding, making obtaining a large amount of fine-grained annotations extremely infeasible. This poses a pressing need for developing label-efficient detection models to alleviate radiologistsâ€™ labeling burden. To tackle this challenge, the literature on object detection has witnessed an increase of weakly-supervised and semi-supervised approaches, yet still lacks a unified framework that leverages various forms of fully-labeled, weakly-labeled, and unlabeled data. In this paper, we present a novel omni-supervised object detection network, ORF-Netv2, to leverage as much available supervision as possible. Specifically, a multi-branch omni-supervised detection head is introduced with each branch trained with a specific type of supervision. A co-training-based dynamic label assignment strategy is then proposed to enable flexible and robust learning from the weakly-labeled and unlabeled data. Extensive evaluation was conducted for the proposed framework with three rib fracture datasets on both chest CT and X-ray. By leveraging all forms of supervision, ORF-Netv2 achieves mAPs of 34.7, 44.7, and 19.4 on the three datasets, respectively, surpassing the baseline detector which uses only box annotations by mAP gains of 3.8, 4.8, and 5.0, respectively. Furthermore, ORF-Netv2 consistently outperforms other competitive label-efficient methods over various scenarios, showing a promising framework for label-efficient fracture detection. The code is available at: https://github.com/zhizhongchai/ORF-Net.",Annotations;Ribs;Object detection;Biomedical imaging;Computed tomography;Data models;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398253,IEEE Journals,,,,,,
Geometric Correspondence-Based Multimodal Learning for Ophthalmic Image Analysis,Y. Wang; L. Zhen; T. -E. Tan; H. Fu; Y. Feng; Z. Wang; X. Xu; R. S. M. Goh; Y. Ng; C. Calhoun; G. S. W. Tan; J. K. Sun; Y. Liu; D. S. W. Ting,IEEE Transactions on Medical Imaging,########,2024,"Color fundus photography (CFP) and Optical coherence tomography (OCT) images are two of the most widely used modalities in the clinical diagnosis and management of retinal diseases. Despite the widespread use of multimodal imaging in clinical practice, few methods for automated diagnosis of eye diseases utilize correlated and complementary information from multiple modalities effectively. This paper explores how to leverage the information from CFP and OCT images to improve the automated diagnosis of retinal diseases. We propose a novel multimodal learning method, named geometric correspondence-based multimodal learning network (GeCoM-Net), to achieve the fusion of CFP and OCT images. Specifically, inspired by clinical observations, we consider the geometric correspondence between the OCT slice and the CFP region to learn the correlated features of the two modalities for robust fusion. Furthermore, we design a new feature selection strategy to extract discriminative OCT representations by automatically selecting the important feature maps from OCT slices. Unlike the existing multimodal learning methods, GeCoM-Net is the first method that formulates the geometric relationships between the OCT slice and the corresponding region of the CFP image explicitly for CFP and OCT fusion. Experiments have been conducted on a large-scale private dataset and a publicly available dataset to evaluate the effectiveness of GeCoM-Net for diagnosing diabetic macular edema (DME), impaired visual acuity (VA) and glaucoma. The empirical results show that our method outperforms the current state-of-the-art multimodal learning methods by improving the AUROC score 0.4%, 1.9% and 2.9% for DME, VA and glaucoma detection, respectively.",Feature extraction;Retina;Three-dimensional displays;Imaging;Lesions;Glaucoma;Correlation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10388423,IEEE Journals,,,,,,
Computer Aided Cervical Cancer Diagnosis Using Gazelle Optimization Algorithm With Deep Learning Model,M. K. Nour; I. Issaoui; A. Edris; A. Mahmud; M. Assiri; S. S. Ibrahim,IEEE Access,29-Jan-24,2024,"Cervical cancer (CC), the most common cancer among women, is most commonly diagnosed through Pap smears, a crucial screening process that includes collecting cervical cells for examination. Artificial intelligence (AI)-powered computer-aided diagnoses (CAD) system becomes a promising tool for improving CC diagnosis. Deep learning (DL), a branch of AI, holds particular potential in CAD systems for early detection and accurate diagnosis. DL algorithm is trained to identify abnormalities and patterns in Pap smear images, such as dysplasia, cellular changes, and other markers of CC. So, this study presents a Computer Aided Cervical Cancer Diagnosis utilizing the Gazelle Optimizer Algorithm with Deep Learning (CACCD-GOADL) model on Pap smear images. The foremost objective of the CACCD-GOADL approach is to examine the image detection of CC. To accomplish this, the CACCD-GOADL methodology uses an improved MobileNetv3 model for extracting complex patterns in Pap smear images. In addition, the CACCD-GOADL technique designs a new GOA for the hyperparameter tuning of the improved MobileNetv3 system. For the classification and identification of cancer, the CACCD-GOADL technique uses a stacked extreme learning machine (SELM) methodology. The simulation validation of the CACCD-GOADL approach is verified on a benchmark dataset of Herlev. Experimental results highlighted that the CACCD-GOADL algorithm reaches superior outcomes over other methods.",Feature extraction;Solid modeling;Classification algorithms;Cervical cancer;Optimization;Medical diagnostic imaging;Computer aided diagnosis;Deep learning;Machine learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10384881,IEEE Journals,,,,,,
Unlocking the Potential of XAI for Improved Alzheimerâ€™s Disease Detection and Classification Using a ViT-GRU Model,S. M. Mahim; M. S. Ali; M. O. Hasan; A. A. N. Nafi; A. Sadat; S. A. Hasan; B. Shareef; M. M. Ahsan; M. K. Islam; M. S. Miah; M. -B. Niu,IEEE Access,19-Jan-24,2024,"Alzheimerâ€™s Disease (AD) is a significant cause of dementia worldwide, and its progression from mild to severe affects an individualâ€™s ability to perform daily activities independently. The accurate and early diagnosis of AD is crucial for effective clinical intervention. However, interpreting AD from medical images can be challenging, even for experienced radiologists. Therefore, there is a need for an automatic diagnosis of AD, and researchers have investigated the potential of utilizing Artificial Intelligence (AI) techniques, particularly deep learning models, to address this challenge. This study proposes a framework that combines a Vision Transformer (ViT) and a Gated Recurrent Unit (GRU) to detect AD characteristics from Magnetic Resonance Imaging (MRI) images accurately and reliably. The ViT identifies crucial features from the input image, and the GRU establishes clear correlations between these features. The proposed model overcomes the class imbalance issue in the MRI image dataset and achieves superior accuracy and performance compared to existing methods. The model was trained on the Alzheimerâ€™s MRI Preprocessed Dataset obtained from Kaggle, achieving notable accuracies of 99.53% for 4-class and 99.69% for binary classification. It also demonstrated a high accuracy of 99.26% for 3-class on the AD Neuroimaging Initiative (ADNI) Baseline Database. These results were validated through a thorough 10-fold cross-validation process. Furthermore, Explainable AI (XAI) techniques were incorporated to make the model interpretable and explainable. This allows clinicians to understand the modelâ€™s decision-making process and gain insights into the underlying factors driving the AD diagnosis.",Magnetic resonance imaging;Brain modeling;Computational modeling;Biological system modeling;Solid modeling;Medical diagnostic imaging;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385046,IEEE Journals,,,,,,
Pseudo-Bag Mixup Augmentation for Multiple Instance Learning-Based Whole Slide Image Classification,P. Liu; L. Ji; X. Zhang; F. Ye,IEEE Transactions on Medical Imaging,########,2024,"Given the special situation of modeling gigapixel images, multiple instance learning (MIL) has become one of the most important frameworks for Whole Slide Image (WSI) classification. In current practice, most MIL networks often face two unavoidable problems in training: i) insufficient WSI data and ii) the sample memorization inclination inherent in neural networks. These problems may hinder MIL models from adequate and efficient training, suppressing the continuous performance promotion of classification models on WSIs. Inspired by the basic idea of Mixup, this paper proposes a new Pseudo-bag Mixup (PseMix) data augmentation scheme to improve the training of MIL models. This scheme generalizes the Mixup strategy for general images to special WSIs via pseudo-bags so as to be applied in MIL-based WSI classification. Cooperated by pseudo-bags, our PseMix fulfills the critical size alignment and semantic alignment in Mixup strategy. Moreover, it is designed as an efficient and decoupled method, neither involving time-consuming operations nor relying on MIL model predictions. Comparative experiments and ablation studies are specially designed to evaluate the effectiveness and advantages of our PseMix. Experimental results show that PseMix could often assist state-of-the-art MIL networks to refresh their classification performance on WSIs. Besides, it could also boost the generalization performance of MIL models in special test scenarios, and promote their robustness to patch occlusion and label noise. Our source code is available at https://github.com/liupei101/PseMix.",Training;Data models;Data augmentation;Interpolation;Semantics;Robustness;Pathology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385148,IEEE Journals,,,,,,
A Novel Transformer Model With Multiple Instance Learning for Diabetic Retinopathy Classification,Y. Yang; Z. Cai; S. Qiu; P. Xu,IEEE Access,15-Jan-24,2024,"Diabetic retinopathy (DR) is an irreversible fundus retinopathy. A deep learning-based automated DR diagnosis system can save diagnostic time. While Transformer has shown superior performance compared to Convolutional Neural Network (CNN), it typically requires pre-training with large amounts of data. Although Transformer-based DR diagnosis method may alleviate the problem of limited performance on small-scale retinal datasets by loading pre-trained weights, the size of input images is restricted to $224\times 224$ . The resolution of retinal images captured by fundus cameras is much higher than $224\times 224$ , reducing resolution in training will result in the loss of valuable information. In order to efficiently utilize high-resolution retinal images, a new Transformer model with multiple instance learning (TMIL) is proposed for DR classification. A multiple instance learning approach is firstly applied on the retinal images to segment these high-resolution images into $224\times 224$ image patches. Subsequently, Vision Transformer (ViT) is used to extract features from each patch. Then, Global Instance Computing Block (GICB) is designed to calculate the inter-instance features. After introducing global information from GICB, the features are used to output the classification results. When using high-resolution retinal images, TMIL can load pre-trained weights of Transformer without being affected by weight interpolation on model performance. Experimental results using the APTOS dataset and the Messidor-1 dataset demonstrate that TMIL achieves better classification performance and reduces inference time by 62% compared with that directly inputting high-resolution images into ViT. And TMIL shows highest classification accuracy compared with the current state-of-the-art results. The code will publicly available at https://github.com/CNMaxYang/TMIL.",Transformers;Feature extraction;Retina;Head;Lesions;Computational modeling;Image resolution;Diabetes;Retinopathy;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10384372,IEEE Journals,,,,,,
A Test Statistic Estimation-Based Approach for Establishing Self-Interpretable CNN-Based Binary Classifiers,S. Sengupta; M. A. Anastasio,IEEE Transactions on Medical Imaging,########,2024,"Interpretability is highly desired for deep neural network-based classifiers, especially when addressing high-stake decisions in medical imaging. Commonly used post-hoc interpretability methods have the limitation that they can produce plausible but different interpretations of a given model, leading to ambiguity about which one to choose. To address this problem, a novel decision-theory-inspired approach is investigated to establish a self-interpretable model, given a pre-trained deep binary black-box medical image classifier. This approach involves utilizing a self-interpretable encoder-decoder model in conjunction with a single-layer fully connected network with unity weights. The model is trained to estimate the test statistic of the given trained black-box deep binary classifier to maintain a similar accuracy. The decoder output image, referred to as an equivalency map, is an image that represents a transformed version of the to-be-classified image that, when processed by the fixed fully connected layer, produces the same test statistic value as the original classifier. The equivalency map provides a visualization of the transformed image features that directly contribute to the test statistic value and, moreover, permits quantification of their relative contributions. Unlike the traditional post-hoc interpretability methods, the proposed method is self-interpretable, quantitative. Detailed quantitative and qualitative analyses have been performed with three different medical image binary classification tasks.",Task analysis;Decoding;Biomedical imaging;Closed box;Retina;Computational modeling;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10378976,IEEE Journals,,,,,,
SPCB-Net: A Multi-Scale Skin Cancer Image Identification Network Using Self-Interactive Attention Pyramid and Cross-Layer Bilinear-Trilinear Pooling,X. Qian; T. Weng; Q. Han; C. Wu; H. Xu; M. Hou; Z. Qiu; B. Zhou; X. Gao,IEEE Access,04-Jan-24,2024,"Deep convolutional neural networks have made some progress in skin lesion classification and cancer diagnosis, but there are still some problems to be solved, such as the challenge of small inter-class feature differences and large intra-class feature differences, which might limit the classification performance of the model as high-level and low-level features are not properly utilized. This paper proposes a multi-scale skin cancer image identification network using self-interactive attention pyramid and cross-layer bilinear-trilinear pooling(SPCB-Net), which mainly consists of three proposed sub-modules that are the self-interacting attention pyramid (SAP), the across-layer bilinear-trilinear pooling operation and the global average algorithm(GAA). The SPCB-Net is applied to two representative datasets of medical images in dermatology and histopathology (HAM10000 and NCT-CRC-HE-100K) to demonstrate the effectiveness of in the skin lesion classification. SPCB-Net(ResNet101) achieves 97.10% and 99.87% accuracy on HAM10000 and NCT-CRC-HE-100K respectively, which are both achieved performance improvements of 0.4% compared to the state-of-the-art models. In addition, a large number of experiments on HAM10000 show that the interactive attention pyramid(SPA) proposed in this paper is superior to the common attention module, and the method with a cross-layer bilinear-trilinear pooling is superior to the cross-layer trilinear pooling method. SPCB-Net is configured on Vgg19 and ResNet101 to evaluate the effectiveness of our proposed module. The experimental results show that SPCB-Net has shown state-of-the-art performance in the two field of dermatology and histopathology. Therefore, it is not only well qualified for the task of identifying skin cancer image but also has the potential to identify skin cancer by identifying pathological tissue.",Feature extraction;Cross layer design;Medical diagnostic imaging;Convolutional neural networks;Skin cancer;Semantics;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10374026,IEEE Journals,,,,,,
Deep Learning-Based YOLO Models for the Detection of People With Disabilities,M. Alruwaili; M. N. Atta; M. H. Siddiqi; A. Khan; A. Khan; Y. Alhwaiti; S. Alanazi,IEEE Access,08-Jan-24,2024,"The current methods, while in use, continue to grapple with accuracy and effectiveness concerns. It is imperative to establish dependable solutions capable of distinguishing and categorizing people according to their assistive devices to tackle these issues. People with disabilities, such as those experiencing paralysis, limb deficiencies, or amputations, may encounter issues related to discrimination and inadequate support. Hence, this research was undertaken to detect and track people with conditions like paralysis, limb deficiency (Amelia), or amputation among the differently-abled population. Earlier investigations have predominantly focused on recognizing people and their mobility aids, utilizing a variety of methods such as Fast R-CNN, Faster R-CNN, RGB or RGB-D cameras, Kalman filters, and hidden Markov models. Modern deep learning models, including YOLO (You Only Look Once) and its variations, have gained substantial acceptance in current applications owing to their distinctive architectural designs and performance attributes. In this study, a substantial dataset comprising 4,300 images and 8,447 labels spanning five distinct categories is employed to assess the efficacy of YOLOv8, YOLOv5, and YOLOv7 models in the identification of people with disabilities. The evaluation findings show that YOLOv8, which achieved an overall precision of 0.907, performs better than both YOLOv5 (precision: 0.885) and YOLOv7 (precision: 0.906). Notably, YOLOv8 has the best wheelchair detection precision (0.998). Furthermore, YOLOv8 outperforms YOLOv5 (recall: 0.887) and YOLOv7 (recall: 0.925) in terms of recall performance (recall: 0.943). YOLOv8 achieves the greatest mean average accuracy (mAP@.5) value of 0.951, followed by YOLOv5 (mAP@.5: 0.942), and YOLOv7 (mAP@.5: 0.954). In a similar vein, of the three models, YOLOv8 has the best performance (mAP@.5:.95: 0.713). The analysis of detection time also shows that YOLOv8 performs best, processing 5,597 frames in just 5.9 milliseconds and achieving a remarkable frame rate of 169.49 frames per second.",Feature extraction;Convolutional neural networks;YOLO;Object recognition;Proposals;Paralysis;Deep learning;Biomedical image processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10373841,IEEE Journals,,,,,,
Multi-Template Meta-Information Regularized Network for Alzheimerâ€™s Disease Diagnosis Using Structural MRI,K. Han; G. Li; Z. Fang; F. Yang,IEEE Transactions on Medical Imaging,########,2024,"Structural magnetic resonance imaging (sMRI) has been widely applied in computer-aided Alzheimerâ€™s disease (AD) diagnosis, owing to its capabilities in providing detailed brain morphometric patterns and anatomical features in vivo. Although previous works have validated the effectiveness of incorporating metadata (e.g., age, gender, and educational years) for sMRI-based AD diagnosis, existing methods solely paid attention to metadata-associated correlation to AD (e.g., gender bias in AD prevalence) or confounding effects (e.g., the issue of normal aging and metadata-related heterogeneity). Hence, it is difficult to fully excavate the influence of metadata on AD diagnosis. To address these issues, we constructed a novel Multi-template Meta-information Regularized Network (MMRN) for AD diagnosis. Specifically, considering diagnostic variation resulting from different spatial transformations onto different brain templates, we first regarded different transformations as data augmentation for self-supervised learning after template selection. Since the confounding effects may arise from excessive attention to meta-information owing to its correlation with AD, we then designed the modules of weakly supervised meta-information learning and mutual information minimization to learn and disentangle meta-information from learned class-related representations, which accounts for meta-information regularization for disease diagnosis. We have evaluated our proposed MMRN on two public multi-center cohorts, including the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) with 1,950 subjects and the National Alzheimerâ€™s Coordinating Center (NACC) with 1,163 subjects. The experimental results have shown that our proposed method outperformed the state-of-the-art approaches in both tasks of AD diagnosis, mild cognitive impairment (MCI) conversion prediction, and normal control (NC) vs. MCI vs. AD classification.",Feature extraction;Metadata;Self-supervised learning;Mutual information;Alzheimer's disease;Aging;Minimization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10365189,IEEE Journals,,,,,,
DeepTree: Pathological Image Classification Through Imitating Tree-Like Strategies of Pathologists,J. Li; J. Cheng; L. Meng; H. Yan; Y. He; H. Shi; T. Guan; A. Han,IEEE Transactions on Medical Imaging,03-Apr-24,2024,"Digitization of pathological slides has promoted the research of computer-aided diagnosis, in which artificial intelligence analysis of pathological images deserves attention. Appropriate deep learning techniques in natural images have been extended to computational pathology. Still, they seldom take into account prior knowledge in pathology, especially the analysis process of lesion morphology by pathologists. Inspired by the diagnosis decision of pathologists, we design a novel deep learning architecture based on tree-like strategies called DeepTree. It imitates pathological diagnosis methods, designed as a binary tree structure, to conditionally learn the correlation between tissue morphology, and optimizes branches to finetune the performance further. To validate and benchmark DeepTree, we build a dataset of frozen lung cancer tissues and design experiments on a public dataset of breast tumor subtypes and our dataset. Results show that the deep learning architecture based on tree-like strategies makes the pathological image classification more accurate, transparent, and convincing. Simultaneously, prior knowledge based on diagnostic strategies yields superior representation ability compared to alternative methods. Our proposed methodology helps improve the trust of pathologists in artificial intelligence analysis and promotes the practical clinical application of pathology-assisted diagnosis.",Pathology;Morphology;Tumors;Lesions;Feature extraction;Deep learning;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10354387,IEEE Journals,,,,,,
A Learnable Counter-Condition Analysis Framework for Functional Connectivity-Based Neurological Disorder Diagnosis,E. Kang; D. -W. Heo; J. Lee; H. -I. Suk,IEEE Transactions on Medical Imaging,03-Apr-24,2024,"To understand the biological characteristics of neurological disorders with functional connectivity (FC), recent studies have widely utilized deep learning-based models to identify the disease and conducted post-hoc analyses via explainable models to discover disease-related biomarkers. Most existing frameworks consist of three stages, namely, feature selection, feature extraction for classification, and analysis, where each stage is implemented separately. However, if the results at each stage lack reliability, it can cause misdiagnosis and incorrect analysis in afterward stages. In this study, we propose a novel unified framework that systemically integrates diagnoses (i.e., feature selection and feature extraction) and explanations. Notably, we devised an adaptive attention network as a feature selection approach to identify individual-specific disease-related connections. We also propose a functional network relational encoder that summarizes the global topological properties of FC by learning the inter-network relations without pre-defined edges between functional networks. Last but not least, our framework provides a novel explanatory power for neuroscientific interpretation, also termed counter-condition analysis. We simulated the FC that reverses the diagnostic information (i.e., counter-condition FC): converting a normal brain to be abnormal and vice versa. We validated the effectiveness of our framework by using two large resting-state functional magnetic resonance imaging (fMRI) datasets, Autism Brain Imaging Data Exchange (ABIDE) and REST-meta-MDD, and demonstrated that our framework outperforms other competing methods for disease identification. Furthermore, we analyzed the disease-related neurological patterns based on counter-condition analysis.",Diseases;Feature extraction;Brain modeling;Adaptation models;Analytical models;Support vector machines;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10334478,IEEE Journals,,,,,,
EAG-RS: A Novel Explainability-Guided ROI-Selection Framework for ASD Diagnosis via Inter-Regional Relation Learning,W. Jung; E. Jeon; E. Kang; H. -I. Suk,IEEE Transactions on Medical Imaging,03-Apr-24,2024,"Deep learning models based on resting-state functional magnetic resonance imaging (rs-fMRI) have been widely used to diagnose brain diseases, particularly autism spectrum disorder (ASD). Existing studies have leveraged the functional connectivity (FC) of rs-fMRI, achieving notable classification performance. However, they have significant limitations, including the lack of adequate information while using linear low-order FC as inputs to the model, not considering individual characteristics (i.e., different symptoms or varying stages of severity) among patients with ASD, and the non-explainability of the decision process. To cover these limitations, we propose a novel explainability-guided region of interest (ROI) selection (EAG-RS) framework that identifies non-linear high-order functional associations among brain regions by leveraging an explainable artificial intelligence technique and selects class-discriminative regions for brain disease identification. The proposed framework includes three steps: (i) inter-regional relation learning to estimate non-linear relations through random seed-based network masking, (ii) explainable connection-wise relevance score estimation to explore high-order relations between functional connections, and (iii) non-linear high-order FC-based diagnosis-informative ROI selection and classifier learning to identify ASD. We validated the effectiveness of our proposed method by conducting experiments using the Autism Brain Imaging Database Exchange (ABIDE) dataset, demonstrating that the proposed method outperforms other comparative methods in terms of various evaluation metrics. Furthermore, we qualitatively analyzed the selected ROIs and identified ASD subtypes linked to previous neuroscientific studies.",Image reconstruction;Brain modeling;Autism;Training;Technology planning;Task analysis;Support vector machines,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10332206,IEEE Journals,,,,,,
Adversarial Medical Image With Hierarchical Feature Hiding,Q. Yao; Z. He; Y. Li; Y. Lin; K. Ma; Y. Zheng; S. K. Zhou,IEEE Transactions on Medical Imaging,03-Apr-24,2024,"Deep learning based methods for medical images can be easily compromised by adversarial examples (AEs), posing a great security flaw in clinical decision-making. It has been discovered that conventional adversarial attacks like PGD which optimize the classification logits, are easy to distinguish in the feature space, resulting in accurate reactive defenses. To better understand this phenomenon and reassess the reliability of the reactive defenses for medical AEs, we thoroughly investigate the characteristic of conventional medical AEs. Specifically, we first theoretically prove that conventional adversarial attacks change the outputs by continuously optimizing vulnerable features in a fixed direction, thereby leading to outlier representations in the feature space. Then, a stress test is conducted to reveal the vulnerability of medical images, by comparing with natural images. Interestingly, this vulnerability is a double-edged sword, which can be exploited to hide AEs. We then propose a simple-yet-effective hierarchical feature constraint (HFC), a novel add-on to conventional white-box attacks, which assists to hide the adversarial feature in the target feature distribution. The proposed method is evaluated on three medical datasets, both 2D and 3D, with different modalities. The experimental results demonstrate the superiority of HFC, i.e., it bypasses an array of state-of-the-art adversarial medical AE detectors more efficiently than competing adaptive attacks, which reveals the deficiencies of medical reactive defense and allows to develop more robust defenses in future.",Medical diagnostic imaging;Hybrid fiber coaxial cables;Perturbation methods;Iterative methods;Feature extraction;Detectors;Three-dimensional displays,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10328635,IEEE Journals,,,,,,
Self-Supervised Deep Unrolled Reconstruction Using Regularization by Denoising,P. Huang; C. Zhang; X. Zhang; X. Li; L. Dong; L. Ying,IEEE Transactions on Medical Imaging,########,2024,"Deep learning methods have been successfully used in various computer vision tasks. Inspired by that success, deep learning has been explored in magnetic resonance imaging (MRI) reconstruction. In particular, integrating deep learning and model-based optimization methods has shown considerable advantages. However, a large amount of labeled training data is typically needed for high reconstruction quality, which is challenging for some MRI applications. In this paper, we propose a novel reconstruction method, named DURED-Net, that enables interpretable self-supervised learning for MR image reconstruction by combining a self-supervised denoising network and a plug-and-play method. We aim to boost the reconstruction performance of Noise2Noise in MR reconstruction by adding an explicit prior that utilizes imaging physics. Specifically, the leverage of a denoising network for MRI reconstruction is achieved using Regularization by Denoising (RED). Experiment results demonstrate that the proposed method requires a reduced amount of training data to achieve high reconstruction quality among the state-of-the-art approaches utilizing Noise2Noise.",Image reconstruction;Noise reduction;Magnetic resonance imaging;Training;Imaging;Training data;Iterative methods,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10318101,IEEE Journals,,,,,,
A Siamese-Transport Domain Adaptation Framework for 3D MRI Classification of Gliomas and Alzheimer's Diseases,L. Yu; J. Liu; Q. Wu; J. Wang; A. Qu,IEEE Journal of Biomedical and Health Informatics,04-Jan-24,2024,"Accurate and fully automated brain structure examination and prediction from 3D volumetric magnetic resonance imaging (MRI) is a necessary step in medical imaging analysis, which can assist greatly in clinical diagnosis. Traditional deep learning models suffer from severe performance degradation when applied to clinically acquired unlabeled data. The performance degradation is mainly caused by domain discrepancy such as different device types and parameter settings for data acquisition. However, existing approaches focus on the reduction of domain discrepancies but ignore the entanglement of semantic features and domain information. In this article, we explore the feature invariance of categories and domains in different projection spaces and propose a Siamese-Transport Domain Adaptation (STDA) method using a joint optimal transport theory and contrastive learning for automatic 3D MRI classification and glioma multi-grade prediction. Specifically, the learning framework updates the distribution of features across domains and categories by Siamese transport network training with an Optimal Cost Transfer Strategy (OCTS) and a Mutual Invariant Constraint (MIC) in two projective spaces to find multiple invariants in potential heterogeneity. We design three sets of transfer task scenarios with different source and target domains, and demonstrate that STDA yields substantially higher generalization performance than other state-of-the-art unsupervised domain adaptation (UDA) methods. The method is applicable on 3D MRI data from glioma to Alzheimer's disease and has promising applications in the future clinical diagnosis and treatment of brain diseases.",Three-dimensional displays;Feature extraction;Magnetic resonance imaging;Task analysis;Training;Costs;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10316553,IEEE Journals,,,,,,
Moving Beyond Simulation: Data-Driven Quantitative Photoacoustic Imaging Using Tissue-Mimicking Phantoms,J. GrÃ¶hl; T. R. Else; L. Hacker; E. V. Bunce; P. W. Sweeney; S. E. Bohndiek,IEEE Transactions on Medical Imaging,########,2024,"Accurate measurement of optical absorption coefficients from photoacoustic imaging (PAI) data would enable direct mapping of molecular concentrations, providing vital clinical insight. The ill-posed nature of the problem of absorption coefficient recovery has prohibited PAI from achieving this goal in living systems due to the domain gap between simulation and experiment. To bridge this gap, we introduce a collection of experimentally well-characterised imaging phantoms and their digital twins. This first-of-a-kind phantom data set enables supervised training of a U-Net on experimental data for pixel-wise estimation of absorption coefficients. We show that training on simulated data results in artefacts and biases in the estimates, reinforcing the existence of a domain gap between simulation and experiment. Training on experimentally acquired data, however, yielded more accurate and robust estimates of optical absorption coefficients. We compare the results to fluence correction with a Monte Carlo model from reference optical properties of the materials, which yields a quantification error of approximately 20%. Application of the trained U-Nets to a blood flow phantom demonstrated spectral biases when training on simulated data, while application to a mouse model highlighted the ability of both learning-based approaches to recover the depth-dependent loss of signal intensity. We demonstrate that training on experimental phantoms can restore the correlation of signal amplitudes measured in depth. While the absolute quantification error remains high and further improvements are needed, our results highlight the promise of deep learning to advance quantitative PAI.",Phantoms;Optical imaging;Image reconstruction;Training;Imaging phantoms;Biomedical optical imaging;Adaptive optics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10312761,IEEE Journals,,,,,,
A Transformer-Based Knowledge Distillation Network for Cortical Cataract Grading,J. Wang; Z. Xu; W. Zheng; H. Ying; T. Chen; Z. Liu; D. Z. Chen; K. Yao; J. Wu,IEEE Transactions on Medical Imaging,########,2024,"Cortical cataract, a common type of cataract, is particularly difficult to be diagnosed automatically due to the complex features of the lesions. Recently, many methods based on edge detection or deep learning were proposed for automatic cataract grading. However, these methods suffer a large performance drop in cortical cataract grading due to the more complex cortical opacities and uncertain data. In this paper, we propose a novel Transformer-based Knowledge Distillation Network, called TKD-Net, for cortical cataract grading. To tackle the complex opacity problem, we first devise a zone decomposition strategy to extract more refined features and introduce special sub-scores to consider critical factors of clinical cortical opacity assessment (location, area, density) for comprehensive quantification. Next, we develop a multi-modal mix-attention Transformer to efficiently fuse sub-scores and image modality for complex feature learning. However, obtaining the sub-score modality is a challenge in the clinic, which could cause the modality missing problem instead. To simultaneously alleviate the issues of modality missing and uncertain data, we further design a Transformer-based knowledge distillation method, which uses a teacher model with perfect data to guide a student model with modality-missing and uncertain data. We conduct extensive experiments on a dataset of commonly-used slit-lamp images annotated by the LOCS III grading system to demonstrate that our TKD-Net outperforms state-of-the-art methods, as well as the effectiveness of its key components. Codes are available at https://github.com/wjh892521292/Cataract_TKD-Net.",Cataracts;Transformers;Annotations;Feature extraction;Image edge detection;Fuses;Knowledge engineering,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294274,IEEE Journals,,,,,,
A Multi-Graph Cross-Attention-Based Region-Aware Feature Fusion Network Using Multi-Template for Brain Disorder Diagnosis,Y. Ma; W. Cui; J. Liu; Y. Guo; H. Chen; Y. Li,IEEE Transactions on Medical Imaging,########,2024,"Functional connectivity (FC) networks based on resting-state functional magnetic imaging (rs-fMRI) are reliable and sensitive for brain disorder diagnosis. However, most existing methods are limited by using a single template, which may be insufficient to reveal complex brain connectivities. Furthermore, these methods usually neglect the complementary information between static and dynamic brain networks, and the functional divergence among different brain regions, leading to suboptimal diagnosis performance. To address these limitations, we propose a novel multi-graph cross-attention based region-aware feature fusion network (MGCA-RAFFNet) by using multi-template for brain disorder diagnosis. Specifically, we first employ multi-template to parcellate the brain space into different regions of interest (ROIs). Then, a multi-graph cross-attention network (MGCAN), including static and dynamic graph convolutions, is developed to explore the deep features contained in multi-template data, which can effectively analyze complex interaction patterns of brain networks for each template, and further adopt a dual-view cross-attention (DVCA) to acquire complementary information. Finally, to efficiently fuse multiple static-dynamic features, we design a region-aware feature fusion network (RAFFNet), which is beneficial to improve the feature discrimination by considering the underlying relations among static-dynamic features in different brain regions. Our proposed method is evaluated on both public ADNI-2 and ABIDE-I datasets for diagnosing mild cognitive impairment (MCI) and autism spectrum disorder (ASD). Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art methods. Our source code is available at https://github.com/mylbuaa/MGCA-RAFFNet.",Feature extraction;Diseases;Fuses;Brain modeling;Autism;Time series analysis;Organizations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294288,IEEE Journals,,,,,,
Distinctive Phase Interdependency Model for Retinal Vasculature Delineation in OCT-Angiography Images,M. Challoob; Y. Gao; A. Busch,IEEE Transactions on Medical Imaging,########,2024,"Automatic detection of retinal vasculature in optical coherence tomography angiography (OCTA) images faces several challenges such as the closely located capillaries, vessel discontinuity and high noise level. This paper introduces a new distinctive phase interdependency model to address these problems for delineating centerline patterns of the vascular network. We capture the inherent property of vascular centerlines by obtaining the inter-scale dependency information that exists between neighboring symmetrical wavelets in complex Poisson domain. In particular, the proposed phase interdependency model identifies vascular centerlines as the distinctive features that have high magnitudes over adjacent symmetrical coefficients whereas the coefficients caused by background noises are decayed rapidly along adjacent wavelet scales. The potential relationships between the neighboring Poisson coefficients are established based on the coherency of distinctive symmetrical wavelets. The proposed phase model is assessed on the OCTA-500 database (300 OCTA images + 200 OCT images), ROSE-1-SVC dataset (9 OCTA images), ROSE-1 (SVC+ DVC) dataset (9 OCTA images), and ROSE-2 dataset (22 OCTA images). The experiments on the clinically relevant OCTA images validate the effectiveness of the proposed method in achieving high-quality results. Our method produces average ${F}_{{\textit {Scor}{e}}}$ of 0.822, 0.782, and 0.779 on ROSE-1-SVC, ROSE-1 (SVC+ DVC), and ROSE-2 datasets, respectively, and the ${F}_{{\textit {Scor}{e}}}$ of 0.910 and 0.862 on OCTA_6mm and OCT_3mm datasets (OCTA-500 database), respectively, demonstrating its superior performance over the state-of-the-art benchmark methods.",Retina;Feature extraction;Image segmentation;Diseases;Faces;Wavelet coefficients;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292662,IEEE Journals,,,,,,
Deep Fusion of Multi-Template Using Spatio-Temporal Weighted Multi-Hypergraph Convolutional Networks for Brain Disease Analysis,J. Liu; W. Cui; Y. Chen; Y. Ma; Q. Dong; R. Cai; Y. Li; B. Hu,IEEE Transactions on Medical Imaging,########,2024,"Conventional functional connectivity network (FCN) based on resting-state fMRI (rs-fMRI) can only reflect the relationship between pairwise brain regions. Thus, the hyper-connectivity network (HCN) has been widely used to reveal high-order interactions among multiple brain regions. However, existing HCN models are essentially spatial HCN, which reflect the spatial relevance of multiple brain regions, but ignore the temporal correlation among multiple time points. Furthermore, the majority of HCN construction and learning frameworks are limited to using a single template, while the multi-template carries richer information. To address these issues, we first employ multiple templates to parcellate the rs-fMRI into different brain regions. Then, based on the multi-template data, we propose a spatio-temporal weighted HCN (STW-HCN) to capture more comprehensive high-order temporal and spatial properties of brain activity. Next, a novel deep fusion model of multi-template called spatio-temporal weighted multi-hypergraph convolutional network (STW-MHGCN) is proposed to fuse the STW-HCN of multiple templates, which extracts the deep interrelation information between different templates. Finally, we evaluate our method on the ADNI-2 and ABIDE-I datasets for mild cognitive impairment (MCI) and autism spectrum disorder (ASD) analysis. Experimental results demonstrate that the proposed method is superior to the state-of-the-art approaches in MCI and ASD classification, and the abnormal spatio-temporal hyper-edges discovered by our method have significant significance for the brain abnormalities analysis of MCI and ASD.",Diseases;Time series analysis;Neurological diseases;Fuses;Correlation;Brain modeling;Functional magnetic resonance imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286564,IEEE Journals,,,,,,
Learning From Incorrectness: Active Learning With Negative Pre-Training and Curriculum Querying for Histological Tissue Classification,W. Hu; L. Cheng; G. Huang; X. Yuan; G. Zhong; C. -M. Pun; J. Zhou; M. Cai,IEEE Transactions on Medical Imaging,########,2024,"Patch-level histological tissue classification is an effective pre-processing method for histological slide analysis. However, the classification of tissue with deep learning requires expensive annotation costs. To alleviate the limitations of annotation budgets, the application of active learning (AL) to histological tissue classification is a promising solution. Nevertheless, there is a large imbalance in performance between categories during application, and the tissue corresponding to the categories with relatively insufficient performance are equally important for cancer diagnosis. In this paper, we propose an active learning framework called ICAL, which contains Incorrectness Negative Pre-training (INP) and Category-wise Curriculum Querying (CCQ) to address the above problem from the perspective of category-to-category and from the perspective of categories themselves, respectively. In particular, INP incorporates the unique mechanism of active learning to treat the incorrect prediction results that obtained from CCQ as complementary labels for negative pre-training, in order to better distinguish similar categories during the training process. CCQ adjusts the query weights based on the learning status on each category by the model trained by INP, and utilizes uncertainty to evaluate and compensate for query bias caused by inadequate category performance. Experimental results on two histological tissue classification datasets demonstrate that ICAL achieves performance approaching that of fully supervised learning with less than 16% of the labeled data. In comparison to the state-of-the-art active learning algorithms, ICAL achieved better and more balanced performance in all categories and maintained robustness with extremely low annotation budgets. The source code will be released at https://github.com/LactorHwt/ICAL.",Annotations;Training;Data models;Cancer;Predictive models;Uncertainty;Costs,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10244066,IEEE Journals,,,,,,
Clinically-Inspired Multi-Agent Transformers for Disease Trajectory Forecasting From Multimodal Data,H. H. Nguyen; M. B. Blaschko; S. Saarakkala; A. Tiulpin,IEEE Transactions on Medical Imaging,02-Jan-24,2024,"Deep neural networks are often applied to medical images to automate the problem of medical diagnosis. However, a more clinically relevant question that practitioners usually face is how to predict the future trajectory of a disease. Current methods for prognosis or disease trajectory forecasting often require domain knowledge and are complicated to apply. In this paper, we formulate the prognosis prediction problem as a one-to-many prediction problem. Inspired by a clinical decision-making process with two agentsâ€“a radiologist and a general practitioner â€“ we predict prognosis with two transformer-based components that share information with each other. The first transformer in this framework aims to analyze the imaging data, and the second one leverages its internal states as inputs, also fusing them with auxiliary clinical data. The temporal nature of the problem is modeled within the transformer states, allowing us to treat the forecasting problem as a multi-task classification, for which we propose a novel loss. We show the effectiveness of our approach in predicting the development of structural knee osteoarthritis changes and forecasting Alzheimerâ€™s disease clinical status directly from raw multi-modal data. The proposed method outperforms multiple state-of-the-art baselines with respect to performance and calibration, both of which are needed for real-world applications. An open-source implementation of our method is made publicly available at https://github.com/Oulu-IMEDS/CLIMATv2.",Transformers;Diseases;Prognostics and health management;Imaging;Task analysis;Medical services;Trajectory,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10242080,IEEE Journals,,,,,,
An Interpretable and Accurate Deep-Learning Diagnosis Framework Modeled With Fully and Semi-Supervised Reciprocal Learning,C. Wang; Y. Chen; F. Liu; M. Elliott; C. F. Kwok; C. PeÃ±a-Solorzano; H. Frazer; D. J. McCarthy; G. Carneiro,IEEE Transactions on Medical Imaging,02-Jan-24,2024,"The deployment of automated deep-learning classifiers in clinical practice has the potential to streamline the diagnosis process and improve the diagnosis accuracy, but the acceptance of those classifiers relies on both their accuracy and interpretability. In general, accurate deep-learning classifiers provide little model interpretability, while interpretable models do not have competitive classification accuracy. In this paper, we introduce a new deep-learning diagnosis framework, called InterNRL, that is designed to be highly accurate and interpretable. InterNRL consists of a student-teacher framework, where the student model is an interpretable prototype-based classifier (ProtoPNet) and the teacher is an accurate global image classifier (GlobalNet). The two classifiers are mutually optimised with a novel reciprocal learning paradigm in which the student ProtoPNet learns from optimal pseudo labels produced by the teacher GlobalNet, while GlobalNet learns from ProtoPNetâ€™s classification performance and pseudo labels. This reciprocal learning paradigm enables InterNRL to be flexibly optimised under both fully- and semi-supervised learning scenarios, reaching state-of-the-art classification performance in both scenarios for the tasks of breast cancer and retinal disease diagnosis. Moreover, relying on weakly-labelled training images, InterNRL also achieves superior breast cancer localisation and brain tumour segmentation results than other competing methods.",Training;Prototypes;Predictive models;Retina;Mammography;Deep learning;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225391,IEEE Journals,,,,,,
An Efficient Deep Neural Network to Classify Large 3D Images With Small Objects,J. Park; J. ChÅ‚Ä™dowski; S. JastrzÄ™bski; J. Witowski; Y. Xu; L. Du; S. Gaddam; E. Kim; A. Lewin; U. Parikh; A. Plaunova; S. Chen; A. Millet; J. Park; K. Pysarenko; S. Patel; J. Goldberg; M. Wegener; L. Moy; L. Heacock; B. Reig; K. J. Geras,IEEE Transactions on Medical Imaging,02-Jan-24,2024,"3D imaging enables accurate diagnosis by providing spatial information about organ anatomy. However, using 3D images to train AI models is computationally challenging because they consist of 10x or 100x more pixels than their 2D counterparts. To be trained with high-resolution 3D images, convolutional neural networks resort to downsampling them or projecting them to 2D. We propose an effective alternative, a neural network that enables efficient classification of full-resolution 3D medical images. Compared to off-the-shelf convolutional neural networks, our network, 3D Globally-Aware Multiple Instance Classifier (3D-GMIC), uses 77.98%-90.05% less GPU memory and 91.23%-96.02% less computation. While it is trained only with image-level labels, without segmentation labels, it explains its predictions by providing pixel-level saliency maps. On a dataset collected at NYU Langone Health, including 85,526 patients with full-field 2D mammography (FFDM), synthetic 2D mammography, and 3D mammography, 3D-GMIC achieves an AUC of 0.831 (95% CI: 0.769-0.887) in classifying breasts with malignant findings using 3D mammography. This is comparable to the performance of GMIC on FFDM (0.816, 95% CI: 0.737-0.878) and synthetic 2D (0.826, 95% CI: 0.754-0.884), which demonstrates that 3D-GMIC successfully classified large 3D images despite focusing computation on a smaller percentage of its input compared to GMIC. Therefore, 3D-GMIC identifies and utilizes extremely small regions of interest from 3D images consisting of hundreds of millions of pixels, dramatically reducing associated computational challenges. 3D-GMIC generalizes well to BCS-DBT, an external dataset from Duke University Hospital, achieving an AUC of 0.848 (95% CI: 0.798-0.896).",Three-dimensional displays;Training;Lesions;Breast cancer;Image segmentation;Hospitals;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223434,IEEE Journals,,,,,,
UKSSL: Underlying Knowledge Based Semi-Supervised Learning for Medical Image Classification,Z. Ren; X. Kong; Y. Zhang; S. Wang,IEEE Open Journal of Engineering in Medicine and Biology,11-Jun-24,2024,"Goal: Deep learning techniques have made significant progress in medical image analysis. However, obtaining ground truth labels for unlabeled medical images is challenging as they often outnumber labeled images. Thus, training a high-performance model with limited labeled data has become a crucial challenge. Methods: This study introduces an underlying knowledge-based semi-supervised framework called UKSSL, consisting of two components: MedCLR extracts feature representations from the unlabeled dataset; UKMLP utilizes the representation and fine-tunes it with the limited labeled dataset to classify the medical images. Results: UKSSL evaluates on the LC25000 and BCCD datasets, using only 50% labeled data. It gets precision, recall, F1-score, and accuracy of 98.9% on LC25000 and 94.3%, 94.5%, 94.3%, and 94.1% on BCCD, respectively. These results outperform other supervised-learning methods using 100% labeled data. Conclusions: The UKSSL can efficiently extract underlying knowledge from the unlabeled dataset and perform better using limited labeled medical images.",Biomedical imaging;Head;Image augmentation;Task analysis;Semisupervised learning;Semantics;Self-supervised learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10218990,IEEE Journals,,,,,,
Transformer-Based Spatio-Temporal Analysis for Classification of Aortic Stenosis Severity From Echocardiography Cine Series,N. Ahmadi; M. Y. Tsang; A. N. Gu; T. S. M. Tsang; P. Abolmaesumi,IEEE Transactions on Medical Imaging,02-Jan-24,2024,"Aortic stenosis (AS) is characterized by restricted motion and calcification of the aortic valve and is the deadliest valvular cardiac disease. Assessment of AS severity is typically done by expert cardiologists using Doppler measurements of valvular flow from echocardiography. However, this limits the assessment of AS to hospitals staffed with experts to provide comprehensive echocardiography service. As accurate Doppler acquisition requires significant clinical training, in this paper, we present a deep learning framework to determine the feasibility of AS detection and severity classification based only on two-dimensional echocardiographic data. We demonstrate that our proposed spatio-temporal architecture effectively and efficiently combines both anatomical features and motion of the aortic valve for AS severity classification. Our model can process cardiac echo cine series of varying length and can identify, without explicit supervision, the frames that are most informative towards the AS diagnosis. We present an empirical study on how the model learns phases of the heart cycle without any supervision and frame-level annotations. Our architecture outperforms state-of-the-art results on a private and a public dataset, achieving 95.2% and 91.5% in AS detection, and 78.1% and 83.8% in AS severity classification on the private and public datasets, respectively. Notably, due to the lack of a large public video dataset for AS, we made slight adjustments to our architecture for the public dataset. Furthermore, our method addresses common problems in training deep networks with clinical ultrasound data, such as a low signal-to-noise ratio and frequently uninformative frames. Our source code is available at: https://github.com/neda77aa/FTC.git",Valves;Training;Location awareness;Transformers;Heart;Doppler effect;Analytical models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10218329,IEEE Journals,,,,,,
DNA Encoding-Based Nucleotide Pattern and Deep Features for Instance and Class-Based Image Retrieval,J. Pradhan; A. K. Pal; S. H. Islam; C. Bhaya,IEEE Transactions on NanoBioscience,03-Jan-24,2024,"Recently, DNA encoding has shown its potential to store the vital information of the image in the form of nucleotides, namely ${A}, {C}, {T}$ , and ${G}$ , with the entire sequence following run-length and GC-constraint. As a result, the encoded DNA planes contain unique nucleotide strings, giving more salient image information using less storage. In this paper, the advantages of DNA encoding have been inherited to uplift the retrieval accuracy of the content-based image retrieval (CBIR) system. Initially, the most significant bit-plane-based DNA encoding scheme has been suggested to generate DNA planes from a given image. The generated DNA planes of the image efficiently capture the salient visual information in a compact form. Subsequently, the encoded DNA planes have been utilized for nucleotide patterns-based feature extraction and image retrieval. Simultaneously, the translated and amplified encoded DNA planes have also been deployed on different deep learning architectures like ResNet-50, VGG-16, VGG-19, and Inception V3 to perform classification-based image retrieval. The performance of the proposed system has been evaluated using two corals, an object, and a medical image dataset. All these datasets contain 28,200 images belonging to 134 different classes. The experimental results confirm that the proposed scheme achieves perceptible improvements compared with other state-of-the-art methods.",DNA;Feature extraction;Image retrieval;Image coding;Encoding;Semantics;Visualization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214607,IEEE Journals,,,,,,
Hierarchical Knowledge Guided Learning for Real-World Retinal Disease Recognition,L. Ju; Z. Yu; L. Wang; X. Zhao; X. Wang; P. Bonnington; Z. Ge,IEEE Transactions on Medical Imaging,02-Jan-24,2024,"In the real world, medical datasets often exhibit a long-tailed data distribution (i.e., a few classes occupy the majority of the data, while most classes have only a limited number of samples), which results in a challenging long-tailed learning scenario. Some recently published datasets in ophthalmology AI consist of more than 40 kinds of retinal diseases with complex abnormalities and variable morbidity. Nevertheless, more than 30 conditions are rarely seen in global patient cohorts. From a modeling perspective, most deep learning models trained on these datasets may lack the ability to generalize to rare diseases where only a few available samples are presented for training. In addition, there may be more than one disease for the presence of the retina, resulting in a challenging label co-occurrence scenario, also known as multi-label, which can cause problems when some re-sampling strategies are applied during training. To address the above two major challenges, this paper presents a novel method that enables the deep neural network to learn from a long-tailed fundus database for various retinal disease recognition. Firstly, we exploit the prior knowledge in ophthalmology to improve the feature representation using a hierarchy-aware pre-training. Secondly, we adopt an instance-wise class-balanced sampling strategy to address the label co-occurrence issue under the long-tailed medical dataset scenario. Thirdly, we introduce a novel hybrid knowledge distillation to train a less biased representation and classifier. We conducted extensive experiments on four databases, including two public datasets and two in-house databases with more than one million fundus images. The experimental results demonstrate the superiority of our proposed methods with recognition accuracy outperforming the state-of-the-art competitors, especially for these rare diseases.",Diseases;Retina;Training;Glaucoma;Pathology;Feature extraction;Tail,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10210137,IEEE Journals,,,,,,
Structural Priors Guided Network for the Corneal Endothelial Cell Segmentation,Y. Zhang; R. Xi; L. Zeng; D. Towey; R. Bai; R. Higashita; J. Liu,IEEE Transactions on Medical Imaging,02-Jan-24,2024,"The segmentation of blurred cell boundaries in cornea endothelium microscope images is challenging, which affects the clinical parameter estimation accuracy. Existing deep learning methods only consider pixel-wise classification accuracy and lack of utilization of cell structure knowledge. Therefore, the segmentation of the blurred cell boundary is discontinuous. This paper proposes a structural prior guided network (SPG-Net) for corneal endothelium cell segmentation. We first employ a hybrid transformer convolution backbone to capture more global context. Then, we use Feature Enhancement (FE) module to improve the representation ability of features and Local Affinity-based Feature Fusion (LAFF) module to propagate structural information among hierarchical features. Finally, we introduce the joint loss based on cross entropy and structure similarity index measure (SSIM) to supervise the training process under pixel and structure levels. We compare the SPG-Net with various state-of-the-art methods on four corneal endothelial datasets. The experiment results suggest that the SPG-Net can alleviate the problem of discontinuous cell boundary segmentation and balance the pixel-wise accuracy and structure preservation. We also evaluate the agreement of parameter estimation between ground truth and the prediction of SPG-Net. The statistical analysis results show a good agreement and correlation.",Image segmentation;Endothelial cells;Task analysis;Shape;Microscopy;Computer science;Convolution,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198489,IEEE Journals,,,,,,
Masked Conditional Variational Autoencoders for Chromosome Straightening,J. Li; S. Zheng; Z. Shui; S. Zhang; L. Yang; Y. Sun; Y. Zhang; H. Li; Y. Ye; P. M. A. van Ooijen; K. Li; L. Yang,IEEE Transactions on Medical Imaging,02-Jan-24,2024,"Karyotyping is of importance for detecting chromosomal aberrations in human disease. However, chromosomes easily appear curved in microscopic images, which prevents cytogeneticists from analyzing chromosome types. To address this issue, we propose a framework for chromosome straightening, which comprises a preliminary processing algorithm and a generative model called masked conditional variational autoencoders (MC-VAE). The processing method utilizes patch rearrangement to address the difficulty in erasing low degrees of curvature, providing reasonable preliminary results for the MC-VAE. The MC-VAE further straightens the results by leveraging chromosome patches conditioned on their curvatures to learn the mapping between banding patterns and conditions. During model training, we apply a masking strategy with a high masking ratio to train the MC-VAE with eliminated redundancy. This yields a non-trivial reconstruction task, allowing the model to effectively preserve chromosome banding patterns and structure details in the reconstructed results. Extensive experiments on three public datasets with two stain styles show that our framework surpasses the performance of state-of-the-art methods in retaining banding patterns and structure details. Compared to using real-world bent chromosomes, the use of high-quality straightened chromosomes generated by our proposed method can improve the performance of various deep learning models for chromosome classification by a large margin. Such a straightening approach has the potential to be combined with other karyotyping systems to assist cytogeneticists in chromosome analysis.",Biological cells;Deep learning;Task analysis;Image reconstruction;Feature extraction;Training;Microscopy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177779,IEEE Journals,,,,,,
DSMT-Net: Dual Self-Supervised Multi-Operator Transformation for Multi-Source Endoscopic Ultrasound Diagnosis,J. Li; P. Zhang; T. Wang; L. Zhu; R. Liu; X. Yang; K. Wang; D. Shen; B. Sheng,IEEE Transactions on Medical Imaging,02-Jan-24,2024,"Pancreatic cancer has the worst prognosis of all cancers. The clinical application of endoscopic ultrasound (EUS) for the assessment of pancreatic cancer risk and of deep learning for the classification of EUS images have been hindered by inter-grader variability and labeling capability. One of the key reasons for these difficulties is that EUS images are obtained from multiple sources with varying resolutions, effective regions, and interference signals, making the distribution of the data highly variable and negatively impacting the performance of deep learning models. Additionally, manual labeling of images is time-consuming and requires significant effort, leading to the desire to effectively utilize a large amount of unlabeled data for network training. To address these challenges, this study proposes the Dual Self-supervised Multi-Operator Transformation Network (DSMT-Net) for multi-source EUS diagnosis. The DSMT-Net includes a multi-operator transformation approach to standardize the extraction of regions of interest in EUS images and eliminate irrelevant pixels. Furthermore, a transformer-based dual self-supervised network is designed to integrate unlabeled EUS images for pre-training the representation model, which can be transferred to supervised tasks such as classification, detection, and segmentation. A large-scale EUS-based pancreas image dataset (LEPset) has been collected, including 3,500 pathologically proven labeled EUS images (from pancreatic and non-pancreatic cancers) and 8,000 unlabeled EUS images for model development. The self-supervised method has also been applied to breast cancer diagnosis and was compared to state-of-the-art deep learning models on both datasets. The results demonstrate that the DSMT-Net significantly improves the accuracy of pancreatic and breast cancer diagnosis.",Transformers;Task analysis;Medical diagnostic imaging;Pancreatic cancer;Image edge detection;Medical devices;Ultrasonic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164159,IEEE Journals,,,,,,
Robust Vascular Segmentation for Raw Complex Images of Laser Speckle Contrast Based on Weakly Supervised Learning,S. Fu; J. Xu; S. Chang; L. Yang; S. Ling; J. Cai; J. Chen; J. Yuan; Y. Cai; B. Zhang; Z. Huang; K. Yang; W. Sui; L. Xue; Q. Zhao,IEEE Transactions on Medical Imaging,02-Jan-24,2024,"Laser speckle contrast imaging (LSCI) is widely used for in vivo real-time detection and analysis of local blood flow microcirculation due to its non-invasive ability and excellent spatial and temporal resolution. However, vascular segmentation of LSCI images still faces a lot of difficulties due to numerous specific noises caused by the complexity of blood microcirculationâ€™s structure and irregular vascular aberrations in diseased regions. In addition, the difficulties of LSCI image data annotation have hindered the application of deep learning methods based on supervised learning in the field of LSCI image vascular segmentation. To tackle these difficulties, we propose a robust weakly supervised learning method, which selects the threshold combinations and processing flows instead of labor-intensive annotation work to construct the ground truth of the dataset, and design a deep neural network, FURNet, based on UNet++ and ResNeXt. The model obtained from training achieves high-quality vascular segmentation and captures multi-scene vascular features on both constructed and unknown datasets with good generalization. Furthermore, we intravital verified the availability of this method on a tumor before and after embolization treatment. This work provides a new approach for realizing LSCI vascular segmentation and also makes a new application-level advance in the field of artificial intelligence-assisted disease diagnosis.",Image segmentation;Speckle;Biomedical imaging;Imaging;Blood flow;Supervised learning;Annotations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155229,IEEE Journals,,,,,,
A Scalable Federated Learning Approach for Collaborative Smart Healthcare Systems With Intermittent Clients Using Medical Imaging,F. Ullah; G. Srivastava; H. Xiao; S. Ullah; J. C. -W. Lin; Y. Zhao,IEEE Journal of Biomedical and Health Informatics,06-Jun-24,2024,"The healthcare industry is one of the most vulnerable to cybercrime and privacy violations because health data is very sensitive and spread out in many places. Recent confidentiality trends and a rising number of infringements in different sectors make it crucial to implement new methods that protect data privacy while maintaining accuracy and sustainability. Moreover, the intermittent nature of remote clients with imbalanced datasets poses a significant obstacle for decentralized healthcare systems. Federated learning (FL) is a decentralized and privacy-protecting approach to deep learning and machine learning models. In this article, we implement a scalable FL framework for interactive smart healthcare systems with intermittent clients using chest X-ray images. Remote hospitals may have imbalanced datasets with intermittent clients communicating with the FL global server. The data augmentation method is used to balance datasets for local model training. In practice, some clients may leave the training process while others join due to technical or connectivity issues. The proposed method is tested with five to eighteen clients and different testing data sizes to evaluate performance in various situations. The experiments show that the proposed FL approach produces competitive results when dealing with two distinct problems, such as intermittent clients and imbalanced data. These findings would encourage medical institutions to collaborate and use rich private data to quickly develop a powerful patient diagnostic model.",Servers;Medical services;Data models;Training;Federated learning;Machine learning;Hospitals,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10144616,IEEE Journals,,,,,,
Unsupervised Domain Adaptation of Object Detectors: A Survey,P. Oza; V. A. Sindagi; V. VS; V. M. Patel,IEEE Transactions on Pattern Analysis and Machine Intelligence,########,2024,"Recent advances in deep learning have led to the development of accurate and efficient models for various computer vision applications such as classification, segmentation, and detection. However, learning highly accurate models relies on the availability of large-scale annotated datasets. Due to this, model performance drops drastically when evaluated on label-scarce datasets having visually distinct images, termed as domain adaptation problem. There are a plethora of works to adapt classification and segmentation models to label-scarce target dataset through unsupervised domain adaptation. Considering that detection is a fundamental task in computer vision, many recent works have focused on developing novel domain adaptive detection techniques. Here, we describe in detail the domain adaptation problem for detection and present an extensive survey of the various methods. Furthermore, we highlight strategies proposed and the associated shortcomings. Subsequently, we identify multiple aspects of the problem that are most promising for future research. We believe that this survey shall be valuable to the pattern recognition experts working in the fields of computer vision, biometrics, medical imaging, and autonomous navigation by introducing them to the problem, and familiarizing them with the current status of the progress while providing promising directions for future research.",Object detection;Computational modeling;Detectors;Training;Adaptation models;Task analysis;Proposals,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075484,IEEE Journals,,,,,,
Brain MR Image Classification Using Superpixel-Based Deep Transfer Learning,T. K. Behera; M. A. Khan; S. Bakshi,IEEE Journal of Biomedical and Health Informatics,########,2024,"Nowadays, brain MR (Magnetic Resonance) images are widely used by clinicians to examine the brain's anatomy to look into various pathological conditions like cerebrovascular incidents and neuro-degenerative diseases. Generally, these diseases can be identified with the MR images as â€œnormalâ€ and â€œabnormalâ€ brains in a two-class classification problem or as disease-specific classes in a multi-class problem. This article presents an ensemble transfer learning-inspired deep architecture that uses the simple linear iterative clustering (SLIC)-based superpixel algorithm along with convolutional neural network (CNN) to classify the MR images as normal or abnormal. Superpixel algorithm segments the input MR images into clusters of regions defined by similarity measures using perceptual feature space. These superpixel images are beneficial as they can provide a compact and meaningful role in computationally demanding applications. The superpixel images are then fed to the deep convolutional neural network (CNN) to classify the images. Three brain MR image datasets, NITR-DHH, DS-75, and DS-160, are used to conduct the experimentation. Through the use of deep transfer learning, the model achieves performance accuracy of 88.15% (NITR-DHH), 98.15% (DS-160), and 98.33% (DS-75) even with the small-scale medical image dataset. The experimentally obtained results demonstrate that the proposed method is promising and efficient for clinical applications for diagnosing different brain diseases via MR images.",Feature extraction;Diseases;Computer architecture;Machine learning;Image classification;Pathology;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926058,IEEE Journals,,,,,,
MVCNet: Multiview Contrastive Network for Unsupervised Representation Learning for 3-D CT Lesions,P. Zhai; H. Cong; E. Zhu; G. Zhao; Y. Yu; J. Li,IEEE Transactions on Neural Networks and Learning Systems,03-Jun-24,2024,"With the renaissance of deep learning, automatic diagnostic algorithms for computed tomography (CT) have achieved many successful applications. However, they heavily rely on lesion-level annotations, which are often scarce due to the high cost of collecting pathological labels. On the other hand, the annotated CT data, especially the 3-D spatial information, may be underutilized by approaches that model a 3-D lesion with its 2-D slices, although such approaches have been proven effective and computationally efficient. This study presents a multiview contrastive network (MVCNet), which enhances the representations of 2-D views contrastively against other views of different spatial orientations. Specifically, MVCNet views each 3-D lesion from different orientations to collect multiple 2-D views; it learns to minimize a contrastive loss so that the 2-D views of the same 3-D lesion are aggregated, whereas those of different lesions are separated. To alleviate the issue of false negative examples, the uninformative negative samples are filtered out, which results in more discriminative features for downstream tasks. By linear evaluation, MVCNet achieves state-of-the-art accuracies on the lung image database consortium and image database resource initiative (LIDC-IDRI) (88.62%), lung nodule database (LNDb) (76.69%), and TianChi (84.33%) datasets for unsupervised representation learning. When fine-tuned on 10% of the labeled data, the accuracies are comparable to the supervised learning models (89.46% versus 85.03%, 73.85% versus 73.44%, 83.56% versus 83.34% on the three datasets, respectively), indicating the superiority of MVCNet in learning representations with limited annotations. Our findings suggest that contrasting multiple 2-D views is an effective approach to capturing the original 3-D information, which notably improves the utilization of the scarce and valuable annotated CT data.",Lesions;Computed tomography;Task analysis;Solid modeling;Lung;Representation learning;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901485,IEEE Journals,,,,,,
GMILT: A Novel Transformer Network That Can Noninvasively Predict EGFR Mutation Status,W. Zhao; W. Chen; G. Li; D. Lei; J. Yang; Y. Chen; Y. Jiang; J. Wu; B. Ni; Y. Sun; S. Wang; Y. Sun; M. Li; J. Liu,IEEE Transactions on Neural Networks and Learning Systems,03-Jun-24,2024,"Noninvasively and accurately predicting the epidermal growth factor receptor (EGFR) mutation status is a clinically vital problem. Moreover, further identifying the most suspicious area related to the EGFR mutation status can guide the biopsy to avoid false negatives. Deep learning methods based on computed tomography (CT) images may improve the noninvasive prediction of EGFR mutation status and potentially help clinicians guide biopsies by visual methods. Inspired by the potential inherent links between EGFR mutation status and invasiveness information, we hypothesized that the predictive performance of a deep learning network can be improved through extra utilization of the invasiveness information. Here, we created a novel explainable transformer network for EGFR classification named gated multiple instance learning transformer (GMILT) by integrating multi-instance learning and discriminative weakly supervised feature learning. Pathological invasiveness information was first introduced into the multitask model as embeddings. GMILT was trained and validated on a total of 512 patients with adenocarcinoma and tested on three datasets (the internal test dataset, the external test dataset, and The Cancer Imaging Archive (TCIA) public dataset). The performance (area under the curve (AUC) $=0.772$ on the internal test dataset) of GMILT exceeded that of previously published methods and radiomics-based methods (i.e., random forest and support vector machine) and attained a preferable generalization ability (AUC $=0.856$ in the TCIA test dataset and AUC $=0.756$ in the external dataset). A diameter-based subgroup analysis further verified the efficiency of our model (most of the AUCs exceeded 0.772) to noninvasively predict EGFR mutation status from computed tomography (CT) images. In addition, because our method also identified the â€œcore areaâ€ of the most suspicious area related to the EGFR mutation status, it has the potential ability to guide biopsies.",Lung;Transformers;Predictive models;Hospitals;Computed tomography;Testing;Electronic mail,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836969,IEEE Journals,,,,,,
Refine Diagnostic Accuracy of Skin Cancer with A Hybrid Deep Network,X. Liu; G. Wu; W. Liu,"2023 International Conference on Machine Vision, Image Processing and Imaging Technology (MVIPIT)",05-Jul-24,2023,"With the rapid development of computer-aided diagnosis technology, many efforts have been made to refine the early diagnostic accuracy on skin cancer so as to improve the patientâ€™s survival rate. In this work, a hybrid deep network on the basis of a dedicated combination of Resnet-50 and CrossViT is proposed in this research field. Compared to recent baselines, the proposed network shows its strengths in two aspects: 1) the feature representation ability is significantly enhanced in this case by engaging two sub-networks with deconvolution layers, enabling better classification performance with less image detail and information loss; 2) it further boosts the diagnostic accuracy with a distinguishable and stable feature fusion process via adding dense layers in CrossViT, aiming to ensure unimpeded gradient propagation between network layers. Extensive experiments show that the proposed model can outperform recent baselines regarding classification accuracy on diverse types of skin cancer.",Analytical models;Accuracy;Image analysis;Deconvolution;Machine vision;Transformers;Propagation losses,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578483,IEEE Conferences,,,,,,
Classification of Alzheimerâ€™s Disease Using Improved MeshCNN based on Residuals Connection,Y. Liu; D. Feng; L. Xing; W. Liu,"2023 International Conference on Machine Vision, Image Processing and Imaging Technology (MVIPIT)",05-Jul-24,2023,"Itâ€™s great to see the potential of deep learning being applied to medical imaging for the diagnosis of Alzheimerâ€™s disease. The challenges of small data size and overfitting are common issues in many deep learning applications, and itâ€™s encouraging to see that the ADNI dataset was utilized to address these challenges. The use of MeshCNN network with residual connections borrowed from ResNet is a clever approach to improve the classification accuracy. The Mesh data representation of brain surfaces as triangular meshes is an interesting and innovative technique to incorporate into the classification model. The improvements in accuracy of 0.01 for AD / MCI / NC and 0.02 for AD / NC may seem small, but in the context of Alzheimerâ€™s disease diagnosis, even small improvements in accuracy can have significant implications for early intervention and treatment. Overall, this study demonstrates the potential of deep learning to advance research on Alzheimerâ€™s disease diagnosis and underscores the importance of continued innovation in medical imaging techniques to improve the accuracy and effectiveness of diagnosis and treatment.",Deep learning;Technological innovation;Accuracy;Machine vision;Magnetic resonance;Brain modeling;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578523,IEEE Conferences,,,,,,
Detection of Lung Cancer using Deep Learning based Convolution Neural Network Models,M. A. Muqeet; H. K. Gill; M. A. N. Hasnain; A. Fatima; A. Mohammed; M. K. Saifullah,"2023 IEEE Fifth International Conference on Advances in Electronics, Computers and Communications (ICAECC)",27-Jun-24,2023,"One of the leading cause for cancer based deaths worldwide is because of Lung cancer. Successful treatment and survival can be significantly improved by early detection of lung cancer. The proposed model has the possibility to aid radiologists by detecting lung cancer in early stages, which can lead to more timely and effective treatment. This thesis focuses in developing a CNN-based model to detect the possibility of lung cancer in medical images. The proposed model uses CNN models, which are trained in advance, such as ResNet50, Inception and Xception, as a feature extractor, and then trains a fully connected neural network on top of the extracted features which classifies if the image is either cancerous or non-cancerous. The model is trained and evaluated based upon a dataset of computed tomography (CT) imaging of the lungs. Experimental results concludes that the model proposed accomplish a high level of accuracy in classifying lung cancer, with an average accuracy of 93.75% for ResNet50, 94.56% for inceptionNetv3 and 97.85% for Xception. The study also compares the accomplishment of the model proposed with other highly developed models, and the results illustrates that the proposed model outperforms other models in terms of accuracy, precision, and recall. The proposed model has the capability to assist radiologists in detecting lung cancer in early stages, which can lead to more timely and effective treatment. The evaluation also gives insight in the usage of deep learning approaches for analysing medical image, particularly in the context of lung cancer detection.",Deep learning;Accuracy;Computational modeling;Neural networks;Lung cancer;Lung;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560318,IEEE Conferences,,,,,,
Explaining Deep Learning Decisions Via Fuzzy Inference System on Medical Images,N. Y. Murad; M. H. Hasan; M. H. Azam; N. Yousuf; S. A. Khalique,2023 IEEE 21st Student Conference on Research and Development (SCOReD),25-Jun-24,2023,"Explaining deep learning models has become crucial in various domains, including medical image processing. Understanding these modelsâ€™ decision making processes is complicated as deep learning act like a black box when it comes on decision making. This can also help in advancements of medical imaging analysis, by allowing for more accurate diagnostic and treatment decisions and, ultimately, better patient care. In this study, we addressed the requirement for interpretability, by fusing the strength of CNNs with a Takagi-Sugeno-Kang (TSK) fuzzy inference system. We have utilized ResNet50, a transfer learning form of CNN, to classify a batch of 1000 images from dataset of chest cancer into four classes: â€™Adenocarcinoma,â€™ â€™Squamous Cell,â€™ â€™Normal,â€™ and â€™Large Cellâ€™. The resolution of each image was varied, which was afterwards rectified in the preprocessing step, and this model attained 94.5 percent accuracy. We tried to improve the transparency and dependability of the modelâ€™s decision-making process. In this regard, we applied PCA to minimize the dimensions of predicted probabilities obtained from CNN for looking insight into the CNN modelâ€™s decision making process. Fuzzy c-means clustering is used to group the decision depending on the number of fuzzy rules. We extracted important insights from CNNâ€™s predictions and offer a thorough justification of its decision-making process by incorporating the TSK fuzzy inference system.",Deep learning;Accuracy;Decision making;Transfer learning;Predictive models;Feature extraction;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10563415,IEEE Conferences,,,,,,
Research on low-dose CT image enhancement algorithm based on CycleGAN improvement,Z. Wang; M. J. C. Samonte,"2023 4th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)",11-Jun-24,2023,"Low-dose CT imaging has a wide range of applications in the field of medical imaging. However, problems such as noise and low contrast in low-dose CT images pose challenges to the diagnosis and analysis of the images. Therefore, researchers have been working on developing and improving low-dose CT image enhancement methods to improve image quality and feasibility of clinical applications. Aiming at the problems of insufficient image feature extraction and large number of network parameters in CT images by CycleGAN, this thesis proposes an improved low-dose CT image enhancement network based on CycleGAN. A Shallow Feature pre-Extraction Module, which can extract rich semantic information, is designed to enhance the CycleGAN generatorâ€™s ability to extract image features in CT images; and some ordinary convolutions in the generator are replaced by Depthwise Separable Convolution with a smaller number of parameters to reduce the network parameters. The experimental results show that the improved CycleGAN exhibits better performance in terms of structural detail preservation, noise and artifacts suppression compared to both the traditional algorithm and other unsupervised algorithms.",Image quality;Computed tomography;Noise;Semantics;Big Data;Feature extraction;Generators,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10551153,IEEE Conferences,,,,,,
Dental Disease Detection and Classification in Radiograph Images using Deep Learning Model,V. Kumar; A. Kumar; P. R. Gautam,"2023 5th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)",05-Jun-24,2023,"Dental illnesses are common global health disorders that, if neglected, can have serious consequences for your oral health. Radiographs, which may prove expensive and time-consuming, are a major component of traditional methods for diagnosing and classifying dental diseases. The goal of this study is to suggest a model based on deep learning for dental illness identification and classification sans the use of radiographs, offering an effective and affordable substitute for early detection and care. The Deep Neural Network that has been pre-trained by applying a vast collection of radiograph pictures can be fine-tuned, which is a popular alternative. Radiography picture classification in the database was done using five different levels. The anatomical position, ancillary features, radiodensity, impact on the surrounding framework, and the category of abnormality were all classified here. By making this information available, we hope to improve your capacity to translate radiologist models, automate anomaly identification and categorization in dentistry panoramic radiographs, and progress teeth segmentation algorithms.",Deep learning;Image segmentation;Databases;Lighting;Teeth;Reflection;Dentistry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541747,IEEE Conferences,,,,,,
Brain Tumor Detection using Deep Learning,V. Rai; H. Dwivedi; S. Tiwari; P. K. Gupta,"2023 5th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)",05-Jun-24,2023,"At first, people were unaware of brain tumors, it was believed that there was no cure for the disease, and the main control of the humanoid system was carried out by the human brain. Brain tumors form when brain cells grow and divide abnormally, and brain cancers form when brain tumors continue to grow. Computer purpose is essential in the field of medical because it removes the need for human judgment to produce correct results. The safest MRI techniques are likes of X-ray, CT-Scan and many more useful techniques. Such techniques can even identify small objects. Brain cancer using brain MRI. Abilateral (BF) filter was used in the pre-processing of this study to remove any noise present. For clinical anaysis there is a method termed as Medical imaging is used which basically provides visual representation of the inshore of body and also to show its functioning. Medical imaging is becoming increasingly important as the need for automatic, quick and efficient diagnosis increases, which can provide better image understanding than the human eye. Seeing different trends among young age group it can be said the brain tumor might be the major source of deaths caused by cancer among age group of 20-39. Brain tumors are painful and, if not treated properly, can eventually lead to various diseases.",Performance evaluation;Image segmentation;Visualization;Filtering;Magnetic resonance imaging;Noise;Brain cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541821,IEEE Conferences,,,,,,
Accurate and Efficient Brain Tumor Segmentation using transfer learning and residual networks,S. Kumar; V. Agarwal; V. Sharma; S. Garg; M. K. Sharma; R. Gupta; S. Prashant,"2023 5th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)",05-Jun-24,2023,"Accurate and specific segmentation of brain tumour is essential in medical imaging for accurate diagnosis and planning of treatments. In this research paper, we suggest the use of a Densenet121 model for tumor recognition and a ResUnet model for segmentation. We fine-tune the DenseNet121 model using transfer learning techniques on a new dataset, with pre-trained weights obtained from a large-scale dataset (ImageNet). By leveraging the strengths of transfer learning with Densenet121â€™s and ResUnetâ€™s robust feature extraction capabilities, our model effectively detects and segments tumors from Brain MRI Images. This approach can speed up the exercise procedure and improve the modelâ€™s accuracy, especially for small datasets. We validated our approach on â€œThe Cancer Genome Atlas (TCGA)â€ dataset from The Cancer Imaging Archive (TCIA) and compared it with several classification and segmentation deep learning models. The experimental results on the testing data demonstrate that our approach is more efficient and accurate than other deep learning networks. This approach can substantially accelerate the training process and enhance the accuracy of the model, particularly in scenarios where the new dataset is small. We have validated our proposed approach on â€œThe Cancer Genome Atlas (TCGA)â€ dataset from The Cancer Imaging Archive (TCIA). We have compared our approach with various classification and segmentation deep learning models, and the experimental results on the testing data demonstrate that our method performs better in terms of precision and effectiveness than other deep learning networks.",Deep learning;Training;Image segmentation;Three-dimensional displays;Magnetic resonance imaging;Transfer learning;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541530,IEEE Conferences,,,,,,
A deep learning approach to identify and classify lung tumor from CT â€“ Scans,N. Sudha; I. N. Margret; S. B. Kasetty,"2023 5th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)",05-Jun-24,2023,"In the world, lung cancer is the primary origin of tumor and a major contributor to mortality. A lung cancer problem is an extremely complex issue. Lung cancer patients are the most exposed to Sars-Covid-19 virus, but premature detection has a high survival rate. In recent years, lung cancer detection with computed tomography (CT) has gained popularity among medical imaging system researchers. In order to diagnose lung cancer accurately, it is necessary to read, detect, classify, and evaluate CT scans rapidly. Therefore, it is crucial to be able to identify, categorize and assess CT scans precisely. An effective method to identify & classify lesions/nodules in lung is presented here in this work. This paper consists of 2 portions: finding lesions and lesion classification (segregating nodules according to whether they are Non-Cancerous or cancerous/Benign). To classify & detect Lung nodules, CT-Scans of the lung are used. CT scans are segmented using the U-Net architecture, whereas 3D-Images from Lidc â€“ Idri & Luna 16 are calculated using the VGG-net.",Geometry;Deep learning;Computer viruses;Computed tomography;Lung cancer;Lung;Computer architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541443,IEEE Conferences,,,,,,
A gallbladder cancer classification model based on global-local NET,Q. Jiang; J. Zhang; Z. Geng; Z. Cai,"13th International Conference on Quality, Reliability, Risk, Maintenance, and Safety Engineering (QR2MSE 2023)",########,2023,"Medical imaging data-based computer-aided diagnosis technology is being employed more and more frequently due to the quick advancement of artificial intelligence technology. The most frequent malignant tumor in the biliary tract, gallbladder cancer has a high degree of aggressiveness and a bad prognosis. As a benign chronic gallbladder inflammation, xanthogranulomatous cholecystitis is non-specific in clinical manifestation compared with gallbladder cancer. Preoperative correct identification of gallbladder cancer and xanthogranulomatous cholecystitis is of great importance for doctors to choose surgical methods. Therefore, this paper intends to establish a classification model for gallbladder cancer and xanthogranulomatous cholecystitis through deep learning methods to provide clinical decision support for doctors. This paper analyzes the performance of convolutional neural network and Transformer algorithms in terms of global feature and local feature extraction, and proposes Global-Local Net. Using CT images as datasets, classification models for gallbladder cancer and xanthogranulomatous cholecystitis were constructed based on EfficientNet, Vision Transformer and Global-Local Net, respectively. According to the testing findings, the accuracy of the Global-Local Net can reach 0.9000 and 0.943, and the AUC can reach 0.848 and 0.895 on the arterial phase CT dataset and venous phase CT dataset, respectively.",,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542020,IET Conferences,,,,,,
CGPNet: Enhancing Medical Image Classification through Channel Grouping and Partial Convolution Network,K. Chen; S. Chen; G. Wang; C. Wang,"2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)",########,2023,"With the rapid development of artificial intelligence (AI), various industries have been propelled forward. In the field of medicine, AI has proven to be invaluable in aiding doctors to gain further insights into medical conditions, medical imaging is a prime example. Indeed, the availability of large-scale datasets like ImageNet has significantly contributed to the success of deep learning models in various computer vision tasks. However, when it comes to medical image classification, the availability of large, labeled datasets is relatively limited. As a result, the number of models specifically trained for medical image classification is comparatively smaller. This paper presents a novel neural network, named Channel Grouping and Partial Convolution Network (CGPNet), built upon the foundation of the InceptionNext model. Leveraging the commonly used lightweight technique, along with the introduction of grouped partial convolutions and dynamic convolutions, our proposed network exhibits promising performance. In particular, experiments were conducted on the ISIC-2019 (International Skin Imaging Collaboration) dataset, which demonstrates that our model achieves 2.61% improvement in top-1 accuracy compared to InceptionNext.",Convolution;Computational modeling;Neural networks;Propulsion;Skin;Security;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10538989,IEEE Conferences,,,,,,
DP-ProtoNet: An interpretable dual path prototype network for medical image diagnosis,L. Kong; L. Gong; G. Wang; S. Liu,"2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)",########,2023,"The significant success of deep learning has sparked interest in its application in medical diagnosis. Some deep learning models have achieved expert-level accuracy on some medical datasets, but these models are rarely used in clinical practice due to the lack of interpretability. Therefore, the research topic of explainable artificial intelligence (XAI) has emerged to make the reasoning process of the model transparent and interpretable. In this case, we applied interpretable artificial intelligence to dermatoscopy image diagnosis for the first time. Specifically, we use an interpretable prototype network for dermoscopy image diagnosis. To solve the problem of weak generalization performance of a single network, we propose to construct a new prototype network using the dual-path network. Besides, we propose a new gate similarity calculation method to reduce the activation of low-similarity regions, thereby reducing the generation of inaccurate prototypes and improving the diagnostic ability of the model. We conducted experiments on the dermoscopy datasets HAM10000 to test the model and compare it with other baseline models. Experimental results show that DP-ProtoNet has made improvements in accuracy while preserving the interpretability of the model.",Deep learning;Privacy;Computational modeling;Prototypes;Network architecture;Logic gates;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10538621,IEEE Conferences,,,,,,
Enhancing Diabetic Retinopathy Screening with Sequential Deep Learning Models,V. Mohith; K. Raja; I. R. Oviya,2023 Seventh International Conference on Image Information Processing (ICIIP),########,2023,"Diabetic retinopathy is a serious medical disorder that, if left untreated, can result in visual impairment or blindness. The precise and timely categorization of its severity is critical for appropriate medical management. This research describes an automated method for determining the severity of diabetic retinopathy using deep learning algorithms. The suggested methodology adopts a two-step process for identifying diabetic retinopathy and then categorizing its severity. In this technique, two pre-trained deep learning models, InceptionResNetV2 and DenseNet121, are used. For training and assessment purposes, the â€œAPTOS 2019 Blindness Detectionâ€ dataset which consists of five severity classes is used as the foundation. To overcome class imbalance, data augmentation approaches are used to improve the modelâ€™s performance. The results of the experiment show that the strategy is effective, with an accuracy of 83.10 % and an F1 score of 82.77 %. This study contributes to the development of automated systems for assessing diabetic retinopathy, perhaps enhancing early detection and treatment.",Deep learning;Training;Diabetic retinopathy;Visual impairment;Transfer learning;Blindness;Information processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537711,IEEE Conferences,,,,,,
Computer-Aided Design for Skin Disease Identification and Categorization Using Deep Learning,K. Varalakshmi; A. Revathi,2023 Seventh International Conference on Image Information Processing (ICIIP),########,2023,"Slin diseases are disorders that affect the tissues of the human skin. The common skin disease includes eczema, psoriasis, acne, moles and various fungal infections. Skin disorders contribute 1.79% of the global burden of disease across the world. The identification and categorization of skin diseases are done through advanced methodologies which include deep learning techniques with computer-aided designs. This helps the medical professional to accurately diagnose and categorize skin diseases. The initial step of skin disease identification involves image processing and feature extraction through the Convolutional Neural Network (CNN).This helps in analyzing the skin images and classifying the distinct patterns and anomalies which forms the essential for the identification of particular skin conditions. This helps in accurately recognizing various skin diseases like melanoma, psoriasis and eczema through the evaluation of large amounts of datasets. The categorization of skin disease is obtained through RNN and LSTM. This is done by analyzing the temporal characteristics and symptom progression. These skin disease exhibits unique patterns and minute changes in appearance over time. The accurate identification of skin disease progression is obtained through modelling sequential data using LSTM and RNN. This provides an accurate solution for disease progression with personalized treatment recommendations. They help the dermatologist by providing automatic decision-making techniques. Thus the integration of CNN, RNN and LSTM helps to achieve higher efficiency and accuracy in skin disease diagnosis and monitoring. This helps to obtain improved potential to reduce diagnostic errors and enhance patient outcomes.",Deep learning;Solid modeling;Design automation;Dermatology;Eczema;Skin;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537681,IEEE Conferences,,,,,,
Enhancing Gastric Cancer Diagnosis Through Ensemble Learning for Medical Image Analysis,N. V. Sai Manoj; M. Rithani; R. S. SyamDev,2023 Seventh International Conference on Image Information Processing (ICIIP),########,2023,"Even with the present state of medical imaging technology, accurately and quickly detecting stomach cancer is still a major difficulty. Even though many detection techniques have advanced, current research noticeably lacks a cohesive strategy that integrates multiple models for reliable detection. In response, this research uses stacking ensembling methods to present a unique paradigm for stomach cancer diagnosis. Our goal is to increase the accuracy of stomach cancer diagnosis by using the power of many sophisticated semantic segmentation models, such as U-Net, DeepLab, and SegNet, in order to overcome the current constraints. We comprehensively examine the effectiveness of the various models and the ensemble technique via a rigorous review of important performance measures, such as accuracy (Ensemble: 97.8%, U-Net: 95.8%, DeepLab: 93.3%, SegNet: 92.5%), F1 score, ROC-AUC, precision, recall, and IoU. According to our findings, the ensemble modelâ€”which is enabled by a Random Forest Classifierâ€”performs better than the individual models in identifying stomach cancer. By offering a complete and accurate method for the accurate and dependable identification of stomach cancer, this research contributes significantly to the area of medical image analysis and facilitates early diagnosis and better patient outcomes.",Deep learning;Solid modeling;Stomach;Analytical models;Image analysis;Semantic segmentation;Stacking,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537790,IEEE Conferences,,,,,,
Detection of Brain Tumor Using Novel Convolutional Neural Network with Magnetic Resonance Imaging,K. Devi; A. K. Sharma,2023 Seventh International Conference on Image Information Processing (ICIIP),########,2023,"Brain tumor is a dangerous form of cancer. Hence, the early detection and recognition of brain tumors is a crucial process for saving the lives of patients. Recently, deep learning has played an important role in the diagnosis of medical imaging. Deep learning-based Convolutional Neural Network (CNN) is intensively used to detect brain tumors from Magnetic Resonance Imaging (MRI) images. Automatic detection and classification of brain tumors is the most prominent step in assessing the abnormal tissues of the brain and providing the appropriate treatment in patient recovery. In this work, for the detection of brain tumors, we proposed the 20-layered novel CNN model by using 253 MRI images from the publicly available Kaggle dataset. The dataset consists of 155 MRI images of tumorous and 98 non-tumorous images. Firstly, input images are resized and split into multiple styles (60:20:20, 70:15:15, 80:10:10, 90:05:05) to get the higher performance of our proposed model. It has been found that the proposed model achieved a higher accuracy of 96%, f1-score of 96%, recall of 96%, and precision of 97% with a splitting ratio of 80:10:10 of the dataset in 20 epochs as compared to other splitting ratios for the detection of brain tumors. The main goal of this research work is to build a less layered CNN architecture and to get the best splitting ratio of the dataset, which can detect brain tumors of MRI images without augmentation and gives better performance as compared to the previous work which has been done on the same dataset.",Deep learning;Magnetic resonance imaging;Information processing;Brain modeling;Convolutional neural networks;Tumors;Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537668,IEEE Conferences,,,,,,
AI-Enhanced Diagnosis: Pediatric Chest X-ray Classification for Bronchiolitis and Pneumonia,N. Gehlot; K. Soni; P. Kothari; A. Vijayvargiya; R. Kumar,2023 Seventh International Conference on Image Information Processing (ICIIP),########,2023,"The pediatric diseases in question are bronchiolitis and pneumonia, which pose a significant threat to children, especially those under ten years of age. Rapid diagnosis often requires a chest X-ray; reading and interpreting these images is challenging, and requires the expertise of a skilled doctor. It is essential to take advantage of advanced image recognition techniques to aid in interpreting these examinations and extracting necessary information. This study employed deep transfer learning models, including VGG16, VGG19, MobileNetV2, and InceptionResNetV2, to diagnose bronchiolitis and pneumonia in pediatric chest X-rays (PCXr) for the first time. Our findings show that the InceptionResNetV2 model has achieved the highest recall rate for bronchiolitis, with an impressive value of 78.82%. Following that, VGG16 achieved a recall rate of 77.64%, MobileNetV2 at 74.11%, and VGG19 at 62.35%. Furthermore, when assessing the models comprehensively based on their performance in terms of the F-score, InceptionResNetV2 outperformed the others with an F-score of 65.68%.",Pediatrics;Pneumonia;Image recognition;Transfer learning;Medical services;Information processing;Data mining,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537715,IEEE Conferences,,,,,,
A ResNet-Powered Approach for Brain Tumor Detection with Particle Swarm Optimization,R. Polaki; V. Umamaheswari,2023 Seventh International Conference on Image Information Processing (ICIIP),########,2023,"Brain tumors cause significant distress, affect the brain or surrounding tissues, and cause severe damage. Timely and accurate brain tumor diagnosis is essential for effective treatment and improving patient outcomes. Recent advancements in deep tissue research, particularly in the field of neuroimaging, offer hope and potential. Magnetic resonance imaging (MRI) is still used to diagnose brain problems because it generates precise brain structure images non-invasively utilizing powerful magnetic fields and radio frequencies. In this research, we proposed a unique method for classifying brain tumors into three distinct classes: meningioma, pituitary, and glioma, using a Residual Neural Network (ResNet) architecture and Particle Swarm Optimisation (PSO) to maximize efficiency. Renowned for its deep learning capabilities, the ResNet50 architecture has been optimized to efficiently extract complex information from images of brain tumors. We have optimized the classification process by fine-tuning the networkâ€™s hyperparameters to maximize its discriminative capability by utilizing PSO. The combination of optimization and deep learning methods has produced remarkable outcomes, raising the bar for brain tumor classification accuracy. The results show that our method routinely achieves 99.3% test accuracy, outperforming previous techniques and creating a new standard for brain tumor classification.",Deep learning;Three-dimensional displays;Magnetic resonance imaging;Refining;Brain modeling;Particle swarm optimization;Optimization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537731,IEEE Conferences,,,,,,
Machine Learning for Detection and Classification of Human Brain Tumor: A Survey,S. A. Al Hussen; E. M. T. A. Alsaadi,"2023 International Conference on Information Technology, Applied Mathematics and Statistics (ICITAMS)",########,2023,"Intracranial solid tumors arise due to uncontrolled and abnormal cell division. Classification of brain tumors depends on factors such as the tumorâ€™s location, the type of tissue it consists of, its malignant or benign nature, and other factors. Magnetic resonance imaging (MRI) is one of the techniques frequently used to detect tumors in the brain Magnetic resonance imaging (MRI). It provides important detail that is used in accurately scanning the human bodyâ€™s internal organization. problem with brain tumors is the complexity and variations in tumor location, shape, size, and intensity from patient to patient. Also, tumor boundaries are usually unclear and irregular. Some studies have appeared in the detection and classification of human brain tumors. Therefore, some of these studies will be discussed in this paper. Humans have developed many algorithms to get better results in brain tumor detection. They used the latest machine-learning techniques to solve this problem. Through these studies, we conclude that the best accuracy was achieved using deep learning techniques like Deep Auto-Encoder (DAE) based on Jaya Optimization Algorithm (JOA) for classification and Bayesian Fuzzy clustering (BFC) to segment up to 98.5%.",Surveys;Image segmentation;Shape;Magnetic resonance imaging;Clustering algorithms;Organizations;Solids,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525497,IEEE Conferences,,,,,,
The Smart Analysis of Machine Learning-Based Diagnostics Model of Cardiovascular Diseases in Patients,J. Mistry; S. C. Patil; B. Muniandi; N. Jiwani; J. Logeshwaran,2023 IEEE Technology & Engineering Management Conference - Asia Pacific (TEMSCON-ASPAC),########,2023,"An accurate way to identify and diagnose cardiovascular diseases in patients is to create a machine learning-based diagnostic tool called the Smart Analysis of Machine Learning-Based Diagnostics Model of Cardiovascular Diseases in Patients. It is based on the machine learning and used to classify cardiac conditions in real-time on the basis of electrocardiograms (ECGs) and other physiological data. After classification, the system then gives the physician the confidence level and the degree of certainty they should have in a decision. The projectâ€™s goal is to create a deep learning-based model that can increase accuracy even more. The model will be optimized to function with medical imaging data after being trained on a sizable dataset of ECGs and other physiological data. The group is employing a number of strategies, including convolution neural networks, deep networks, and recurrent networks, to accomplish this goal. In addition, they are investigating the application of contemporary optimization techniques and parallel computing to raise the systemâ€™s efficiency. In order to extract more meaningful insights from the data, the researchers are also creating fresh and enhanced knowledge representation and data mining methodologies.",Analytical models;Neural networks;Medical services;Machine learning;Predictive models;Electrocardiography;Physiology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531319,IEEE Conferences,,,,,,
Advanced Deep Learning Approach for Multiclass Breast Cancer Detection from Mammogram Images,Z. B. T. Baig; X. Han; M. Azam; R. Ahmad,2023 International Conference on Engineering and Emerging Technologies (ICEET),########,2023,"Breast cancer continues to be a significant health concern worldwide, the leading cause of cancer-related deaths among women. However, early detection of breast cancer can significantly enhance survival rates and the efficacy of treatment plans. This study harnesses the power of artificial intelligence (AI) to aid in the accurate and prompt breast cancer diagnosis from histopathological images. We implement a two-stage approach involving image preprocessing using MediCorrectNet, an innovative deep-learning model designed to enhance the diagnostic quality of medical images, and a Convolutional Neural Network (CNN) for classifying the preprocessed images. Specifically, we utilize the EfficientNetB0 model, a pre-trained network known for its effectiveness in image classification tasks. With the combination of MediCorrectNet and EfficientNetB0, our approach achieved an impressive accuracy of 93%, precision of 92%, recall of 91%, F1-Score of 0.915, and an AUC-ROC of 0.98. These results demonstrate our proposed approach's feasibility and high potential for effective breast cancer diagnosis, setting a promising direction for future research and real-world applications.",Deep learning;Histopathology;Refining;Breast cancer;Robustness;Convolutional neural networks;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525730,IEEE Conferences,,,,,,
Novel Machine Learning Approach for High-Precision Brain Tumor Detection from MRI Images,Z. B. T. Baig; X. Han; M. Azam; R. Ahmad,2023 International Conference on Engineering and Emerging Technologies (ICEET),########,2023,"In the ever-evolving medical imaging landscape, the accurate diagnosis of brain tumors presents formidable challenges. Our study introduces a groundbreaking approach harnessing the formidable capabilities of Convolutional Neural Networks (CNN) in tandem with an attention mechanism, significantly elevating the efficacy of brain tumor detection from magnetic resonance imaging (MRI) scans. This innovative attention-based CNN model prioritizes crucial features within MRI scans and substantially enhances overall diagnostic accuracy. Through rigorous training and validation on a diverse Kaggle dataset encompassing four distinct tumor types, glioma, meningioma, pituitary, and no tumor, we have achieved a remarkable overall accuracy rate of 94%, firmly establishing the model's effectiveness. Notably, the precision, recall, and F1-score metrics for each tumor category further validate our methodology's robustness, exemplified by a remarkable F1-score of 0.97 in identifying â€˜No Tumorâ€™ cases. This study pioneers a fresh perspective in integrating machine learning into medical imaging. It underscores the potential of attention-driven CNNs as powerful tools for precise and early brain tumor diagnosis. Our results spotlight the model's exceptional balance between sensitivity and specificity, positioning it as an invaluable asset in brain tumor detection.",Training;Measurement;Neurology;Magnetic resonance imaging;Sensitivity and specificity;Brain modeling;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525991,IEEE Conferences,,,,,,
Co-Teaching Learning from Noisy Labeled FMRI Data for Diagnostic Classification of Major Depression,X. Zhang; J. Su; M. Gan; Y. Zhang; Z. Fan; L. -L. Zeng,2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT),########,2023,"Brain imaging provides an effective tool for identifying brain abnormalities, and functional Magnetic Resonance Imaging (fMRI) is widely used due to its non-invasive nature and high spatiotemporal resolution. However, low signal-to-noise ratio, large inter-subject differences, and high cost of manual labeling may lead to noisy labels in fMRI-based diagnostic classification of neuropsychiatric disorders. In this study, we developed a co-teaching learning framework to address the issue of noisy labels in the fMRI-based diagnostic classification of Major Depression Disorder (MDD), in which a sample selection strategy based on loss value of predicted labels and true labels was realized through the dual-network collaborative architecture, dynamically screening out samples that may have noisy labels. Then the screened samples were used to update the parameters of the two networks. Experimental results revealed that our proposed method could achieve accuracy improvements of 2.2~3.2% in the classification between MDD patients and normal controls, suggesting that our method is helpful to address the issue of noisy labels in image-based diagnostic classification of neuropsychiatric disorders.",Neuroimaging;Manuals;Learning (artificial intelligence);Functional magnetic resonance imaging;Depression;Spatiotemporal phenomena;Noise measurement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10528543,IEEE Conferences,,,,,,
Medical Image Segmentation of Intracranial Hemorrhage: A Review,X. Shi; H. Xiao; D. Chen; Y. Wei,2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT),########,2023,"Intracranial hemorrhage (ICH) is a common clinical emergency that can lead to brain damage or death in a serious situation with extremely high disability and mortality rates. In order to obtain effective treatment for patients timely, the rapid and accurate detection of ICH is crucial. Considering that different subtypes of bleeding correspond to different treatment strategies in clinical practice, it is necessary to quickly diagnose ICH at the early stage. Computed tomography (CT) is one of the most effective ways to detect ICH. For quantifying the lesions and facilitating the subsequent treatments, the segmentation of ICH based on CT images is an extremely important step. The paper presents a review of the task of ICH segmentation, elaborates on the properties of ICH data and the available datasets for research, provides a detailed introduction to the traditional and deep-learning methods, and outlines various improvement schemes for deep-learning models. Finally, the current research achievements in this field are summarized, some urgent problems are proposed, and future research development is foreseen.",Image segmentation;Reviews;Computed tomography;Learning (artificial intelligence);Data models;Lesions;Hemorrhaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10528593,IEEE Conferences,,,,,,
Automated Cardiac Diagnosis Based on Weakly Supervised Learning,L. Zhu; P. Shi; C. Li; W. Li,2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT),########,2023,"Semantic segmentation of medical images is a significant and challenging task, yet traditional fully supervised methods are difficult to achieve satisfactory results due to the scarcity and difficulty of image data annotation. Thus, this paper proposes a semi-supervised semantic segmentation method based on cross-pseudo-supervision and attention modulation. This method uses an EfficientNet as a feature encoder to extract multi-level features for feature fusion; in the encoding stage, it adds an over-parameterized convolution layer with depthwise separable convolution operation, replacing the original convolution method; in the decoding stage, it uses Unet decoder to process the multi-level feature signals, and introduces activation modulation and recalibration mechanisms to enhance the secondary features and suppress the most sensitive and least sensitive feature information. This method trains the model using a cross-pseudo-supervision approach, and achieves a Dice index of 86.7% on the ACDC 1/10 annotated dataset, which is 6% higher than the CPS method; after using 1/3 annotated data, it achieves results similar to the traditional fully supervised semantic segmentation task. Additionally, this method has training friendliness, and when reaching 85% Dice, the training speed is more than ten times faster than traditional fully supervised learning.",Training;Image coding;Convolution;Semantic segmentation;Supervised learning;Modulation;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10528504,IEEE Conferences,,,,,,
Design of an Efficient Deep Learning Framework for Covid-19 Image Classification,J. Himasree; K. Aravindhan; K. P. Keerthana; C. Gobinath,"2023 International Conference on New Frontiers in Communication, Automation, Management and Security (ICCAMS)",########,2023,"The worldwide health crisis caused by COVID-19 and the subsequent control measures have had significant and widespread impacts on various aspects of human life. Detecting COVID-19 early is crucial for effective diagnosis, and ML algorithms have played a vital role in speeding up the process and improving efficiency. However, factors like delivery time and access to training data are also crucial considerations. Extensive analysis of lung ultrasound pictures using the COVID-19 database and deep learning algorithms show that the former is superior to the latter in terms of illness diagnosis. Without specialised testing, it might be difficult to tell COVID-19 from other viral fevers. The research suggests a method of COVID-19 image classification using Convolutional Neural Networks (CNNs). Further evidence of the superiority of the proposed model is provided by a loss and accuracy comparison study.",COVID-19;Deep learning;Ultrasonic imaging;Training data;Lung;Convolutional neural networks;Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525806,IEEE Conferences,,,,,,
Content Based Image Retrieval System using Modified LSTM with Clustering and K-D Tree Indexing Techniques,Lakshmana; P. V. B. Reddy; M. P. Arakeri,"2023 International Conference on New Frontiers in Communication, Automation, Management and Security (ICCAMS)",########,2023,"A Robust Content-Based Medical Image Retrieval System must be designed to support the ever-increasing volume of digital photographs across various medical disciplines in order to successfully return medical photos that are aesthetically comparable to the query image. The CBMIR approach for optimum medical image retrieval is presented in this research work, with a focus on reducing deviation in medical image mining and a full analysis of significant aspects of medical photos. The purpose of this research was to develop a LSTM based deep learning model for classifying CT Liver tumor images from the CT Liver tumor images dataset. The dataset is preprocessed with thresholding, extreme point computation, and bicubic interpolation. Second, the proposed method uses a convolutional neural network based LSTM to extract information from cropped photos. There are four measures used to evaluate the modelâ€™s efficacy: accuracy, precision, recall. Accuracy at 96.88%, precision at 97.96%, recall at 96.60% are all optimally achieved by the proposed model. According to the findings, the proposed LSTM based RNN Approach with KD Tree model is the most efficient method for identifying CT Liver Tumor images detection and classification.",Visualization;Computed tomography;Computational modeling;Image retrieval;Liver;Vegetation;Data mining,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525784,IEEE Conferences,,,,,,
Breast Cancer Diagnosis in Radiology Images Using Transfer Learning Technique,S. L. Nesamani; D. Lissy; S. Sumathy; S. N. S. Rajini,"2023 International Conference on New Frontiers in Communication, Automation, Management and Security (ICCAMS)",########,2023,"Diseases like cancer which are life-threatening needs accurate diagnosing techniques at a very early stage. This research work is focused on identifying one such technique for identifying breast cancers at a very early stage. This work focuses on employing radiology images of breast mammograms using a transfer learning technique, which is a very powerful deep learning technique that has proven to produce excellent results on computer vision projects. As medical data are very sparse and difficult to obtain transfer learning technique is employed where the knowledge gained through a previous task could be transferred to a new task of a different domain. Breast mammogram images were taken from the publicly available Mini DDSM Dataset which consists of breast mammogram images in the Medio Lateral Oblique (MLO) and bilateral Cranio Caudal (CC) views of both thebreasts. Three experiments were performed on the dataset to demonstrate the effectiveness of the technique by choosing three pre-trained models VGG19, ResNet 50, and Xception and the results were compared. The overall performance of the pre-trained models was found to be elevated due to the deployment of a Logistic Regression (LR) classifier. VGG19 pre-trained model proved to produce the topmost result of 88% accuracy on both the training as well as on the validation datasets. The other two pretrained models exhibited a slightly lower performance of ResNet50 with 80% and Xception with 79% accuracy.",Training;Logistic regression;Transfer learning;Computer architecture;Radiology;Mammography;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10526195,IEEE Conferences,,,,,,
Deep Learning Classification and Segmentation of Brain MRI Images,S. Pooja Ahuja; S. R. Likhith; A. Shiva Balan; K. Navya,"2023 International Conference on New Frontiers in Communication, Automation, Management and Security (ICCAMS)",########,2023,"Unique Clinical pictures have a basic impact on the specialistâ€™s capability to the right conclusion and in the treatment of the patient. Diagnostic imaging can quickly identify lesions thanks to sophisticated algorithms, and it is essential to glean characteristics from photographs. In a number of studies, a number of algorithms have been incorporated into imaging in medicine. Using convolutional neural network, CNNâ€™s fundamental architecture is constructed using picture component extraction. To be able to circumvent the limitations imposed by both machine vision and human vision, the study is extended to include CNN with multiple input channels for visual feature extraction. This study uses approximately 3300 MRI samples from Kaggle to investigate four classifications: tumors of the pituitary gland, meningioma, glioma, and none. The implemented BrainNet has a accuracy of 98.31 percent and a validation accuracy of 87.80 percent. In order to determine which strategy performed better, deep architectures like InceptionNet and ResNet were also put through tests with or without learning through transfer.",Visualization;Image segmentation;Magnetic resonance imaging;Object segmentation;Feature extraction;Pituitary gland;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10526082,IEEE Conferences,,,,,,
A Framework for Generalization Error Evaluation in Deep Convolutional Neural Networks,G. Chugh; S. Kumar; N. Singh,2023 IEEE Engineering Informatics,########,2023,"Deep learning has introduced various paradigms in the healthcare industry. Deep Convolutional Neural Network models assist doctors in diagnosis, surgery, and other areas. Incidences of breast tumors are growing at a frightening pace. Thus, it is necessary to diagnose it so that the mortality count can be decreased. Generalizability defines how well the model performs on unseen data. When capturing mammograms different types of noise get added to the images. Noise may significantly diminish classification ability and make class separation more difficult. Thus, analyzing the model's generalization on noisy or unseen data is very important. This paper proposes an approach for analyzing generalization errors in Deep Convolutional Neural Networks by inducting noises such as Gaussian, Salt and pepper, and Speckle. We have utilized the CBIS-DDSM dataset. Three prominent deep neural network models- Inception v3, Dense Net 201, and EfficientNetB4- are fine-tuned to assess the model's efficiency on noisy data. Results proved that the DenseNet201 has minimum Generalization errors i.e. 0.12 and performs pretty well on noisy data with a minimum loss rate. On the other hand, maximum distortion is caused by Speckle Noise in Inceptionv3 and Efficient NetB4 leading to a considerable drop in accuracy. Inception v3 has the highest Generalization Error i.e. 0.61 and thus exhibits minimum generalization capability.",Ultrasonic imaging;Noise;Medical services;Speckle;Distortion;Data models;Mammography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10520375,IEEE Conferences,,,,,,
Progressive Neural Networks for Continuous Classification of Retinal Optical Coherence Tomography Images,D. Wang; K. Huang; Q. Chen,2023 Eleventh International Conference on Advanced Cloud and Big Data (CBD),########,2023,"Deep learning models have demonstrated a great effectiveness on the classification of retinal lesions in optical coherence tomography images. However, the performance of these models deteriorates significantly when classifying images from different devices compared to the training data, which greatly limits their practicality. To address the issue of poor performance of the model in real-world applications, we propose a method of multi-granularity feature space constraint by studying samples from different devices as domain incremental tasks, to maintain the stability and plasticity of the model, thereby making the model having good generalization performance. Experimental results demonstrate that the proposed method effectively maintains performance on both new and old datasets. Compared to some state-of-the-art baselines focusing on natural images, the proposed method in this paper achieves higher average accuracy and lower forgetting rate on publicly available medical image datasets.",Performance evaluation;Deep learning;Optical coherence tomography;Neural networks;Training data;Retina;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516494,IEEE Conferences,,,,,,
Gradient-Based Fine-Tuning Strategy for Improved Transfer Learning on Surgical Images,A. Davila; J. Colan; Y. Hasegawa,2023 International Symposium on Micro-NanoMehatronics and Human Science (MHS),########,2023,"Transfer learning is a widely used technique to leverage pre-trained models on new tasks, but it often suffers from out-of-distribution shifts when the source and target domains are different. This is especially common in surgical images, where the appearance and context of the images vary significantly across different procedures and instruments. To address this problem, we propose a novel gradient-based fine-tuning strategy that selectively freezes layers of a pre-trained model based on their weight gradients. Our method aims to preserve the generalizable features learned from the source domain while adapting the model to the target domain. We evaluate our method on two tasks: surgical instrument recognition and gesture recognition. We compare our method with several existing fine-tuning strategies, including full fine-tuning, linear probing, and gradual unfreezing variations. Our experimental results show that our method achieves the best performance on both tasks. Our approach enables more efficient and robust transfer learning for surgical image segmentation, which is essential for various applications in computer-assisted surgery.",Deep learning;Image segmentation;Adaptation models;Instruments;Transfer learning;Surgery;Gesture recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10510130,IEEE Conferences,,,,,,
An Interpretable Deep Learning Approach for Skin Cancer Categorization,F. Mahmud; M. M. Mahfiz; M. Z. I. Kabir; Y. Abdullah,2023 26th International Conference on Computer and Information Technology (ICCIT),29-Apr-24,2023,"Skin cancer is a serious worldwide health issue, precise and early detection is essential for better patient outcomes and effective treatment. In this research, we use modern deep learning methods and explainable artificial intelligence (XAI) approaches to address the problem of skin cancer detection. To categorize skin lesions, we employ four cutting-edge pre-trained models: XceptionNet, EfficientNetV2S, InceptionResNetV2, and EfficientNetV2M. Image augmentation approaches are used to reduce class imbalance and improve the generalization capabilities of our models. Our modelsâ€™ decision-making process can be clarified because of the implementation of explainable artificial intelligence (XAI). In the medical field, interpretability is essential to establish credibility and make it easier to implement AI-driven diagnostic technologies into clinical workflows. We determined the XceptionNet architecture to be the best performing model, achieving an accuracy of 88.72%. Our study shows how deep learning and explainable artificial intelligence (XAI) can improve skin cancer diagnosis, laying the groundwork for future developments in medical image analysis. These technologiesâ€™ ability to allow for early and accurate detection could enhance patient care, lower healthcare costs, and raise the survival rates for those with skin cancer. Source Code: https://github.com/Faysal-MD/An-Interpretable-Deep-Learning-Approach-for-Skin-Cancer-Categorization-IEEE2023",Deep learning;Visualization;Explainable AI;Computational modeling;Medical services;Skin;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10508527,IEEE Conferences,,,,,,
Ensemble Deep Learning Approaches for Liver Tumor Detection and Prediction,R. Archana; L. Anand,2023 Third International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS),29-Apr-24,2023,"Cancer is a fatal illness with several underlying causes in metabolism and genetics. Liver cancer ranks as the third leading cause of cancer-related mortality globally and the sixth most prevalent illness in terms of total incidence. The liver plays a pivotal part in the physiological functioning of the human body. The liver controls cholesterol homeostasis and stores fat-soluble vitamins. Liver cancer may be treated more effectively if detected and classified early on. Brain, breast, liver, and other tumors may be examined using CT and MRI, two types of imaging technology. Convolutional neural networks (CNNs) have improved medical imaging for identifying and classifying cancer compared to earlier methods. To help radiologists notice anomalies more quickly, this research presents a classification model for recognizing liver tumors utilizing a convolutional neural network (CNN), an enhanced optimization technique, and transfer learning. The MPA (algorithm for detecting marine predators) was the basis for our optimization strategy. Using an improved version of the marine predator's algorithm (IMPA), developed as part of this project, I was able to identify the ideal values for the CNN's meta parameters. The proposed method combines a version of the ResNet50 CNN model that has already been trained with the IMPA algorithm to create the IMPA-ResNet50 architecture. This IMPA offers superior precision than competing optimization techniques accuracy is 99.20%. IMPA modifies the primary discovery and exploitation procedures to zero down on the most relevant features for reliable categorization.",Deep learning;Liver cancer;Metaheuristics;Liver;Computer architecture;Classification algorithms;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10505955,IEEE Conferences,,,,,,
Deep Learning Based Blood Flow Analysis in MRI Images,U. M. Hiremath; S. Chickerur,2023 Third International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS),29-Apr-24,2023,"The study, deep learning algorithms, and compu-tational fluid dynamics (CFD) are combined to analyze blood flow patterns in MRI images. Due to complicated physiological structures and turbulent flows, traditional approaches for blood flow analysis in medical imaging frequently face challenges. In the current study, a deep learning architecture is suggested to accurately extract flow data from MRI scans, offering thorough insights into vascular behavior. Following the extraction, the data are merged into a CFD simulation, allowing for accurate hemodynamic modeling. The combination of deep learning and CFD improves the precision and effectiveness of blood flow analysis, paving up the possibility of potential developments in cardiovascular disease diagnosis and treatment planning. The suggested method employs deep learning-based techniques for flow analysis, ROI extraction, and image pre-processing. It's significant that the method presents a possible classification system for both normal and abnormal aortas. Fundamental flow characteristics, such as velocity, pressure, and turbulence, may be calculated using CFD and are crucial for understanding the underlying flow patterns. It is now possible to analyze and identify a variety of aortic issues with the aid of MRI images and the integration of CFD and deep learning, improving diagnosis and patient care. Experimental findings show the effectiveness with which the suggested method captures complex flow patterns and advances our knowledge of vascular dynamics.",Deep learning;Magnetic resonance imaging;Heuristic algorithms;Fluid dynamics;Ubiquitous computing;Physiology;Planning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506005,IEEE Conferences,,,,,,
A Comprehensive Study on Computer Aided Approaches for Multiclass Neurological Disorder Classification,B. R. M R; K. Kavitha; J. J S,2023 Third International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS),29-Apr-24,2023,"Visualizing anatomical structure benefits from medical imaging and analysis. However, employing basic imaging methods is challenging and ineffective. Brain abnormalities diagnosis and classification manually is especially a tedious task. Additionally, the approach currently in use experiences inter-observer variability while interpreting images. Diagnosis must be done with extreme care because even a small human error can have serious effects. An MRI can be used as a non-invasive method to find tumors. Early disease diagnosis can be accomplished by Computer Aided Diagnosis (CAD) systems, potentially improving survival odds and reducing the requirement for MRI analysis by a specialist. CNN have demonstrated to be quite efficient at finding tumors in brain MRIs. The automated diagnosis of brain MRI images using a unique computer-aided diagnosis method is described here. In recent decades, neuroimaging, especially MRI, has played a pivotal role in advancing our understanding of brain function and the various illnesses that can impact. Accurately identifying such illnesses from acquired neuroimaging data poses significant challenges due to the similarities in disease characteristics. This study analyses and evaluates the effectiveness of deep learning methods to identify neurological diseases. This study focuses on Alzheimer's, Picks, Huntington's, Cerebral Carcinoma and Tumors.",Neurological diseases;Neuroimaging;Deep learning;Magnetic resonance imaging;Ubiquitous computing;Computer aided diagnosis;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506034,IEEE Conferences,,,,,,
Enhancing Healthcare Through Machine Learning: A Rigorous Evaluation and Comparative Study of Algorithms,H. Sharma; R. Kumar; M. Gupta,2023 Third International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS),29-Apr-24,2023,"The integration of machine learning in healthcare is rapidly growing and holds a great potential to transform the analysis and application of medical data. This study explores the latest developments in machine learning to significantly improve the speed and precision of disease diagnosis. Main goals of this research is to obtain current research landscape in this domain by identifying the existing research gaps, and then develop a potential solution for future investigation. By conducting a comprehensive search through Google Scholar database, 20-research publications were selected in order to address two key research questions, i.e. why researchers choose machine learning in healthcare and what are the limitation/weakness of ML in healthcare. Based on the analysis, it was determined that the PIMA and MESSIDOR datasets are the most frequently utilized datasets for diabetes classification. Additionally, in this work a machine learning algorithm that effectively reduce the time constraints while simultaneously improving diagnostic performance is proposed.",Deep learning;Machine learning algorithms;Transforms;Ubiquitous computing;Robustness;Diabetes;Internet,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506004,IEEE Conferences,,,,,,
Label Semantic Improvement with Graph Convolutional Networks for Multi-Label Chest X-Ray Image Classification,D. Cai; H. Lu; Z. Chai; R. Wang; W. Zhu; Y. Yao,2023 13th International Conference on Information Technology in Medicine and Education (ITME),25-Apr-24,2023,"Multi-label classification of Chest X-ray images is one of the important tasks in the field of medical image analysis, and current methods only capture co-occurrence relationships between labels, making it difficult to establish complex connections among labels. We face a series of challenges when considering global label correlation, visual correlation, and dealing with unbalanced data. In this study, we introduce a Label Semantic Improvement Graph Convolutional Network (LSI-GCN) framework based on GCN, which consists of an Image Representation Learning module, a GCN module with Label Semantic Improvement, and a Distance Metric module to improve classification performance. The task of multi-label classification of chest X-ray images is approached from different perspectives and levels by combining their functionalities to improve the performance, robustness, and accuracy of the model. The Image Representation Learning module is a feature extractor that learns high-level label-specific features from chest X-ray images. The GCN captures label correlations and models global label relationships and category visual correlations, respectively. With this approach, the correlation between disease labels and the relevance of image features can be considered simultaneously, and the accuracy of multi-label classification can be improved. The image features were further optimized using a distance metric module. Experiments were conducted using Chest X-ray14 and CheXpert images as multi-label datasets, and the mean Area Under Curve (AUC) scores obtained were better than those of comparative models such as CheXGCN [3] and the accuracy and robustness of multi-label classification were improved.",Measurement;Visualization;Correlation;Semantics;Image representation;Feature extraction;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10505562,IEEE Conferences,,,,,,
Unsupervised Anomaly Detection for Mild Cognitive Impairment Using Diffusion Model,Y. Lei; M. Nieuwoudt; H. Matsumoto; N. Zhong; P. Yin; S. Wang,2023 Asia Conference on Cognitive Engineering and Intelligent Interaction (CEII),25-Apr-24,2023,"Mild Cognitive Impairment (MCI) is an important stage between Normal Control (NC) and Alzheimer's disease (AD), thus the early detection of MCI carries important clinical significance. Anomaly detection can identify abnormal regions in medical images and serve as a tool for early diagnosis and treatment of MCI patients. However, the absence of pixel-level labels in the MCI database poses a challenge to performing anomaly detection using supervised learning. Unsupervised learning can be utilized as an alternative approach to address the above-mentioned issues. In this paper, we propose an unsupervised anomaly detection framework based on the diffusion model for the early detection of MCI. The proposed framework learns the distribution of healthy data and guides the abnormal data to conform to the healthy distribution, thereby achieving ""healing"" and locating anomaly regions. Experimental results demonstrate that the proposed framework can effectively distinguish between healthy and abnormal data, and highlight anomaly regions associated with MCI, showing its practical significance in the early diagnosis of MCI.",Training;Databases;Supervised learning;Asia;Data models;Alzheimer's disease;Anomaly detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506892,IEEE Conferences,,,,,,
Endoscopic Bladder Tissue Classification Through Fusion of Handcrafted and Deep Features,R. G. Tiwari; H. Maheshwari; A. K. Agarwal; O. Sharma; N. Bharathiraja,2023 3rd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),18-Apr-24,2023,"The proper identification of tissue types during endoscopy may greatly affect both the patientâ€™s care and the treatment options that are made, making the categorization of endoscopic bladder tissue a crucial responsibility in the field of urology. Combining handmade characteristics with deep features is the innovative strategy that is presented in this study to improve the accuracy of bladder tissue categorization. To extract handmade characteristics from endoscopic images, the proposed technique makes use of textural feature extraction and the Bag-of-Visual-Word (BoVW) paradigm. This research proposes a hybrid convolutional neural network (CNN) model as a means of further enhancing classification performance. This model takes handcrafted features in addition to CNN-based features as input. This hybrid model is intended to make use of the complementing qualities that both kinds of features possess to provide a depiction of bladder tissue that is more accurate and distinct. During the experimental assessment, the proposed hybrid CNN model is evaluated in comparison to fundamental machine learning approaches as well as alternative deep learning architectures. The findings show that the suggested model can achieve an amazing accuracy rate of 95%, which is higher than the performance of other techniques. This accomplishment demonstrates the ability to integrate handmade cues with deep features for endoscopic bladder tissue categorization. This work has major implications for increasing diagnosis accuracy and the outcomes for patients.",Endoscopes;Deep architecture;Machine learning;Bladder;Feature extraction;Convolutional neural networks;Urology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426495,IEEE Conferences,,,,,,
Detection of Cardiac Arrhythmia using Machine Learning,D. Jyothirmai; P. Muktevi; G. R. Varun; H. V. Mantada; J. Moturi; R. Pitchai,2023 3rd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),18-Apr-24,2023,"The testing and evaluation of cardiac arrhythmia requires the use of Electro cardio gram (ECG), a diagnostic tool. Cardiac arrhythmia refers to abnormalities in the rate of the heartbeat. It happens when the heartâ€™s electrical signals are malfunctioning and the heart beats too quickly, too slowly, or irregularly. The usage of machine learning in detecting cardiac arrhythmia can help improve the performance and efficiency of diagnosis, but it is important to note that machine learning algorithms should always be used hand-to-hand with clinical expertise and judgment. Several factors, including excessive blood pressure, alcohol misuse, and genetics, can contribute to cardiac arrhythmia. If not attended it can be life-threatening and lead to death.",Training;Support vector machines;Machine learning algorithms;Heart beat;Arrhythmia;Machine learning;Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425867,IEEE Conferences,,,,,,
Accurate Brain Tumor Detection using Deep Learning: A Comprehensive Review and Implementation Study,G. Shruthi; M. Saravanan; U. Samyuktha; Thirumalaipathy,2023 3rd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),18-Apr-24,2023,"The objective of this study is to develop a detailed method for applying deep learning techniques to the early diagnosis of brain tumors. It is imperative that brain tumor is diagnosed as early as possible to ensure the best possible treatment. Due to its potential to enhance the precision and efficiency of cancer detection, deep learning techniques have become increasingly popular in areas related to medical image analysis. In the proposed system, Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) are used to analyze medical images, including CT and MRI scans. To raise the caliber of the input data, preprocessing methods like segmentation and picture enhancement are used. Brain scan abnormalities, lesions, and tumor regions can be automatically recognized by the trained deep-learning model.",Deep learning;Recurrent neural networks;Brain modeling;Medical diagnosis;Reliability;Medical diagnostic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425825,IEEE Conferences,,,,,,
Empowering Medical Diagnosis with Deep Learning-Driven Image Segmentation,R. K. Saw; H. Jain; K. Chowdhury; P. Singh,2023 3rd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),18-Apr-24,2023,"This research explores advanced computational techniques for medical image analysis, focusing on Chest X-ray pneumonia images. Integrating Convolutional Neural Networks (CNN) and Fully Convolutional Networks (FCN), this approach enhances disease detection through image segmentation. Comparative analysis with traditional models reveals distinct diagnostic accuracies, shedding light on model selection for specific tasks. The study aims to bridge technology and clinical practice, leveraging deep learning to improve healthcare outcomes.",Deep learning;Image segmentation;Image analysis;Computational modeling;Convolutional neural networks;Medical diagnosis;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426324,IEEE Conferences,,,,,,
Improved Classification for Corona Virus Disease using XceptionNet in X-Ray Images,S. S. M; K. Srividya; V. C. Jadala; U. Chandrasekhar; K. Durga; M. M. Ammangatambu,2023 3rd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),18-Apr-24,2023,"Diagnosis is necessary to confirm SARS-CoV-2 infection. Imaging methods are vital in aiding the diagnosis, assessing disease severity, directing treatment, identifying complications, and monitoring the response to treatment. The lungs are the foremost organ impacted, and chest X-rays are the preferred initial imaging exam due to their accessibility and cost-effectiveness, whether obtained at conventional X-ray centers or using portable equipment. This research study has used a combination of virus-infected people as a dataset. The classification of diseases will help doctors to treat the patients and save their life. For classifying Normal, COVID-19 and Viral Pneumonia classesâ€™ the architecture of the pre-trained network XceptionNet is modified. XceptionNet is the method to use for the classification of diseases. The proposed model has resulted in a 98.80% accuracy for classifying the diseases.",COVID-19;Computed tomography;Pulmonary diseases;Object detection;Reliability;X-ray imaging;Monitoring,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425851,IEEE Conferences,,,,,,
Advancing Brain Lesion Classification in CT Images: A Transfer Learning Approach with Convolutional Neural Networks,T. Ramya; U. L. Marksia; R. V. Suseela; P. Bhuvana; B. A. Geetha; K. L. Narayanan,2023 3rd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),18-Apr-24,2023,"This study presents a novel hybrid deep learning approach for precise segmentation and detection of various types of brain lesions in CT images, with a particular emphasis on hemorrhagic strokes.â€ Convolutional neural networks (CNNs) are used to process CT brain images and perform image recognition in the proposed framework. For the purpose of pre-processing adaptive median filtering is used, which improves the quality of the image, which successfully removes noise. Afterward, the pre-processed image will act as an input to the segmentation block, where it is broken into a number of segments for additional processing. The recommended network uses K-means clustering to improve segmentation accuracy. This technique helps in distinguishing the hemorrhagic area from healthy brain tissue by enhancing the contrast. The experimental results obtained using the CNN Classifier demonstrate precise lesion detection. However, it is important to address the computational speed and the limitation of signal flow in one direction only in feed-forward setups to optimize the overall performance of the proposed method.",Image segmentation;Brain;Computed tomography;Transfer learning;Lesions;Convolutional neural networks;Hemorrhaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426047,IEEE Conferences,,,,,,
GCNN-based Combined Denoising and Classification for Improved MRI Brain Tumor Identification,G. Dharani Devi; S. Sandra Doss; S. Sanjitha; N. Sai Chaithanya,2023 3rd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),18-Apr-24,2023,"Medical image analysis is fundamental in modern healthcare, providing vital insights for early diagnosis and effective treatment planning. In the realm of neuroimaging, Magnetic Resonance Imaging (MRI) serves as a main resource for brain tumor detection. However, the clinical utility of MRI is intrinsically linked to image quality, wherein enhancing the noisy MRI scans remain as a significant challenge. This study introduces a novel GCNN approach for improving MRI image quality in brain tumor detection by jointly addressing the denoising and classification challenges. This study efficiently remove noise from MRI images by using the Denoising Convolutional Neural Network (DnCNN). A Convolutional Neural Network discriminator evaluates the quality of denoised images. Following denoising, a CNN-based classifier detects brain tumors in the denoised images. Empirical validation across diverse datasets demonstrates the proposed approachâ€™s efficacy in enhancing image quality and achieving robust tumor classification. GCNN presents a promising solution for advancing brain tumor diagnosis through integrated GAN-based image enhancement and classification. A cutting-edge technique for medical image analysis is demonstrated by GCNN.",Image quality;Magnetic resonance imaging;Noise reduction;Planning;Convolutional neural networks;Medical diagnostic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425821,IEEE Conferences,,,,,,
Acute Lymphoblastic Leukemia Classification Method Based on Attention Residual Network,Y. Shao; W. Yu,"2023 3rd International Signal Processing, Communications and Engineering Management Conference (ISPCEM)",18-Apr-24,2023,"Aiming at the problems of error-prone and time-consuming classification of Acute lymphoblastic leukemia (ALL) blood cell microscopic images in medicine, this paper proposes a method based on the attention mechanism residual convolutional neural network model to obtain the information in medical images. Complicated pathological information. This method first preprocesses the sample data, cleans out the training set and verification set that meet the requirements, and also uses the threshold segmentation method to extract the target area of the training sample, and then inputs the preprocessed data into the network. In the model, it is trained, and finally the verification set is input into the model for verification. The experimental results show that this classification method can effectively classify whether ALL blood cell microscopic images are diseased.",Training;Image segmentation;Pathology;Microscopy;Engineering management;Data preprocessing;Signal processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499344,IEEE Conferences,,,,,,
Medical Data Enhancement and Gastric Cancer Detection Based on Improved WGAN and Multi-Channel Fusion CNNs,W. Qiu,"2023 3rd International Signal Processing, Communications and Engineering Management Conference (ISPCEM)",18-Apr-24,2023,"Stomach cancer is now one of the most common malignant tumor diseases of digestive tract, and its prevalence as well as mortality rate are located in the top few tumor diseases. At the same time, China is a country with a high prevalence of gastric cancer, which is expected to turn into a more significant problem with the accelerated aging of the population, so detecting gastric cancer at an early stage can effectively improve the treatment outcome. One of the main problems faced by deep learning for processing medical images is the small size of the dataset. When the amount of data is severely insufficient, the network model is difficult to be trained stably and leads to weak generalization performance, which will directly affect the performance of the algorithm, so the study of medical data enhancement is a very important measure to alleviate the phenomenon of small-sized datasets. In this paper, we design and improve the WGAN network model by introducing the perceptual and structural loss information of medical image data on the basic structure of WGAN network. Through comparative experiments, our designed model has better performance in all evaluation indexes, which verifies the effectiveness and feasibility of the improvement. On the basis of data enhancement, we also designed a multi-channel fusion CNN model combining VGG-16 and GoogLeNet features for early gastric cancer detection. From the experimental results, it can be seen that the fusion CNN we designed can better enhance the effectiveness of early gastric cancer detection, making its accuracy reach 97.08%.",Deep learning;Stomach;Sociology;Cancer detection;Feature extraction;Data models;Indexes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499417,IEEE Conferences,,,,,,
Efficient Brain Tumor Segmentation in MRI Images: A ResNet-based Approach with SegNet Classification,S. Biradar; Virupakshappa,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"Magnetic Resonance Imaging (MRI) stands as one of the extensively tested medical imaging methods utilized for disease detection and diagnosis. It plays a pivotal role in identifying various abnormalities within the human body, including tumors, lesions, and other internal issues. Here, the manual distribution technique consumes more time and the method is complex. Because of this, programmed division strategies are utilized. A precise, productive, and high-level computational division technique is expected for deadly infections like brain tumors from brain X-ray pictures. Therefore, this research work focuses on the effective segmentation of brain tumors from brain tumor MRI datasets via image pre-processing, feature segmentation as well and feature extraction. The data pre-processing undergoes two vital steps namely gray scale conversion and image blurring. The preprocessed images are extracted using ResNet which minimizes diverse decomposition as well as gradient problems. A SegNet model is employed in segmenting the images which paves the way for effective classification. To evaluate the segmentation performances, the ResNet is compared with various other approaches for diverse metrics. The evaluation results demonstrated that the proposed approach attained enhanced segmentation results when compared with other techniques.",Measurement;Image segmentation;Magnetic resonance imaging;Feature extraction;Brain modeling;Lesions;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425981,IEEE Conferences,,,,,,
Tuberculosis Classification Using EfficientNet B3 Deep Learning Architecture,G. Kaur; N. Sharma; R. Chauhan; S. Thapliyal; R. Gupta,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"Tuberculosis is a communicable illness that is transmitted through the inhalation of airborne pathogens between individuals. The main purpose of this study is to categorize Tuberculosis disease using X-ray images. Utilizing the EfficientNetB3 transfer learning model enables the attainment of this objective. The potential of EfficientNetB3 in automatically diagnosing tuberculosis (TB) inside medical images has been highlighted by a recent study. This study introduces a novel methodology that employs EfficientNetB3 to classify photos of tuberculosis (TB) disease into two distinct categories. The methods adopted in this study utilize a dataset including 4,200 photographs. The input photos are used for training the model throughout 40 epochs, utilizing a batch size of 30 and a learning rate of 0.001. The findings derived from our study reveal substantial findings, wherein the proposed EfficientNetB3 model showcases an impressive accuracy rate of 99%. The findings presented in this study show the potential efficacy of EfficientNetB3 as a reliable instrument for the automatic categorization of tuberculosis in X-ray pictures. The implications of the research findings are significant, as achieving high levels of accuracy can significantly enhance the rapid and accurate identification of tuberculosis (TB), leading to enhanced patient outcomes and more successful strategies for disease management. Moreover, the successful implementation of this model underscores the importance of employing advanced deep learning techniques in examining medical images, hence opening up new possibilities for developing intelligent diagnostic tools in the healthcare industry.",Training;Industries;Deep learning;Tuberculosis;Computational modeling;Transfer learning;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426140,IEEE Conferences,,,,,,
MedDQN: A Deep Reinforcement learning approach for Biomedical Image classification,P. K. R; J. S. L; K. G. Sindhu; T. V. S. S. Chaitanya; B. Ganesh; H. K. N,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"The categorization of medical pictures is crucial for biomedical research and treatment, yet real-world data are sometimes difficult to use because of uneven class distributions. Conventional classification algorithms frequently fall short in such scenarios, rendering their effectiveness in doubt, particularly when faced with highly imbalanced data distributions. This study offers a novel approach to this pressing problem: a thorough unbalanced classification model based on deep reinforcement learning. Through the use of a deep Q-learning network, the method reframes the classification problem as a sequential decision-making process. Within this dynamic setting, an agent takes on the responsibility of classifying individual samples at each temporal step, while the environment assesses these actions and furnishes the agent with feedback in the form of rewards. Notably, the reward system is strategically calibrated to accord greater importance to minority class samples, thereby enhancing the agent's sensitivity toward these often-underrepresented instances. Our model systematically adjusts an ideal classification policy under the constraints of unbalanced data as a consequence. Achieving this goal is facilitated by the presence of a well-defined reward system and a nurturing learning environment. Its remarkable ability to not only identify but also accurately classify a larger number of minority class samples underscores its exceptional performance.",Deep learning;Training;Q-learning;Sensitivity;Biological system modeling;Classification algorithms;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426453,IEEE Conferences,,,,,,
White Blood Cell Classification using Deep Learning Convolutional Neural Network Model,G. Kaur; N. Sharma; R. Chauhan; G. Verma; R. Gupta,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"White blood cells protects human body from the various diseases. This study aimed to address the critical issue of distinguishing between different subtypes of Leukocytes (WBS) using deep learning techniques. Recent progress in biomedical diagnostics has been made possible by convolutional neural network (CNN) models, which have showed promise in automating the classification of white blood cell images. This proposed work introduces a unique CNN model architecture that can categorize white blood cell pictures into Polynuclear and Mononuclear classification. 12,500 images of white blood cells made up a large dataset that was used. With a batch size of 128 and a default learning rate, the suggested CNN model gone through intense training that lasted 30 iterations. The outcomes of this experiment show how successful the proposed technique is. When correctly categorizing the various white blood cell classification, the CNN model performed exceptionally well, obtaining a fantastic accuracy rate of 95%. For the discipline of medical image analysis, these discoveries have significant implications. This research adds to the creation of automated and precise systems for classifying white blood cell subtypes by utilizing the power of deep learning, particularly the CNN architecture. This CNN model's successful deployment sets the way for more complex AI-driven solutions in medical imaging, transforming healthcare procedures and enhancing overall medical decision-making. This proposed strategy can help with better categorization planning by allowing for the early identification of white blood cell subgroups.",White blood cells;Training;Deep learning;Image analysis;Biological system modeling;Convolutional neural networks;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426496,IEEE Conferences,,,,,,
Pneumonia Disease Classification Utilizing Artificial Intelligence on VGG19 Model using Chest Xray Images,K. S. Gill; V. Anand; R. Gupta,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"Pneumonia is a sickness that causes swelling in the sacs in your lungs that help you to breathe. It can affect one or both lungs. The small bags of air in the body might become filled with liquid or pus, which can make a person cough up mucus or pus, have a fever, feel cold, and have trouble breathing. Many different tiny things like bacteria, viruses, and fungi can give you pneumonia. Pneumonia can be mild or very dangerous. This is very risky for babies, young kids, elderly people, and those with health issues or weak immune systems. In this study on pneumonia illness, classification is carried out utilising VGG19 Model using chest X-ray pictures, with the aim of improving accuracy which proves out to be 93% accurate. This appears to be an important and promising use of artificial intelligence in the medical industry. The study also suggests ways to enhance the practises now used by the medical community. It's crucial to keep in mind that creating a classification model for pneumonia requires a combination of deep learning, medical imaging, and domain knowledge skills. To assure the model's clinical validity and safety, collaboration with medical specialists is necessary.",Pulmonary diseases;Lung;Data models;Safety;Medical diagnostic imaging;X-ray imaging;Tuning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426395,IEEE Conferences,,,,,,
Malaria Parasite Disease Classification using Deep Learning Neural Networks,K. K. Babu; M. S. R. Reddy; S. R. Bhavanam; A. Chimma; P. Nuthalapati; S. Kovvuri,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"Malaria is a global health threat caused by the Plasmodium parasite. The early identification and classification of the parasite are vital for prompt treatment and improved patient outcomes. However, traditional detection methods depend heavily on expert knowledge for accurate diagnosis, making them challenging to implement. This paper aims to conduct a comprehensive analysis of 6 existing deep learning models including AlexNet, GoogleNet, VGG19, MobileNet, ResNet, and RCCNet utilizing the malaria parasite dataset. Additionally, we perform a comparative evaluation of these models by implementing various Data Augmentation techniques and improving accuracy by 3% reaching maximum accuracy up to 96.73 %. The assessment is centered around key factors such as training duration, precision, weighted f1 score, specificity, and sensitivity.",Deep learning;Training;Sensitivity;Malaria;Neural networks;Data models;Information technology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426537,IEEE Conferences,,,,,,
A Comparative Study of Machine Learning Algorithms for Benign-Malignant Breast Tumor Classification,P. Sankhe; J. Jhaveri; B. Shah,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"Breast cancer is still a major worldwide health concern, and mortality rates are greatly impacted by late-stage diagnoses. Globally, late-stage breast cancer claims the lives of about 685,000 women each year.This study explores new developments in breast cancer detection, highlighting the importance of an early diagnosis. This article examines cutting-edge medical imaging methods and applications of artificial intelligence that could lessen this burden by increasing precision and enabling earlier diagnoses, ultimately improving patient outcomes and lowering breast cancer-related mortality. Additionally, we used three different classifiersâ€”Logistic Regression, Decision Tree, and Random Forestâ€”to thoroughly assess our expanded dataset. Because of their adaptability and appropriateness for handling complicated information, these classifiers were chosen. Our findings indicate promising results, with specific classifiers demonstrating increased sensitivity and specificity in early breast cancer detection.",Training;Data analysis;Forestry;Breast cancer;Regression tree analysis;Random forests;Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426552,IEEE Conferences,,,,,,
Detection and Segmentation of Skull Fractures via CNN and U-Net Hybrid Model using Computed Tomography Images,T. Kodavati; R. P. Kumar,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"Computer tomography (CT) are now widely used for the diagnosis of head various head injuries. A CT scan slice has a lot of data that canâ€™t always be thoroughly analyzed quickly and precisely enough with standard visual inspection. To help doctors, a computer-assisted skull fracture detection and classification expert system is required. The most widely used deep learning models for image classification are convolutional neural networks (CNNs), as their accuracy and output consistently beat those of other models. This research aims to develop a model with a hybrud model of CNN with a custom classifier, DenseNet-161 and U-Net, that shows an increased efficiency and ease, through detection and segmentation of the CT scan images in the CQ500 dataset. The model achieved an accuracy of 98.37%, AUC score of 0.972, Recall score of 0.88 and Precision score of 0.87. These results represent a notable improvement over other models.",Deep learning;Image segmentation;Head;Computational modeling;Computed tomography;Medical services;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426088,IEEE Conferences,,,,,,
Underwater Image Classification using Efficient Fine-tuned DenseNet-121 with Image Albumentation and Adagrad Optimizer for Minimum Loss,G. George; Anusuya.s,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"Underwater image classification is a difficult task due to the unique properties of underwater surroundings. We provide an effective and fine-tuned DenseNet-121 model for underwater picture classification in this paper. We use picture augmentation techniques and the Adagrad optimizer to minimize loss during training to improve the modelâ€™s performance. Our goal is to obtain high accuracy, minimal loss, and an excellent F1-score in the classification of underwater image categories. It employs a dataset underwater photographs of various marine species and objects. Our model is built on the DenseNet-121 architecture, which is noted for its efficiency and powerful feature extraction capabilities.The model accuracy achieved is an Accounting 97.76%, with a loss of only 0.07. Furthermore, the model has a strong F1-score of 94%, indicating its ability to properly balance precision and recall in classification tasks. These findings demonstrate the modelâ€™s ability to reliably classify underwater photos across several categories. The efficiency of combining the DenseNet-121 architecture, image augmentation techniques, and the Adagrad optimizer for underwater picture classification is demonstrated in this paper. The modelâ€™s high accuracy, minimal loss, and solid F1-score demonstrate its promise for use in marine research, environmental monitoring, and undersea exploration.",Training;Solid modeling;Solids;Image augmentation;Environmental monitoring;Task analysis;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425927,IEEE Conferences,,,,,,
Utilizing Hybrid CNN Model and Machine Learning Techniques for the Identification of Pulmonary Fibrosis,A. Saraswat; Upasana; B. Naik K,2023 Global Conference on Information Technologies and Communications (GCITC),18-Apr-24,2023,"Pulmonary fibrosis is a progressive lung disorder characterized by scar tissue formation, leading to a deterioration of the patientâ€™s respiratory function. The identification of such diseases heavily relies on medical imaging techniques, such as high-resolution computed tomography (HRCT) and radiographic images (X-rays). Unfortunately, the high cost and potential risks associated with radiation exposure make HRCT inaccessible to the majority of people. This paper proposes a hybrid model for classifying fibrotic and healthy lungs from chest X-ray images. Convolutional Neural Networks (CNNs) have emerged as the most widely used deep learning models for medical image classification. However, different CNN architectures employ distinct approaches to learning and interpreting images. So, the features of three well-known CNNs: Xception, InceptionResnetV2, and DenseNet121, are fused to form a comprehensive feature representation. Principal Component Analysis (PCA) is applied to alleviate overfitting and reduce dimensionality. Subsequently, a random forest classifier is employed on the principal components for accurate classification. By integrating these methods, we aim to enhance the classification of pulmonary fibrosis and promote early detection for improved patient outcomes.",Lung;X-rays;Convolutional neural networks;X-ray imaging;Random forests;Biomedical imaging;Principal component analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425894,IEEE Conferences,,,,,,
Advancements in Early Lung Cancer Detection Using Convolutional Neural Networks,S. Middha; A. Kumar,2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI),15-Apr-24,2023,"It presents a groundbreaking approach for lung cancer detection utilizing Convolutional Neural Networks. Cancer remains a leading cause of global morbidity and mortality, underscoring the urgency for accurate and timely diagnostics. Leveraging the power of deep learning, our proposed CNN-based model demonstrates exceptional efficacy in analyzing medical imaging data, particularly chest radiographs, to identify subtle patterns indicative of lung malignancies. The model's architecture integrates advanced features for feature extraction and classification, enhancing sensitivity and specificity in comparison to conventional methods. Through extensive validation on diverse datasets, our CNN exhibits robust performance, showcasing its potential as a reliable tool for early and precise lung cancer detection. The implementation of this technology has significant implications for improving patient outcomes by facilitating earlier interventions and personalized treatment strategies in the fight against lung cancer.",Technological innovation;Sensitivity;Precision medicine;Refining;Lung cancer;Sensitivity and specificity;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489806,IEEE Conferences,,,,,,
Classification of Thyroid Cancer Subtypes With Imagenet Pretrained CNNS,A. Gavade; M. V. Shitole; V. Pendse; V. K. Swarnkar; J. Somasekar; R. K. Manik,2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI),15-Apr-24,2023,"This study applies interpretivism, a deductive method, and a descriptive design to prepared Convolutional Neural Networks (for identifying thyroid cancer subgroups from histopathology pictures. Utilized is secondary data made up of several annotated photos. Strong subtype categorization performance is displayed by the fine-tuned CNN, which achieves high accuracy, preciseness, and recall. Key diagnostic aspects are clarified by interpretability metrics like Grad-CAM as well as Layer-wise Resonance Propagation. Potential discrepancies in forecasts are addressed by bias analysis as well as fairness measures. Comparing performance to traditional approaches reveals superiority. A prospective clinical verification study, research into cutting-edge deep learning structures, and dataset expansion are suggested. Future research will focus on molecular markers along with multi-modal interfacing. This study improves the diagnosis of thyroid cancer by providing an accurate method for subtype differentiation",Measurement;Industries;Deep learning;Technological innovation;Thyroid cancer;Histopathology;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488958,IEEE Conferences,,,,,,
Advancements in Breast Cancer Detection: A Comprehensive Review of Deep Learning Techniques,D. Arora; S. Gupta; J. S. Chawla; R. Garg,2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI),15-Apr-24,2023,"This paper addresses the pressing issue of breast cancer-related mortality by comprehensively analyzing six distinct deep learning models utilized in the past decade. With a focus on timely and precise diagnosis, the study evaluates the performance metrics of each model across diverse datasets, image modalities, and methodologies. The primary goal is to identify the most efficient and precise deep learning architecture tailored to classify breast cancer tumors, with potential implications for optimizing early detection strategies and advancing clinical decision-making processes. The findings hold significant implications for refining clinical decision-making processes, potentially leading to more efficacy in timely detection strategies. This research emphasizes on the broader field of image analysis and its role in combating breast cancer.",Deep learning;Measurement;Technological innovation;Ultrasonic imaging;Decision making;Collaboration;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489805,IEEE Conferences,,,,,,
Empowering Intracranial Tumor Diagnosis: Deep Learning with DenseNet-256 and CNN-Based Enhanced Classification Algorithms,P. S. Chaitanya; S. Mastanbi; G. Manasa; M. Vohida; M. Dharani,2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI),15-Apr-24,2023,"Brain tumours are a serious public health issue, and improving patient outcomes depends heavily on early and correct detection. Deep learning methods have become effective tools for automatically identifying brain cancers in MRI and CT scan pictures. In medical imaging, accurately and quickly diagnosing brain malignancies is still a major difficulty. In this work, we suggest a novel method based on deep learning techniques to enhance the diagnostic performance for the categorization of brain tumors. We use convolutional neural network (CNN)-based techniques to augment the DenseNet-256 architecture and enable robust and accurate categorization of different types of tumors in brain MRI data. Our model performs better in by using sophisticated feature extraction and classification techniques distinguishing between various tumor types, such as pituitary tumors, meningiomas, and glioblastomas, among others. Our suggested method achieves state-of-the-art accuracy and sensitivity in tumor classification through rigorous testing and validation on a variety of datasets, demonstrating its potential to provide accurate and quick diagnosis. Our results highlight the need of combining cutting-edge CNN-based algorithms with deep learning approaches to achieve more accurate and dependable intracranial tumor identification, providing encouraging opportunities for better patient care and treatment planning.",Deep learning;Technological innovation;Sensitivity;Magnetic resonance imaging;Transfer learning;Brain modeling;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489558,IEEE Conferences,,,,,,
Feature Extraction and Analysis for Diabetic Retinopathy Classification using Pre-trained Deep Networks,S. Kumar; K. K. Dewangan; K. Yadav,2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI),15-Apr-24,2023,"In the quest to improve the accuracy of diagnosing diabetic retinopathy, we explore a novel approachâ€”leveraging pre-trained deep networks for feature extraction. This method involves harnessing the knowledge acquired by neural networks through prior training on diverse datasets. By utilizing these pre-trained networks, we aim to extract intricate features from retinal images, crucial for discerning the stages of diabetic retinopathy. This approach not only capitalizes on the depth of information learned by the networks but also enhances the efficiency of feature extraction. Our study delves into the application of this technique, evaluating its performance in diabetic retinopathy classification. Through sophisticated technology and medical insight, we seek to unveil a more nuanced and accurate means of identifying and classifying diabetic retinopathy, paving the way for improved patient care and timely interventions. The MobileNet V2 has obtained the highest classification accuracy of 92.42% over the other considered pre-trained models through the significant feature extraction.",Training;Diabetic retinopathy;Technological innovation;Neural networks;Medical services;Feature extraction;Retina,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489452,IEEE Conferences,,,,,,
Identifying Leukemia Subtypes with Deep Learning and Blood Cell Image,V. Sharma; D. Mukhopadhyay; K. Gupta; Y. Rathore; P. Jagadeesan,2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI),15-Apr-24,2023,"Through the evaluation of blood cell pictures, this study uses deep comprehension and interpretive techniques to advance the categorization of leukemia subgroups. A strong convolutional neural network (CNN) structure was created using a deductive method and the idea of interpretivism. Adequate representation was guaranteed via a descriptive approach and secondary gathering of information from various sources. The model demonstrated impressive interpretability as well as accuracy (94.5%) while utilizing layer-wise significance propagation. Evaluation in comparison to conventional diagnostic techniques revealed improved performance. Transparency, bias analysis, and moral considerations were given top priority. Longitudinal research and multimodal interaction should both be investigated in future study. This study has implications for larger classification of hematologic malignancies and advances the field of lymphoma diagnosis.",Industries;Deep learning;Technological innovation;Ethics;Image analysis;Medical services;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489661,IEEE Conferences,,,,,,
Detection and Classification of Breast Cancer Using Different Machine Learning Classifier,K. K. Dewangan; S. P. Sahu; R. R. Janghel,2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI),15-Apr-24,2023,"Navigating the complex landscape of breast cancer diagnostics, this study addresses the critical challenge of early detection and classification. Breast cancer remains a significant health concern, underscoring the urgency for advanced method-ologies. Employing various machine learning classifiers kMPN (k-Most Proximate Neighbor), Naive Bayes, and the formidable Support Vector Machine (SVM)-we rigorously assessed their capabilities in discriminating between benign and malignant tumors. In a groundbreaking revelation, SVM emerged as the indisputable leader, achieving an exceptional accuracy of 93.72%. This milestone not only represents a pivotal advancement in breast cancer diagnostics but also underscores the indispensable role of machine learning in healthcare. As we grapple with the intricacies of breast cancer, SVM's outstanding accuracy positions it as the optimal choice for refining and advancing detection methodologies, offering a beacon of hope for improved patient outcomes and transformative progress in addressing this pervasive health issue.",Support vector machines;Measurement;Technological innovation;Navigation;Refining;Machine learning;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489154,IEEE Conferences,,,,,,
Resnet Transfer Learning For Enhanced Medical Image Classification In Healthcare,N. Varshney; M. Sharma; V. Saravanan; N. SHALINI; V. K. Yadav; N. Kumar,2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI),15-Apr-24,2023,"This work overcomes the limitations of sparsely labeled data by optimizing ResNet transfer learning methods in medical classification of images. Using a deductive approach along with interpretive philosophy, we optimize ResNet for better diagnostic performance on healthcare data sets. Our team of technical approach includes preprocessing datasets, configuring model architectures, and fine-tuning hyperparameters using secondary data. The improved model performance as demonstrated by the results is confirmed by metrics such as precision, reliability, and recall. Analyses of comparisons demonstrate superiority over basic models. Upcoming tasks include working together to create standardized benchmarks, improving interpretability along with scalability, and verifying in actual clinical settings.",Measurement;Technological innovation;Philosophical considerations;Scalability;Transfer learning;Medical services;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489521,IEEE Conferences,,,,,,
Deep Learning-based Algorithms for Nucleus Segmentation: An Overview,S. Bao; H. Ding; L. Zhang; J. Li,2023 International Conference on Artificial Intelligence and Automation Control (AIAC),15-Apr-24,2023,"Nucleus segmentation is a crucial task in biological and medical imaging, facilitating in-depth investigations into cellular nuclear morphology, cell tracking, and cellular phenotype classification. With the advent of deep learning, we've seen significant advancements in this domain, leading to higher accuracy and the ability to process images at an unprecedented scale. In this overview, we specifically discuss the application of various deep learning methods in nucleus segmentation and present various evaluation metrics. Nevertheless, we also discuss the current challenges being faced and try to outline the future directions of development.",Measurement;Deep learning;Image segmentation;Pathology;Recurrent neural networks;Microprocessors;Morphology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10491626,IEEE Conferences,,,,,,
Deep Learning-Based Brain Tumor Prediction: An Analysis of Performance Evaluation of Convolutional Neural Network,Jason; F. Venesius; Y. Sie; R. Fredyan; H. Pranoto,2023 15th International Congress on Advanced Applied Informatics Winter (IIAI-AAI-Winter),09-Apr-24,2023,"Over the years, Magnetic Resonance Imaging (MRI) has become increasingly prominent in the field of medical science for diagnosing brain tumors. This study presents an analysis of the performance evaluation of a convolutional neural network (CNN) for deep learning-based brain tumor prediction. The study aims to investigate the accuracy and effectiveness of using a CNN for the early detection of brain tumors in medical imaging. The proposed CNN architecture was trained and evaluated on a large dataset of brain MRI images, and the performance metrics were compared with traditional machine learning techniques. The results demonstrate that the CNN model outperforms the traditional machine learning models in terms of accuracy and specificity, showing promise as an effective tool for brain tumor detection. The findings of this study have important implications for the development of accurate and efficient tools for brain tumor prediction, which could potentially lead to earlier diagnosis and improved treatment outcomes for patients.",Performance evaluation;Magnetic resonance imaging;Computational modeling;Machine learning;Predictive models;Brain modeling;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488270,IEEE Conferences,,,,,,
Segmentation of Skin Cancer and Intensity Classification Using Deep Convolutional Neural Network,H. Aljuaid; A. Alhammad; N. Alsakran; R. Alnoghaimshi; F. Aldhafer; Y. Alahmari,"2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)",09-Apr-24,2023,"Skin cancer is an excessively common type of cancer. It occurs when mutations appear in the DNA of skin cells. The four main forms of skin cancer are Basal cell carcinoma (BCC), Squamous cell carcinoma (SCC), Merkel cell carcinoma (MCC), and the deadliest form Melanoma. Skin cancer is usually diagnosed late because of the unnoticeable symptoms. Therefore, Reliable automatic detection of skin tumors is needed to help increase the accuracy and efficiency of pathologists. In this paper the DCNN method is used which is designed to perform complex analysis of 2594 images and 2594 of corresponding ground truth (response masks) for training and 1000 images for testing of data using image segmentation and classification for creating model that detect skin cancer in early stages. The models testing produced positive outcome with accuracy 0.95 for classification and 0.895 for segmentation. The results are promising for future enhancement.",Training;Image segmentation;Computational modeling;Predictive models;Skin;Convolutional neural networks;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487581,IEEE Conferences,,,,,,
Visual Memory Transfer for Imbalanced Medical Image Classification,K. Yao; Y. Liu,"2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)",09-Apr-24,2023,"In deep learning of medical image data, skin lesion classification remains a challenging problem due to imbalanced distribution of training data. We propose a visual memory transfer (VMT) method by means of transferring visual knowledge from majority classes to minority classes. As a result, our method enriches feature of minority classes with pre-calculated memory features. In addition, VMT defines a refined feature map to perform fine-grained classification. Our classification results outperform SOTA methods on the largest public available dermoscopic image dataset on averaged F-score and Top-1 classification accuracy.",Deep learning;Visualization;Training data;Skin;Lesions;Biomedical imaging;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487491,IEEE Conferences,,,,,,
Multi-Label Concept Classification in Imaging Entities of Biomedical Literature Using CNN and Vision Transformers,M. M. Rahman; B. Regmi,"2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)",09-Apr-24,2023,"Biomedical images are frequently used in articles to illustrate medical concepts and highlight regions-of-interests (ROIs) by using annotation markers (pointers) such as different arrows, letters or symbols overlaid on figures. Also, in many cases multiple markers in the same image are often pointing to different concepts relevant to the article. Hence, each image might be assigned with one or more concepts for multi-label classification and object detection based machine-learning tasks. This work reports such a proof-of-concept (POC) experiment by annotating ROIs and classifying (multi-label classification) 200 Chest CT images appeared in biomedical articles with eleven (11) different concept (similar to UMLS) categories such as, ground-glass, bronchi, honeycomb, cyst, nodules, etc. For annotation, we use an online tool (Labelimg) to annotate image ROIs with concepts based on the information content in associated captions. To demonstrate the feasibility of the POC, this study conducts experiments with different Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) using both transfer learning (fine-tuning) and training from scratch. We achieved encouraging results (around 70% micro average precision and recall accuracies) in a test set, whereas the dataset images are in very low resolution, non-uniform lighting conditions and with varied shapes and sizes. Overall, this study demonstrates the effectiveness of deep learning models in multi-label classification in medical images and establishes the feasibility and rationale of the POC. The ultimate goal of this work is to develop a large-scale concept detection framework towards building a visual ontology of images in biomedical articles.",Training;Visualization;Annotations;Unified modeling language;Ontologies;Transformers;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487591,IEEE Conferences,,,,,,
Steganalysis of Medical Radiographs: a Deep Learning Approach Comparing the Importance of Using Content Pixels and Content-Free Pixels,F. G. Mohammadi; R. Sebro,"2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)",09-Apr-24,2023,"Content-free pixels, which do not contain actual information, have been used in research studies for passive watermarking of radiographs. However, no studies have compared the effectiveness of content-free pixels to content pixels for watermarking. In this study, we propose a radiograph steganalysis solution that can be used to identify the source of a radiograph and be potentially used to identify whether a radiograph is fake. The solution uses a deep-learning architecture for automating computer-generated fake radiograph detection and compares the performance of passive watermarking using the content-free pixels to that using the content pixels. We use patients who had radiographs of the abdomen, pelvis, and lumbar spine at Mayo Clinic (01/01/2010 - 12/31/2021). The patients (n = 4722, radiographs = 10937) were randomly split into training/validation (80%, n = 3778, radiographs = 8998) and test (20%, n = 944, radiographs = 1939) datasets by patient. We evaluate and obtain the highest source identification for patient level model classification of pelvis (accuracy (ACC) = 98.6%, area under curve (AUC) = 95.34%, precision = 99.11%, recall = 98.6%) in content pixel analysis and for patient level model classification of lumbar spine (ACC = 97.2%, AUC = 97.34%, precision = 97.18%, recall = 97.2%) in content-free pixel analysis. This research confirms that the steganalysis can be performed on content-free and content pixels from radiographs. These results will be valuable for medical forensic and legal communities.",Deep learning;Analytical models;Forensics;Spine;Watermarking;Forgery;Pelvis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487506,IEEE Conferences,,,,,,
FXAI: Fusing XAI for Predicting COVID-19 Using Diverse Chest X - Ray Images,R. A. A. Saleh; S. M. Farea; Z. Al-Huda; M. Ertunc; D. Kvak; M. A. Al-antari,2023 18th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),08-Apr-24,2023,"Fusing explainable artificial intelligence (FXAI) is currently a prominent research topic in medical imaging interpretations. The proposed FXAI has the capability to provide the following benefits. Firstly, it can extract strong and reliable high-level deep features by combining various standard AI networks. Secondly, it can simultaneously generate visual explainable saliency maps associated with each chest X-ray (CXR) scan. Such heat maps not only demonstrate the most relevant regions of the AI decision-making process but also offer advantages to radiologists and patients. Thirdly, it enhances prediction performance to deliver an optimal intelligent solution for communities worldwide. These advantages can support the development of an optimal treatment plan, reduce medical costs, and enhance the capabilities of health care systems. We have trained and evaluated the proposed FXAI using a diverse benchmark medical CXR dataset that has been collected from various public resources. Our findings encourage researchers and stakeholders in the medical industry to validate this proposed framework in a practical manner.",COVID-19;Visualization;Pulmonary diseases;Medical services;Feature extraction;Reliability engineering;Stakeholders,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481318,IEEE Conferences,,,,,,
An Adaptive and Interpretable Framework for Biomedical Image Analysis,S. Singh; S. T. Acton; S. Moosa; N. D. Sheybani,"2023 57th Asilomar Conference on Signals, Systems, and Computers",01-Apr-24,2023,"Biomedical image analysis has benefited tremendously from the advent of artificial intelligence. Machine learning and deep learning-based algorithms are increasingly utilized in real time to assist clinicians with making crucial decisions for patients. Explainability and interpretability of these algorithms are critical for doctors and patients to develop trust in the automated decision making-process. Furthermore, designing specialized solutions based on clinical applications of these algorithms is non-trivial, due to the heterogeneity of imaging data available. Therefore, we propose an adaptive and interpretable framework for biomedical image analysis with novel applications to transcranial magnetic resonance-guided focused ultrasound thalamotomy for the treatment of essential tremor. The algorithm automatically configures itself to analyze brain lesions based on heterogeneous magnetic resonance images and subsequently predicts short-term clinical outcomes utilizing random forest and SHAP values, while ensuring interpretability for this process.",Ultrasonic imaging;Machine learning algorithms;Adaptive systems;Magnetic resonance imaging;Magnetic resonance;Medical services;Prediction algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476986,IEEE Conferences,,,,,,
Detection of Alzheimer's Disease using Deep Learning: An Optimized Approach,G. Ahmed; M. Joo Er; S. Zikria; M. Sadiq Fareed,2023 6th International Conference on Intelligent Autonomous Systems (ICoIAS),########,2023,"This paper suggests a new CNN that requires just a few parameters to diagnose AD and is perfect for training on smaller datasets. Compared to existing state-of-the-art models, the proposed model uses a more optimized technique to identify the early stages of Alzheimer's disease. The image is sharpened and normalized as part of the preparation phase to help the model learn more quickly and succinctly. We used a bespoke CNN model that makes precise predictions with minimum parameters. Five Convolutional layers and three fully connected layers, which have equally learned samples from each class, make up the proposed model. The approach was compared using a variety of accuracy metrics, such as accuracy, AUC, precision, recall, and Fl-Score, as well as categorical cross-entropy loss, self, and other comparisons. To accurately forecast the method, several trials were carried out. The values for the evaluation measures are as follows: accuracy, AUC, precision, recall, fl-score, and loss are 99.95%, 99.99%, 99.97%, 99.95%, and 0.0016 respectively. According to simulation results, the proposed model performs better than existing cutting-edge models in all evaluation metrics.",Training;Deep learning;Autonomous systems;Simulation;Predictive models;Loss measurement;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476557,IEEE Conferences,,,,,,
Deep Learning-Based Model with XAI for Brain Tumor Classification and Segmentation Using MRI Images,S. Hasan; M. M. Nabila; R. B. Anis; R. Rab,2023 IEEE 9th International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE),########,2023,"Brain tumor classification & segmentation are crucial challenges in medical image analysis. Brain tumors are often incurable malignancies that arise from glial support cells. Medical study reveals that manual classification with human-assisted support might result in inaccurate prognosis and diagnosis because of the diversity and similarity of malignancies & normal tissues. Brain tumors are classified & segmented using T1, T1c, T2, and FLAIR MRI modalities. In this paper, we provide a technique for extracting brain tumours from 2D MRI images using several deep-learning tumor classification models (ResNet50, InceptionV3, VGG19, VGG16, DenseNet121, and EfficientNet). To give interpretability and explainability for the classification findings, the Lime (Local interpretable Model-Agnostic explanations) approach is also applied. During the segmentation step, we explore three widely used architectures: U-Net, SegNet, & ResUNet.The application of sophisticated classification models, XAI, and a fresh dataset establishes the framework for better tumor diagnosis and segmentation from 2D MRI images. In our experimental study, the ResNet50 model beat the other deep learning models, obtaining 96% accuracy. Furthermore, the suggested segmentation models ResUnet, Segnet, and U-Net have dice scores of 0.851, 0.772, and 0.7424, respectively.",Image segmentation;Explainable AI;Magnetic resonance imaging;Computational modeling;Manuals;Brain modeling;Prognostics and health management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456517,IEEE Conferences,,,,,,
Alzheimer's Disease Detection Model Based on Multimodal Data Early Fusion of Medical Neuroimaging,S. Wu; S. Qin; K. Wang; S. Yang; S. Zhang,"2023 IEEE International Conference on High Performance Computing & Communications, Data Science & Systems, Smart City & Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)",########,2023,"Alzheimer's disease (AD) is a long-term neurodegenerative disease. The research in the field of AD is rapidly devel-oping, but most research methods are based on unimodal without considering the complementarity between different modes. In this work, a multimodal-based deep learning model is proposed for AD detection. Specifically, two medical neural image modes (i.e. magnetic resonance imaging and Positron emission computed to-mography) are used to fuse into a feature map. The convolutional block attention modules and convolutional neural networks are utilized to extract the features. A multilayer perceptron is used as a model classifier for the diagnostic classification of the learned AD features. To validate the model, experiments are conducted using the Alzheimer's Disease Neuroimaging Initiative dataset. Compared with existing AD diagnostic models, this work is better, and the accuracy of this model reaches 99.17%. It provides an optional solution for AD detection.",Deep learning;Neuroimaging;Support vector machines;Computational modeling;Magnetic resonance imaging;Feature extraction;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10466959,IEEE Conferences,,,,,,
A Deep Learning Method for Classification of Interictal and Preictal EEG Signals in Epileptic Seizure Prediction,L. Chen,2023 9th Annual International Conference on Network and Information Systems for Computers (ICNISC),########,2023,"Epilepsy is one of the most common nervous system diseases in the world, which will cause serious consequences to the patient's nerves, cognition and psychology, and affect the patient's physical and mental health and quality of life. Early detection of seizures can help prevent seizures and reduce harm through medical intervention. In this study, a deep learning (DL) framework was proposed that employs two branches to extract time-domain and frequency-domain features of EEG signals, which are then fused together to improve feature extraction and classification abilities. The proposed model demonstrates satisfactory performance in epileptic seizure prediction (ESP). To ascertain the effectiveness of the model, four experiments were conducted. Initially, five different segmentation lengths were tested in order to determine the optimal one. Subsequently, ablation experiments were executed to assess the effectiveness of the modelâ€™s components. Furthermore, the model was tested on specific patient data, resulting in an average accuracy of 0.9636 and an Fl-score of 0.9638 on the CHB-MIT dataset, outperforming other advanced models. Finally, when compared to other advanced models, the proposed framework yielded the highest accuracy in both the CHB-MIT dataset and the newborns with epilepsy annotation EEG recording dataset (DNERSA), with accuracies of 0.9943 and 0.9745, respectively. The proposed model exhibits promising potential in the field of ESP.",Training;Deep learning;Frequency-domain analysis;Time series analysis;Epilepsy;Brain modeling;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473430,IEEE Conferences,,,,,,
Multiinstitutional Lung Image Classification Using Privacy-Preserving Horizontal Federated Learning with Homomorphic Encryption,D. Cao; C. Wang; H. Sun; C. Cao; M. Kang; H. Zheng; S. Zhou; X. Guan; Y. Cao; Q. Tong,"2023 IEEE International Conference on E-health Networking, Application & Services (Healthcom)",########,2023,"Nowadays, deep learning is widely used for medical image analysis and computer-aided diagnosis. However, the process of training them requires a large amount of data and accurate annotations. Medical data is scarce, sensitive, and heterogeneous across multiple institutions, which poses challenges for building accurate models. To address this, we develop a hor-izontal Federated Learning system with Homomorphic Encryption (FLHE) and a deep convolutional neural network ResNet-50 for multiinstitutional lung image classification. The proposed system enables multiple medical institutions to collaboratively train a model without sharing sensitive data and uses encryption to ensure privacy. Experimental results demonstrate that our system achieves high accuracy while preserving privacy, and has the potential to improve diagnosis and treatment efficiency. Our work highlights the potential of federated learning in addressing the challenges of medical image classification, facilitating collaborative research, and promoting medical data sharing in healthcare and medical domains.",Training;Privacy;Federated learning;Lung;Medical services;Data models;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10472358,IEEE Conferences,,,,,,
"Pre-Trained Tabular Transformer for Real-Time, Efficient, Stable Radiomics Data Processing: A Comprehensive Study",Z. Jiang; R. Jia; L. Zhang; K. Li,"2023 IEEE International Conference on E-health Networking, Application & Services (Healthcom)",########,2023,"Radiomics is an important research direction in the field of medical image analysis. Although the number of publications is increasing year by year, it has been difficult to translate into clinical practice due to the small size of clinical data. In most cases, Radiomics data can be considered as small tabular data. Deep learning is often less effective than classical machine learning algorithms in processing tabular data. Recently, table representation learning has started to receive more widespread attention which is often an easily overlooked but very important area of research, helping improve the status of tabular deep learning. Here, we first apply a pre-trained Transformer model named Tabular Prior-Data Fitted Network (TabPFN) to the field of Radiomics analysis. We implement extensive experiments on three real-world clinical datasets: (a) Ultrasound Radiomics dataset for the classificatory diagnosis of Kidney tumor, (b) CT Radiomics dataset for the prediction of EGFR gene mutations in non-small cell lung cancer, (c) MRI Radiomics dataset for the prediction of treatment response of brain metastases to gamma knife radiosurgery. By comprehensive analysis, we demonstrate that the pre-trained tabular Transformer can be used as a realtime, efficient, and stable Radiomics data processor with superior performance over other tabular machine learning methods in different clinical tasks. We also simulate an ideal clinical practice scenario for evaluating the clinical translation potential of pretrained models. Finally, we explore the advantages and limitations of pre-trained tabular models for Radiomics analysis.",Training;Representation learning;Adaptation models;Analytical models;Transformers;Data models;Real-time systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10472397,IEEE Conferences,,,,,,
A Novel Deep Ensemble Learning Approach to Predict Eye Diseases from OCT Images,N. Uddin; M. S. Mia; M. N. Uddin; P. Mahmud,2023 5th International Conference on Sustainable Technologies for Industry 5.0 (STI),########,2023,"As the world advances in technology, the classification of eye diseases is thought to have clinical applications since it evaluates and analyzes the results. Optometrists research and diagnose age-related eye problems such as Choroidal Neovascularization (CNV), Diabetic Macular Edema (DME), and Drusen using optical coherence tomography (OCT) of the human eye. Because their symptoms are similar, identifying these diseases is challenging and requires hours of expert analysis. The requirements for eye illness classification should be straightforward in nature, structure, and evaluation. Deep Learning Convolutional Neural Network Classification can create a hierarchical representation of images to differentiate between normal and diseased eye patterns for diagnostic purposes. Therefore, this paper aims to implement and define deep learning to classify and identify eye diseases. Using a four-class classification problem, this work investigates the automatic detection of Choroidal Neovascularization (CNV), Diabetic Macular Edema (DME), Drusen, and Normal in Optical Coherence Tomography (OCT) pictures. Multi-transfer learning architecture was used in the suggested deep learning Ensemble classification model. The most incredible accuracy of our proposed Ensemble model for multi- class classification was 83%.",Deep learning;Optical coherence tomography;Transfer learning;Predictive models;Eye diseases;Diabetes;Noise measurement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465048,IEEE Conferences,,,,,,
SegJSW: A Hybrid Model for Knee Osteoarthritis Classification,H. P. Trung; T. N. Trung; T. T. Quang; T. Q. Thanh; S. N. Thiet; D. T. Tuan; T. T. Minh,2023 RIVF International Conference on Computing and Communication Technologies (RIVF),########,2023,"Knee osteoarthritis is currently one of the most serious and prevalent health issues in society. Over the years, numerous research have investigated applying Artificial Intelligence for early disease detection. However, most current approaches have merely relied on image processing techniques and thus not incorporating domain medical knowledge, which somehow limits the ultimate results. Therefore, in this paper, we propose a new method called SegJSW. Our model carries out the prediction process based on two branches of information: (1) overall diagnostic outcomes obtained from image classification models; and (2) Joint Space Width Evalutaion, developed from our proposed method. Experimented on public dataset of actual patients, SegJSW yields significant accuracy improvements compared to the baseline method (81.09% vs. 77.65%), while also enjoying recall enhancement as well (77.38% vs. 68.73%).",Image segmentation;Computational modeling;Predictive models;Communications technology;Medical diagnosis;Reliability;Osteoarthritis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10471829,IEEE Conferences,,,,,,
Abnormality Detection of Effluent Dialysate Images on Continuous Ambulatory Peritoneal Dialysis Using Deep Learning,D. A. Navastara; F. Indriati Eka Sari; C. Fatichah; M. Maroqi Abdul Jalil; M. Thaha; S. Dwi Suryantoro; W. Putri Sulistyaning; M. R. Haryati,2023 6th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI),########,2023,"Continuous Ambulatory Peritoneal Dialysis (CAPD) offers independent dialysis for chronic kidney disease (CKD) patients in Indonesia, enhancing quality of life. However, high mortality risks due to complications prompted by negligence and technical errors persist. In this research, an innovative contribution was suggested: designing a system to detect complications related to CAPD based on abnormality detection of effluent dialysate images using deep learning. The images are harnessed as input for the deep learning model, employing transfer learning techniques atop pre-trained models such as InceptionV3, ResNet50, EfficientNetB7, and MobileNetV2. After this, the model is refined by incorporating supplementary layers, and an array of experiments encompassing diverse dataset resampling scenarios and augmentation strategies are conducted. Empirical findings underscore that the optimal model manifests with ResNet50 under precise circumstances, yielding a recall rate of 75.00% and an f1-score of 78.95% on the test dataset.",Deep learning;Seminars;Transfer learning;Effluents;Manuals;Chronic kidney disease;Intelligent systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467296,IEEE Conferences,,,,,,
An enhanced hybrid model paradigm for transforming breast cancer prediction,A. V. S. Swetha; M. Bala; K. Sharma; R. Katarya,2023 17th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),########,2023,"Breast cancer is the most common cancer among women worldwide, underlining the significance of early identification for successful treatment. Deep learning (DL) has shown promising results in breast cancer prediction, but traditional DL models struggle with imbalanced datasets making the model biased. To overcome this problems hybrid models are built but, these hybrid models often consume huge resources making them computationally inefficient. This study introduces an innovative breast cancer classification framework using deep convolutional neural networks. The framework relies on weight factors and threshold values to create an effective hybrid model. Initially, two separate deep convolutional neural network models are employed, and their test accuracies are compared to a predefined threshold. If both models accuracy falls below the threshold, a hybrid model is constructed. This hybrid model merges features from both models through a multimodal fusion approach, expanding the feature set but potentially affecting computational efficiency. To address this efficiency challenge, an optimal feature selection algorithm is employed to choose the most relevant features from the expanded set. Empirical evidence validates the frameworkâ€™s excellence, even when dealing with imbalanced datasets, as it surpasses evaluation criteria. The suggested hybrid model achieves an impressive binary classification accuracy of 99.69% while maintaining a minimal processing time of just 3.52 seconds.",Deep learning;Computational modeling;Transfer learning;Predictive models;Sensitivity and specificity;Feature extraction;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10472844,IEEE Conferences,,,,,,
Enhancing Privacy-Preserving Brain Tumor Detection in Medical Cyber-Physical Systems through Deep Learning Algorithms,P. Singh; K. Lata; Y. Gupta; G. Sachdeva; S. Saini,2023 IEEE International Symposium on Smart Electronic Systems (iSES),########,2023,"The proliferation of deep learning in medical image processing has spurred the development of a diverse range of medical imaging applications, leading to exponential growth in treatment and diagnostic solutions for various medical images issues. In the age of the Internet of Things (IoT), establishing high levels of privacy and security for medical data in Medical Cyber-Physical Systems (MCPS) is critical to facilitating the progress of complicated medical imaging diagnostic applications. The authors' goal in this research is to create and analyze a secure hospital management system geared toward MCPS for brain tumor identification. The proposed system is designed in three distinct phases and then all of them are integrated to complete the design. In the first phase, a smart healthcare system is introduced to deliver effective health services, particularly targeted towards patients with brain tumors. To achieve this, an application is developed, compatible Microsoft-based operating systems. This application enables patients to access the system either in person or remotely. As a result, patient data remains secure, accessible only to the hospital and the patient. The application involves uploading the patient's MRI images, followed by entering a unique 10-digit number to access the predicted results. In the second phase, the authors propose a deep learning-based tumor detection method specifically designed for brain MRI scans. Lastly, AES-128 algorithm is integrated with the proposed platform for secured medical image storage on the server and the transmission through internet from client to server and back to client after prediction. The proposed system has achieved a level of performance that competes with state-of-the-art (SOTA) methods, as demonstrated through various deep learning classifiers.",Deep learning;Magnetic resonance imaging;Prediction algorithms;Classification algorithms;Encryption;Servers;Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467090,IEEE Conferences,,,,,,
Mobile Application Using CNN For Skin Disease Classification with User Privacy,M. Z. Hussain; M. Z. Hasan; S. Nosheen; M. M. Khan; M. Sufyan; A. Qureshi; A. A. Siddiqui; Z. Mubarak; A. Bilal; M. A. Yaqub; M. Mustafa; S. H. Chuhan,"2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)",########,2023,"Skin diseases present a complex challenge for mechanical analysis due to the inherent irregularities in skin texture, varying complexions, and the presence of hair and other surface features. The need for an accurate and automated system for skin disease detection is paramount. However, the task is compounded by issues such as dataset imbalance and stringent data privacy concerns associated with medical images. In this study, we harnessed the power of Convolutional Neural Networks (CNNs) in the domain of Medical Image Analysis (MIA) to classify skin diseases. Additionally, we employed a federated learning approach to safeguard data privacy. Our results demonstrate the remarkable performance of CNNs, achieving an accuracy score of 0.90 in skin disease classification. Building upon these findings, we propose the development of a mobile application tailored for skin disease classification, leveraging CNNs and the federated learning strategy. This mobile app offers an innovative solution for skin analysis while maintaining the highest data security and privacy standards. In conclusion, our research underscores the potential of CNNs and federated learning in the realm of skin disease classification. We introduce a promising path for the creation of an efficient mobile application for skin disease diagnosis, meeting the rigorous demands of modern medical data handling and analysis without compromising data security or privacy.",Data privacy;Image analysis;Federated learning;Data security;Skin;Mobile applications;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10466130,IEEE Conferences,,,,,,
Design of an efficient deep Learning Model for Segmentation and Classification of Psoriasis and Vitiligo Skin diseases,D. A. Reddy; S. Shambharkar; D. Chaudhari; K. Jyothsna; R. K. Somkunwar; A. Srinish Reddy,"2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)",########,2023,"Skin diseases such as psoriasis and vitiligo can greatly impact a patient's quality of life. For correct diagnosis and therapy planning, it is crucial to accurately segment and classify various skin disorders. Existing segmentation & classification models for identification of skin diseases either have low efficiency or have higher complexity when used for clinical scenarios. In order to address these problems, we provide a deep learning model in this study that can automatically separate and categorize vitiligo and psoriasis skin lesions from clinical photos. The suggested model is built on a Convolutional Neural Network (CNN) architecture that has been specifically designed for use in medical image processing applications. Initially, all the collected images are augmented via scale, rotate, width shift, height shift, shear and zoom operations. In proposed approach, train the model on this augmented dataset of clinical images for psoriasis and vitiligo lesions via Transfer Learning based ResNet50 CNN, and evaluate its performance using various metrics such as accuracy, precision, recall, and F1-scores. Proposed approach results show that the proposed model achieves high accuracy in both segmentation and classification tasks. The proposed deep learning model can assist dermatologists and medical professionals in accurately diagnosing and treating skin diseases, potentially leading to better patient outcomes.",Deep learning;Computational modeling;Transfer learning;Skin;Planning;Convolutional neural networks;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465712,IEEE Conferences,,,,,,
Visualizing Wellness: The Impact of Deep Learning on Healthcare Image Analysis,D. K. Sinha; P. Naval; S. Fiza,"2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)",########,2023,"Deep convolution networks have quickly established itself as the main method for deciphering medical pictures due to their widespread use. This extensive study examined almost 300 recent papers, mostly from the preceding year, that summarized the basic artificial intelligence ideas related to medical picture analysis. Applications of deep learning were explored in detail in a number of disciplines, including object recognition, picture grouping, classification, and registering. This book presents a succinct summary of studies conducted in the fields of neurological disorders, retinal cells, asthma, malignant scans, breasts, cardiovascular and cardiac pelvic, and nerve imaging. Study conducted concludes our assessment with a review of the state-of-the-art at this time, a thorough investigation of outstanding problems, and suggestions for further research. All areas, including computerized pathology, musculoskeletal, pulmonary, the heart, breasts, muscles and tendons, retinal, and neurology imaging, are summarized in a summary of current studies. This thorough review ends with an assessment of the field's current state, a critical analysis of ongoing issues, and specific suggestions for further research and development.",Deep learning;Visualization;Image analysis;Reviews;Medical services;Breast;Retina,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465825,IEEE Conferences,,,,,,
Connecting the Dots: Graph Neural Network Powered Ensemble and Classification of Medical Images,A. Singh; P. Van de Ven; C. Eising; P. Denny,2023 31st Irish Conference on Artificial Intelligence and Cognitive Science (AICS),########,2023,"Deep learning models have demonstrated remarkable results for various computer vision tasks, including the realm of medical imaging. However, their application in the medical domain is limited due to the requirement for large amounts of training data, which can be both challenging and expensive to obtain. To mitigate this, pretrained models have been fine-tuned on domain-specific data, but such an approach can suffer from inductive biases. Furthermore, deep learning models struggle to learn the relationship between spatially distant features and their importance, as convolution operations treat all pixels equally. Pioneering a novel solution to this challenge, we employ the Image Foresting Transform to optimally segment images into superpixelse These superpixels are subsequently transformed into graph-structured data, enabling the proficient extraction of features and modeling of relationships using Graph Neural Networks (GNNs). Our method harnesses an ensemble of three distinct GNN architectures to boost its robustness. In our evaluations targeting pneumonia classification, our methodology surpassed prevailing Deep Neural Networks (DNNs) in performance, all while drastically cutting down on the parameter count. This not only trims down the expenses tied to data but also accelerates training and minimizes bias. Consequently, our proposition offers a sturdy, economically viable, and scalable strategy for medical image classification, significantly diminishing dependency on extensive training data sets. Our code is available at Github.",Deep learning;Image segmentation;Biological system modeling;Computational modeling;Training data;Transforms;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10470787,IEEE Conferences,,,,,,
Lightweight Deep Learning for Breast Cancer Diagnosis Based on Slice Selection Techniques,O. Oladimeji; H. Ayaz; I. McLoughlin; S. Unnikrishnan,2023 31st Irish Conference on Artificial Intelligence and Cognitive Science (AICS),########,2023,"Breast cancer is a prevalent form of cancer with significant mortality and morbidity rates among women worldwide. Early detection is vital in increasing the chances of survival and one of the major approaches for breast cancer screening and detection is medical imaging. Advancements in technology have given rise to 3D medical imaging such as Abbreviated Breast MRI and DBT, overcoming the challenges of tissue overlapping in 2D modalities. However, deep learning model development using this 3D medical imaging comes with higher computational costs and complexity. This study proposes a lightweight deep learning technique based on three slice selection techniques using entropy, variance, and gradient magnitude values from DBT medical imaging modality. The selection techniques help select only the most informative slices making computational cost and complexity reduced. Entropy value-based slice selection performed best with an accuracy of 91%. The results obtained using the slice selection techniques for lightweight deep learning model development show that it can diagnose breast cancer with a lower number of slices and less computational complexity compared to existing methods.",Deep learning;Solid modeling;Three-dimensional displays;Computational modeling;Brain modeling;Breast cancer;Entropy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10470496,IEEE Conferences,,,,,,
Unveiling Image Classifiers: An In-Depth ComparativeExploration of Machine Learning Algorithms,A. Pathak; K. V. Arya; V. Tiwari; M. Bhende,2023 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI),########,2023,"The challenging problem of image classification serves as a critical hub with important implications for a variety of fields in the rapidly growing field of computational intelligence. This research study carries out on an in-depth journey, carefully traversing through a broad range of machine learning algorithms carefully made for the important task of image classification. The main goal of this research is to understand the unique properties, natural obstacles, and elaborate metrics for performance that such algorithms feature. The work develops through a broad and thorough comparative examination, thoroughly analyzing the positive aspects and flaws of each method. The broad Support Vector Machines (SVM), the fundamental Convolutional Neural Networks (CNN), the combined skill of Random Forest, the perceptive K- Nearest Neighbors (KNN), and the interpretable Decision Trees make up the set of algorithms that went through comprehensive examination. This extensive research provided an array of findings that are ready to give an illumination on both the individual capability of these algorithms and their combined impact on the broad area of image classification. This research gives a map for analyzing the complex details that support the effectiveness and contributions of these algorithms within the dynamic world of image classification through the integration of real-life data with logical interpretations.",Support vector machines;Machine learning algorithms;Navigation;Heuristic algorithms;Classification algorithms;Convolutional neural networks;Decision trees,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10464718,IEEE Conferences,,,,,,
Lung Disease Classification in Chest X-Rays with Feature Fusion via Stacked Convolutional Denoising Autoencoders,J. Borah; H. K. Singh; K. Sarmah,2023 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI),########,2023,"Deep Learning recently has made a breakthrough and tremendous advancement in medical imaging domain. While training supervised deep learning models require large number of annotated data. This is costlier, challenging and time-consuming and requires domain expertise. Nevertheless, these challenges can be tackled with models trained in unsupervised manner., that requires no annotations. Stacked convolutional denoising autoencoders are unsupervised models with convolutional layers that reconstruct the original input data that requires no annotated data with random perturbations added to the original input data. The ability to combine information from various sources in enhancing classification performance is showing promising results. In this paper, we show unsupervised models with stacked convolutional denoising autoencoders for extracting salient features from input un annotated data and fine tuning it for supervised classification of lung diseases on chest X-ray images with feature fusion.",Training;Deep learning;Pulmonary diseases;Perturbation methods;Noise reduction;Feature extraction;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10464643,IEEE Conferences,,,,,,
Performance Analysis for Convolutional Neural Network Architectures using Brain Tumor Datasets: A Proposed System,M. J. U. Chowdhury; S. Kibria,2023 International Workshop on Artificial Intelligence and Image Processing (IWAIIP),########,2023,"Brain tumors remain a pressing global health concern, with high mortality rates despite significant medical advancements. The brain tumor is a potentially deadly condition, and its categorization creates an enormous challenge for radiologists due to the diverse composition of tumor cells. In recent times, there has been a growing interest in the development of computer-aided diagnostic systems that use magnetic resonance imaging (MRI) to help in the identification of brain tumors. These systems have the potential to provide valuable support in the medical field. In this paper, we have proposed an experimental method for the extraction of brain tumors from 2D Magnetic Resonance Imaging (MRI) scans using a convolutional neural network (CNN). The qualitative study was conducted using a real-time online dataset that included a wide range of tumor sizes, locations, forms, and varying image intensities. The proposed methodology aims to deliver the most effective model capable of distinguishing between normal brain structures and various types of brain tumors, such as glioma, meningioma, and pituitary tumors, using these MRI images. The architecture of the network, the choice of hyperparameters, and the nuances of the optimization algorithmsâ€”all these factors play a crucial role in determining the modelâ€™s performance. In our work, CNN achieved an accuracy rate of 98.91%, which is quite impressive. In addition, results from experiments demonstrated that the proposed method is efficient and suitable for computer-assisted brain tumor detection.",Training;Magnetic resonance imaging;Two-dimensional displays;Transfer learning;Computer architecture;Brain modeling;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10462886,IEEE Conferences,,,,,,
A Spatiotemporal Cross-Recognition Network with Robust Generalization for Lesion Classification in Different Medical Images,H. Zhu; H. He; Z. Yang; Y. Duan,2023 China Automation Congress (CAC),########,2023,"Ultrasound images, magnetic resonance imaging slices, computed tomography scans, X-ray images and other medical images obtained through medical imaging technologies have different problems of speckle noise and low contrast, which seriously affect the accuracy of diagnosis. However, the currently proposed deep learning models can only address the single issue of the single-type slices and seriously ignore the temporal features, resulting in a lack of generalizability. Therefore, to solve the aforementioned limitations, a spatiotemporal cross-recognition network (STCR-Net) with robust generalization is proposed in this paper and validated on different types of medical images. The dualmodal spatiotemporal encoder is first constructed to achieve the indepth extraction of real long short-term temporal sequence features and global spatial features. Subsequently, a spatiotemporal crossskip mechanism is developed to realize the sharing of temporal and spatial feature weights. Finally, a spatiotemporal hybrid attention mechanism is designed to extract spatiotemporal fusion features that are closer to the lesion areas. Compared to other advanced classification frameworks, extensive experimental results have proved that the STCR-Net assists in solving different problems of multi-type medical images and has promising prospects for clinical application because of the state-of-the-art classification performance and robust generalization ability.",Deep learning;Ultrasonic imaging;Magnetic resonance imaging;Computed tomography;Speckle;Feature extraction;Spatiotemporal phenomena,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452096,IEEE Conferences,,,,,,
Image Registration with Learned Regularization,X. Lai; W. Yang; L. Wang; S. Ying,2023 China Automation Congress (CAC),########,2023,"Image registration is one of the important tasks in medical image processing. The accuracy of image registration greatly affects the subsequently intelligent analysis. In this paper, we focus on the medical image registration via deep learning, and propose the unsupervised deep learning framework based on model decoupling and regularization learning. Specifically, as a highly ill-posed inverse problem, we first decompose the image registration into two simpler sub-problems to reduce the complexity of model solving. Further, two light neural networks are constructed to approximate the solutions of the two subproblems, where the training strategy of alternative iteration is used. Secondly, we introduce regularization learning module to form an end-to-end deep learning architecture. Here, we approximate the regularization term via the ISTA net, and make the regularization constraints more consistent with the actual data distribution. It reduces the registration errors caused by the deviation between the pre-set prior knowledge and the actual data distribution, and then realizes a higher accurate and data adaptive registration approach. Finally, to validate the performance of the proposed algorithm, we compare it with the VoxelMorph algorithms on brain MRI images dataset LPBA40 and lung CT image dataset DIRLAB. The experimental results show that the proposed algorithms obtain the best performance.",Deep learning;Image registration;Adaptation models;Magnetic resonance imaging;Computed tomography;Lung;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450865,IEEE Conferences,,,,,,
LightMoDAD: A Lightweight Diagnosis Network for Alzheimer's Disease with Small-Scale Multi-Modal Data,G. Wang; Z. Bai; Y. Xu; S. Song; M. Chen; H. Chen,2023 China Automation Congress (CAC),########,2023,"Alzheimer's Disease (AD) is a chronic neurodegenerative disease without effective medications or supplemental treatment. Early and accurate diagnosis of AD is crucial for effective treatment and patient management. However, AD diagnosis model training is often suffered from small datasets. This paper proposes a lightweight AD diagnosis network trained by using small multi-modal datasets. First, multimodal medical images are produced by fusing structural information from magnetic resonance imaging (MRI) of AD patients with brain activity information from positron emission tomography (PET) images. Then, a lightweight neural network is constructed by integrating convolutional neural networks (CNNs) with transformer networks to extract essential features and classify Alzheimer's disease images. Meanwhile, transfer learning makes the AD diagnosis model less dependent on data. Our model achieves promising results in terms of accuracy, sensitivity, and specificity using a small subset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) public dataset.",Training;Magnetic resonance imaging;Transfer learning;Brain modeling;Feature extraction;Data models;Alzheimer's disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10451828,IEEE Conferences,,,,,,
Automated Lung Cancer Detection Using Lightweight Neural Network,M. M. Ali; V. Jain; A. Chauhan; V. Ranjan; M. Raj,"2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)",########,2023,"Due to growing lung cancer rates and COVID19 pandemic awareness, automated lung cancer diagnosis has garnered focus in recent years. In medical image processing, Convolutional Neural Networks (CNNs) have shown promise for accurate and efficient lung cancer identification. This research proposes the use of a lightweight CNN architecture to identify lung cancer. The lightweight CNN balances model complexity with performance, making it suited for deployment on mobile devices and edge computing systems. The proposed technique is tested using a heterogeneous lung cancer dataset with chest X-ray pictures. During training, our lightweight CNN achieved 99% dataset accuracy, superseding other models in comparison. The model's sensitivity and specificity, critical in lung cancer screening, reduces false negatives and positives. Precision and automated lung cancer detection may speed up early identification and care, improving patient outcomes. CNN's lightweight architecture allows real-time inference on resource-constrained devices, expanding lung cancer screening in rural and underprivileged areas. In conclusion, a lightweight convolutional neural network can identify lung cancer automatically. This approach's 99% validation accuracy on a difficult lung cancer dataset shows itâ€™s potential to help doctors make accurate diagnoses leading to aids in timely lung cancer detection and improved health outcomes.",Training;Computational modeling;Lung cancer;Lung;Computer architecture;X-rays;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10458737,IEEE Conferences,,,,,,
Reducing Labeling Costs in Alzheimerâ€™s Disease Diagnosis: A Study of Semi-Supervised and Active Learning with 3D Medical Imaging,R. A. Qayyum Patel; R. -C. Mihailescu,"2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)",########,2023,"Alzheimerâ€™s Disease (AD) is a neurodegenerative condition that is the most common cause of dementia. While there is no cure, its early detection is crucial for effective medical intervention. Deep learning models trained on brain Magnetic Resonance Imaging (MRI) scans have shown promise in this regard, but obtaining annotations for medical imaging data is expensive. In this study, we explore three network training approaches that aim to minimize labeling costs â€“ Active Learning (AL), Semi-Supervised Learning (SSL), and Semi-Supervised Active Learning (SSAL). These were applied to train a 3D subject-level convolutional neural network to diagnose AD using 3D brain MRI scans. Our results confirm the significant impact of the annotation budget and the initial training set on model performance. We observe that all approaches consistently outperform random sampling. Uncertainty-based AL achieves comparable performance to the traditional supervised baseline using only 30 percent of the annotated data. Representative AL and joint SSAL outperform the traditional supervised baseline using 30 percent of the annotated data, with the latter showing robustness even with a restricted initial training set.",Training;Three-dimensional displays;Costs;Annotations;Magnetic resonance imaging;Brain modeling;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10458754,IEEE Conferences,,,,,,
Automating CT Diagnostic Image Quality Control Using Transfer Learning,A. Gupta; M. S. Shehata; W. J. Braun; R. E. Feldman; T. A. Bjarnason,"2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)",########,2023,"Healthcare outcomes are intrinsically tied to the quality of medical imaging, which, in turn, relies on the precision of devices such as Computed Tomography (CT) scanners. Ensuring the high quality of medical images necessitates routine manual quality control testing, a process known for its time-consuming, error-prone, and costly nature. This paper presents a study that evaluates state-of-the-art deep convolutional neural network models to automate the quality control process for medical images. We evaluate the performance of 5 different models wherein InceptionResNetV2 achieves the best accuracy of 99%. We further examine out-of-domain datasets, highlighting a crucial need for more resilient models capable of automating quality control within healthcare facilities.",Image quality;Computed tomography;Computational modeling;Transfer learning;Process control;Quality control;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10458738,IEEE Conferences,,,,,,
Robust and Transferable Graph Neural Networks for Medical Images,S. H. Gheshlaghi; N. Yahyasoltani,2023 International Conference on Machine Learning and Applications (ICMLA),########,2023,"Graph neural networks (GNNs) are extensively deployed in handling various network structured data. However, similar to other deep learning models, GNNs are vulnerable to adversarial perturbations in the input samples. This paper addresses this limitation by evaluating the effects of adversarial training and principle component analysis (PCA) defense methods on enhancing the GNN's robustness. More specifically, adversarial attacks in classification of medical images has been considered and the numerical tests verify that adversarial training with PCA can improve the GNN classification accuracy with different levels of adversarial perturbation budget. It is further shown that using adversarial training jointly with PCA as defense approach can improve model transferability.",Training;Privacy;Perturbation methods;Medical services;Graph neural networks;Robustness;Numerical models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459960,IEEE Conferences,,,,,,
Multi-scale Loss based Electron Microscopic Image Pair Matching Method,C. Duan; K. Komatsu; M. Sato; H. Kobayashi,2023 International Conference on Machine Learning and Applications (ICMLA),########,2023,"Nanodiffraction Imaging (NDI), a novel imaging technique based on the scanning transmission electron mi-croscopy (STEM), helps the understanding of the relationships between micro-structure and macro-properties. However, the analysis requires image pair matching tasks through meticulous and time-consuming observation and selection by domain experts. Therefore, this paper proposes an image pair matching method for NDI images. The proposed method adopts a two-step training approach. The first step is to perform pre-training by contrastive learning on specialized NDI images. The second step is fine-tuning by a customized model on image pair matching tasks. In the second step, the training is performed with the incorporation of a special loss function, OriDist loss. This loss function is designed to focus on orientation and distribution of multi-scale features. The evaluation results demonstrate the ability of the proposed method to achieve high-accuracy NDI image pair matching, and efficiently reduce the search space of candidate matching images, resulting in a significant reduction in the human workload. Through an extensive ablation study, each component of the proposed method shows positive contributions to the overall performance.",Training;Self-supervised learning;Machine learning;Electron microscopy;Task analysis;Electrons,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459812,IEEE Conferences,,,,,,
Vision Transformer-based Classification for Lung and Colon Cancer using Histopathology Images,M. Hasan; M. S. Rahman; S. Islam; T. Ahmed; N. Rifat; M. Ahsan; R. Gomes; M. Chowdhury,2023 International Conference on Machine Learning and Applications (ICMLA),########,2023,"In this day and age, a considerable number of fatalities are caused by colon and lung cancer. Although their appearance simultaneously is rare, the likelihood of cancer cells spreading between these two organs in the absence of early diagnosis is relatively significant. The second highest cause of death worldwide is cancer. Colon and lung cancers are the most typical and lethal malignancies among the many different forms. Historically, physicians had to go through a lengthy and time-consuming process to evaluate histopathological images and identify cancer cases; however, current technology options make it possible to complete this process more quickly. This study classified the histopathological imagery LC25000 gathered at the Tampa, Florida-based James A. Haley Veterans Hospital of Lung and Colon Cancers using several Convolutional Neural Network (CNN) variants as well as the more recent vision transformer architecture. Based on the outcomes of this study, it has been observed that using the Vision Transformer provides exceptional performance outperforming deep learning CNN, VGG19, and Resnet 50 algorithms with 100% accuracy.",Deep learning;Machine learning algorithms;Histopathology;Hospitals;Lung cancer;Lung;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459734,IEEE Conferences,,,,,,
Breast Cancer Segmentation on Ultrasound Images using U-Net and Grad-CAM,S. U. Bhuiyan; M. R. Hasan Mahin; M. J. Siddique; N. Hossain; S. F. Ahmed; M. Abeer; A. A. Rasel,2023 International Conference on Machine Learning and Applications (ICMLA),########,2023,"Breast cancer has remained as the most commonly diagnosed cancer worldwide, with the rate of infections among women continuously increasing with time. Hence, this issue requires appropriate tools for diagnosis to avoid casualties, misdiagnosis and much more. In regards to detection, classification and segmentation, significant progress has been made through breast cancer research facilities, and the accuracies received have been progressively improving through time, however interpretations and visualization methods can prove to be lackluster. Thus, Machine Learning (ML) classifiers are implemented to a higher degree to facilitate the early prediction process. In this paper, we will be dealing with the semantic segmentation aspect, and further refine the visualisation procedure in ML classifiers using Grad-CAM. To be more elaborate, we will implement U-Net Architecture and Adam Optimizer, which is a deep learning technique. Then we will demonstrate how Grad-CAM influences the decision-making process of Convolutional Neural Networks (CNNs), validating model predictions and providing heat-maps to identify regions of interest, thus increasing the chances of a correct diagnosis. Our model is not pretrained and has achieved accuracy, recall, precision, IoU and F1-score of 99.37%, 86.20%, 81.30%, 84.60% and 83.70% respectively and performed better than other models compared in this paper.",Visualization;Ultrasonic imaging;Semantic segmentation;Scalability;Instruments;Predictive models;Rendering (computer graphics),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459967,IEEE Conferences,,,,,,
Attention-Based Regularisation for Improved Generalisability in Medical Multi-Centre Data,D. Silva; G. Agrotis; R. Beets-Tan; L. F. Teixeira; W. Silva,2023 International Conference on Machine Learning and Applications (ICMLA),########,2023,"Deep Learning models are tremendously valuable in several prediction tasks, and their use in the medical field is spreading abruptly, especially in computer vision tasks, evaluating the content in X-rays, CTs or MRIs. These methods can save a significant amount of time for doctors in patient diagnostics and help in treatment planning. However, these models are significantly sensitive to confounders in the training data and generally suffer a performance hit when dealing with out-of-distribution data, affecting their reliability and scalability in different medical institutions. Deep Learning research on Medical datasets may overlook essential details regarding the image acquisition procedure and the preprocessing steps. This work proposes a data-centric approach, exploring the potential of attention maps as a regularisation technique to improve robustness and generalisation. We use image metadata and explore self-attention maps and contrastive learning to promote feature space invariance to image disturbance. Experiments were conducted using Chest X-ray datasets that are publicly available. Some datasets contained information about the windowing settings applied by the radiologist, acting as a source of variability. The proposed model was tested and outperformed the baseline in out-of-distribution data, serving as a proof of concept.",Deep learning;Training;Computational modeling;Training data;X-rays;Self-supervised learning;Metadata,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459854,IEEE Conferences,,,,,,
Integrating Ensemble Learning into Remote Health Monitoring for Accurate Prediction of Oral and Maxillofacial Diseases,S. M. Hussain; S. A. Zaidi; A. Hyder; M. M. Movania,2023 25th International Multitopic Conference (INMIC),########,2023,"Due to excessive use of tobacco, oral and maxillofacial diseases are prevalent in Pakistan. This paper presents a deep learning-based approach for the accurate diagnosis of oral diseases, specifically focusing on mouth ulcers, hypodontia, and dental caries, using RGB images. Unlike previous studies that primarily utilize X-ray images, this research uses a diverse dataset of over 6,000 annotated RGB images. The methodology involves training and evaluating three models including VGG16, MobileNet, and InceptionV3 for individual disease classification. The models achieve high validation accuracies ranging from 90% to 95%. The weighted ensemble model, combining the predictions of the three models, is also implemented which resulted in an improved accuracy of 97%. The proposed methodology demonstrates the potential of deep learning in enhancing the precision and effectiveness of oral disease diagnosis, enabling timely intervention, and optimizing patient care. Future work could focus on expanding the dataset size to further improve the modelâ€™s accuracy.",Training;Mouth;Focusing;Predictive models;Distance measurement;Medical diagnosis;Ensemble learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465788,IEEE Conferences,,,,,,
A Transfer Learning Based Detection and Grading of Cataract using Fundus Images,A. Abbas; A. Alzahrani; A. Imran; A. Almuhaimeed; A. H. Khan,2023 25th International Multitopic Conference (INMIC),########,2023,"Among the most prominent causes for visual impairment, especially among older adults, is cataract. As per the World Health Organisation (WHO), there are 2.2 billion individuals globally are estimated to be blind or possess an eyesight problem. One of the most identified and important causes of this is cataracts. Cataracts should be identified and treated as soon as possible to avoid blindness. Ophthalmologists use an expensive slit lamp to diagnose cataracts in regions with few medical facilities. Consequently, the issue is that a lack of skilled ophthalmologists may delay the identification of cataracts, for which medical treatment is unavoidable. Medical image analysis based on artificial intelligence provides a rapid and precise diagnosis in modern healthcare. We utilized deep learning models based on transfer learning, namely VGG19, and ResNet-50, to diagnose cataracts using fundus images and enhance classification accuracy. The performance metric for the model is accuracy; the highest achieved accuracy was 98%.",Cataracts;Deep learning;Measurement;Glaucoma;Image analysis;Transfer learning;Visual impairment;Medical treatment;Older adults;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465934,IEEE Conferences,,,,,,
Leukemia Cells Classification using Deep Learning Approaches,S. Mohsin; S. U. Warsi; M. Asif; A. Hassan; R. Khan; A. Fatima,2023 25th International Multitopic Conference (INMIC),########,2023,"Leukemia falls under the category of blood cancer that originates in the bone marrow and causes proliferation of a significant quantity of irregular cells. Early detection and treatment offer the possibility of a cure for this disease. Considering this context, rapid analysis of blood cells for leukemia becomes a critical priority within the healthcare industry. Identifying and categorizing white blood cells poses a significant challenge in image processing due to labor-intensive manual data analysis and frequently inaccurate nature. To tackle this challenge, this research article proposes a technique to classify blood smears that uses multiple deep learning architectures including SqueezeNet, ResNet-50 and AlexNet. To develop a technique, Acute Lymphoblastic Leukemia image dataset is used. Moreover, a comparative assessment is conducted among applied deep learning models to choose the most suitable one for the intended domain. The experimental finding demonstrate that the AlexNet surpasses SqueezeNet and ResNet-50 with 99% accuracy. Additionally, comparative evaluation of the proposed technique with existing ones illustrates its supremacy.",Deep learning;White blood cells;Industries;Image recognition;Data analysis;Cells (biology);Manuals,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10466065,IEEE Conferences,,,,,,
Brain Image Segmentation via GLCM Features and CNN Classification for Improved Image Retrieval Using Machine Learning,R. Sekar; H. P. R; K. Acharjya,2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG),########,2023,"Improvement methods based on picture segmentation are commonly employed in medical image analysis. Pixels are commonly put together in groupings of varied sizes to generate an image. The available pixel intensities in an image allow for the separation of distinct image regions. Edge-based and boundary-based segmentation methods are the two primary classifications of this type of analysis. The boundary-based segmentation technique is the more widespread of the two. The image of the tumor needs to be segmented into numerous fragments, each of which can then be analyzed separately. It can annotate data and locate locations of interest in medical image processing. The issues with the existing paper are addressed in the suggested study. Feature extraction using GLCM has been used for classification of tumor images. Analysis of data in segments is routine. The CNN classifier's segments are labelled by an analytical procedure. The proposed method boosts information quality and reduces error-prone training in MRI brain tumor pictures by increasing the learning coefficient of the back propagation layer in the neural network. As a result, the current CNN Shallow Net architecture achieves a higher classification rate of 99.61% thanks to its increased specificity (98.56%) and sensitivity (98.12%). The deep web achieves a segmentation accuracy of 99.58 percent, with an additional diagnostic accuracy of 98.5 percent in identifying cancerous spots.",Image segmentation;Sensitivity;Magnetic resonance imaging;Feature extraction;Convolutional neural networks;Tumors;Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456270,IEEE Conferences,,,,,,
An Operative Investigation of ML Methods for Image Retrieval with MPKDC-Based Brain Image Segmentation,M. R; G. D; R. Kumar,2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG),########,2023,"Processing images is crucial in making judgements based on a wide range of criteria. Image processing techniques include enhancing, restoring, and compressing photos all help save space in the long run. Segmentation is a crucial stage that plays a significant role in every aspect of the image processing workflow. When assessing medical images, image segmentation-based improvement techniques are frequently used. This work proposes a unique MPKDC method for determining which hypoechoic areas in a brain imaging are most likely to contain a tumor. The proposed algorithm can help the radiologist determine what areas need to be scanned using MRI for diagnostic purposes. Multimodal Brain Tumor Image Segmentation datasets were used to evaluate the proposed method. This dataset includes 220 high-quality brain tumor images and 54 low-quality tumor images. Promising efficiency of the suggested segmentation methodology was demonstrated via cross-validation of the succeeding methods.",Deep learning;Neuroimaging;Image segmentation;Machine learning algorithms;Image recognition;Magnetic resonance imaging;Image retrieval,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456186,IEEE Conferences,,,,,,
Enhancing Unsupervised Domain Adaptation via Multi-Layered Domain Alignment in Multisource Environments,M. Singh,2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG),########,2023,"Even in current times, lung cancer is recognized as one of the most lethal varieties of the disease. Early detection and exact diagnosis are two of the most important factors in improving patient outcomes. This study proposes a unique way for evaluating medical images with the purpose of identifying lung cancer. In terms of accuracy, precision, recall, and F1-score, the suggested technique beats both standard histological classification methods and genomic classification methods. The method employs cutting-edge machine learning and neural network methods. The inquiry looks thoroughly into the procedures that occur between the moment an image is captured and its classification. It emphasizes the significance of feature extraction, segmentation, and the convolutional layer. The findings indicate that the developed approach has the potential to become a reliable tool for the early identification of lung cancer, allowing the technology to be used in clinical settings.",Image segmentation;Neural networks;Government;Lung cancer;Genomics;Machine learning;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10455967,IEEE Conferences,,,,,,
Multimodal Data Fusion and Machine Learning for Comprehensive Management of Parkinson's Disease in Healthcare,G. D; A. K. Gautam; P. Bhambu,2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG),########,2023,"Parkinson's disease is a neurological ailment that affects millions of individuals throughout the world. Successful treatment will greatly enhance the quality of life for those who have it. To effectively treat Parkinson's disease, this study investigates how multimodal data and machine learning technology might be combined. To improve illness diagnosis, symptom severity prediction, and medication efficacy evaluation, the suggested technique draws from a broad variety of data sources such as clinical records, medical imaging, and sensor data. In this study, novel data fusion and analytic techniques are used to generate a comprehensive picture of the patient's health. Classification and prediction can be accomplished using either the Random Forest, Convolutional Neural Network, or Long Short-Term Memory Network machine learning techniques. Data fusion and feature extraction, the first steps in building reliable models, are graphically shown by a set of mathematical equations. Data presented in the tables of comparison show how much of an improvement the proposed remedy represents over the status quo. This strategy has the potential to significantly alter the way doctors treat Parkinson's disease, leading to better outcomes for patients and setting a new standard for healthcare in general.",Parkinson's disease;Soft sensors;Data integration;Medical services;Transforms;Feature extraction;Mathematical models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456100,IEEE Conferences,,,,,,
Advancing Brain Tumor Diagnosis through Machine Learning and the Power of Combined CNN and RNN in Medical Imaging,P. Jayadharshini; N. Abinaya; G. Rithanya; N. Bhavatharini; V. Balaji; T. Madhavan,2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG),########,2023,"A precise and prompt diagnosis is essential for efficient treatment planning in the complex and possibly fatal illness known as brain tumours. The diagnosis of brain tumours through diagnostic imaging information, such as MRI, or magnetic resonance imaging, scans, has shown considerable promise during the past few years. This research provides a thorough analysis of different machine learning methods and algorithms applied to brain tumour identification. The article starts off by talking about the difficulties in detecting brain tumours, such as the natural complexity and variety of brain tumour pictures. The following section gives an overview of the many machine learning methods used in this industry, including more contemporary techniques like deep learning as well as more established ones like SVM, ANN, and decision trees. To address the challenges posed by the complexity and variety of brain tumors. This study proposes an innovative machine learning-based approach that leverages the power of recurrent neural networks (RNNs) in addition to convolutional neural networks (CNNs). RNNs are particularly well-suited for processing sequential data, such as sequential slices of MRI scans. By incorporating RNNs, our model can capture the temporal dependencies and spatial relationships present in the sequential MRI slices, leading to more accurate brain tumor detection. The suggested technique demonstrates promising outcomes in precisely detecting brain cancers, showcasing the potential of combined machine learning, including both CNNs and RNNs, in advancing medical image analysis methods for more accurate detection and diagnosing of brain tumors. . By making it possible to identify and diagnose brain tumors early and precisely, this discovery has the potential to have a substantial impact on the area of medical imaging and improve patient outcomes.",Support vector machines;Training;Logistic regression;Recurrent neural networks;Magnetic resonance imaging;Brain modeling;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456243,IEEE Conferences,,,,,,
Detection of Lung Cancer Using Watershed Algorithm and CNN,V. A; P. K. G B; T. B,2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS),########,2023,"Lung cancer is a serious public health issue that requires better early detection. To establish a novel framework for lung cancer detection, we combine the Watershed Algorithm for feature extraction with the VGG-16 Convolutional Neural Network (CNN) architecture. This comprehensive system has the potential to improve diagnosis accuracy. An advanced feature extraction method, the Watershed Algorithm, segments lung areas while maintaining small features, reducing data loss during preprocessing. These segments are fed into the CNN VGG-16, which is well-known for recognizing complex patterns. To detect small lung anomalies, we fine-tune and train it using a huge dataset of lung CT scans. Our method, which incorporates the Watershed Algorithm and CNN VGG-16, provides a realistic way for early lung cancer diagnosis.",Transfer learning;Lung cancer;Lung;Watersheds;Feature extraction;Classification algorithms;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448847,IEEE Conferences,,,,,,
Automated Classification of MR Brain Images based on Transfer Learning Approach,K. Jaspin; S. Selvan,2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS),########,2023,"Accurate and precise classification of MR images plays a vital role in the clinical evaluation of patients with brain tumors. However, a fundamental challenge arises from the disparity between the intricate details perceptible to human assessors and the information captured by MRI machines. Traditional methods for categorization often emphasize significant features and incorporate manually crafted attributes. Recent advancements in deep learning, particularly Convolutional Neural Networks (CNNs), have shown success in image classification. Despite its reliance on substantial training data, deep learning effectively captures features and seamlessly integrates feature extraction and categorization through self-learning. Yet, applying CNN training directly to small medical imaging datasets poses challenges due to their limited size. To overcome this issue, we propose a solution involving transfer learning, utilizing a pre-trained deep-supervised CNN model specialized in discriminating between tumor and non-tumor classes. We validate our approach using a benchmark dataset. Notably, our method eschews handcrafted features, requires minimal preprocessing, and achieves an impressive accuracy of 80.67%, underscoring its broader applicability. We compare our results with alternative deep learning techniques employing CNNs.",Deep learning;Brain;Transfer learning;Training data;Feature extraction;Convolutional neural networks;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449265,IEEE Conferences,,,,,,
Detecting Brain Tumor at Early Stage Tumor Attest,B. Latha; D. V; Ramaditya; G. P; S. B,2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS),########,2023,"A brain tumor is an accumulation of aberrant brain tissue. Tumors are typically separated into benign and malignant categories when they initially develop. Since tumors can be lethal, it is vital to identify and specify their presence in a brain image. The proposed method employs CNN and support vector machines to identify if a brain tumor is present or absent from an MRI image. In the first stage, when spots are discovered, the input image is converted to grayscale using binary thresholding. The discovered spots are shown according to their intensities in order to differentiate between the normal and tumor brains. Tumor identification is done by employing the SVM-CNN hybrid technique after characterizing the extracted set of characteristics.",Support vector machines;Computational modeling;Magnetic resonance imaging;Gray-scale;Brain modeling;Tumors;Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449231,IEEE Conferences,,,,,,
Malaria Disease Prediction using Faster RCNN,S. Mathupriya; R. R. S. V; N. T. M,2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS),########,2023,"Malaria is a disease caused by Plasmodium parasites that remains a major threat to global health. It affects 200 million people and causes 400,000 deaths each year. The Plasmodium parasite, which is spread through the bites of female Anopheles mosquitoes, is the main cause of malaria. Typically, the use of a microscope in microbiological studies facilitates the identification of infected cells in a blood sample, followed by expert analysis of the results to complete the diagnostic process. This type of expert analysis does not give a completely 100% perfect conclusion and it also costs more. Identifying such targets is also a challenge, largely due to differences in cell shape, density and color, and the ambiguity surrounding certain cell classes. Therefore, a method based on deep learning is used to predict malaria with the highest accuracy. Deep learning-based technologies have proven to be able to achieve human-level accuracy in object detection/classification in image data. These approaches can be used to automate many of the monotonous tasks involved in analyzing micrographs of blood samples. It uses Faster R-CNN. The proposed models were trained and tested on a publicly available erythrocyte image dataset containing both infected and uninfected cells. Methods developed to fine-tune the images using image pre-processing successfully identified malariainfected erythrocytes with the highest accuracy in ashort period of time. The software is also adapted to a budget microcomputer to speed up prototyping.",Support vector machines;Red blood cells;Shape;Malaria;Medical treatment;Object detection;Software,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448699,IEEE Conferences,,,,,,
State-of-Art Techniques for effective Segmentation and Classification of Pancreatic Cancer Using Deep Learning: A Review,C. K.V; G. R. G. King,2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS),########,2023,"Pancreatic Cancer (PC) is a highly aggressive tumor of the digestive system that is also difficult to identify in the outset for Healthline personnel. The traditional technique for diagnosing these malignancies is time-consuming and error-prone. As a result, a reliable and precise model for diagnosis is required. As technology advances in this period, incorporating cutting-edge technology such as Deep Learning (DL) into healthcare equipment will make things more efficient and convenient. Pre-processing for noise removal, as we all know, is one of the three primary phases in the diagnostic process. Segmentation is used to segment tumour areas, and then a classifier is used to classify them accurately. We have chosen these three phases in this review article, where the most recent 11-year publication (2010â€“2021) by various specialists is examined in this state-of-the-art, which clearly define the relevance of various pancreatic image preprocessing, segmentation, and classification techniques. New computations are being produced in this case for possible clients to assess the study area in question. This evaluation recognises the diagnostic measures' main successes as well as their qualitative and quantitative assessment success markers. This research study delves into the significant findings and the underlying reasons behind the acquisition of knowledge, aiming to pave the way for future discoveries.",Deep learning;Image segmentation;Reviews;Pancreatic cancer;Reliability;Personnel;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449138,IEEE Conferences,,,,,,
An Efficient Learning Assisted Malaria Parasites Identification Methodology based on Blood Smear Images,S. D. Lalitha; P. Remya; R. Priyanka; N. Rajendran; N. Sivakamasundari,"2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)",########,2023,"Malaria remains a global health challenge, necessitating accurate and efficient methods for the identification of malaria parasites from blood smear images. In this study, we propose an innovative approach for malaria parasites identification that leverages the power of deep learning and gradient boosting. Our methodology combines the feature extraction capabilities of the MobileNet convolutional neural network with the robust classification capabilities of XGBoost. The MobileNet model is employed to extract discriminative features from blood smear images, providing a rich representation of the parasitesâ€™ morphological characteristics. Subsequently, XGBoost is utilized as the classification model to distinguish between infected and uninfected blood samples based on these extracted features. This hybrid approach showcases remarkable accuracy and efficiency, offering a reliable solution for malaria diagnosis. Experimental results demonstrate the superiority of our methodology, achieving precise identification of malaria parasites while minimizing false positives. The proposed model takes the lead with an impressive accuracy score of 0.98, signifying its exceptional capability in distinguishing between different classes within the dataset. This model, tailored for the specific task at hand, utilizes a combination of innovative techniques to achieve superior accuracy.This innovative learning-assisted approach holds great promise for enhancing malaria diagnosis, particularly in resource-constrained settings, and represents a significant step towards combatting this life-threatening disease more effectively.",Deep learning;Malaria;Lead;Feature extraction;Boosting;Reliability;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465510,IEEE Conferences,,,,,,
A Robust Predictive Model Using Combined Residual Network and Long Short â€“ Term Memory for Lung Cancer Identification,S. Sivakumar; G. Ramkumar,"2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)",########,2023,"Lung cancer is the primary cause of cancer mortality in many nations. Both benign and malignant tumors exist. Malignant tumors are meant to be referred to as ""cancer"" here. Lung cancer has several potential origins, such as secondhand smoke, active smoking, chemical exposure, and genetics. In this study, we present a Residual Network (ResNet)/Long Short-Term Memory (LSTM) hybrid model for lung cancer categorization on CT scan pictures, and we compare the suggested model's effectiveness to that of UNet Architecture. The ability to accurately identify or forecast early-stage lung cancer is crucial. Future medical expenses might be cut significantly as a result. The suggested ResNet - LSTM hybrid technique aims to enhance overall accuracy in lung cancer prediction. It can make surgery safer or perhaps unnecessary, and it has been shown to improve survival rates. Thus, technologies for identifying lung cancer are not only easy to use, cheap, and convenient, but also very successful methods for initial identification of lung cancer. As a result, the network is crucial to making an accurate diagnosis. According to our findings, the steps involved in the detection process are as follows: image preprocessing, image segmentation, extraction of features, and finally, identifying the neural system. The primary goals of this study are to (1) compare and contrast different methods to identify lung cancer, and (2) analyze the current best strategy. The ResNet-LSTM algorithm has shown outstanding accuracy, suggesting that its usage in lung cancer identification might improve detection rates. Lung cancer sufferers may find it simpler to detect and treat their condition if the cancer can be predicted at an earlier stage. We discovered that the accuracy of a hybrid ResNet-LSTM model for diagnosing lung cancer was 97%, whereas that of a standard UNet Model was 93%. In comparison to the previous model, the new one had better accuracy, precision, and recall. Clinicians can improve the accuracy and timeliness of lung nodule detection and classification using medical imaging technologies powered by deep learning. This study details the most up-to-date imaging methods for spotting lung cancer in its earliest stages using deep learning. When compare to the most recent state-of-the-art methods, the suggested method's effectiveness and dependability shine through in experimental findings.",Deep learning;Computed tomography;Malignant tumors;Computational modeling;Lung cancer;Surgery;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465359,IEEE Conferences,,,,,,
Transforming Lung Carcinoma Analysis: Deep Learningâ€˜s Efficient Segmentation and Classification,D. Divya; D. Saveetha; V. C. Bharathi; P. Shanmugam,"2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)",########,2023,"Cancerous cells can develop in the lungs, a condition known as lung carcinoma. One of the most deadly and challenging malignancies to cure, lung cancer often spreads to other parts of the body at an early stage. The World Health Organisation reports that for several decades, lung cancer has topped all cancers globally in terms of incidence. Early detection and treatment of lung cancer are of the utmost importance. More effective treatments, less intrusive surgeries, and longer survival times may result from this. This paper presents a method for efficiently segmenting and classifying lung carcinomas using convolutional neural networks (CNNs) for the purpose of identifying malignant tumors. Here, additional effective methods such as an adaptive median filter for preprocessing, histogram equalization, edge-based segmentation, and Convolutional Neural Network (CNN) for successful classification are used to identify the lung carcinoma. To make segmentation more efficient, we use the Adaptive Median Filter in conjunction with the Histogram Equalization technique. It is also possible to transfer the important properties to the classification algorithm by deriving them from feature similarity. When compared to another prediction model that is already in use, the suggested model has the potential to attain superior accuracy. Reduced time and effort spent on detecting lung cancer are two benefits of the enhanced performance.",Histograms;Adaptation models;Lung cancer;Lung;Adaptive filters;Surgery;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465333,IEEE Conferences,,,,,,
"COVID-19 Classification using CNN, Inception V3 and Transfer Learning",M. Mohammed Iqbal; G. Tamilmani; C. H. Tej Pavan; P. Sri Pranav,"2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)",########,2023,"The outbreak of the COVID-19 pandemic has posed unprecedented challenges for global healthcare systems, neces-sitating accurate and rapid diagnostic solutions. The suggested method utilizes the capabilities of Convolutional Neural Networks (CNN) and the Inception V3 module to provide a novel deep learning strategy for COVID-19 identification. The capacity of the Inception V3 architecture to extract nuanced features from challenging medical imaging data is famous. This work evaluates the performance of the CNN-Inception V3 model in differentiating COVID-19 instances from other respiratory illnesses, such as pneumonia, fibrosis, tuberculosis, and normal lung scans, through a thorough examination employing a broad dataset of chest X-ray and CT images. Important parameters like accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC) are used to assess the modelâ€™s performance. The study also explores the interpretability of CNN-Inception V3 model predictions, providing information on possible clinical uses. The results of this study add to the corpus of research on deep learning-based COVID-19 diagnostics and show the value of cutting-edge technology in the fight against infectious diseases.",COVID-19;Deep learning;Pneumonia;Pandemics;Medical services;Predictive models;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465433,IEEE Conferences,,,,,,
"Strategic Integration of CNN, SVM, and XGBoost for Early-stage Tumor Detection using Hybrid Deep Learning Method",T. P. G. James; B. Y. Karthikeyan; P. Ashok; Dhaasarathy; R. Suganya; K. Maharaja,"2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)",########,2023,"A tumor is an unnatural cell development that can develop into cancer. Most often, cancer affects the brain, kidneys, lungs, liver, and other organs of the body. MRI (Magnetic Resonance Imaging) scans are the official method for locating tumors. Detecting a tumor in its early stages and using the resulting image to spot aberrant tissue growth allows for early cancer treatment. Early detection of tumors plays a pivotal role in improving the prognosis and overall survival rates of patients. A Hybrid Model was constructed which composed of a self-defined Convolution Neural Network (CNN), Support Vector Machine (SVM), and Extreme Gradient Boosting (XGBoost) is applied in detecting the presence of tumors and their performance is analyzed and the Proposed system detects the Tumor in the beginning stage with the accuracy of 98%. The proposed approach leverages the strengths of each individual technique to address the challenges associated with tumor detection. CNNs are employed to extract intricate spatial features from medical imaging data, capturing relevant patterns and representations. SVM is then applied to reduce the dimensionality of the feature space, enabling efficient and effective classification. Finally, XGBoost is used to optimize the classification performance by leveraging ensemble learning and gradient boosting techniques. Experimental results on a diverse dataset demonstrate the superiority of the integrated approach, achieving state-of- the-art accuracy and outperforming traditional methods for early-stage tumor detection.",Support vector machines;Magnetic resonance imaging;Lung;Liver;Feature extraction;Boosting;Prognostics and health management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465307,IEEE Conferences,,,,,,
Detection of Stroke in Brain CT with Fused Deep-Features and Hummingbird Optimizer,S. Ramadasan; M. Ramadasan; K. Vijayakumar; G. Balaji,"2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)",########,2023,"Brain is one of the sensitive organs and the abnormality in the brain is a medical emergency. Hemorrhagic Stroke (HS) is one of the harsh brain condition and appropriate screening and treatment is necessary to reduce its harshness. Medical imaging supported brain screening is a common clinical practice to examine the HS. This work proposes Deep-Learning Technique (DT) to detect HS from the CT scan slices. The stages in this scheme includes; (i) CT slice collection and resizing, (ii) feature extraction using DT, (iii) Hummingbird-Algorithm based feature selection and serial features concatenation and (iv) classification and verification of the performance. This research considered N=2000 images (1000 healthy and 1000 stroke) along with the skull section. The performance of EfficientNet and DenseNet variants are considered to examine the CT slices. The experimental outcome of this study confirms that the fused optimal features helps to attain an accuracy of >96%. In future, this performance of this system can be improved by considering ensemble of features.",Databases;Computed tomography;Feature extraction;Hemorrhaging;Biomedical imaging;Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465543,IEEE Conferences,,,,,,
Cross Organ Bridge Transfer Learning for Lung Cancer Detection,S. S S; K. R; S. M. Kishorkumar; S. G.; K. P. Venkatesh,2023 IEEE 11th Region 10 Humanitarian Technology Conference (R10-HTC),########,2023,"Classification of medical CT scan images is a challenging task that requires high accuracy and sensitivity. In this paper, we propose a method based on VGG 19 and transfer learning for the classification of normal and tumor of lungs from CT scan images. A popular deep convolutional neural network for image identification applications is VGG19. VGG 19 is used as a feature extractor and the last layers are fine-tuned to adapt to our task. We also use transfer learning to leverage the knowledge learned from other datasets, such as chest CT images for different types of lung cancer, viz. adenocarcinoma, basal cell carcinoma, squamous cell carcinoma and kidney dataset. We use a public dataset to assess our methodology. Our findings demonstrate the great accuracy and sensitivity of our approach. Using CT scan pictures, our technique may be helpful for the early identification and diagnosis of lung cancer. In the area of medical imaging, convolutional neural networks (CNNs) in particular have shown promising results in picture categorization tasks. Deep learning techniques, however, face difficulties because of the medical domain's lack of appropriate labelled data. In our study, we provide a novel transfer learning-based approach to medical picture categorization. By using a bridged database created using the same medical imaging modality as the target database, we utilize modality-bridge transfer learning to overcome the domain imbalance between the source and target datasets. Even with a small number of labelled target medical pictures, we are able to obtain good classification accuracy by learning from the source to the bridge and then from the bridge to the target. In order to extract deep characteristics and provide highly accurate findings, our research uses VGG19, a popular deep convolutional neural network, to improve the classification performance of pictures of lung cancer.",Bridges;Adaptation models;Ultrasonic imaging;Databases;Computed tomography;Transfer learning;Lung cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10461796,IEEE Conferences,,,,,,
The Complete Transformation of Usefulness of Image Classification in the Field of Medical Clinical Diagnosis,J. a. Singh,"2023 International Conference on Power Energy, Environment & Intelligent Control (PEEIC)",########,2023,"Large-scale labeled dataset gathering is often difficult in the field of medical imaging because of issues with privacy, the scarcity of specific illnesses, and the expense of expert annotation. This limitation presents a difficult issue for conventional profound learning models, which typically need a lot of information to be prepared. In this work, we explore how move learning might be utilized to resolve the issue of information shortage in the order of clinical pictures. Move learning is utilized to calibrate on more modest clinical datasets by utilizing the data from pre-prepared models for enormous scope non-clinical datasets. Utilizing front line convolutional brain organizations (CNNs) that have been pre-prepared on ImageNet, we then, at that point, adjust the CNNs on specific clinical imaging assignments. We show that our technique conveys tantamount, and in certain examples far superior, execution than, models prepared without any preparation on scanty information. We likewise investigate the interpretability of our models, ensuring that the qualities that are learnt are both vital to medication and simple for clinicians to comprehend. Our outcomes feature move learning's true capacity as a strong technique to further develop clinical picture characterization execution, especially when dataset size is a requirement constraint.",Training;Computational modeling;Transfer learning;Computer architecture;Brain modeling;Data models;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10451188,IEEE Conferences,,,,,,
Lung Cancer Detection and Classification using Transfer Learning with Pre-trained VGG19 Convolutional Neural Networks,S. Das; L. G; S. J. Prakash; N. S. Dey; J. Panuganti; R. Poojitha,2023 3rd International Conference on Emerging Frontiers in Electrical and Electronic Technologies (ICEFEET),########,2023,"It is possible to diagnose and identify a variety of illnesses with the use of high-quality figures obtained from medical equipment. However, obtaining and storing these kinds of photos may be quite costly, and diagnosing a patient might take a long time. Artificial intelligence (AI) solutions for automatic diagnosis may play a major role in addressing the challenges of time and cost. The categorization of medical images may be effectively solved by pre-trained deep learning models. Lung cancer continues to be a widespread and serious type of cancer on a global scale. Timely and precise identification is crucial for successful treatment and enhanced patient results. This study presents a novel approach to the identification and categorization of lung cancer by using transfer learning via a VGG19 Convolutional Neural Network (CNN) that has been trained beforehand. Our work harnesses the power of transfer learning by leveraging the VGG 19 architecture, provides the basis for lung cancer diagnosis, having been pre-trained on a large dataset. Our carefully curated dataset encompasses over 14600 diverse lung images, covering benign, malignant, and normal cases, providing the model with a rich source of information. The model demonstrated outstanding performance metrics, achieving a training accuracy of 99.68%, 98.11% for validation accuracy and 97.93% for test accuracy. Such high accuracy holds great promise for expediting and improving the diagnostic capabilities of healthcare professionals. By leveraging Transfer Learning from VGG19, our model demonstrates adaptability to diverse medical imaging tasks, making it a valuable tool for real-world clinical applications.",Training;Deep learning;Adaptation models;Transfer learning;Lung cancer;Medical services;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452195,IEEE Conferences,,,,,,
Additive Attention for Medical Visual Question Answering,Y. Liu; N. Yang; S. Yao; X. Wang,2023 5th International Conference on Frontiers Technology of Information and Computer (ICFTIC),########,2023,"Medical visual question answering is a prominent research area, presenting significant challenges within the domain of visual question answering. In traditional medical visual question answering, the initial step commonly involves employing Convolutional Neural Network (CNN) for image information extraction. Subsequently, bilinear attention mechanisms are employed to merge textual question characteristics with image visual features. However, the method of extracting visual attributes through convolutional neural networks often overlooks the global contextual within the image, which is crucial for answering questions accurately. Consequently, this paper introduces an Additive Attention Network (AANet) to capture comprehensive image information features. Specifically, CNN is employed to obtain local visual features of images, while the additive attention mechanism serves to acquire global contextual features of images. These components complement each other, enhancing the representation of visual features and augmenting the modelâ€™s global contextual awareness capability. The proposed method demonstrated superior performance on the VQA-RAD dataset, achieving an overall accuracy of 72.5%, especially for closed questions, achieving an accuracy of 81.9%.",Visualization;Additives;Decision making;Information retrieval;Feature extraction;Question answering (information retrieval);Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456252,IEEE Conferences,,,,,,
Detection of Thoracic Diseases Using Chest X-Ray: A Comparative Study of Binary Class and Multiclass Classification Using Deep Learning,A. Agarwal; V. L; A. R. Paduri; R. Mabiyan; M. S. Wattamwar; D. L; N. Darapaneni,2023 IEEE Pune Section International Conference (PuneCon),########,2023,"Identifying thoracic ailments stands pivotal in medical imaging, offering insights for disease recognition and treatment. Chest X-ray (CXR) imaging emerges as a prevalent non-invasive and cost-effective technique for thoracic disease detection [1â€“4]. This research delves into a comparative exploration between binary class and multiclass classification methodologies in identifying thoracic diseases through CXR images. Our approach involves training and assessing deep learning models leveraging a publicly accessible CXR image dataset, juxtaposing the performance of binary and multiclass models using diverse performance metrics. We formulated 17 distinct binary class models and orchestrated their ensemble, alongside crafting a solitary multiclass classification model [25â€“26]. The assessment of these models encompassed various performance metrics, including F1 score, accuracy, precision, and recall. The outcomes advocate for binary classification's pragmatic suitability, attributed to its heightened accuracy and simplified implementation. This inquiry strives to refine the precision and efficacy of CXR image analysis in clinical practice.",Measurement;Deep learning;Training;X-ray imaging;Diseases;Biomedical imaging;Pragmatics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450111,IEEE Conferences,,,,,,
Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based Approaches and Handcrafted Feature Extraction Techniques,S. Paul; S. Mallick; A. Sen,2023 IEEE Pune Section International Conference (PuneCon),########,2023,"Computer vision has transformed medical diagnosis, treatment, and research through advanced image processing and machine learning techniques. Fracture classification, a critical area in healthcare, has greatly benefited from these advancements, yet accurate detection is challenged by complex patterns and image noise. Bit plane slicing enhances medical images by reducing noise interference and extracting informative features. This research explores partial denoising techniques to provide practical solutions for improved fracture analysis, ultimately enhancing patient care. The study explores deep learning model DenseNet, and handcrafted feature extraction. Decision Tree and Random Forest, were employed to train and evaluate distinct image representations. These include the original image, the concatenation of the four bit planes from the LSB, the four bit planes from the MSB, the fully denoised image, and an image consisting of six bit planes from MSB and two denoised bit planes from LSB. The purpose of forming these diverse image representations is to analyze SNR as well as classification accuracy and identify the bit planes that contain the most informative features. Moreover, the study delves into the significance of partial denoising techniques in preserving crucial features, leading to improvements in classification results. Notably, this study shows that employing the Random Forest classifier, the partially denoised image representation exhibited a testing accuracy of 95.61%, surpassing the performance of other image representations. These numerical results underscore the effectiveness of the proposed method in accurately identifying fractures. The outcomes of this research provide valuable insights into the development of efficient preprocessing, feature extraction and classification approaches for fracture identification. By enhancing diagnostic accuracy, these advancements hold the potential to positively impact patient care and overall medical outcomes.",Thresholding (Imaging);Noise reduction;Image representation;Feature extraction;Classification algorithms;Random forests;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450051,IEEE Conferences,,,,,,
A Critical Evaluation of Deep Learning Methods for Classifying Brain Tumor: A Review,S. Jain; V. Jain,"2023 International Conference on Computational Intelligence, Networks and Security (ICCINS)",########,2023,"According to a February 2018 WHO report, Asia has the highest brain or CNS cancer death rate. Early tumor detection can save many of these lives. Targeted therapy requires tumor classification. Due to the high cost, lengthy process, and risk of infection associated with diagnosing a tumor, we have an immediate need for non-invasive, cost-effective, and efficient methods of describing and determining the grade of brain tumors.Â MRI, CT, and other brain scans can detect tumors quickly and safely. This study summarizes brain cancer pathophysiology, imaging modalities, and machine and deep learning-based brain tumor characterization.",Deep learning;Representation learning;Reviews;Imaging;Classification algorithms;Security;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450040,IEEE Conferences,,,,,,
Few Shot Learning for Medical Imaging: A Review of Categorized Images,H. M. Imran; M. A. A. Asad; T. A. Abdullah; S. I. Chowdhury; M. Alamin,2023 IEEE 7th Conference on Information and Communication Technology (CICT),########,2023,"Deep learning systems have advanced significantly in numerous medical applications, improving various aspects of patient care. However, they still need to work on the issue of dependence on the availability of training data. Few-shot learning (FSL) is a topic of active study that aims to overcome this limitation. FSL techniques require only a few labeled examples for training. FSL-based Medical Imaging (MI) approaches show great potential because many unknown rare diseases have limited annotated imaging data in the real world. In this study, we conducted a systematic review to discover the state of FSL techniques for medical images. We categorized different types of images, such as X-rays, computed tomography (CT), magnetic resonance imaging (MRI), tissues, and other images.",Training;Systematics;Reviews;Magnetic resonance imaging;Computed tomography;Transfer learning;Training data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10455365,IEEE Conferences,,,,,,
Noise and performance analysis on fundus images with CNN and transformer models,N. Vannadil; P. Kokil,2023 IEEE 7th Conference on Information and Communication Technology (CICT),########,2023,"Fundus imaging is a valuable diagnostic tool in ophthalmology, providing clinicians with detailed visualizations of the retina and aiding in the detection and monitoring of various eye diseases, including age-related macular degeneration (AMD), glaucoma, diabetic retinopathy (DR), and cataract. However, the quality of fundus images can be significantly affected by noise, mainly additive white Gaussian noise (AWGN), which is inherent in many imaging systems. The presence of noise in real-world data poses significant challenges for computer vision tasks. In the field of medical image classification, a wrong diagnoisis has heavy consequences. Understanding the impact of AWGN on fundus images is crucial for developing practical denoising algorithms and improving diagnostic accuracy. This work presents an analysis of AWGN noise in fundus images aims to characterize its effects on image quality and assess its impact on diagnostic tasks. The work also analyzes the performance of six models (3 each) of two popular deep learning architectures, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) in the presence of AWGN. AWGN is first introduced to the clean image datasets to conduct the analysis. The CNN and ViT models are trained on the noisy datasets to evaluate the performance of the image classification task. The work also involves six denoising algorithms and a popular image enhancement algorithm- Contrast Limited Adaptive Histogram Equalization (CLAHE).",Analytical models;Adaptation models;AWGN;Noise reduction;Transformers;Retina;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10455148,IEEE Conferences,,,,,,
Pulmonary Nodule Classification with Fine-Tuned Deep Learning Models,Sakshiwala; M. P. Singh,2023 IEEE 7th Conference on Information and Communication Technology (CICT),########,2023,"Lung cancer is one of the deadliest cancer types that exists today. At an early stage, deep learning techniques assist radiologists in classifying the malignant pulmonary nodules of lung cancer. This paper studies the various pre-trained deep learning models trained on the IMAGENET dataset and fine-tuned on LIDC-IDRI computed tomography images. Seven pre-trained deep learning architectures, ResNet152, ResNet18, SENet154, ResNext101, SEResNext50, MobileNetV2 and EfficientNetB0 are studied. These architectures are selected based on their specific characteristics. Finally, the fine-tuned EfficientNetB0 shows better performance compared to other models. Classification in medical imaging is challenging due to the invariant background present in images. This study may provide researchers with a base to understand the malignancy classification of lung cancer.",Deep learning;Computational modeling;Computed tomography;Lung cancer;Lung;Computer architecture;Information and communication technology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10455628,IEEE Conferences,,,,,,
Dynamic Convolutional Attention for Classification of Diabetic Retinopathy,S. Bharathraj; S. Sudharson; R. Annamalai; P. N. Senthil Prakash,2023 IEEE 7th Conference on Information and Communication Technology (CICT),########,2023,"Deep learning algorithms in medical image processing have showed considerable promise, notably in the identification of DR. However, due to the diseaseâ€™s complexity and variability, precisely identifying DR remains difficult. A recently developed deep learning approach known as DCA has shown promising outcomes in a range of image classification applications, including medical imaging. The DCA model excels at recognising essential parts and patterns by using attention mechanism to focus on significant areas and exclude irrelevant ones. Our findings add to the body of knowledge about deep learning approaches in medical picture processing. We discovered that combining DCA approaches with ResNet-50 and ensembling it with other state of art models improved DR classification accuracy considerably. The ensemble model, with weights assigned to each architecture, outperformed the standalone ResNet-50-DCA modelâ€™s accuracy of 73.59%, precision of 75.52%, and recall of 74.37%. The ensemble technique maximises performance while minimising weaknesses by harnessing the strengths of diverse models.",Deep learning;Diabetic retinopathy;Image processing;Neural networks;Pattern recognition;Information and communication technology;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10455722,IEEE Conferences,,,,,,
Deep Learning Techniques for Lung Cancer Detection: A Systematic Literature Review,S. H. Mahmud; I. Soesanti; R. Hartanto,2023 6th International Conference on Information and Communications Technology (ICOIACT),########,2023,"Lung cancer has been a leading cause of cancer-related deaths, with the number of fatalities in the United Kingdom between 2017 and 2019 reaching 34771, as reported by Cancer Research UK. Lung cancer is when cells inside the lung grow uncontrollably. Detecting nodules in lung cancer at an early stage can increase the chances of survival for humans. Researchers have been investigating the potential of artificial intelligence and deep learning to develop computer-aided detection (CAD) systems for automated lung cancer detection and classification. CAD systems could help radiologists detect lung cancer and improve lung cancer diagnosis accuracy. Our systematic literature review provided an overview of the performance of current deep-learning methods and datasets for detecting and classifying lung cancer using CT images. We conducted a systematic literature review using the PRISMA 2020. This paper gives the reader insights into various facets of lung cancer detection and motivates researchers to further explore opportunities for crafting models that can be seamlessly integrated into a CAD system.",Deep learning;Training;Solid modeling;Systematics;Bibliographies;Computed tomography;Lung cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10455848,IEEE Conferences,,,,,,
Cataract Disease Diagnosis Using SURF Features and Pre-Trained Variants of an EfficientNet Model: Comparative Analysis,N. Kaur; G. Gupta,"2023 4th International Conference on Computation, Automation and Knowledge Management (ICCAKM)",########,2023,"Cataracts are a common eye condition that affects millions of people worldwide. The major symptom of cataracts is poor or cloudy vision. Accurate and timely detection and diagnosis of cataracts can prevent vision loss. Therefore, an inexpensive system that diagnoses cataracts at an early stage needs to be developed. The goal of this paper is to use SURF features and pre-trained versions of an effective model trained on a publicly available image dataset to sort cataract disease into different groups. The variants of the EfficientNet model were applied to SURF features detected from the image dataset. The comparative analysis was conducted on the basis of different evaluation parameters and found that variant B0 has outperformed the rest of the variants of EfficientNet with an accuracy of 96.3%. The accuracy of EfficientNet B2 is greater than that of EfficientNet B1, but it can be considered to have better performance.",Cataracts;Computational modeling;Feature extraction;Knowledge management;Computational efficiency;Medical diagnosis;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449495,IEEE Conferences,,,,,,
Machine and Deep Learning Classifications for IoT-Enabled Healthcare Devices,S. S. Priya; M. H. Al-Fatlawy; N. Khare; V. Mahalakshmi; S. S. Ganesh,"2023 4th International Conference on Computation, Automation and Knowledge Management (ICCAKM)",########,2023,"The emergence of the Internet of Things (IoT) is profoundly influencing academic research and will have longterm consequences on several industries, chief among them the healthcare sector. The healthcare industry has changed as a result of the Internet of Things (IoT), which has replaced old, centralized processes with decentralized networks of smart devices. Systems of customized healthcare have been developed as a result of this (PHS). This change has been made possible by recent advancements in fields like wearables, sensor networks, and cloud computing, which have simplified the use of IoT in healthcare settings on a large scale. Notwithstanding a few significant disadvantages, IoT has the potential to significantly transform the medical industry. Keeping up with a large number of devices, rising costs, and the requirement for more data storage space are the challenges. This paper offers a thorough examination of IoT as a flexible and robust technology, emphasizing its applications in the healthcare industry. Research is being done on the interaction between machine learning and deep learning. In addition to examining the various advantages and real-world applications of the technology, this article covers the architecture of an Internet of Things (IoT)-enabled healthcare system. It draws attention to the challenges posed by the IoT healthcare environment and emphasizes the need for academics to come up with novel solutions to these problems. The present work of several researchers in the fields of machine learning and deep learning, which are extensively used in IoT-enabled healthcare systems, is highlighted in this study.",Industries;Deep learning;Medical services;Transforms;Real-time systems;Internet of Things;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449617,IEEE Conferences,,,,,,
Unveiling Alzheimer's Disease Diagnosis with Convolutional Neural Networks: Visualizing Features and Class Activation Mapping,A. N. Mohammed,2023 IEEE 11th International Conference on Systems and Control (ICSC),########,2023,"This research paper explores the utilization of CNNs for diagnosing Alzheimer's disease, delving into techniques that enhance their interpretability and decision-making process. Feature visualization and class activation mapping provide insights into how CNNs detect the disease, emphasizing the importance of interpretability in medical imaging. The methodology involved skull stripping to isolate brain structures by removing non-brain tissues, followed by skull cropping to focus on the region of interest, improving the CNN model's accuracy and efficiency. Through training on the preprocessed dataset, the research achieved an impressive 99.92% accuracy in Alzheimer's disease detection, highlighting CNNs' potential for diagnosis. The study underscores the significance of understanding neural network decision-making, offering valuable insights into deep-learning models for medical imaging tasks. These findings contribute to improved Alzheimer's detection methods and enhance our understanding of applying deep-learning models in medical imaging, benefiting other diagnostic imaging tasks and advancing disease detection.",Visualization;Decision making;Feature extraction;Convolutional neural networks;Alzheimer's disease;Task analysis;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449840,IEEE Conferences,,,,,,
Deep Learning-based Neural Network for Automated Brain Tumor Diagnosis from MRI Images,G. Uthradevi; G. Mohanbabu; B. R. Senthil Kumar; A. Tamilselvi; A. Ashok; R. Sangeetha,"2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)",########,2023,"The identification and categorization of brain tumors are crucial issues in medical imaging as they affect treatment choices and diagnosis. While several approaches for brain tumor detection and classification have been developed, many are restricted in their capacity to offer accurate and efficient findings, particularly when it comes to tumor localization and classification within medical imaging. Existing models frequently use distinct pipelines for tumor location and classification, resulting in poor performance and inefficiency. Furthermore, these models frequently struggle with imbalanced datasets and outcomes that are challenging to understand. In this research, we address a significant research gap in existing models by presenting a unique method for classifying brain tumors using RetinaNet, an object detection model. We tested our method using a publicly available dataset of brain MRI scans (Figshare), proving its accuracy in detecting and classifying tumors. Our approach contributes to better clinical decision support by not only achieving high classification accuracy but also providing the precise and comprehensible location of brain tumors.",Magnetic resonance imaging;Pipelines;Surgery;Brain modeling;Planning;Tumors;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452581,IEEE Conferences,,,,,,
Classification Techniques to Predict the Risk of Myelodysplastic Syndromes using Deep Learning,K. Srilakshmi; D. V. Lakshmi,"2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)",########,2023,This paper presents a new hybrid deep learning model of CNN and Logistic Regression (LR) for risk prediction of Myelodysplastic syndromes (MDS). This model uses image processing procedures for patient's risk factor detection. The goal is to predict MDS risk with high specificity and compassion. The hybrid neural network model first obtains the multilevel feature representation of medical imaging data. Then these features are inputted into the logistic regression classifier to predict the risk. Experiments are run on the real-world MDS dataset to evaluate the model performance. Evidence demonstrates that the suggested approach surpasses traditional machine learning techniques and other deep learning methods for MDS risk prediction. The results of this study demonstrate the promise of the proposed hybrid model for risk prediction in medical applications. It is expected that the hybrid deep learning model suggested here paper will provide an effective way for early diagnosis of MDS and reduce the death hazard of human beings.,Deep learning;Logistic regression;Sensitivity;Predictive models;Classification algorithms;Convolutional neural networks;Random forests,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452580,IEEE Conferences,,,,,,
Glaucoma Detection using Fundus Image of the Retina,S. Ramkumar; K. Kathirvel; J. Kiranprasath; M. Prasanthkumar,"2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)",########,2023,"Glaucoma is often regarded as the primary cause of irreversible visual loss. Early glaucoma diagnosis is essential for effective treatment and the preservation of vision. Identifying Glaucoma Using Image Processing Techniques the main effect that glaucoma has on the optic disc is an enlargement of the eye socket. The second most prevalent cause of blindness is glaucoma, which has historically been challenging to identify in its early stages. Glaucoma is the most common cause of permanent blindness worldwide. Therefore, early detection is crucial for the prevention and proper treatment of vision loss. The development of computer-aided glaucoma diagnostic tools has improved significantly in recent years with the use of convolution neural networks (CNNs). This study offers an overview of contemporary CNN-based glaucoma diagnosis algorithms and concentrates on investigations completed up to 2021. To understand when irregularities occur, preprocessing techniques such as filtering, green channel extraction, and CLAHE are applied. The suggested classifier examines these images to determine if glaucoma is present or not computationally. CDR of the desired image. Compare the accuracy of the proposed classifier with that of rival methods. They use soft computing methods and hybrid algorithms for morphology-based image classification.",Glaucoma;Neural networks;Medical services;Blindness;Data models;Classification algorithms;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452518,IEEE Conferences,,,,,,
Detection and Categorization of Brain Tumors Through Deep Learning,V. A. Devi; E. Bhuvaneswari; R. K. Tummala,"2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)",########,2023,"Brain tumor identification can be extremely challenging in the early stages of life. But now that many deep learning and machine learning algorithms have been developed, it has advanced. Automatic identification of brain tumors is a hot topic these days. The brain tumor is located using the patient's MRI pictures. The proposed system aims to determine whether a patient has a brain tumor. Early tumor detection is critical to the patient's long-term health. Identifying these brain tumors and improving the detection precision have been the subject of numerous studies. Radiologists or clinical specialists perform the most difficult and time-consuming tasks: tumor area segmentation, detection, and extraction.",Deep learning;Machine learning algorithms;Magnetic resonance imaging;Brain modeling;Real-time systems;Task analysis;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452566,IEEE Conferences,,,,,,
Modeling Prediction Uncertainty in Regression using the Regression Tsetlin Machine,K. D. Abeyrathna; A. Hafver; L. Y. Edward,2023 International Symposium on the Tsetlin Machine (ISTM),########,2023,"Machine learning is being widely used in various industries, and its impact varies based on the application. However, high-risk domains like autonomous vehicles and medical imaging require a higher level of accuracy and uncertainty estimation. Among other approaches, probabilistic deep learning methods have been developed to quantity both aleatoric and epistemic uncertainty by estimating probability distributions for aleatoric variables and fitting distributions to model parameters, respectively. Tsetlin Machines (TMs) are an emerging technology in machine learning known for their fast learning, low energy and memory consumption, high prediction accuracy, and interpretability. While previous research can be modified to identify epistemic and aleatoric uncertainty in classification tasks, it remains unclear how TMs can quantity aleatoric uncertainty in their regression predictions. In this research, we demonstrate how the Regression Tsetlin Machine (RTM) can be adapted to quantity uncertainty in regression tasks by modifying its structure and learning mechanism. Essentially, we divide the clauses into two groups where one group predicts the mean regression output while the other group learns to predict the variance associated with the predicted mean. Learning involves regular RTM learning, however with slight modifications to learn the variance where clauses in this group are trained based on the error of the predicted mean. We evaluate our proposed approach to estimate the uncertainty using artificial data, both when the variance is random and variance is a function of the input. We compare the performance of the proposed method against a deep learning model. The results demonstrate that RTM based model performs on par or better compared to the deep learning model.",Deep learning;Learning systems;Adaptation models;Uncertainty;Memory management;Predictive models;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10454923,IEEE Conferences,,,,,,
Detection of Skin Cancer: A Deep Learning Approach,A. Shafiullah; F. Faisal; S. M. R. Arnab; R. H. Badhon; S. Ali; M. M. Nishat; I. Ahmed; M. R. Muttaqi; M. A. Billah; S. R. Sadik; I. Marshad; M. S. Islam,2023 IEEE Smart World Congress (SWC),########,2023,"Despite recent advances in medical imaging and machine learning, the detection of skin cancer remains an enormous challenge. Traditional techniques like visual inspection and physical examination by dermatologists are prone to subjectivity and heavily dependent on the clinicianâ€™s experience, resulting in variable diagnostic accuracy. This manifests the pressing need for more accurate, efficient and available diagnostic techniques. Deep learning techniques, particularly transfer learning, have demonstrated promise in addressing problems in multiple disciplines, especially in medical imaging domain. This research seeks to address the issue by developing a transfer learning model for skin cancer detection from images using pre-trained deep learning architectures such as: DenseNet201, ResNet50, and VGG16 with task-specific additional layers. The developed skin cancer detection model attained an aggregate validation accuracy of 91% with F1-scores ranging from 0.94 to 1.00 for some attributes which is highly accurate, efficient, and potentially more accessible than current methods.",Deep learning;Aggregates;Transfer learning;Skin;Task analysis;Skin cancer;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448820,IEEE Conferences,,,,,,
Computer-aided Diagnosis Technology Based on Deep Learning on CT Images of Pulmonary Nodules: A Survey,W. Yutao; L. Yihong,2023 IEEE Smart World Congress (SWC),########,2023,"Lung cancer is one of the most life-threatening diseases in the world. The early manifestation of lung cancer is pulmonary nodules; Computed Tomography(CT) examination is the most commonly used method for early screening of pulmonary nodules. With the continuous development of computer technology, computer-aided diagnosis (CAD) system is gradually applied in the intelligent diagnosis and pre-diagnosis of pulmonary nodules and automatically analyzes medical images for different clinical diagnostic purposes. The application of computer-aided diagnosis technology based on deep learning to CT images of pulmonary nodules mainly includes five stages: data preprocessing, lung parenchyma segmentation, pulmonary nodule detection, pulmonary nodule segmentation, and benign and malignant classification. This paper conducts a comprehensive review of deep learning-assisted decision support for pulmonary nodule diagnosis, including the analysis of data set characteristics and the induction, summary, and comparison of indicators of different model algorithms on various data sets. Computer-aided diagnosis technology has laid a certain foundation for the research on intelligent diagnosis and prognosis of pulmonary nodules.",Deep learning;Reviews;Computed tomography;Lung;Lung cancer;Computer aided diagnosis;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449233,IEEE Conferences,,,,,,
A2PAN: Anatomy-Aware Probe Attention Network for OCT Multi-Label Fluid Categorization,Q. Wu; R. Dan; S. Yang; L. Tu; X. Ji; X. Chen; S. Wang; X. Ye; Y. Wang,2023 IEEE Smart World Congress (SWC),########,2023,"Retinal swelling caused by fundus fluids is one of the most sight-threatening retinal abnormalities, which may lead to irreversible blindness. Common retinal diseases, such as diabetic macular edema (DME), can cause multiple fundus fluids, including intraretinal fluid (IRF) and subretinal fluid (SRF). Hence, highly accurate and automatic diagnosis systems are vital for modern digital healthcare centers and the progress of ubiquitous intelligence. Although extensive efforts have been made to develop deep-learning-based automatic diagnosis systems under the modal of optical coherence tomography (OCT), complex retina anatomies and fine-grained fundus fluid spatial features still significantly challenge the existing methodsâ€™ clinical applications. Therefore, we proposed a novel Anatomy-Aware Probe Attention Network (A2PAN) for OCT multi-label fundus fluid categorization. Specifically, we designed an Anatomy-Aware Probe Attention (A2PA) module to probe and capture multi-scale anatomy features. Moreover, we proposed an Anatomy Attention Consistency Loss (A2C-Loss) to promote modeling retinal anatomy structures and spur the recognition of multiple fundus fluids in a simple yet effective paradigm. Extensive quantitative results and qualitative analysis of our dataset demonstrate the effectiveness of our A2PAN and its related clinical prospect. Furthermore, our A2PAN outperforms several state-of-the-art networks dedicated to multi-label medical image categorization on a publicly available fluid dataset. The code will be available at: https://github.com/wpppc/A2PAN.",Fluids;Retina;Feature extraction;Electronic healthcare;Probes;Task analysis;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448801,IEEE Conferences,,,,,,
Aggregating Dual Attention Residual Network and Convolutional Sparse Autoencoder to Enhance the Diagnosis of Alzheimerâ€™s Disease,S. Jing; R. Li; S. Mu; S. Shan; L. Li; J. Li; Y. Sun; X. Cui,2023 International Conference on Human-Centered Cognitive Systems (HCCS),########,2023,"For the diagnosis of Alzheimerâ€™s disease, we proffer a pioneering computer-assisted methodology, the salient innovations of our approach are threefold: 1) A dual attention residual deep neural network has been proposed to capture localized features inherent in MR images; 2) A convolutional sparse autoencoder, fortified with dual hidden strata, was devised to extract both global and spatial intelligence from PET images, thereby bolstering diagnostic efficacy; 3) An innovative multi modal framework has been introduced, which integrates MRI and PET images into a unified network architecture for end-to-end learning. Pursuant to rigorous experimental evaluations on the publicly accessible dataset from the ADNI, empirical results underscore that our propounded multi-feature fusion model attains an impressive classification accuracy, registering at 95.12%.",Technological innovation;Magnetic resonance imaging;Network architecture;Feature extraction;Alzheimer's disease;Residual neural networks;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452464,IEEE Conferences,,,,,,
User Friendly Web Based Deep Learning System for TB and Pneumonia Diagnosis in Developing Countries,M. E. Dirie; H. A. Ali; A. A. Mohamed; S. D. Siyad; S. A. Ali; S. A. Kahie,2023 2nd International Conference on Multidisciplinary Engineering and Applied Science (ICMEAS),########,2023,"Tuberculosis (TB) and pneumonia are two of the leading causes of death and disability worldwide. Both diseases are preventable and treatable, but early diagnosis and treatment are crucial, especially in developing countries. Chest X-ray (CXR) is the most widely used imaging modality for diagnosing TB and pneumonia, but it is time-consuming and subjective to interpret. Deep learning is a subfield within the domain of machine learning that uses artificial neural networks as its primary computational framework for acquiring knowledge from data. Deep learning models have demonstrated efficacy in a range of medical imaging applications, encompassing the accurate detection of tuberculosis (TB) and pneumonia. This paper proposes a deep learning-based system for accurately and efficiently diagnosing TB and pneumonia from CXR images using the VGG19 architecture. The system was trained and evaluated on a large dataset of CXR images from patients with TB, pneumonia, and normal cases, achieving an accuracy of 99%. The authors also evaluated the performance of eight different deep learning algorithms for the classification of normal and abnormal CXR images. The VGG19 algorithm achieved the highest accuracy (99%), followed by DenseNet121 (98%) and Inception V3 (97%). The system is user-friendly and accessible through a web interface, making it accessible to healthcare professionals in all settings, including developing countries. The suggested method has the potential to greatly enhance TB and pneumonia detection and treatment., especially in developing countries. By automating the image analysis process and improving diagnosis accuracy, the system can help reduce mortality and morbidity associated with these diseases",Deep learning;Tuberculosis;Pulmonary diseases;Developing countries;Classification algorithms;X-ray imaging;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10429832,IEEE Conferences,,,,,,
Exploring CNNs for Blood Subtype Recognition: Binary and Multi-Class Image Classification,P. Shourie; V. Anand; R. Chauhan; H. S. Pokhariya; S. Gupta,"2023 3rd International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)",########,2023,"Blood cell subtype classification accuracy is essential for the diagnosis of many illnesses and ailments. This paper introduces a novel methodology for the classification of multi and binary subtypes of blood cells through the utilization of Convolutional Neural Networks (CNNs). The principal aim of this study is to provide a resilient and effective framework for the automated categorization of blood cell subtypes, with the intention of aiding healthcare practitioners in delivering prompt and precise diagnoses. In order to tackle the issue of multi-class classification, a CNN architecture is devised to proficiently acquire distinguishing characteristics from the unfiltered visual data and subsequently associate them with distinct subtypes of blood cells. The classification scenario of binary subtypes, emphasizing crucial distinctions has also been undertaken. For this, the model is being modified to the CNN structure and engaged in fine-tuning the model to augment its capacity to discern variations in the features of blood cells. The study introduces a resilient and adaptable CNN architecture for the categorization of both multi and binary subtypes of blood cells. The technique exhibits a notable level of precision and holds significant promise for practical medical applications, rendering it a useful asset within the realm of medical imaging and diagnostics. of many metrics, including accuracy, precision, recall, and F1-score, which serve to demonstrate its capacity to deliver dependable and consistent outcomes.",Adaptation models;Microprocessors;Collaboration;Computer architecture;Convolutional neural networks;Task analysis;Blood,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442491,IEEE Conferences,,,,,,
Improved Brain Tumor Classification Made Possible Through the Use of Transfer Learning Model ResNet50,V. Tanwar; V. Anand; R. Chauhan; D. Rawat,"2023 3rd International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)",########,2023,"This study gives a detailed assessment of the performance of a classification model in the context of brain tumor classification, employing a dataset consisting of four separate classes: glioma tumors, meningioma tumors, normal brain tissue, and pituitary tumors. The results of this evaluation are presented in the form of a comparison. The model, which was constructed using transfer learning using the architecture of ResNet50, has been subjected to a stringent evaluation based on accuracy, recall, and F1-score for each class. The findings reveal high classification skills, with considerable accuracy, recall, and F1-Score value distributions across all classes, suggesting successful tumor type discrimination. The model's ability to accurately classify brain tumors is shown by the high overall accuracy score of 91%. In addition, macro and weighted averages provide additional evidence that the model's performance is well-balanced when taking into account class distribution. These discoveries contribute to the progress of medical image analysis, which promises more accurate and quick detection of brain cancers, leading to improvements in patient care.",Image analysis;Transfer learning;Computer architecture;Brain modeling;Medical diagnostic imaging;Tumors;Residual neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442138,IEEE Conferences,,,,,,
Enhancing Haemorrhage Detection in Head CT Scans Using Deep Learning,A. Kumar; L. Nelson; S. Kumar,"2023 3rd International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)",########,2023,"This work investigates the application of deep learning for haemorrhage detection in head CT scans. The aim of this work is to develop a robust model for accurate detection, even with limited data. The Sequntional CNN model is utilised to detect the haemorrhage CT scans, consisting of convolutional layers to extract features, dense layers for classification, pooling layers for dimensionality reduction, and dropout layers for regularisation. This model uses the data augmentation technique to enhance the training data through transformations like rescaling, shearing, and rotation. This model mitigates overfitting and enhances the overall robustness of the model. Moreover, this model provides significant improvement in accuracy, reaching 94.99%. This performance highlights data augmentation, enhancing the generalisation capacity of the model. The developed model outperforms other deep learning models used in similar applications. This work highlights the adaptability of deep learning in medical image analysis and its potential to enhance healthcare applications, with implications for improved patient outcomes.",Deep learning;Adaptation models;Head;Computed tomography;Medical services;Data augmentation;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442342,IEEE Conferences,,,,,,
Precision Kidney Disease Classification Using EfficientNet-B3 and CT Imaging,R. Singh; N. Sharma; R. Chauhan; A. Choudhary; R. Gupta,"2023 3rd International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)",########,2023,"Kidney disorders are a prominent issue in global health, and the quick and precise identification of these conditions is crucial for the optimal care of patients. This study introduces an innovative methodology for the categorization of kidney diseases by using advanced deep learning algorithms. In this study, we conducted fine-tuning of the EfficientNet-B3 architecture using a large dataset of CT kidney images. The dataset consisted of four unique classes, namely Normal, Cyst, Tumour, and Stone. The dataset exhibits a wide range of variations, enabling a thorough evaluation of kidney disorders often seen in the context of clinical practice. Through the use of this dataset for model training, a notable classification accuracy of 95.77% was attained, hence showcasing the efficacy of our methodology in discerning between distinct kidney diseases. The considerable practical value of the model's high accuracy lies in its capacity to accelerate the diagnostic process and provide valuable assistance to healthcare professionals in making well-informed judgements. The present study work offers a comprehensive description of the methods used, including data preparation, model design, and the fine-tuning procedure. Moreover, it explores the significance of interpretability and model explain ability within the medical field. In addition, we provide a comparative study of our fine-tuned EfficientNet-B3 model in comparison to other frequently used designs, emphasizing its better performance. Moreover, this research highlights the significant importance of deep learning models in enhancing the diagnosis of kidney illness and their capacity to assist healthcare professionals in making more precise and prompt judgements, eventually resulting in improved patient outcomes. The obtained accuracy of 95.77% underscores the potential benefits of using deep learning models in clinical environments for the purpose of classifying and diagnosing renal diseases.",Deep learning;Computed tomography;Transfer learning;Kidney;Medical diagnostic imaging;Diseases;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442291,IEEE Conferences,,,,,,
A Fine-Tuned EfficientNet B5 Transfer Learning Model for the Classification of Knee Osteoarthritis,R. Singh; N. Sharma; D. Upadhyay; S. Devliyal; R. Gupta,"2023 3rd International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)",########,2023,"This comprehensive investigation explores the complex realm of knee osteoarthritis (OA), a widespread and incapacitating joint condition that affects a substantial segment of the populace. Osteoarthritis of the knee is a prominent contributor to physical discomfort, limited mobility, and a decline in overall well-being, predominantly affecting the older population. This study utilizes state-of-the-art technology, namely the EfficientNet B5 Transfer Learning Model, to detect and classify knee osteoarthritis at an early stage. The study carefully assembles a meticulously balanced dataset consisting of three distinct categories: â€œHealthy,â€ â€œModerate,â€ and â€œSevere.â€ The dataset serves as the fundamental basis for both training and evaluating models, thereby guaranteeing the development of a classification system that is both robust and unbiased. The findings demonstrate a noteworthy improvement in the performance of the model throughout the training process, as evidenced by a gradual decrease in losses and a corresponding increase in accuracy. The model exhibits a notable overall accuracy rate of 97% in the classification of knee osteoarthritis (OA) cases, highlighting its efficacy in the diagnosis of this ailment. This study highlights the significant significance of precise diagnosis and treatment of knee osteoarthritis (OA), particularly among the elderly demographic, to improve their overall quality of life. This research makes a valuable contribution to the current endeavors aimed at reducing the consequences of knee osteoarthritis. By utilizing advanced technology and employing Deep learning techniques, the study explores potential strategies for identifying and addressing the condition at an early stage.",Deep learning;Training;Transfer learning;Sociology;Osteoarthritis;Task analysis;Statistics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442712,IEEE Conferences,,,,,,
Segmentation Techniques for Detection of Tuberculosis Using Deep Learning: A Review,T. Bansal; S. Gupta; N. Jindal,"2023 3rd International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)",########,2023,"Among many uses of image segmentation are scene understanding, image analysis for medical purposes, robotic understanding, video monitoring, augmented reality (AR), and compression of images. There are many different image segmentation computational methods available. Due to growing popularity of models using deep learning in vision applications, many current efforts have focused on advancing deep learning-based methods for image segmentation. This survey provides a state-of-the-art overview of state-of-the-art in field of segmentation regarding Tuberculosis (TB), including a wide range of ground-breaking computational models such as Unet, Vnet, SegNet, and Fully Convolutional Neural Networks (FCN). X-rays of the lungs are the primary method used by medical professionals in previous research for diagnosing tuberculosis (TB). Mycobacterium tuberculosis has been infectious agent that causes tuberculosis (TB), condition that can spread from person to person. Around the world in year 2019, tuberculosis was responsible for mortality of almost 1.4 million people. Using deep learning algorithms for classification has improved TB detection accuracy nearly comparable with that of a human doctor. When applied to lung segments rather than the full X-ray, techniques for classification improve the likelihood of detecting tuberculosis.",Deep learning;Image segmentation;Tuberculosis;X-rays;Classification algorithms;Convolutional neural networks;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442736,IEEE Conferences,,,,,,
Automated Lung Size Estimation in Chest X-Ray Images Using deep learning,B. S. Penugonda; A. Koganti; A. Unnam; S. Chinnadurai,2023 IEEE 20th India Council International Conference (INDICON),########,2023,"Chest X-Rays (CXRs) are the most performed radiological procedure, accounting for roughly one-third of all radiological procedures. These images are used to study various structures such as the heart and lungs to diagnose diseases like lung cancer, tuberculosis, and pneumonia. Anatomical structure segmentation in chest X-rays is a critical component of computer-aided diagnostic systems. The measurements of irregular shape and size and total lung area can provide insight into early signs of life-threatening conditions such as cardiomegaly and emphysema. Lung segmentation is a challenge due to variance caused by age, gender, or health status; it becomes even more difficult when external objects like cardiac pacemakers, surgical clips, or sternal wire are present. As a result, accurate lung field segmentation is regarded as an important task in medical image analysis. A comparison of the efficacy of two deep-learning algorithms to detect lung-related pathologies via an investigation into the size of the lungs is enumerated herein. Utilizing X-ray images and the accompanying masks, Deep Learning Models were employed to predict the lung masks respective to the X-Ray Images with an exceptional level of accuracy achieved by one of the Deep Learning models at a 99.64%, determining the lung condition if it is normal or abnormal by calculating the sizes of the lung mask.",Deep learning;Image segmentation;Wires;Lung;Predictive models;Size measurement;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440878,IEEE Conferences,,,,,,
Counter-CAM : An Improved Grad-CAM based Visual Explainer for Infrared Breast cancer Classification,K. Raghavan; S. B; K. V,2023 IEEE 20th India Council International Conference (INDICON),########,2023,"Understanding the decision-making process of machine learning models is essential for establishing confidence and interpretability. Counter-CAM combines the power of Grad-CAM and counterfactual explanations to provide intuitive and comprehensive insights into model decisions. Counter-CAM enables end users to comprehend why a model makes particular predictions by displaying the critical regions and their modifications in counterfactual images. Initially, we use Grad-CAM to generate heatmaps that emphasize regions of an input image that are crucial for the modelâ€™s prediction. These heatmaps provide localized explanations but cannot demonstrate how modifications to these regions would affect the modelâ€™s conclusion. To overcome this limitation, we incorporate counterfactual explanations, demonstrating how minor image changes can result in a different model prediction. By superimposing Grad-CAM on counterfactual images, we generate Counter-CAM. This visual representation juxtaposes the significance of various regions in the original image with their influence on the modelâ€™s decision in the counterfactual image. We demonstrate Counter-CAMâ€™s ability to provide intuitive and visual explanations for model predictions by validating its efficacy on multiple datasets and demonstrating its capacity to provide intuitive and visual explanations for model predictions. Counter-CAM improves the interpretability and explainability of machine learning models, enabling end users to comprehend and trust the decision-making procedure.",Visualization;Cogeneration;Decision making;Machine learning;Predictive models;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440898,IEEE Conferences,,,,,,
COVID-19 Detection from Chest X-ray Images Using Deep Learning Approach,A. Bushra; M. A. Habib; M. J. H. Jahid,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Lung diseases has multiplied in recent years and now kills millions of people every year. An effective, trustworthy, and reasonably priced method of diagnosing lung diseases has become crucial to addressing the epidemic. In addition to the less accurate traditional technique of identifying lung illnesses, computer-aided technology can be a very helpful addition for the automatic diagnosis and categorization of lung diseases such COVID-19, pneumonia, and lung cancer from the chest X-ray. To enhance the performance of automated lung disease (COVID-19 and Pneumonia) detection model, a CNN based model is proposed for the classification of COVID-19, normal and Pneumonia from X-ray images. For detecting COVID-19, biomedical imaging exams such as CT scans or CXR images can be used. As the chest X-ray images are more widespread, faster and cheaper, datasets containing COVID positive, Normal and Pneumonia is being used in this work. Furthermore, to extract information from the lung region, lung segmentation method is being implemented. Thus, this study aims to illustrate the effect of lung segmentation on the identification of pulmonary diseases, particularly COVID-19 based on CXR images. To segment the lung region from the plain CXR images, a U-Net CNN architecture has been proposed. For classification, several transfer learning Convolutional Neural Network (CNN) such as VGG19, DenseNet121, ResNet50, InceptionV2 and Xception were used. The models with their extensive filter family, abstraction and weight distribution capabilities can automatically reveal the discriminating features of the X-ray images. Overall, the classification of COVID-19 from the lung segmented dataset achieved an accuracy of 86% for binary and 79% for ternary classification. The potential of image segmentation in medical imaging, particularly in the identification of COVID-19 from radiological images, is demonstrated in this work.",COVID-19;Image segmentation;Pneumonia;Computational modeling;Lung;Lung cancer;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441034,IEEE Conferences,,,,,,
"An Approach to Detect Cardiomegaly, COVID-19, Pneumonia, Pneumothorax and Tuberculosis from CXR Images Using Ensembles of Deep Learning",T. Muthaki; S. I. Masuk; A. Maksud; M. A. H. Rafi; N. Sakib,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Recent advancements in medical image analysis have harnessed the potential of deep learning, especially in diagnosing thoracic ailments using chest X-ray (CXR) images. This study introduces a novel approach to detecting a range of thoracic diseases, including COVID-19, pneumonia, pneumothorax, cardiomegaly, and tuberculosis, through ensembles of deep learning models. The development of a reliable and efficient diagnostic system is crucial given the increasing significance of early disease detection and the growing number of CXR images in medical databases. The proposed ensemble method combines features of multiple deep learning models to improve overall performance, leveraging their distinct strengths. The study includes extensive experimentation and comparative analyses on a diverse and extensive dataset to highlight the superior performance of this approach. The results demonstrate that the ensemble-based model outperforms individual models, achieving a higher accuracy of 98.37%, sensitivity of 0.9837, and precision of 0.9836, thus presenting significant potential for robust and multi-disease detection in CXR images. Our results hold promise for advancing healthcare and clinical decision-making, underlining the valuable contribution of this research. This work not only underscores the potency of deep learning in medical imaging but also highlights its pivotal role in elevating patient care.",Deep learning;COVID-19;Pneumonia;Sensitivity;Tuberculosis;X-ray imaging;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441082,IEEE Conferences,,,,,,
Automated Multi-Class Brain Tumor Classification for MRI Image Using Transfer Learning,A. Hasan; R. Toufiq; M. Z. Islam,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Recently, brain tumor detection has become crucial for effective treatment to enhance the lives of those suffering humans. Our research is centered around achieving a primary objective is fine-tuning pre-existing transfer learning models, making them well-suited and optimized for our specific research context, conducting a comprehensive comparative analysis of our implemented models, and strategically selecting the most optimal transfer learning model based on the outcomes of our comparative evaluations. Regarding medical Imaging and training datasets tend to be tiny, making it difficult to use deep learning and train a CNN from scratch. The primary obstacle in MRI image categorization is the semantic gap that exists between machine-acquired low-level visuals and human-interpreted high-level features. Addressing the global shortage of ophthalmologists, this research strives to automate the classification of gliomas, meningiomas, pituitary tumors, and normal brain MRI images. For this reason, six pre-trained transfer learning modelsâ€”VGG16, ResNet-50, Inception-ResNet-v2, ResNet-101, Xception and EfficientNetB0â€”are considered in this work and propose EfficientNetB0 as the best transfer learning model to classify brain tumor on MRI images. A dataset of 3264 detailed brain MRI images included glioma tumors (926 images), meningioma tumors (937 images), pituitary tumors (901 images), and non-tumor cases (500 images), as well as a combined dataset comprising MRI, CE-MRI, and Br35H datasets that contained 7022 detailed brain MRI images featuring glioma tumors (1621 images), meningioma tumors (1645 images), pituitary tumors (1757 images), and non-tumor cases (2000 images). Data augmentation techniques mitigate overfitting while enhancing dataset diversity. Notably, the optimized EfficientNetB0 architecture exhibited the highest accuracy (up to 96.33%) for Dataset-I, and for multi-class classification, InceptionResNetv2 also excelled with the best accuracy (up to 99.16%) for Dataset-II.",Analytical models;Visualization;Magnetic resonance imaging;Transfer learning;Brain modeling;Tumors;Context modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441355,IEEE Conferences,,,,,,
Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification,A. Mustari; R. Ahmed; A. Tasnim; J. S. Juthi; G. M. Shahariar,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"This paper proposes an efficient system for classifying cervical cancer cells using pre-trained convolutional neural networks (CNNs). We first fine-tune five pre-trained CNNs and minimize the overall cost of mis-classification by prioritizing accuracy for certain classes that have higher associated costs or importance. To further enhance the performance of the models, supervised contrastive learning is included to make the models more adept at capturing important features and patterns. Extensive experimentation are conducted to evaluate the proposed system on the SIPaKMeD dataset. The experimental results demonstrate the effectiveness of the developed system, achieving an accuracy of 97.29%. To make our system more trustworthy, we have employed several explainable AI techniques to interpret how the models reached a specific decision. The implementation of the system can be found at - https://github.com/isha-67/CervicalCancerStudy.",Visualization;Costs;Sensitivity;System performance;Self-supervised learning;Cervical cancer;Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441352,IEEE Conferences,,,,,,
Comparative Study Between ResNet And EfficientNet Family For Classification of Leukemia,T. Ibtekar; M. R. Haque; A. Y. Srizon,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Leukemia, a prevalent hematological malignancy, presents significant challenges in diagnosis and classification. Traditional diagnostic methods, often reliant on microscopic examination, suffer from subjectivity and inconsistency. In an era where timely and accurate diagnosis is critical, identifying the most effective deep learning architecture for leukemia classification becomes imperative. This paper addresses this urgent need by conducting a systematic comparative study between two leading deep learning architectures, ResNet and EfficientNet. Utilizing a generalized method architecture for both models, the study aims to discern which architecture offers a superior balance of accuracy, computational efficiency, and clinical adaptability. EfficientNetB5, the top-performing model in our study, achieved an impressive accuracy of 95.87% and an ROC area of 0.94, underscoring its robustness and reliability in leukemia classification. Performance evaluation metrics, including F1-Score, Recall, Precision, Accuracy, are utilized to conduct a thorough assessment of the effectiveness of these architectural models. By providing a transparent and replicable research process, this study not only contributes to the broader discourse on methodological rigor in deep learning research but also offers a standardized approach for leukemia classification. The findings are intended to guide future research and clinical applications, thereby addressing a critical challenge in healthcare.",Performance evaluation;Adaptation models;Systematics;Computational modeling;Microscopy;Computer architecture;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441438,IEEE Conferences,,,,,,
A Federated Learning Approach to Bone Metastasis Prediction Using Convolutional Neural Network,S. M. Ali; S. Ibne Eunus; T. A. Bushra; M. Humaion Kabir Mehedi; A. A. Rasel,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Bone metastasis is a frequently occurring disease and can be a consequence of a number of different cancers such as - prostrate, lung and breast cancers, and predicting them can be really useful for the diagnosis of patients with such diseases. Classifying images of bone scan for bone metastasis prediction requires a huge amount of data to produce a prediction output which is reliable and accurate, but a single medical organization usually do not have access to such amounts of data from other organizations and those organizations are not also ready to share their patientsâ€™ private data as well due to data security issues. For such scenarios, it is not often possible to train a model with enough data, thus leading to an inaccurate prediction model for bone metastasis. This can be devastating at times due to the occurrence of many false positives or false negatives, if bone metastasis is wrongly classified. In order to find a better solution, so that there is less data protection and privacy issues and therefore more availability of data, we are proposing to use a Federated Learning (FL) based approach for bone metastasis prediction using convolutional neural network. As per our knowledge and background study, we are the first to use federated learning for bone metastasis prediction on the BS-80K dataset. Federated Averaging (FedAvg) strategy was used for implementing the federated learning methodology where different client models were built along with a Global Model.",Federated learning;Organizations;Predictive models;Bones;Data models;Metastasis;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441154,IEEE Conferences,,,,,,
"SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification",S. M. N. Ashraf; M. A. Mamun; H. M. Abdullah; M. G. R. Alam,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Chest X-rays are widely used to diagnose thoracic diseases, but the lack of detailed information about these abnormalities makes it challenging to develop accurate automated diagnosis systems, which is crucial for early detection and effective treatment. To address this challenge, we employed deep learning techniques to identify patterns in chest X-rays that correspond to different diseases. We conducted experiments on the ""ChestX-ray14"" dataset using various pre-trained CNNs, transformers, hybrid(CNN+Transformer) models, and classical models. The best individual model was the CoAtNet, which achieved an area under the receiver operating characteristic curve (AUROC) of 84.2%. By combining the predictions of all trained models using a weighted average ensemble where the weight of each model was determined using differential evolution, we further improved the AUROC to 85.4%, outperforming other state-of-the-art methods in this field. Our findings demonstrate the potential of deep learning techniques, particularly ensemble deep learning, for improving the accuracy of automatic diagnosis of thoracic diseases from chest X-rays.",Deep learning;Predictive models;Transformers;Vectors;X-ray imaging;Biomedical imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441433,IEEE Conferences,,,,,,
Privacy-Preserving Knee Osteoarthritis Classification: A Federated Learning Approach with GradCAM Visualization,R. H. Rifat; A. Chakraborty Shruti; M. Kamal; M. G. Rabiul Alam,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Knee Osteoarthritis (KOA) poses a significant global health challenge, impacting a substantial population. The conventional detection process involves multiple tests and meticulous examination by experienced physicians, which is time-consuming and susceptible to misclassification due to subtle variations in X-ray images. Additionally, privacy concerns hinder the sharing of sensitive data like X-rays. This study employs Federated Learning with pre-trained architectures (DenseNet-169, Inception-v2, and MobileNet-v2) to classify three KOA severity grades, utilizing two clients to ensure data privacy. The aim is to develop a generalized model for disease classification, improving efficiency while ensuring the confidentiality of patient information. DenseNet-169 excelled with an F1 score of 81% and an accuracy of 82%, while Inception-v2 and MobileNet-v2 performed well with slight F1 score variations. Moreover, the exploration of GradCAM visualization techniques is conducted to improve interpretability, highlighting the capability of this approach to effectively tackle the intricate challenges linked to Knee Osteoarthritis detection.",Data privacy;Federated learning;Sociology;Medical services;Osteoarthritis;Statistics;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441001,IEEE Conferences,,,,,,
Deep Learning-Based Novel Image Noise Classification Model â€˜MobileNoiseNetâ€™,M. F. H. Shiblee; M. F. A. Limon; M. S. Iqbal,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Image noise refers to random fluctuations in pixel intensity values caused by low light, high ISO settings, sensor limitations, compression, transmission errors, imaging sensor defects, etc., which distort the visual content and reduce the overall quality and clarity of the image. Therefore, understanding the types and sources of noise in an image is important for selecting the appropriate noise reduction techniques (filtering) and improving the overall quality of the image. Filtering is an important technique in image processing that is used to enhance the quality of images by removing unwanted noise or artifacts. However, distinguishing between diverse image noises is challenging, especially since various noise types can coexist within the same domain. For instance, MRI images might exhibit Salt and Pepper (SP), Speckle, Gaussian, Erlang, and Rayleigh noises. Therefore, the development of a mechanism to categorize these various forms of noise holds considerable significance. In this research, two deep learning-based models, MobileNoiseNet1 and MobileNoiseNet2, have been introduced. Both models are trained using the MobileNet architecture, departing from conventional transfer learning methods. Their role is to classify nine distinct types of image noise. Notably, our proposed models attain remarkable classification accuracy. In the validation set, the overall accuracy of both models stands at 94% and 92%, and this performance is further heightened in a distinct test set, achieving an impressive accuracy of 95% and 96%, respectively. This surpasses the capabilities of previous models in this domain.",Training;Filtering;Computational modeling;Biological system modeling;Magnetic resonance imaging;Noise level;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441272,IEEE Conferences,,,,,,
An Attention-Based Deep Learning Approach to Knee Injury Classification from MRI Images,K. D. Nath; A. F. M. M. Rahman; M. A. Hossain,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Knee injuries, prevalent in athletic and aging populations, pose significant challenges to healthcare professionals due to their complex nature and the critical function of the knee joint. Early and accurate diagnosis is paramount to ensure effective treatment and minimize long-term complications. Traditional diagnostic methods, including physical examinations and imaging techniques like MRI, require expert interpretation and can sometimes be inconclusive. This study introduces an approach to knee injury classification using deep learning techniques by leveraging convolutional neural networks (CNNs) with Attention Mechanism. This research work integrates powerful feature extraction capabilities of CNN and feature refinement of attention mechanism for the binary and multi-class classification of knee MRI images, with the aim of accurately identifying specific knee injury types. Based on our experiment on two comprehensive knee MRI datasets, our custom CNN model achieved 88% testing accuracy on Dataset-1 (Binary classification) and 77% accuracy on Dataset-2 (Multi-class classification). Meanwhile, the Attention-based CNN model achieved 100% accuracy on Dataset- 1 (Binary Classification) and 91% accuracy on Dataset-2 (MultiClass Classification). This approach not only holds promise for enhancing diagnostic accuracy but also for reducing the time to diagnosis.",Deep learning;Magnetic resonance imaging;Computational modeling;Computer architecture;Feature extraction;Convolutional neural networks;Injuries,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441340,IEEE Conferences,,,,,,
Enhanced Brain Tumor Classification from MRI Images Using Deep Learning Model,A. B. Rahman; M. Touhid Islam; M. R. Islam; M. Sohrawordi; M. N. Sultan,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"In the realm of image classification, traditional algorithms, encompassing both machine learning and deep learning, grapple with formidable challenges arising from uneven pixel ranges and dimensionality reduction. This results in a significant impediment to achieving accurate image categorization. Numerous examples of such traditional methods, including KNN, Random Forest, SVM, DNN, CNN etc, have encountered persistent issues such as inefficient performance of feature engineering, limited accuracy, etc. In response to these challenges, this paper introduces a novel image classification method that integrates pixel mapping, DWT, and CNN for improved efficiency and reliability. By resolving irregular pixel ranges through initial pixel mapping, our method establishes uniformity as a foundation for subsequent image analysis. Subsequently, DWT is employed to dissect and reduce image dimensionality, extracting essential features while lowering computational complexity. This two-step preprocessing approach forms a robust foundation for effective data classification. Within this framework, our proposed CNN architecture plays a pivotal role, utilizing both spectral and spatial information to address image categorization challenges. The networkâ€™s capacity to learn complex patterns enhances classification accuracy. In extensive evaluations, our methodology surpasses conventional classification techniques, yielding impressive results. With an Overall Accuracy (OA) of 96.9% and a Kappa statistic of 95.16%, our method showcases excellence and practical potential. These compelling achievements underscore the significance of our approach in tackling image classification challenges, paving the way for enhanced precision and efficiency across various domains.",Dimensionality reduction;Deep learning;Transfer learning;Feature extraction;Discrete wavelet transforms;Image classification;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441064,IEEE Conferences,,,,,,
Brain Tumor Detection and Classification Using Hybrid VGG Network,A. Sakib; N. J. Khan; L. Shahrier; K. Plabon; J. Al Mahmud; M. F. Mridha,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"In their highest degree, brain tumors can be extremely deadly. Misdiagnosis can lead to the incorrect course of treatment and lower the likelihood of recovery for patients. To tackle these problems, a hybrid VGG network is extensively researched in the proposed framework to classify brain tumors, including meningioma, pituitary, glioma, and healthy brain MRI slices. In this work, we propose a hybrid model for brain tumor classification. We made use of a publicly available Kaggle brain tumor dataset that included MRI scans showing no brain tumors and three different types of brain tumors: meningiomas, pituitary tumors, and gliomas. We performed preprocessing on the dataset. As our base model, we employed VGG16 and added more CNN layers to it. We trained the additional CNN layers while using transfer learning to freeze the base model layers. The pre-trained VGG16 model is familiar with a variety of textures, forms, and features related to medical imaging. We also added more CNN layers that discovered more complex and distinct task-related brain tumor pattern features that might not have been present in the original ImageNet dataset. We obtained 96.52% test accuracy with our model.",Training;Computational modeling;Magnetic resonance imaging;Transfer learning;Brain modeling;Task analysis;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441065,IEEE Conferences,,,,,,
Spatial Attention-guided Deep Learning for Accurate Kidney Disease Classification in CT Scans,M. N. Islam; M. Al Mamun; M. F. Faruk; A. Y. Srizon; S. M. M. Hasan; B. Roy,2023 26th International Conference on Computer and Information Technology (ICCIT),########,2023,"Kidney diseases are globally prevalent and often lead individuals to seek urgent medical attention due to severe discomfort. The timely identification of these conditions necessitates a reliable automated diagnostic approach. Imaging techniques are integral in the diagnosis of kidney diseases, frequently requiring specialized expertise for comprehensive interpretation. Leveraging deep learning methodologies for kidney disease detection can offer valuable support to medical practitioners during diagnosis. This investigation delved into the categorization of kidney conditions based on CT scan images encompassing four distinct classes: cysts, normal, stone, and tumor. To facilitate this, a deep learning framework with spatial attention was introduced. Spatial attention modules were seamlessly integrated within Convolutional Neural Network (CNN) architecture to bolster interpretability, offering insights into influential regions guiding the modelâ€™s decisions and enhancing the modelâ€™s capacity to effectively process visual data, leading to improved classification performance. The proposed spatial attention-guided CNN model exhibited remarkable accuracy at 99.52%, accompanied by the precision, recall, and f1-score metrics of 99.54%, 99.52%, and 99.52% respectively. Impressively, the proposed framework surpassed the majority of recent comparable studies in terms of both accuracy and computational efficiency.",Deep learning;Computed tomography;Computational modeling;Convolutional neural networks;Kidney;Diseases;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441347,IEEE Conferences,,,,,,
Rethinking False Positives with Class Activation Maps: Can we define false positives without ground-truths?,E. Civil; S. A. Karatopak; A. H. Ornek,"2023 International Conference on Electrical, Communication and Computer Engineering (ICECCE)",########,2023,"In this study, we have used Class Activation Mapping (CAM) technique to improve the accuracy of our Gloves classification model. By extracting the CAM and computing its areas, we have created a mechanism that effectively reduces false positives. We have trained the ResNet18 model on 2000 images of hands with and without gloves, achieving 98.65% accuracy. We have used the last convolutional layer of the model to extract the CAM outputs using the Gradient-CAM technique. Then, we have taken the extracted areas into account to decide whether a hand exists in the given image. Our results show that this approach is effective in reducing false positive detections.",Thresholding (Imaging);Computational modeling;Surveillance;Robustness;Task analysis;Optimization;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442548,IEEE Conferences,,,,,,
Spinal Cord Disease Identification Using Transfer Learning Techniques,R. Mittal; V. Malik; S. B. Goyal; S. A. Yadav; A. P. Srivastava; A. Sankhyan,"2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)",########,2023,"Spinal cord diseases pose a significant challenge in the medical field due to their complex nature and diverse manifestations. Early and accurate diagnosis is crucial for effective treatment and management. With the advent of deep learning techniques, the field of medical image analysis has witnessed remarkable advancements. Transfer learning, a subset of deep learning, has gained prominence for its ability to leverage pre-trained models on new tasks with limited labeled data, making it ideal for medical image classification tasks. In this study, we propose a novel approach for the identification of spinal cord diseases using transfer learning techniques, specifically employing a modified ResNet (Residual Neural Network) architecture. In order to train incredibly deep networks while avoiding the vanishing gradient issue, ResNet makes use of a deep structure and skip connections. This modification aims to extract subtle features that are crucial for accurate disease classification. A huge collection of spinal cord pictures, including both diseased and healthy instances, is used to train the suggested model. The model's capability to generalize and correctly diagnose spinal cord disorders is enhanced by the use of a pre-trained ResNet backbone, which draws on information gathered from a wide variety of datasets.",Deep learning;Spinal cord;Transfer learning;Feature extraction;Task analysis;Medical diagnostic imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434575,IEEE Conferences,,,,,,
Alzheimer Disease Classification Using Deep Neural Network,R. Haque; M. B. Islam; G. Chhabra; S. Sharma; K. Kaushik; M. D. Hosen; R. H. Dip,"2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)",########,2023,"Alzheimer's disease is a complex neurodegenerative disorder with profound implications for individuals and healthcare systems. Early and accurate diagnosis is critical for effective intervention and treatment. This study explores the potential of deep learning algorithms to predict Alzheimer's disease stages using MRI segmentation data. Four distinct algorithms, namely MobileNet, CNN, DenseNet, and Inception V3, were evaluated for their performance in classifying AD stages. MobileNet emerged as the top-performing algorithm, followed by CNN, DenseNet, and Inception V3. The study's findings highlight the promise of utilizing deep learning techniques for early Alzheimer's disease detection. However, the study's shortcomings, such as the quantity of the dataset and the assessment limits, are acknowledged. Despite these limitations, the findings are encouraging, and show the promise of using cutting-edge technologies to improve the early diagnosis and care of Alzheimer's More research is needed to have been validated to improve these systems for better patient care for better treatment of Alzheimer's disease.",Deep learning;Neurological diseases;Magnetic resonance imaging;Prediction algorithms;Classification algorithms;Alzheimer's disease;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434852,IEEE Conferences,,,,,,
Enhancing Knee Osteoarthritis Severity Classification using Improved Efficientnet,A. Pandey; V. Kumar,"2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)",########,2023,"Knee Osteoarthritis (KOA), caused by the gradual degradation of joints resulting in discomfort and limited mobility, poses a significant healthcare challenge, emphasizing the importance of early detection for effective management. In this study, we present an improved EfficientNet-B0 based approach aimed at enhancing the accuracy of osteoarthritis (OA) severity classification. On publicly available dataset, the proposed model resulted in an accuracy rate of 69.74% which is better to other cutting-edge approaches.",Location awareness;Degradation;Refining;Medical services;Computer architecture;Osteoarthritis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434740,IEEE Conferences,,,,,,
Heart Guardian: A Model for Predicting Cardiovascular Diseases,N. Bansal; D. Garg,2023 9th International Conference on Signal Processing and Communication (ICSC),########,2023,"Heart disease is a major cause of mortality worldwide and timely detection and intervention being essential to enhancing patient health condition. Recently, deep learning techniques, have shown promising results in various healthcare applications. In this paper, we present a novel deep learning-based model for heart disease prediction that leverages a diverse range of risk factors, medical imaging data, and patient demographics. The proposed model utilizes a Convolutional Neural Network (CNN) for extraction of features from the medical images, which are then fused with the other risk factors and fed into a Multi-Layer Perceptron (MLP) for classification. Deep learning techniques are used to provide insights into the features contributing to the predictions, enhancing the interpretability and transparency of our model. The performance of the model is evaluated using the real-world data set of patients with and without heart disease and compare it to traditional risk assessment approaches. Our study adds to the expanding body of literature on deep learningâ€™s potential in the healthcare industry, highlighting the transformative impact of these models in improving heart disease prediction and enabling personalized interventions. Our findings show that the suggested model performs better than others in terms of accuracy, sensitivity, and specificity, outperforming traditional risk assessment methods.",Heart;Deep learning;Medical services;Predictive models;Risk management;Cardiovascular diseases;Graphical user interfaces,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441202,IEEE Conferences,,,,,,
Predicting Diseases Using Chest X-Ray Medical Images,L. K. Shisodiya; A. Chauhan; R. K. Nadesh; V. Kallil,2023 Innovations in Power and Advanced Computing Technologies (i-PACT),########,2023,"Pneumonia and Coronavirus are the key disease that has affected people rapidly in the last couple of years across the globe. Medical Practitioners confirm the diseases using chest X-ray medical images. Computed Radiology, Computer Aided Diagnosis has an edge over the traditional process as it helps to find the ailment at a primary stage, which saves lives at a growing scale day by day. To predict the chest diseases, deep learning methods are promising in predicting the type of chest disease with greater accuracy. In the proposed model to detect chest diseases, DenseNet, Image processing, and deep-learning classification techniques are used. The proposed methodology optimizes the output that includes the inclusion of augmentation layers, dataset filtering, etc., and results in achieving an accuracy of 95.96%.",COVID-19;Solid modeling;Pneumonia;Computational modeling;Medical diagnostic imaging;X-ray imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434700,IEEE Conferences,,,,,,
Explainable AI for Medical Imaging: Advancing Transparency and Trust in Diagnostic Decision-Making,R. PV; U. Shanmugam,2023 Innovations in Power and Advanced Computing Technologies (i-PACT),########,2023,"Osteoarthritis (OA) is an existing degenerative joint disease with a potential societal impact, requiring accurate and early diagnosis for effective treatment. In this work, we have deployed deep learning techniques, specifically ResNet50 and VGG16, to predict osteoarthritis from medical imaging data. Remarkably, our models produced an impressive accuracies of 91.0% and 89.5% respectively, showing the efficacy of CNN in OA classification. To further better the interpretability in the predictions of our model ,we used GradCam++ visualization, a technique that enabled us in generating heat maps that highlights the regions crucial for model decision-making. Optimizing GradCam++, we noticed an improvement in accuracy to 93.0%, reinforcing its value in refining model interpretability. The generated heat maps are used not only in understanding the model validation but also in providing clinicians with useful information regarding the areas of concern within the medical images. Our outcomes shows the ability of combining deep learning techniques with visualization tools like GradCam++ to enhance predictive accuracy and to offer trustable and interpretable results. The areas of concern in the heat maps throw light on the specific features influencing the model's decision. This combined method promises for advancing the field of medical image analysis, fostering more accurate and clinically applicable diagnostic tools for osteoarthritis and potentially other medical conditions.",Deep learning;Explainable AI;Decision making;Data models;Osteoarthritis;Heat maps;Residual neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434658,IEEE Conferences,,,,,,
Evaluating the Effectiveness of Synthetic Datasets for Dementia Diagnosis Using Deep Learning,A. Romitti; J. Shetty; P. Rao,2023 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),########,2023,"Early and accurate diagnosis of dementia can lead to better treatment of the disease and improve patients' quality of life. Advanced neuroimaging technologies such as magnetic resonance imaging (MRI) and deep learning hold promise for early and accurate dementia diagnosis. However, there is limited number of real-world MRI datasets for training deep-learning models to classify a patient's degree of dementia. Generative adversarial networks (GANs) are deep learning-based generative models that can generate synthetic data samples based on a real dataset's data distribution. They have been successfully used in clinical neuroimaging studies. In this work, we investigate how synthetic MRI images generated by GANs can improve the performance of deep learning models for accurately classifying the level of dementia (i.e., very mildly demented, mildly demented, moderately demented, and no dementia.) We trained a state-of-the-art deep learning model for image classification, namely, the Data-Efficient Image Transformer (DeiT) using a real-world MRI dataset along with synthetic MRI images generated by GANs. We combined real and synthetic images during training by varying the proportion of synthetic images in the training set. We evaluated the accuracy and F1-score of the trained DeiT models on real MRI images. Our results showed that DeiT can achieve good performance even with synthetic images in the training set. Hence, GANs can offer a promising solution to improving dementia diagnosis via deep learning especially when real data are scarce.",Training;Deep learning;Neuroimaging;Magnetic resonance imaging;Transformers;Dementia;Synthetic data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440719,IEEE Conferences,,,,,,
An Uncertainty-Aware Deep Learning-Based Model for COVID-19 Diagnosis,L. C. S. Reddy; R. Jayakarthik; A. Kiran; N. Sharma; S. Sharma; P. C. S. Reddy,2023 3rd International Conference on Mobile Networks and Wireless Communications (ICMNWC),########,2023,"The application of deep-learning(DL) to the processing of medical images has been highly successful. A few groundbreaking DL-based works have achieved substantial progress in computerized screening of COVID-19 using chest X-ray( CXR) pictures in the wake of the current epidemic scenario caused by SARS-CoV-2. While DL models have shown promise, they currently lack a crucial component for use in medical image analysis: the ability to communicate uncertainty in the strategies prediction. Therefore, in this paper, we create a design for automatically detecting COVID19 from CXR images using an uncertainty-aware convolutional-neural-network(UACNN) and estimate the uncertainty in its predictions. In the suggested method, an EfficientNetB3 model is tuned using CXR pictures, and then the model is used in conjunction with Monte Carlo (MC) dropout. The posterior predictive distribution has been obtained after M forward passes of inference using MC dropout. The anticipation and design uncertainty are then determined by calculating the mean and entropy of the generated anticipative distribution. The COVID19CXr, XR-pictures, and Kaggle CXR datasets are used to test the suggested approach. Multi-class classification on the COVID19CXr dataset was completed with an accuracy of 99.81% using the proposed UACNN approach. For the purpose of diagnosing COVID-19 cases from CXR pictures, our suggested methodology demonstrates its superiority over the existing methodologies.",COVID-19;Wireless communication;Training;Solid modeling;Uncertainty;Medical diagnostic imaging;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435818,IEEE Conferences,,,,,,
Breast Cancer Detection and Classification with Data-Augmented Ensemble Model,B. Srinivas; M. Sriram; V. Ganesan,2023 3rd International Conference on Mobile Networks and Wireless Communications (ICMNWC),########,2023,"Medical image classification plays a crucial role in modern healthcare, aiding in the early detection and diagnosis of various diseases. This paper introduces an advanced machine-learning model for the classification of medical images, specifically focusing on its application in the context of cancer detection. The proposed model, named MLM-EO (Multi-Level Morphological Extraction Optimization), integrates cutting-edge optimization techniques with machine learning algorithms to achieve highly accurate classification results. The study encompasses several key phases, including image segmentation, feature extraction, and classification. In the segmentation phase, MLM-EO demonstrates exceptional performance in accurately delineating target objects within medical images. The feature extraction process extracts meaningful information from the images, enhancing the model's ability to capture relevant patterns and characteristics. The classification results showcase the model's proficiency in categorizing medical images into â€œCancerâ€ and â€œNon-Cancerâ€ classes, with associated probability estimates. Comparative analysis with other classification methods reveals MLM-EO's superiority in terms of accuracy, precision, recall, and F1-Score. This research contributes to the ongoing efforts to advance medical image analysis and improve disease diagnosis. The MLM-EO model exhibits great promise as a valuable tool for healthcare professionals, offering the potential to enhance the accuracy and efficiency of cancer detection. Further validation and deployment in clinical settings are essential to fully unlock its potential and impact on patient care.",Image segmentation;Predictive models;Feature extraction;Cancer detection;Medical diagnostic imaging;Optimization;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435906,IEEE Conferences,,,,,,
Inception-Resnet V2 Based Eye Disease Classification Using Retinal Images,R. E. Varghese; I. A. Pandian,2023 3rd International Conference on Mobile Networks and Wireless Communications (ICMNWC),########,2023,"Eye diseases are a diverse group of ocular conditions that can affect the visual health and overall wellbeing of individuals. The early detection and classification of eye diseases hold profound significance in the realm of healthcare and ophthalmology. Accurate classification of eye diseases enables healthcare providers to tailor treatment plans, monitor disease progression, and allocate resources efficiently. Furthermore, it facilitates the development of predictive models and the identification of risk factors, paving the way for personalized medicine and targeted preventive strategies. This paper presents an effective eye disease classification model based on Inception Resnet V2 model with fine-tuning mechanism. The Inception-ResnetV2, known for its exceptional feature extraction capabilities, is pretrained on a large-scale dataset and subsequently fine-tuned on a curated dataset of labeled eye disease images. The proposed system provided significant performance after fine-tuning. The model had 81.00% accuracy, which remarkably increased to 94.74% after fine-tuning. The enhanced accuracy of the model reflects improved precision and reliability, with precision increasing from 79.25% to 93.99%, reducing false positives, and recall rising from 82.49% to 94.24%, thereby lowering false negatives. Consequently, the F1-Score, which combines precision and recall, improved significantly from 81.49% to 94.11%. Finally, the model classifies the retinal images into four classes (Cataract, Glaucoma, Diabetic Retinopathy, Normal). This paper represents a significant step towards developing a reliable and efficient tool for the automated diagnosis and early detection of eye diseases, ultimately enhancing patient care in the field of ophthalmology.",Glaucoma;Visualization;Medical services;Eye diseases;Retina;Feature extraction;Ophthalmology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435893,IEEE Conferences,,,,,,
Feature Set Segmentation Model to Detect Lung Tumor Size with Deep Learning Model,S. B; S. M; G. V,2023 3rd International Conference on Mobile Networks and Wireless Communications (ICMNWC),########,2023,"This electronic document is a â€œliveâ€ template and already defines the components of your paper [title, text, heads, etc.] in its style sheet. Accurate and timely detection of lung tumors are critical for early diagnosis and effective treatment planning. This paper presented a novel lung tumor detection, segmentation, and classification using the Median Probabilistic Feature Set Segmentation (MPFSS) method. The proposed framework leverages a combination of image processing, feature extraction, and deep learning techniques to address the complex challenges associated with lung tumor analysis in medical images. The MPFSS method demonstrates robust segmentation capabilities, accurately delineating tumor regions within lung images. Additionally, feature engineering is explored, highlighting the importance of selecting and engineering feature sets to enhance classification performance. Deep learning models are employed for classification, showcasing their effectiveness in achieving high accuracy and balanced precision and recall. Comparative analysis with traditional machine learning models further validates the superiority of the MPFSS approach. The proposed MPFSS model provides accurate lung tumor detection and classification, providing valuable insights for the field of medical image analysis and its potential impact on clinical diagnosis and treatment planning. The propsed model emerges as a strong performer, with balanced precision and recall, high F1-Scores, and an impressive ROC AUC.",Deep learning;Image segmentation;Analytical models;Lung cancer;Feature extraction;Medical diagnostic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435850,IEEE Conferences,,,,,,
Alzheimer's Disorder Identification from MRI Images with EfficientNetB0,P. Shourie; V. Anand; R. Chauhan; A. Garg; S. Gupta,2023 2nd International Conference on Futuristic Technologies (INCOFT),########,2023,"Alzheimer's disease has been defined as a progressive loss of mental function and is a neurodegenerative condition. For prompt management and better patient outcomes, it is essential to make an early and correct analysis of Alzheimer's illness. Models based on deep learning have recently demonstrated encouraging outcomes in medical image analysis applications, such as Alzheimer's disease sorting. Modern deep learning architecture EfficientNet has shown good precision on a number of computer vision applications. Proposed Research has created models for categorizing Alzheimer's ailment with neuroimaging data by utilizing EfficientNet capabilities. The model's higher performance is a result of its capacity to take advantage of the vast spatial and temporal data recorded in neuroimaging scans. This paper offers a unique method for precise and effective Alzheimer's disease classification using brain imaging data by utilizing the EfficientNet architecture. The results demonstrate the potential of deep learning models to enhance early detection and medical treatment plans for Alzheimer's disease patients.",Neuroimaging;Deep learning;Computational modeling;Magnetic resonance imaging;Brain modeling;Feature extraction;Alzheimer's disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425184,IEEE Conferences,,,,,,
Brain Tumor Classification Using ResNet-50,M. Aiswarya; G. A. E. S. Kumar; K. Sanjay; M. Raj Ratan,2023 2nd International Conference on Futuristic Technologies (INCOFT),########,2023,"One of the most common cancers that is impacting a lot of individuals is brain cancer. The illness now poses a threat to life. Early detection is important to make life-saving interventions. Magnetic Resonance Imaging (MRI) is a powerful tool for detecting various brain abnormalities and is widely utilized by radiologists and physicians. We have proposed a Deep Learning-based convolutional neural network technique to identify different types of brain cancers. The proposed model makes use of larger datasets with five classes (meningiomas, gliomas, pituitary, Neurocitoma, Schwannoma tumors). It is based on a Residual Network (ResNet-50), and it is designed with a specific architecture for identifying complex data features. This approach aids medical professionals in deciding if a disease is present or not. Based on Deep Learning technology, it provides a more precise, efficient, and quick way to identify brain tumors. The proposed approach achieves an accuracy of 99.08%. The experimental findings show how effective the suggested strategy for BT multi-class categorization is.",Neuroimaging;Magnetic resonance imaging;Brain cancer;Brain modeling;Servers;Tumors;Residual neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425007,IEEE Conferences,,,,,,
Eye Disease Classification Using ResNet-18 Deep Learning Architecture,G. Kaur; N. Sharma; R. Chauhan; S. Kukreti; R. Gupta,2023 2nd International Conference on Futuristic Technologies (INCOFT),########,2023,"The present study aims to investigate the crucial topic of automated categorization of eye diseases using medical photographs by utilizing the capabilities of the ResNet-18 model. The purpose of doing this research derives from the positive outcomes documented in recent studies that have employed ResNet-18 for comparable objectives. This study introduces a customized ResNet-18 model architecture explicitly designed for classifying eye illness images into four categories with significant medicinal properties. The methodology employed in this study utilizes a dataset consisting of 4,217 photos of eye diseases. The model was trained over 30 iterations, using a batch size 128 and default learning rates. The results demonstrate a noteworthy accomplishment, as the ResNet-18 model suggested in this study achieved a commendable accuracy rate of 94%. This highlights the model's efficacy in distinguishing between various eye illnesses, offering substantial enhancements in the precision and effectiveness of diagnostic protocols. The research findings are significant, as they lay the foundation for advancing automated systems that can efficiently and precisely identify eye disorders. This has the potential to bring about a transformative impact on the field of ophthalmic diagnostics and enhance the quality of patient treatment.",Training;Protocols;Transforms;Eye diseases;Numerical models;Medical diagnosis;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425690,IEEE Conferences,,,,,,
Using Deep Learning and MobileNet50V2 CNN Model to Classify Chest X-Ray Images for Pneumonia Disease Detection,K. S. Gill; V. Anand; R. Chauhan; D. Rawat; R. Gupta,2023 2nd International Conference on Futuristic Technologies (INCOFT),########,2023,"Pneumonia is a pathological condition characterised by the inflammation of the alveoli in either one or both lungs. Symptoms including productive cough, fever, chills, and dyspnea may develop when fluid or purulent material builds up within the air sacs. Many different kinds of organisms, such as bacteria, viruses, and fungi, may cause pneumonia. Pneumonia may range in intensity from quite mild to severe. The population groups most vulnerable to this condition are newborns and young children, those aged 65 and above, as well as those with underlying health conditions or compromised immune systems. This research focuses on the categorization of pneumonia sickness by using the MobileNet50V2 Model and exploiting chest X-ray images. The primary objective is to enhance the accuracy of the classification process. The use of artificial intelligence in the medical sector is seen as a significant and auspicious application. The research further proposes strategies to augment the current practises used by the medical community. It is important to consider that the development of a pneumonia classification model necessitates the integration of deep learning techniques, medical imaging expertise, and domain knowledge proficiency. In order to ensure the clinical validity and safety of the model, it is essential to engage in collaborative efforts with medical professionals.",Deep learning;Pediatrics;Pulmonary diseases;Data models;Task analysis;Medical diagnostic imaging;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425642,IEEE Conferences,,,,,,
Lung Disease Classification Using Convolutional Neural Network (CNNs),J. Jayanthi; A. Kumar; D. Phieri; M. Patil; P. Purohit; V. Metha,2023 2nd International Conference on Futuristic Technologies (INCOFT),########,2023,"Lung and respiratory diseases claim millions of lives annually, and their treatment is hindered by the challenges in fast and accurate diagnosis. Access to specialized medical facilities for diagnosing and treating these conditions is limited globally. Even for those with access, the diagnostic process can be very expensive due to the cost of methods like CT or MRI scans, or the need for highly skilled radiologists when using cheaper alternatives such as x-rays. The demand for these professionals is high but the supply is extremely low as it takes several years of training for such a professional to be able to accurately diagnose conditions. The diagnostic process can also be time-consuming, ranging from half a day to several days. The primary goal of the paper is to maximize the benefits of costeffective x-ray diagnostic methods while leverage the capabilities of computer-aided diagnosis. This approach aims not only to reduce diagnosis time but also to minimize the risk of misdiagnosis. Additionally, it seeks to mitigate the shortage of radiologists in high-demand regions by augmenting their numbers and providing an alternative solution in areas lacking radiology expertise. To achieve these goals, a Web Application will be developed, serving as a diagnostic tool for lung and respiratory conditions. This application will provide an efficient, faster, affordable, and easily accessible means of diagnosing chest x-rays, aiding medical personnel in the detection and diagnosis of lung and respiratory diseases. This will speed up the diagnosis and will serve as a supplementary service for radiologists in areas lacking enough expertise.",Solid modeling;Pulmonary diseases;Lung;X-rays;Convolutional neural networks;Medical diagnostic imaging;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425236,IEEE Conferences,,,,,,
Detection & Diagnosis of COVID-19 from CXR Images Through VGG19 Transfer Learning Model,R. Pillai; N. Sharma; R. Chauhan; G. Verma; R. Gupta,2023 2nd International Conference on Futuristic Technologies (INCOFT),########,2023,"COVID-19 must be diagnosed rapidly and precisely to control the current pandemic effectively. Using a dataset of 21,165 chest X-ray images, this study suggests a transfer learning approach for identifying and stratification of COVID-19. Three subsets of the datasetâ€”training, validation, and testâ€”each including 16,932, 2,116, and 2,117 imagesâ€”are created. The VGG19 transfer learning model uses deep learning approaches to extract pertinent image characteristics and accurately classify the input images. By reaching a high accuracy rate, the key objective of this study is to distinguish COVID-19-infected patients from others with distinct lungrelated abnormalities. The model successfully classifies images with an incredible 94.85% accuracy, highlighting its potential for biomedical applications in the real world. The model's robustness and generalizability are aided by the extensive and varied dataset, which includes four annotated classes. The VGG19 model has much to offer regarding healthcare advantages if successfully implemented. The speed and accuracy of diagnosis can be improved by automatically identifying and classifying COVID-19 instances from chest Xray images, allowing for quicker patient triage and resource allocation. Additionally, it can lighten the load on radiologists and health workers and promote efficient management tactics throughout the pandemic. This study emphasizes the potential of models based on deep learning for COVID-19 identification in digital image processing. Future studies should improve the model's effectiveness, determine if it can be used in various groups, and include more clinical data for thorough research. In the continuing COVID-19 pandemic, the proposed model may help medical practitioners identify patients more quickly and accurately, leading to better patient outcomes.",COVID-19;Deep learning;Pandemics;Transfer learning;Medical services;X-ray imaging;Load modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425343,IEEE Conferences,,,,,,
The Classification of Breast Cancer Using a Transfer Learning Strategy in a Federated Learning Framework,S. Bansal,2023 2nd International Conference on Futuristic Technologies (INCOFT),########,2023,"In order to make an accurate forecast, deep learning algorithms need a sizable quantity of data to learn from. Recent research has shown that transfer learning-based DL techniques to developing CAD systems function properly across a range of use cases. Diseases including lung cancer, brain tumours, and breast cancer are detected and analysed at an early stage by employing these systems and their many modalities. Pre-trained models are often used for DL-based activities in computer vision instead of creating models of neural networks from scratch. This article explains why transfer learning models may be used to automate tumour classification without the need for augmentation or preprocessing. On the BreakHis dataset, seven machine learning models are used for tumour classification; Xception achieved the highest accuracy (83.07%) among these seven models. DarkN et53 also excels in computing a new measure called Balanced Accuracy (BAC) (87.17%), which is necessary for achieving accuracy with an imbalanced dataset. This discovery will help scientists and doctors choose the best model for tumour classification when faced with an imbalanced data set. It will help doctors categorise the illness more accurately and quickly.",Solid modeling;Computational modeling;Transfer learning;Medical services;Brain modeling;Breast cancer;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425784,IEEE Conferences,,,,,,
CT Tumor Image Segmentation Based on Deep Learning,S. Yue; D. Yong,2023 3rd International Conference on Electronic Information Engineering and Computer Science (EIECS),########,2023,"Computed Tomography (CT) images play a crucial role in tumor detection, as it directly impacts subsequent analysis and treatment procedures. To address the issue of decreased accuracy due to manual interference, leveraging the advancements in deep learning computer technology for automated segmentation of CT tumors holds significant promise. The U-Net network is an automatic segmentation algorithm that operates fully on its own, This paper proposes enhancements to the original U-Net network model by increasing the number of convolutional layers in the U-Net encoder and modifying the classification from three-classification to two-classification. Additionally, the training data set sample size is increased, and the system performance is compared with the original basic model. The results show a significant improvement in the Dice coefficient compared to the original model, which holds clinical significance.",Deep learning;Image segmentation;Computed tomography;Computational modeling;System performance;Data models;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435434,IEEE Conferences,,,,,,
Convolution Neural Network with Unsupervised Machine Learning Approach for Feature Extraction and Brain Tumor Detection in Human beings,G. S. Kumar; M. Nivaashini; G. U. Maheshwari; D. Rasi; B. M. Kumar; R. Arun,2023 International Conference on Emerging Research in Computational Science (ICERCS),########,2023,"Brain tumors are atypical progress of cells in the brain or the contiguous tissues. Tumors can be either non-cancerous or cancerous. The prognosis for a person with a brain tumor varies significantly. Benign tumors are generally less aggressive and may be curable with surgery. Malignant tumors can be more challenging to treat and may have a poorer prognosis. The outcome also depends on the stage at which the tumor is diagnosed. Detecting brain tumors using magnetic resonance imaging (MRI) images with convolutional neural networks (CNNs) is a common and effective approach in medical image analysis. In this research, the unsupervised machine learning approach called self-organizing map (SOM) is implemented for effective feature extraction and CNN with ResNet architecture is employed for brain tumor exposure efficiently. The proposed SOMResNet algorithm takes the MRI images as the input and perform feature extraction. Again, the mined essential features are given as the input for SOMResNet to identify the tumor in the human brain. The accuracy of the proposed SOMResNet is compared with softmax, ReLu, Tanh, decision tree classifiers. The accuracy of SOMResNet is 97.5%, Sensitivity is 98.0%, specificity rate is 97.1%, precision rate is 97.3% with 97.6% of F-Score value. The result shows that the SOMResNet algorithm outperforms than the traditional algorithms in brain tumor detection.",Self-organizing feature maps;Machine learning algorithms;Magnetic resonance imaging;Feature extraction;Classification algorithms;Convolutional neural networks;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434117,IEEE Conferences,,,,,,
An Experimental and Clinical Study on Advanced Breast Cancer Detection Using Convolutional Neural Networks with Improved Optimization Techniques,R. Nithya; D. Radhika; K. Rajeswari; A. Saranya,2023 International Conference on Emerging Research in Computational Science (ICERCS),########,2023,"One of the most common and fatal cancers impacting women globally is breast cancer. For the best chance of survival, breast cancer must be found early. In order to help detect cancer, Convolutional Neural Networks (CNNs) have been effectively used in medical picture analysis. However, the presence of noise, artefacts, and other variables in medical pictures frequently affects how well CNNs work. It has been discovered that the performance of CNNs in a variety of applications may be enhanced by the use of sophisticated optimisation techniques. The aim of this study is to investigate the effectiveness of applying improved optimization techniques, namely, the Monarch Butterfly Optimization (MBO) algorithm, in conjunction with CNNs for advanced breast cancer detection. Our study compares the performance of CNNs with traditional statistical methods, such as receiver operating characteristic analysis on breast cancer imaging datasets. We also evaluate the clinical usefulness of CNNs in breast cancer detection in clinical settings. Our experimental results indicate that CNNs with improved optimization techniques achieve high accuracy in breast cancer detection and have the potential to improve the reliability of conventional diagnostic methods. Furthermore, the clinical findings demonstrated the feasibility of using advanced CNN models, combined with novel optimization techniques, for early breast cancer detection. The findings of this study will advance the field of medical imaging and contribute to better cancer management and treatment options for women.",Statistical analysis;Scientific computing;Breast cancer;Convolutional neural networks;Optimization;Medical diagnostic imaging;Resilience,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434110,IEEE Conferences,,,,,,
Developing a Convolutional Neural Model for Segmentation of Medical Images,M. Arunachalam; P. M. Kumar; L. Selvam; S. S. Kumar; B. Arunkumar,2023 International Conference on Emerging Research in Computational Science (ICERCS),########,2023,"Medical image segmentation is a new biomedical image processing method that has made a significant contribution to sustainable health care. It has now become a major study area in the realm of computer vision research. Medical image processing based on deep convolutional neural networks has become a research hotspot due to the fast development of deep learning techniques. An additional cascade component is included to the architecture in this article to better replicate spatial closure tags' direct dependences. It is also utilised for post-segmentation processing, resolving the conflict between classification performance and network intensity and number of mixing times in a typical convolutional net. Numerous state-of-the-art deep recognition models, and a multi-atlas segmentation methodology, are outperformed by our methodology.",Image segmentation;Scientific computing;Medical services;Biomedical image processing;Spatial resolution;Dispersion;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434096,IEEE Conferences,,,,,,
An Automated Classification and Segmentation of Fatty Liver Disease Using SegNet Model on Ultrasound Images,G. Paul; G. Ramkumar,2023 International Conference on Emerging Research in Computational Science (ICERCS),########,2023,"In a lot of regions of the world, liver disease has replaced cancer as the leading cause of death. The use of alcoholic beverages, inhalation of toxic gases, and ingestion of damaged food and pharmaceuticals have all contributed to an increase in the number of individuals diagnosed with a liver condition. Researchers are looking at liver patient data sets to create algorithms for categorization that might foretell liver illness. Using this data collection, prediction and categorization techniques were developed, which lessens the burden on medical professionals. In recent years, liver volumetric analysis has been increasingly useful in the medical field. This work aims to offer a brief summary of automated categorization and segmentation of Fatty Liver Disease using the SegNet Model to radiologists and other medical professionals in a way that is both accessible and informative. Ultrasound and other traditional methods provide just a qualitative, more subjective assessment of fat. This study describes the clinical applications of liver segmentation using ultrasound pictures, as well as the technological techniques used by segmentation software and the expanding roles that liver segmentation is playing in clinical practice. Characterization of NAFLD in ultrasound images utilizing the SegNet model vs the more efficient EfficientNet. Although it is the preferred method in research and clinical practice, manual segmentation is time-consuming and laborious. While automatic segmentation methods are becoming increasingly powerful, they nevertheless may be vulnerable to common mistakes. Segmentation is finding new uses in areas like surgical planning and ultrasound image integration. The EfficientNet categorization achieved 87% accuracy in predicting liver fat from ultrasound scans, while the SegNet Model achieved 91% accuracy. There is a growing number of clinical uses for liver segmentation. Semi-automated and fully-automated segmentation methods are available to clinicians, allowing for smoother incorporation of volumetry into clinical practice.",Image segmentation;Ultrasonic imaging;Liver diseases;Manuals;Data models;Fats;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434261,IEEE Conferences,,,,,,
Performance Analysis of Machine Learning Techniques for Multi-Organ Cancer Detection and Classification: A Comparative Study,M. L. V. A. Priya; M. V. Subbarao,2023 International Conference on Emerging Research in Computational Science (ICERCS),########,2023,"Cancer is a leading cause of mortality worldwide, and early detection is crucial for successful treatment and improved patient outcomes. In recent years, Machine Learning (ML) techniques have shown promising potential in assisting medical professionals with accurate cancer detection and classification. This research paper presents a comprehensive investigation and comparative analysis of various ML algorithms for multi-organ cancer detection and classification. The dataset includes wide variety of images taken from different part of organs for clinical inspections. The dataset is further divided into different cancers groups and the images are further processed using preprocessing techniques and then features are extracted from the processed images. These features are taken as the inputs for the ML algorithms for identification of cancers tumours. This paper presents investigations on different organ cancers using Support Vector Machines (SVM), K-nearest Neighbors (KNN), Decision Trees (DT), and Ensemble classifiers (EC). Different performance metrics are measured to know the ability of each classifier in cancer detection. Results depicted that SVM and Ensemble classifiers are performed better than the other classifiers.",Support vector machines;Machine learning algorithms;Machine learning;Cancer detection;Classification algorithms;Performance analysis;Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434014,IEEE Conferences,,,,,,
Bat Optimized CNN For Skin Cancer Detection Using Deep Learning Approach,S. Sathish; J. Shanmugapriyan; N. Malathy; K. Shruthi,"2023 International Conference on Energy, Materials and Communication Engineering (ICEMCE)",########,2023,"Among the most harmful forms of malignancy that people get easily these days is skin cancer. Skin cancer comes in a variety of forms, including basal, melanoma, carcinoma, and squamous cell, among which melanoma is unexpected. Therefore, earlier skin cancer diagnosis is crucial for successful treatment. In order to diagnose skin cancer, this research proposes a new technique dubbed the bat optimisation algorithm. To reduce the noise and artefacts from the input image, pre-processing is first applied. The pre-processed image is then passed on to the feature extraction stage, where the features are obtained using convolutional neural network features. A bat optimisation is then employed to classify data based on the retrieved features. The precision, efficacy, and specificity of skin cancer diagnosis using the proposed approach are assessed.",Melanoma;Sensitivity and specificity;Feature extraction;Classification algorithms;Convolutional neural networks;Optimization;Skin cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433947,IEEE Conferences,,,,,,
Feature Fusion Tactics in Gastrointestinal Endoscopic Image Retrieval Using Deep Features,M. G. Arulsamy; P. R. Murugan; A. P. Thiyagarajan; K. Ramaraj,"2023 International Conference on Energy, Materials and Communication Engineering (ICEMCE)",########,2023,"Learning outcomes in the vast field of image retrieval (IR) can enhance the effectiveness of IR systems. Medical imaging is crucial because it can quickly and precisely diagnose a patientâ€™s disorder, monitor their response to treatment, and manage their condition. Therefore, it is essential to develop a reliable and effective medical image retrieval system. Endoscopic colored images have a single mode of representation with identical attributes, making it difficult to exploit low-level features. In this work, feature vectors extracted from deep learning networks such as ResNet101 and DenseNet201 are fused together in two ways, namely concatenation and summation, using the canonical correlation analysis-based feature fusion technique. The dimensionality of the feature space chosen for the analysis was reduced using principal component analysis. The effectiveness of this methodology was examined using the well-known medical image dataset, Kvasir. The experimental performance analysis demonstrated that the retrieval strategy is extremely effective when integrating various feature spaces.",Image retrieval;Self-supervised learning;Feature extraction;Gastrointestinal tract;Reliability;Medical diagnostic imaging;Principal component analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434167,IEEE Conferences,,,,,,
The Classification of Dermatologic Image and Diagnostic System Based on Deep Learning,H. Ouyang; W. Jiang; Y. Yu; H. Li,2023 11th International Conference on Information Systems and Computing Technology (ISCTech),########,2023,"Skin is the first line of physiological defense of the human body, and cutaneous venereal disease has become a major problem in the medical field. With the development of medical level, the treatment of cutaneous venereal diseases has also been significantly improved, but it still faces great challenges in terms of difficult diagnosis, small coverage, high treatment cost and low efficiency of doctors' diagnosis. To solve the above problems, this paper uses cutting-edge deep learning algorithms, including Inception V3, GoogLeNet, and EfficentNet, to classify skin disease images. Based on the above deep learning algorithms, an online intelligent clinical diagnosis and treatment system is developed to assist doctors in disease diagnosis. The result shows that EfficientNet is the best in the diagnosis of skin disease. EfficientNet is 83.45%, Inception V3 (63.24%), and GoogleNet (75.32%). The result also shows that EfficientNet provides a more efficient model with the same dataset which has the capability to process more complex medical image classifications task.",Deep learning;Biological system modeling;Skin;Classification algorithms;Clinical diagnosis;Medical diagnostic imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10438268,IEEE Conferences,,,,,,
Automated Brain Tumor Classification and Detection Using Modified Convolutional Neural Networks for Early Diagnosis,A. D; R. R; S. V; V. N; R. R,2023 International Conference on Intelligent Technologies for Sustainable Electric and Communications Systems (iTech SECOM),########,2023,"In this project, we leverage cutting-edge deep learning techniques, including CNN, AlexNet, ResNets, and VGG-16, to classify MRI brain images. Our primary goal is early brain tumor detection, addressing the challenge of asymptomatic or vague symptoms. We employ a robust preprocessing pipeline for accurate brain region isolation, incorporating grayscale conversion, Gaussian blurring, thresholding, and artifact removal. Advanced image enhancement techniques like edge-based segmentation and clustering are applied to improve MRI analysis. Our deep learning-based system automates tumor diagnosis and segmentation, easing the burden on radiologists. With more than 2750 MRI images, we use data augmentation to tackle class imbalance, achieving an impressive 98% test accuracy. Beyond CNN, we explore alternative architectures and transfer learning to find the most effective model for MRI classification, revolutionizing early brain tumor detection and aiding medical professionals. This research has the potential to save lives and streamline the diagnostic process significantly.",Image segmentation;Magnetic resonance imaging;Discrete wavelet transforms;Convolutional neural networks;Kernel;Tumors;Principal component analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435136,IEEE Conferences,,,,,,
A Critical Analysis on Early Stage of Lungs and Colon Cancer Detection Using CNN Algorithm,R. K; B. C; N. V S; A. Samraj; R. Nandhakumar,2023 International Conference on Intelligent Technologies for Sustainable Electric and Communications Systems (iTech SECOM),########,2023,"One of the most dangerous problems that people experience globally is lung and colon tumor that has spread rapidly to become a normal medical challenging problem. It is essential to achieve an accurate and preliminary detection in order to minimize the hike of death. The complexity of the task entirely relies on the histopathologists experience. The analysis of the histopathology images of lung and colon cancer is crucial because early and accurate diagnosis of cancer histology is actually needed and since the treatment of cancer is aimed on the kind of histology, genetic profile and stage of the illness. Histologists may even endanger the patientâ€™s life. In recent times popularity of deep learning increase and recognized in the diagnosis and treatment of medical imaging. In order to identify lung and colon cancer using histopathological image and more effective augmentation techniques, this research involves studying and modifying the current CNN model. In the past few years, CNN has reported amazing progress in the fields of CT, MRI, and ultrasound. A novel technique that combines the advantages of two imaging modalities, 3D CT scan and histopathological image processingâ€”to produce results that are more accurate and dependable. They are trained on the LC25000 dataset. CNN are the most preferred approach of deep learning algorithm for the advanced detection of lung and colon cancer. The level of accuracy offered by CNN in this regard may help to reduce the frequency and improve evaluation and quality",Deep learning;Ultrasonic imaging;Histopathology;Computed tomography;Lung;Cancer detection;Colon,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435341,IEEE Conferences,,,,,,
Automated Classification of Chest Image Using Deep Learning,A. D; N. R. S; M. R. Shankaran; J. K; R. R. M,2023 International Conference on Intelligent Technologies for Sustainable Electric and Communications Systems (iTech SECOM),########,2023,"With the current medical research, it has become challenging to diagnose lung and heart diseases. The absolute and verified methodological examination of the patient's medical testâ€™s results is necessary for the diagnostic analysis. Deep learning has rapidly enhanced greatly in the past few years, making it capable to recognize and categorize patterns in medical images. This project classifies pneumonia for lung diseases, blocks and contractions in heart for heart diseases. Furthermore, the trained deep learning model is then integrated with a web application that helps the doctors to access and use the model anywhere around the world. The model utilizes convolutional neural networks (CNN) to aid classifications of the diseases based on the patientâ€™s clinical data. The experiments conducted and the obtained conclusions and results demonstrate our system may perform efficiently for automated medical picture diagnosis.",Deep learning;Heart;Pulmonary diseases;Medical services;Data models;Convolutional neural networks;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435016,IEEE Conferences,,,,,,
Deep Learning in Object Detection: Advancements in Machine Learning and AI,P. K. Hebbar; P. K. Pullela,"2023 International Conference on the Confluence of Advancements in Robotics, Vision and Interdisciplinary Technology Management (IC-RVITM)",########,2023,"This article provides an overview of object detection and its significance in various applications. Object detection, a process that detects images of visual objects, encompasses tasks such as number plate recognition, face detection, text detection, and pedestrian detection. The article emphasizes the role of deep learning in enhancing the accuracy and speed of object detection, making it a powerful method in machine learning. It explores recent studies and advancements in visual recognition, with a focus on the applications of deep learning in fields like healthcare, robotics, and smart agriculture. The paper also delves into research in object detection, including the use of deep learning in remote sensing, video surveillance, and medical imaging. Finally, the article discusses the future scope of deep learning, highlighting its potential impact on various industries and domains.",Deep learning;Visualization;Technology management;Service robots;Text detection;Object detection;Video surveillance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435048,IEEE Conferences,,,,,,
Efficient Model for Multiview classification for diagnosis of Brain Tumors.,B. Anil Kumar; B. M. Chandrakala; B. V. Shruthi,"2023 International Conference on the Confluence of Advancements in Robotics, Vision and Interdisciplinary Technology Management (IC-RVITM)",########,2023,"Brain tumors are caused mainly by DNA abnormalities, which result in aberrant cell division, prolonged cell lifespans, and the creation of aggregative masses inside the brain. Such tumors prevent the brain from operating at its best, sometimes resulting in long-term problems. These tumorsâ€™ genesis (primary or secondary) and malignancy (benign or malignant) are used to classify them. While some might not be immediately lethal, their growth or contact with healthy brain tissue might cause damage. Investigations into various approaches, such as conventional medical imaging and artificial intelligence (AI), mainly using deep learning (DL) and transfer learning (TL), is prompted by efforts to identify and categorize brain tumors. Implementing the EfficientNetB0 model, which is famous for its adaptability and efficacy, is an excellent demonstration of this strategy. This work uses a well-curated dataset of 3264 photos from various classes to closely analyze the use of the EfficientNetB0 TL model to identify and classify brain tumors from MRI scans. The modelâ€™s adaptability and accuracy are meticulously examined using a dual-pronged strategy incorporating transfer learning and fine-tuning, yielding an astounding accuracy of 98.5%. Accuracy and loss charts illustrate the modelâ€™s learning development in vivid detail, and a well-defined confusion matrix provides extensive insights into categorization effectiveness across various tumor classifications. The achievement of a 98.5% accuracy rate highlights the potential paradigm change made possible by AI and TL in the diagnosis and classification of brain tumors. This accomplishment confirms the efficacy of such sophisticated procedures and promises to speed up diagnosis, enable prompt therapies, and improve patient prognoses. The combination of AI and TL heralds a new era of precision medicine, ready to reshape the field of brain tumor care as research in this area grows.",Adaptation models;Transfer learning;DNA;Brain modeling;Artificial intelligence;Medical diagnostic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435348,IEEE Conferences,,,,,,
Applications of deep learning for confocal fluorescence microscopy image quality enhancement using unpaired data,S. Liu; Z. Lin; Y. Huang; H. Yang; J. Chen,"2023 2nd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)",########,2023,"The development of deep learning method and open access to ample public datasets have provided an excellent solution for confocal microscopy image translation, which severs as a stepping stone for microscopic imaging and biomedical research. However, in microscopic imaging, most applications of deep learning algorithms are applied in a supervised manner, which hinders the development of deep learning in microscopy owing to the need for large paired images and annotations. Here, we use an improved Cycle Generative Adversarial Network-based unsupervised learning algorithm to achieve confocal microscopy image quality enhancement. Two datasets, the self-collected human prostate cancer cells deblurring dataset and public planaria denoising dataset, are used to train and test in the mentioned model. The results show the model has excellent generalization ability, which can be adapted to different image translation tasks and different types of cell imaging for confocal microscopy.",Deep learning;Semiconductor device modeling;Optical microscopy;Microscopy;Noise reduction;Fluorescence;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10439099,IEEE Conferences,,,,,,
A Paradigm Shift in Brain Tumor Classification: Harnessing the Potential of Capsule Networks,Y. Raythatha; V. M.,"2023 IEEE 2nd International Conference on Data, Decision and Systems (ICDDS)",########,2023,"Accurate and timely classification of brain tumors is critical for developing effective treatment plans and predicting treatment outcomes. However, CNN-based models commonly used for this task have limitations, such as their reliance on large amounts of training data and difficulties with input orientation and transformations. To address these limitations, we propose a CapsNet-based model for brain tumor classification designed to effectively handle limited datasets, class imbalance, and input transformations. CapsNet relies on â€œcapsules,â€ groups of neurons that work together to represent specific input image features and are resistant to input orientation and transformations. Our study compares the performance of the proposed CapsNet-based model with state-of-the-art CNN models, and our results demonstrate that the CapsNet-based model outperforms CNN models in terms of accuracy and robustness to input orientation and transformations. These findings suggest that CapsNet has the potential to be a promising alternative to CNNs for accurate and efficient brain tumor classification.",Training data;Brain modeling;Data augmentation;Data models;Robustness;Convolutional neural networks;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434400,IEEE Conferences,,,,,,
Vision Transformer and Attention-Based Melanoma Disease Classification,P. Shobhit; N. Kumar,"2023 4th International Conference on Communication, Computing and Industry 6.0 (C216)",########,2023,"This study delves into the critical domain of melanoma detection, a life-saving endeavor that hinges on early diagnosis. Melanoma, a deadly form of skin cancer, poses a formidable challenge due to its tendency to remain dormant until advanced stages. Dermoscopic images serve as valuable tools, but distinguishing melanoma from non-melanoma lesions is notoriously complex. This paper explores the potential of Vision Transformers (ViTs), a novel deep learning architecture equipped with self-attention mechanisms, to enhance melanoma classification. We investigate how ViT's attention mechanisms can capture intricate features in dermoscopic images. The study also delves into fine-tuning strategies specific to medical image analysis. Through rigorous experimentation, our ViT-based sys-tem demonstrates promising results and training accuracy of 97% and testing accuracy of 91% highlighting its potential to revolutionize melanoma diagnosis.",Training;Image analysis;Melanoma;Visual systems;Transformers;Telecommunication computing;Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430697,IEEE Conferences,,,,,,
"DeepKidney: Multiclass Classification of Kidney Stones, Cysts, Tumors, and Normal Cases Using Convolutional Neural Networks",K. S. K; B. M. K P; N. Patwari; S. D A,"2023 4th International Conference on Communication, Computing and Industry 6.0 (C216)",########,2023,"Accurately classifying kidney diseases, such as stones, cysts, tumors, and normal cases, is vital in effective diagnosis and treatment planning. A method for multi class classification of kidney conditions using Convolutional Neural Networks (CNNs) is proposed in this paper. The main aim is to develop a system, termed DeepKidney, that can accurately classify medical imaging data automatically. This study focuses on the multiclass classification of kidney stones, cysts, tumors, and normal cases using CNNs. We propose a novel approach called DeepKidney, which utilizes CNNs to analyze medical imaging data and extract relevant features for classification. A large and diverse dataset comprising kidney images representing different conditions is used for training the CNN model. DeepKidney aims to achieve high classification accuracy and improve diagnostic outcomes. This approach provides a comprehensive solution for distinguishing between different kidney conditions, aiding in precise diagnoses. DeepKidney's implementation has the potential to increase kidney disease classification's precision and effectiveness, resulting in better patient outcomes and less work for medical professionals.",Training;Feature extraction;Telecommunication computing;Convolutional neural networks;Kidney;Diseases;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431043,IEEE Conferences,,,,,,
Amalgamation of Image Features for Medical Image Classification with Ensemble of Classifiers,R. Bhuvaneswari; K. Ashwini; B. S. Kiran,"2023 4th International Conference on Communication, Computing and Industry 6.0 (C216)",########,2023,"COVID 19 disease happened to be one of the most infectious diseases in the 21st century. This widespread disease was very huge where people from almost all parts of the world were victims. This paper presents an idea of amalgamation of various image features for COVID 19 detection and classification. Four most significant statistical image features namely, Maximally Stable Extremal Regions (MSER), Histogram of Oriented Gradients (HOG), Scale Invariant Feature Transform (SIFT), and Local Binary Pattern (LBP) are extracted from the chest X ray images. Contribution of each feature in accurate classification and detection of covid is analyzed initially with classifiers namely Naive Bayes (NB), Support Vector Machine (SVM), and Decision tree (DT). Simulation results are also obtained to evaluate how ensemble of classifiers comprehend a specific set of features in classification. Through numerous experimental findings, it has been discovered that amalgamation of all four features to categorize the images using ensemble of classifiers yields best results with the classification accuracy of about 97.5%.",COVID-19;Support vector machines;Simulation;Transforms;Feature extraction;Telecommunication computing;Decision trees,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431014,IEEE Conferences,,,,,,
Cardiovascular Disease (CVD) Prediction Using Deep Learning Algorithm,S. Charkha; A. Zade; P. Charkha,2023 International Conference on Integration of Computational Intelligent System (ICICIS),########,2023,"Brain and Heart both organs keep our body working with co-ordination comparing to brain heart is very delicate and need more care for well-functioning. So heart is essential in every living body. Pumping of oxygenated blood and supplies it to all organs in proper flow makes our body well operated. So keeping our heart healthy and disease free is our priority. For this propose prediction of genesis of cardiovascular diseases will be very notable work in healthcare sector. Processing on raw data with useful tools and technique to make it significant for prediction of disease at early stage will be useful for healthcare center to cure disease at its born stage. Enormous disease related data of patients is stored every day in that can be pre-process and can be used to train any machine learning or deep learning model which will help to predict disease at early stage. In this study deep learning algorithms like RNN(Recurrent Neural Network) with its type LSTM(Long short Term Memory) is used to train and predict the cause of CVD so patient can start medication very soon. The proposed work, come up with comparative study of pervious used algorithm and proposed working algorithms with accurate prediction without time delay. The results of the diagnosis model are generated by using classification method, accuracy and severity of causing parameters. This review paper purpose is to develop the model to predict whether patient is having a cardiovascular disease or not and make patient more aware on CVD. The perfection is achieved by using categorization technique, RNN (Recurrent Neural Network) with LSTM to achieve accuracy in prediction.",Heart;Deep learning;Recurrent neural networks;Machine learning algorithms;Predictive models;Prediction algorithms;Cardiovascular diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430254,IEEE Conferences,,,,,,
Brain MRI Classification Using Convolutional Neural Networks and VGG19: A Deep Learning Approach for Accurate Brain Disease Diagnosis,S. Ghodke; S. Nandgave,2023 International Conference on Integration of Computational Intelligent System (ICICIS),########,2023,"Brain MRI classification is a critical task in medical imaging analysis for accurate diagnosis of brain diseases. This study presents a deep learning approach utilizing Convolutional Neural Networks (CNNs) and the VGG19 architecture to achieve precise brain disease diagnosis based on brain MRI scans. The proposed methodology leverages the power of CNNs and transfer learning, employing the pre-trained weights of the VGG19 model. The depth and capability of the VGG19 architecture enable it to extract relevant features from brain MRI scans effectively. The process begins with data preparation and preprocessing to ensure dataset quality and consistency. The VGG19 model is adapted for brain MRI classification by replacing the final layers with task-specific layers. Transfer learning is applied by freezing initial layer weights and fine-tuning the remaining layers on the brain MRI dataset. The training process involves optimization using suitable algorithms and loss functions, with performance monitoring on a validation set. Hyperparameters are fine-tuned to achieve optimal results. The trained model is evaluated on an independent testing set to assess its diagnostic accuracy, utilizing metrics such as accuracy, precision, recall, and F1-score. The proposed deep learning approach demonstrates great potential for accurate brain disease diagnosis based on brain MRI scans. By automating the classification process, it reduces reliance on manual interpretation and enables faster, objective diagnoses. This study contributes to the advancement of medical imaging analysis and emphasizes the significance of deep learning techniques in brain MRI classification.",Deep learning;Magnetic resonance imaging;Transfer learning;Feature extraction;Brain modeling;Convolutional neural networks;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430304,IEEE Conferences,,,,,,
A Comprehensive Study on Medical Image Denoising using Convolutional Neural Networks,A. Mohammed; S. Punniakodi,2023 Second International Conference on Advances in Computational Intelligence and Communication (ICACIC),########,2023,"Medical images are considered as one of the most important medical data for diagnosis and treatment purposes. For remote consultation and diagnosis, the medical images are frequently transferred using different communication methods. There is a chance of noise degradation of these medical images during transmission. Denoising such medical images is crucial for enhancing the quality and diagnostic precision. The remarkable ability of Convolutional Neural Networks (CNNs) to effectively denoise medical images has attracted considerable interest. This paper discusses various CNN architectures, training strategies, and data augmentation techniques for denoising tasks. In addition, the difficulties and limitations of CNN-based denoising techniques, such as computational complexity and data scarcity, are discussed. In addition, incorporating Generative Adversarial Networks (GANs) and transfer learning for improved denoising performance are discussed. This survey aims to serve as a valuable resource for researchers, clinicians, and developers by fostering a better comprehension of the current state-of-the-art techniques and future directions in medical image denoising using CNNs.",Noise reduction;Transfer learning;Training data;Computer architecture;Convolutional neural networks;Medical diagnostic imaging;Image denoising,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435104,IEEE Conferences,,,,,,
Advancements in Interstitial Lung Disease Classification and Segmentation: A Comprehensive Review,G. AmarTej; R. Nakkeeran,2023 Second International Conference on Advances in Computational Intelligence and Communication (ICACIC),########,2023,"Interstitial lung disorders, collectively affecting the interstitium around lung air sacs, are marked by fibrosis, inflammation, and compromised lung function. Conditions like Sarcoidosis, Hypersensitivity Pneumonitis, Connective tissue disease-associated ILDs, Idiopathic Pulmonary Fibrosis (IPF), and more fall under this group. Diagnosis includes clinical assessment, radiographic imaging (e.g., High-Resolution Computed Tomography), lung function tests, and histological lung tissue analysis. Radiologists face difficulties identifying ILDs via X-rays and CT scans, necessitating robust detection, treatment, and monitoring algorithms. Moreover, this review looks into CT scan-related challenges in ILD identification, including image artifacts, data variability, and deep learning model interpretability. By presenting insights and addressing hurdles, this literature review aims to advance important research in medical imaging, particularly in the context of ILD identification.",Image segmentation;Computed tomography;Pulmonary diseases;Computational modeling;Bibliographies;Lung;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435032,IEEE Conferences,,,,,,
Systematic Development of Modified Hybrid Learning based Detection Scheme for White Blood Cancer Cells using Medical Image Processing Methodology with IoT Alert Mechanism,P. Vinayagam; N. V. V. Reddy; P. T. K. Kishore; A. Royappa,2023 Second International Conference on Advances in Computational Intelligence and Communication (ICACIC),########,2023,"The automated detection of white blood cell cancers like cancer and lymphoma is an active area of medicinal study that presents significant challenges. In this research, we unveil a brand-new, cutting-edge tool for aiding in the diagnosis of disorders affecting the white blood cells. In this work, we propose Hybrid Learning based Medical Image Evaluation (HLMIE), a novel deep learning based cancer cell classification technique. The proposed approach is grounded in traditional learning and classification techniques like Neural Network based Learning and Random Forest Classifier (RFC), and entails creating an automated system to aid doctors in making accurate diagnoses of the various forms of this illness. Acute Myolegenous Leukemia (AML) and Acute-Lymphoblastic-Leukemia (ALL) are the two categories, and they share several symptoms that might make diagnosis difficult. Several techniques involving machine learning and deep learning have been employed in the past to try to make predictions about blood cancer; however these researches have their flaws. Therefore, this research proposes a deep learning/classification logic/medical image processing hybrid model to enhance prediction accuracy. Different degrees of prediction, analysis, and learning methods are incorporated into the suggested hybrid learning model enabled with image processing, and various learning criteria, such as development of learning and assessment precision with regard to epochs, are utilized. In addition, limiting fine-tuning of the systemâ€™s output to actual experts and this scheme provides better accuracy in results with respect to better classification performance with prediction logics using hybrid learning methodology, in which the proposed scheme attains 97.84% of prediction accuracy and the reduce the image noise ratio as 2.36% as compared with the conventional classification and learning algorithms. The resulting section shows the proper proof of this quote with exact histographical specification. The resulting summary of the disease prediction is reported properly to the respective care taker or an individual with respect to the principles of trending technology called Internet of Things (IoT).",White blood cells;Image processing;Predictive models;Hybrid learning;Internet of Things;Medical diagnostic imaging;Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435291,IEEE Conferences,,,,,,
An Investigation on Pre-processing Techniques and Image Datasets for Lung Cancer Classification,S. R. Jena; S. T. George; D. N. Ponraj,2023 Second International Conference on Advances in Computational Intelligence and Communication (ICACIC),########,2023,"Lung cancer is a devastating illness that affects people all over the world. As a result, accurate and earlier prediction of lung cancer is critical for improving disease prognosis in patients. To identify lung nodule growth, many studies have been developed using various imaging modalities. Deep Learning (DL) techniques are used in this work for preprocessing, noise reduction, and classification. This paper focuses on cutting-edge ways for maximizing decision analysis as picture modalities, dataset pre-processing, and Deep Neural Networks (DNN) are employed for lung nodule classification. Convolutional Neural Networks (CNN) have recently been utilized in conjunction with DNN for classification purposes. This extensive study provides a greater understanding of how to improve future studies for efficiently predicting lung cancer.",Shape;Noise reduction;Lung;Lung cancer;Convolutional neural networks;Reliability;Prognostics and health management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435021,IEEE Conferences,,,,,,
Kidney Abnormalities Prediction in Ultrasound Images using Transfer Learning Approach,T. Mangayarkarasi; D. Najumnissa Jamal,2023 Second International Conference on Advances in Computational Intelligence and Communication (ICACIC),########,2023,"Transfer Learning Algorithm is becoming popular for medical image analysis, because it has high level of flexibility in dealing with Image dataset with minimum number of Images. It makes convenient for researchers to test and train medical images, assign the determined weights thus reducing the computational complexities and pre-processing time. Presence of stones, tumour, cyst and other kidney abnormalities in Kidney Ultrasound Images is a challenging task for a radiologist. There exists a difference of opinion between an experienced physician and a beginner in the field of urology and nephrology. Requirement of an intelligent tool for diagnosis in the medical field has increased in the recent years due to the enormous growth of Artificial Intelligent and Machine Learning algorithms. In this proposed work Pretrained Efficient Net models B1-B5 are used with the pretrained weights of ImageNet data set thus, reducing the pre-processing time taken for random initialization of weights. Ultrasound Kidney Image Dataset are taken as inputs for training phase and testing Phase. Fine Tuning is adapted by changing the hyper parameter setting. Performance metrics such asF1Score and AUC are evaluated. Efficient Net B4 Model gives satisfactory result in terms of AUC as 89.95% and F1 Score 82.9%. Cross validation when carried out by training the models with predetermined weights of CT scan Kidney Images resulted in improving AUC to 90.9% for the prediction of kidney stones, cyst and tumour.",Training;Ultrasonic imaging;Computational modeling;Transfer learning;Kidney;Tumors;Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435333,IEEE Conferences,,,,,,
Enhancing Toxoplasmosis Chorioretinitis Detection: A Hybrid ResNet-YOLO Classifier with Fitness Sorted-Shark Smell Optimization,R. G. Tiwari; H. Maheshwari; V. Gautam; A. K. Agarwal; N. K. Trivedi,2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART),########,2023,"Ophthalmologists often examine fundus pictures to detect toxoplasmosis chorioretinitis, a serious condition of the eye. Preventing blindness in people who may be impacted by this disorder requires prompt and correct diagnosis. In this study, we present a new method for improving Toxoplasmosis chorioretinitis diagnosis accuracy by using a hybrid deep learning model that fuses features from the ResNet and YOLO (You Only Look Once) architectures. The core difference in our method is that we use the YOLO classifier for illness identification instead of the ResNet model's fully connected layer. We use the Fitness Sorted-Shark Smell Optimization (FS-SSO) technique to fine-tune crucial model parameters to get a high recognition rate with this hybrid deep learning model. Using a wide range of performance indicators and accuracy measures, we rigorously assess the effectiveness of the suggested strategy in our experimental investigation. The findings show that our hybrid model outperformed the state-of-the-art methods for detecting Toxoplasmosis chorioretinitis with an accuracy of 99%. Using the FS-SSO approach for parameter optimization, this study demonstrates the promise of combining the strengths of the ResNet and YOLO models for medical picture analysis. Our method's excellent accuracy suggests that it might be used to enhance patient outcomes and minimize the incidence of blindness caused by Toxoplasmosis chorioretinitis by allowing for earlier diagnosis and treatments.",YOLO;Deep learning;Image recognition;Blindness;Market research;Optimization;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10428594,IEEE Conferences,,,,,,
A Review: Medical Image Analysis Using Deep Learning Models,P. Shrivastava; D. K. Sharma,2023 12th International Conference on System Modeling & Advancement in Research Trends (SMART),########,2023,"The aim of medical image analysis is to enhance the effectiveness of clinical research and treatment approaches. Deep learning has brought about a transformation in medical image analysis, delivering remarkable outcomes across various image-processing tasks, including preprocessing, segmentation, feature extraction, and classification. The deep learning technique observed a valuable hidden pattern in images which is supported for better diagnostic perfection. Multiple deep-learning methods have been introduced to medical image analysis and classification. This paper offers an overview of recent developments in utilizing deep-learning techniques for medical image analysis. First, we begin with the medical image analysis tasks and give the overview of the pre-trained model that aims to improve the performance of Deep neural networks second, we explore the current survey paper of deep learning models in different medical image analysis tasks across a range of modalities.",Deep learning;Surveys;Analytical models;Image analysis;Transfer learning;Task analysis;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10428324,IEEE Conferences,,,,,,
Analysis of Pneumonia detection in X-ray images using different filters based Convolution Neural Networks,S. Das; S. Samanta; M. Rout,2023 OITS International Conference on Information Technology (OCIT),########,2023,"With recent advances and diversity of medical image acquisition technologies and processing, artificial intelligence-based deep learning has proven its indispensable image analysis capability in the most challenging and complicated tasks, such as segmentation and classification. Since early detection of any abnormality is crucial, the method will help in a timely and accurate diagnosis to launch appropriate treatment at the earliest possible stage. The proposed study aims to analyse and compare the results of medical images using a single Convolutional Neural Network by applying different image filters. The work emphasizes various filters and determines which one performs best with a particular CNN architecture. The model has been experimented with the publicly available Kaggle Covid-19 dataset comprising 21,165 images of 4 classes with different state-of-the-art filters such as Adaptive Wiener, Median, Adaptive Median, Gaussian, and Mean filters. In this experiment, the Gaussian filter gave the best accuracy of 92.98% in the improper class classification. Whereas the model with no filter gave an accuracy of 91.94%. Which shows with proper image enhancement techniques image recognition can be further improved.",Adaptation models;Pulmonary diseases;Information filters;Convolutional neural networks;Task analysis;Medical diagnostic imaging;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430781,IEEE Conferences,,,,,,
Integrating Explainable AI with Infrared Imaging and Deep Learning for Breast Cancer Detection,K. R; S. B; K. V,2023 OITS International Conference on Information Technology (OCIT),########,2023,"The interplay between medical imaging and artificial intelligence has set the stage for groundbreaking advancements in early disease detection. In the critical domain of breast cancer diagnosis, accurate, swift, and interpretable models are undeniably paramount. This study presents a pioneering approach to breast cancer detection using infrared breast imagery underpinned by the principles of explainable AI. Our methodologyâ€™s core lies in a novel Gaussian pyramid-driven denoising autoencoder meticulously tailored for infrared breast images. This sophisticated denoising technique enhances the quality of input infrared breast images. It paves the way for more accurate feature extraction, a cornerstone in the diagnostic process. Capitalizing on this refined input, our research introduces an ensemble classifier, blending the deep learning capabilities of DenseNet201 with the feature-rich outputs of the auto-encoder. This synergistic amalgamation sets a new benchmark in precision for breast cancer detection in infrared images. However, accuracy without understanding remains an incomplete victory. Addressing this, our study integrates an attention-guided Grad-CAM mechanism, shedding light on the modelâ€™s decision-making process. This tool accentuates pivotal regions in imagery through saliency-driven heatmaps, offering clinicians an intuitive window into the AIâ€™s verdict. Our research not only pushes the boundaries of precision in breast cancer detection but also champions the cause of transparency and understanding in AI-driven diagnostics, ensuring that technology and human expertise move forward hand in hand.",Heating systems;Deep learning;Visualization;Explainable AI;Noise reduction;Feature extraction;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431160,IEEE Conferences,,,,,,
Deep Learning-Based Medical Image Classification Segmented with Particle Swarm Optimization Technique,S. Singh; P. K. Jain; N. Sharma; M. Pohit,2023 OITS International Conference on Information Technology (OCIT),########,2023,"Medical image segmentation and classification is a prior task in medical image analysis. Therefore, a generalized method is required to process these images efficiently. Objective of our work is to use a nature inspired metaheuristic algorithm, particle swarm optimization (PSO). The algorithms is applied on three types of databases available in repositories. These databases include breast cancer ultrasound, chest-X- Rays, and carotid ultrasound images. After performing PSO clustering, medical images are classified into binary class using state of the art deep learning models VGG16, GoogleNet and ResNet50 models. Out of these DL models GoogleNet performs better classification performance for all types of database. The PSO-based approach effectively produces distinct ROI clusters for cluster 3, while the cluster 4-based method highlights variations in ROI tissue density. In summary, the PSO-based method stands out as a robust tool for delineating regions of interest in medical images.",Image segmentation;Ultrasonic imaging;Databases;Particle swarm optimization;Task analysis;Biomedical imaging;Residual neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430516,IEEE Conferences,,,,,,
Transfer Learning with Deep Convolutional Neural Networks for Respiratory Disease Classification in X-Ray Images,L. DaÅ¡iÄ‡; O. PaviÄ‡; T. Geroski; D. MilovanoviÄ‡; M. PetroviÄ‡; N. FilipoviÄ‡,2023 IEEE 23rd International Conference on Bioinformatics and Bioengineering (BIBE),########,2023,"Medical imaging plays an important role in medicine today, assisting in illness diagnosis and therapy. For limited medical image datasets, training from scratch is not an option, hence transfer learning emerges as a solution, with ImageNet weights being utilized as initial weights, followed by fine-tuning. This paper takes a different approach by introducing transfer learning approach with pretrained architecture DenseNet121 with CheXNeXt weights. Collected dataset consisted of 227269 X-ray images from public databases and 684 chest X-ray images from a retrospective study conducted in the University Clinical Center of Kragujevac and includes information on atelectasis, cardiomegaly, parenchymal consolidation, edema, effusion, emphysema, fibrosis, hiatus hernia, infiltration, pleural thickening, non-viral pneumonia, pneumothorax, viral pneumonia in the form of Covid-19, tuberculosis as well as tumors in the form of mass and nodules. The results show that the model is able to distinguish between the healthy and diseased lungs with average AUC of 0.91 (the lowest AUC of 0.8 for emphysema and the highest AUC for of 0.99 for pneumonia and 0.98 for COVID-19). Although the results seem promising, additional fine tuning may be necessary to improve other metrics. Future research will focus on this aspect, as well as on creating a glass box system for classification.",Measurement;Transfer learning;Lung;Emphysema;X-ray imaging;Medical diagnostic imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431879,IEEE Conferences,,,,,,
Deep Learning Enabled Pneumonia Detection From Chest X-rays: A Transfer Learning Based Ensemble Classification Approach,M. Paul; R. Naskar,"2023 IEEE 3rd International Conference on Smart Technologies for Power, Energy and Control (STPEC)",########,2023,"Deep learning has revolutionized medical image analysis, leading to advancements in diagnosis and treatment. This article examines the application of deep learning techniques, specifically convolutional neural networks (CNNs), for diagnosing pneumonia in chest X-rays. Utilizing transfer learning algorithms and ensemble methods, the proposed method aims to improve feature extraction accuracy and automated pneumonia identification techniques. The research compares three transfer learning modelsâ€”DenseNet169, VGG16, and InceptionV3â€”and employs ensemble techniques to generate more precise predictions. The architecture of the ensemble model combines the probabilistic outputs of each base model using a soft voting layer, resulting in enhanced predictive performance. Deep learning and ensemble approaches can potentially improve pneumonia detection from chest X-ray images, as demonstrated by these findings.",Deep learning;Pulmonary diseases;Transfer learning;Predictive models;Data models;Ensemble learning;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430698,IEEE Conferences,,,,,,
Brain Tumor Diagnosis with MCNN-Based MRI Image Analysis,R. Nidhya; R. Kalpana; G. Smilarubavathy; S. M. Keerthana,2023 1st International Conference on Optimization Techniques for Learning (ICOTL),########,2023,"Brain tumors are critical life-threatening medical condition that requires timely and accurate diagnosis for effective treatment. Magnetic Resonance Imaging (MRI) is a widely used and non-invasive medical imaging technique for the detection and diagnosis of brain tumors. In recent years, deep learning approaches, particularly Convolutional Neural Networks (CNNs), have shown remarkable success in medical image analysis, including brain tumor detection. This paper presents a novel approach for brain tumor detection using a Modified Convolutional Neural Network (MCNN) on MRI images. The proposed solution will utilize a deep learning architecture that employs Convolutional Neural Networks (CNNs) for feature extraction and classification. The MCNN architecture consists of a deep Convolutional Neural Networks with a unique combination of convolutional layers, pooling layers, and fully connected layers. Furthermore, we introduce several modifications to the traditional CNN architecture, including the additional layers to improve feature extraction and spatial attention. These modifications aim to address the challenges associated with the complex and subtle nature of brain tumor images in MRI scans. The developed system will be evaluated using standard metrics such as accuracy, sensitivity, specificity, and F1 score. The results will be compared to existing methods for brain tumor detection to demonstrate the effectiveness and potential clinical utility of the proposed approach. The proposed model's superior performance highlights its potential to assist healthcare professionals in early and accurate brain tumor diagnosis, ultimately contributing to better patient care and outcomes. In the proposed CNN model, we observed the average accuracy value on the training data is 98%, with an average loss value of 0.14181. However, the findings on the test data show a significant difference: the average accuracy value on the test data is 90%, with an average loss value of 0.44037.",Image analysis;Magnetic resonance imaging;Training data;Brain modeling;Feature extraction;Convolutional neural networks;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435262,IEEE Conferences,,,,,,
Advancements in Optimization Algorithms for Lung Nodule Detection and Classification: A Review,M. K. Kumar; A. Amalanathan,2023 1st International Conference on Optimization Techniques for Learning (ICOTL),########,2023,"Lung cancer remains a leading cause of cancer-related deaths globally. Timely identification and categorization of lung nodules are crucial for improving prognoses and survival rates. This review explores various optimization algorithms applied to the detection and classification of lung nodules, analyzing their effectiveness in computed tomography (CT) scans. The study delves into evolutionary, swarm-based, and gradient-based algorithms, evaluating their impact on sensitivity, specificity, accuracy, and computational efficiency. It also compares traditional machine learning, deep learning, and hybrid models while addressing associated challenges and proposing potential solutions.",Deep learning;Machine learning algorithms;Computational modeling;Lung;Lung cancer;Classification algorithms;Optimization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435253,IEEE Conferences,,,,,,
Evaluation and Classification of Kidney Stone Detection Using Deep Learning Techniques,M. Nadeem; G. Tan; M. Altaf; M. K. Mumtaz; A. Fatima; Noshi,2023 6th International Conference on Software Engineering and Computer Science (CSECS),########,2023,"Kidney stone detection is a crucial task in medical diagnostics where early identification can mitigate severe health complications. This research employs advanced deep-learning techniques to classify four types of renal ultrasound images: cyst, normal, stone, and tumor. Three pre-trained and customized neural network architectures-EANet, InceptionV3, and SqueezeNet-are utilized for this purpose. The methodology was rigorously evaluated on a testing dataset consisting of 3,734 renal ultrasound images. Results demonstrate an overall accuracy of 95.8% for EANet, 96.14% for InceptionV3, and 96.1% for SqueezeNet. Comprehensive comparative analysis employing metrics such as accuracy, precision, recall, F1-score, and ROC AUC score reveals that Inception V3marginally outperforms both EANet and SqueezeNet across multiple metrics. The research signifies a substantial advancement in the field of kidney stone detection and poses a promising direction for future clinical implementation.",Measurement;Training;Pathology;Ultrasonic imaging;Neural networks;Medical diagnostic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10428612,IEEE Conferences,,,,,,
Recognition and Classification of Medical Images using Convolutional Neural Network,A. Makandar; N. Jadhav,"2023 International Conference on Communication, Security and Artificial Intelligence (ICCSAI)",########,2023,"Pneumonia diagnosis is indeed challenging and error-prone to diagnose in chest X-ray pictures as other lung infection transparency is also prominent in the scan. Manually analysing X-rays by different professionals can yield distinct conclusions and inaccuracies. Implementing convolutional neural network (CNN) based models that can correctly categorize specific pneumonia categories can help with targeted treatment. The objective of this study is to utilize a pre-trained CNN variation along with Mobile Net in order to train a data set from an open source comprised of clinical images of an X-ray, followed by a training and validation analysis of the method. By analysing the data, the accuracy and adaptability of the CNN model has been evaluated. The highlights of the study defines that the imbalanced data has been balanced and enhanced the true positive prediction with improved accuracy and precision.",Training;Image recognition;Pulmonary diseases;Computational modeling;X-rays;Convolutional neural networks;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421256,IEEE Conferences,,,,,,
UNet Segmentation based Effective Skin Lesion Detection using Deep Learning,A. K. Dubey; A. Jain; A. Panwar; M. Kumar; H. Taneja; P. S. Lamba,"2023 International Conference on Communication, Security and Artificial Intelligence (ICCSAI)",########,2023,"Skin cancer is a common and possibly fatal condition. Effective therapy depends on early discovery and precise diagnosis. This study proposes a two-step, segmentation-and classification-based method for skin lesion analysis. The U-Net architecture, a semantic segmentation model based on deep learning, is used in the initial stage to segment skin lesions. On the test set, the suggested method achieves a promising segmentation accuracy of 94.88%. Precise segmentation helps separate the skin lesions from the surrounding environment and facilitates further classification. Using a Support Vector Machine (SVM) classifier, the segmented lesions are classified into benign and melanoma categories in the second stage. The classification results show a 78% accuracy rate, indicating that the suggested method has the capacity to differentiate between benign and malignant skin lesions.",Deep learning;Semantic segmentation;Support vector machine classification;Medical treatment;Melanoma;Skin;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421443,IEEE Conferences,,,,,,
Brain Tumor Classification Based on Deep Learning Algorithms: A Systematic Literature Review,M. R. Ramadhani; I. Soesanti; I. Hidayah,2023 8th International Conference on Information Technology and Digital Applications (ICITDA),########,2023,"Along with the times, the medical field has changed and developed with the help of technology that is very helpful and makes it easier for both medical personnel and patients. The application of Artificial Intelligence (AI) also helps in the medical field, especially in medical imaging of Magnetic Resonance Imaging (MRI) images in the diagnosis of brain tumors, but in practice, there have been quite a number of studies trying to apply the application of AI to the diagnosis of brain tumors with results which is quite good with a variety of approaches to Deep Learning methods. Therefore, this is a challenge for the authors and this study aims to apply the right and best deep learning method to produce a higher quality brain tumor classification than previous studies so that the diagnosis of brain tumor disease in patients can improve even better. To identifying and extracting relevant and important data from recent studies, we conducted a systematic literature review using the PRISMA 2020 guidelines. The comparative analysis of recent studies correlated with brain tumor classification using deep learning techniques is considered in this systematic literature review. The outcome of this paper states the various research gaps identified from the systematic literature review.",Deep learning;Systematics;Bibliographies;Magnetic resonance imaging;Artificial intelligence;Medical diagnostic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10427361,IEEE Conferences,,,,,,
CoVaD-GAN:An efficient Data Augmentation technique for COVID CXR Image Classification,A. Dash; T. Swarnkar,2023 2nd International Conference on Ambient Intelligence in Health Care (ICAIHC),########,2023,"In recent years, medical image analysis has witnessed a rise in the use of deep learning methods, notably Generative Adversarial Networks (GANs). This research introduces the use of generative adversarial networks (GANs) to supplement the data utilized by deep learning models for disease identification. The COVID-19 CXR chest X-ray image collection is used with the suggested approach to detect the virus. Data augmentation with DCGAN and Vanilla GAN is performed to address data scarcity, and then synthetic data is evaluated using deep learning. The suggested model (VGG16+ResNet50+ Dense-Net and LSTM), with a detection accuracy of 98%, a sensitivity of 99%, and a specificity of 100%, surpasses those trained using just real data, according to the findings. These conclusions are supported by evaluation metrics such as precision, recall, F1-score, specificity, and synthetic images produced using data augmentation that achieve 11.685 peak signal-to-noise ratio (PSNR) and 0.598 structured similarity index method (SSIM) values. These metrics also demonstrate the effectiveness of the synthetic data augmentation method. In the end, the paper shows how GAN-based data augmentation could be used to improve COVID-19 CXR image classification and suggests more research and optimization of GAN architectures for this use. The research contributes to the field of medical image analysis, helping to prepare the way for more precise deep-learning techniques for COVID-19 diagnosis and providing essential assistance to healthcare workers in the Fight against the pandemic.",COVID-19;Deep learning;PSNR;Generative adversarial networks;Data augmentation;X-ray imaging;Synthetic data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431455,IEEE Conferences,,,,,,
"Beyond Pixels: Tackling COVID-19 with CNN, Transfer Learning, and Traditional ML Classifiers",S. C. B. Jaganathan; S. Devaraju; D. M; T. Balamukesh; R. Sah; S. Agarwal; A. Baheti,2023 2nd International Conference on Ambient Intelligence in Health Care (ICAIHC),########,2023,"This study addresses the critical urgency of early Covid-19 detection, a vital aspect in preventing potential fatalities due to delayed treatment. Deep Learning (DL) techniques offer a promising avenue for early screening using Chest X-ray images to facilitate medical diagnosis. In pursuit of this objective, Convolutional Neural Networks (CNNs), namely VGG16, ResNet50, and EfficientNet, were harnessed to detect the presence of Covid-19 in X-ray images, utilizing a dataset sourced from KAGGLE. These CNN models underwent rigorous training on preprocessed images representing both Covid-19 and Pneumonia cases, with a focus on enhancing edge detection specific to Covid-19. The architectural design of these networks was meticulously optimized for the classification task, incorporating layers and learning parameters. The study's findings underscore the remarkable similarity in outcomes produced by VGG16, ResNet50, and EfficientNet, taking into account key factors such as training time, learning rate, and overall accuracy. This suggests that these DL-based models offer comparable performance in the domain of Covid-19 detection, reaffirming their robustness and efficacy in medical imaging analysis, potentially contributing to improved patient care. Furthermore, the study emphasizes the significance of fine-tuning model parameters and considerations regarding computational resources when implementing DL solutions for medical image analysis, thereby presenting important insights for the efficient utilization of resources in clinical settings.",COVID-19;Pulmonary diseases;X-ray imaging;Biomedical imaging;Training;Testing;Microorganisms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431404,IEEE Conferences,,,,,,
AlzNET: Computational Clinical Decision Support for Diagnosing Alzheimer's Disease from MRI Images,G. Mushtaq; V. K,2023 International Conference on Next Generation Electronics (NEleX),########,2023,"Globally Alzheimer's disease (AD) has been recognized as the primary cause of dementia which may eventually lead to fatality. It is a neurodegenerative disorder that causes adverse effects like the death of brain cells and brain atrophy. The deep learning approach has been employed in every field due to its efficacy in producing desired results, especially in medical imaging domain. In this paper, we developed a customized deep learning-based computational model called AlzNET for the classification of Alzheimer's disease. The data set used in this study contains Magnetic Resonance Imaging (MRI) brain scan images of normal and AD-affected people of different stages. The AlzNET model categorizes the MRI images into one of four categories including mild-demented, moderate-demented., non-demented., and very mild-demented based on the severity. To construct the overall framework for our proposed model, we use the Convolutional Neural Network (CNN) that reports an accuracy of 0.821. The images were transformed before feeding into the framework to process the MRI faster. Experimental results demonstrates that the proposed AlzNET computational model outperforms other popular deep neural networks family including VGG19, ResNet 101, DenseNet169, and InceptionV3.",Deep learning;Magnetic resonance imaging;Computational modeling;Brain modeling;Convolutional neural networks;Alzheimer's disease;Next generation networking,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421608,IEEE Conferences,,,,,,
A Review on Brain Tumour Detection and Classification Using Deep Learning Techniques,B. Sachdeva; J. Panda,2023 3rd International Conference on Advancement in Electronics & Communication Engineering (AECE),########,2023,"Successful treatment and better patient outcomes depend on early identification and correct diagnosis of brain tumours. The study looks at how deep learning may help detect brain tumours, including how convolutional neural networks (CNNs) can change the face of diagnostic imaging. It emphasizes the significance of accurate and early detection of brain cancer for effective treatment and improved patient outcomes. The study discusses the challenges associated with tumour segmentation and classification, such as variations in size, shape, and location. To overcome these challenges, deep learning techniques are utilized to analyse brain scans and identify tumour abnormalities. However, the paper underscores the importance of these models serving as tools to assist medical professionals rather than replacing their expertise. Additionally, the paper presents a comparative study that evaluates different techniques, artificial neural networks (ANN) achieve the highest accuracy at 99%, followed by Support Vector Machine (SVM) at 98.9%. vision transformer (ViT), Extreme Gradient Boosting (XG Boost), CNN-based dense Efficient Net, and Deep Neural Network (DNN) also show high accuracies above 98%. However, U-Net, Alex Net, hybrid ensemble method (Random Forest (RF), K-Nearest Neighbour (KNN), Decision Tree (DT), and Region-based convolutional neural networks (R-CNN)) have lower accuracies ranging from 91.66% to 97.305%. Overall, the findings highlight the potential of deep learning in enhancing the efficiency and precision of brain tumour diagnosis, leading to earlier treatment and improved patient outcomes.",Deep learning;Support vector machines;Image segmentation;Brain modeling;Convolutional neural networks;Medical diagnostic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10428097,IEEE Conferences,,,,,,
EfficientAD: A Deep Learning Approach for Multi-Stage AD Classification Using Transfer Learning,A. Thakran; Y. K. Gupta,"2023 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)",########,2023,"Alzheimer's disease (AD) is a neurodegenerative disease that has affected millions of elderly people worldwide. It is the most common cause of dementia among the aged population. It affects the cognitive skills of a person making it difficult to perform daily chores. Due to the incurability and progressive nature of AD, it has garnered a huge interest from the researchers. Neurologists treating AD patients analyze various neuroimaging scans using Computer-Aided Diagnosis (CAD) systems. These systems use DL algorithms to detect AD early and accurately. In this paper, various pretrained CNN models namely EfficientNetB7, VGG16, VGG19 and VGG Ensemble models were employed for binary and multistage classification of AD by using transfer learning. This paper achieved an accuracy of 97.88% using the EfficientNetB7, 91.36%, 92.81 % and 95.16% using VGG16, VGG19 and VGG Ensemble models respectively.",Solid modeling;Computational modeling;Transfer learning;Sociology;Brain modeling;Statistics;Older adults,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425684,IEEE Conferences,,,,,,
A Smart Device Employs Vgg-16 to Identify Brain Stroke Using CT Scan Images,P. Shourie; V. Anand; S. Gupta,"2023 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)",########,2023,"Because brain stroke is a potentially fatal medical illness, prompt and correct diagnosis is essential for prompt health treatment, better patient outcomes, and improved mortality. It can take a long time and be susceptible to human error to manually evaluate medical imaging data conventionally for stroke diagnosis. This study examines how Convolutional Neural Networks (CNNs) can be used to dynamically classify brain strokes in order to overcome these issues. The suggested study employs the VGG-16 CNN model to classify CT scans of the brain following a stroke. For this challenge, the primary extractor is the VGG-16 model, which is initialized using pre-trained weights from the ImageNet dataset. This allows transfer learning to make use of the network's learned features. By using CT scan pictures to diagnose cerebral stroke, the suggested method demonstrates the potential of VGG-16 as a crucial tool for helping radiologists and other medical professionals. The incorporation of these technologies into clinical processes may help to speed up the diagnosis of strokes, allowing for quicker treatments and better patient care.",Computed tomography;Transfer learning;Stroke (medical condition);Brain modeling;Convolutional neural networks;Smart devices;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425282,IEEE Conferences,,,,,,
Image Captioning for Chest X-Rays Using GRU - Based Attention Mechanism,T. Kodavati; G. B. Mohan; R. P. Kumar; E. R,"2023 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)",########,2023,"Medical image captioning is a field that combines medical imaging with natural language processing (NLP). Since the development of deep learning techniques in both NLP and medical image analysis, this area of study has gained significant progress. This research aims to develop a system for providing a brief description when given an image of chest x-ray using a GRU-based Attention Model, which gave better training and validation accuracy and BLEU scores when compared to a simple encoder-decoder, attention mechanism implemented with context flow and global flow and other state-of-the-art models using chexnet. The Chest X-Ray Images dataset, consisting of a large number of images of different chest x-rays suffering from pneumonia with different points of view (anterior-posterior), is utilized for training and validation of the model. The proposed model is trained using a combination of chest x-ray images along with their text features. The findings demonstrate the potential of using the GRU-based attention model for detection and classification of pneumonia, which can be applied for diagnosis in hospitals, clinics, emergency departments and screening programs.",Training;Visualization;Pulmonary diseases;Computational modeling;X-ray imaging;Medical diagnostic imaging;Context modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425174,IEEE Conferences,,,,,,
Early Detection of Alzheimer's Disease Based on Deep Learning Approach Using Brain MRI,A. Chakrabortty; S. R. Rahman; T. Islam; M. R. Islam; M. A. Mamun,2023 6th International Conference on Electrical Information and Communication Technology (EICT),########,2023,"Alzheimer's disease is one kind of neurological condition for which people gradually decrease their mental capacity and memory. The number of Alzheimer's patients nowadays is rising quickly, which is quite concerning for the advancement of humanity. The scenario is much more terrible because this illness has no cure. A professional neurologist needs to examine medical imaging techniques including PET scans, CT scans, and MRIs for diagnosing an Alzheimer's patient. This procedure is time- and resource-intensive. Many people hesitate to treat Alzheimer's disease because of this instability. The potential for early detection of Alzheimer's disease might be achieved by the use of Magnetic Resonance Imaging (MRI). However, the MRI dataset has an issue in class imbalance problem. To address the problems in the dataset, a data balancing technique called SMOTETomek is used and then a lightweight CNN model is developed that uses MRI images as input and determines a patient's stage. In terms of recall, precision, accuracy, ROC curve, and f1 score, the performance of the CNN model has been compared with a few transfer learning algorithms. Therefore, using the proposed model, each Alzheimer's patient may determine what stage of the disease they are now in and take further actions to lower the mortality rate of Alzheimer's patients. The model under consideration demonstrates superior performance when compared to previous models, with an accuracy rate of 97.50%.",Training;Magnetic resonance imaging;Computational modeling;Computed tomography;Transfer learning;Data models;Alzheimer's disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10427876,IEEE Conferences,,,,,,
BI-TLM: Bilinear Interpolation with Transfer Learning Model for Breast Cancer Classification,R. Islam; S. Hossen; S. M. Ariful Islam; S. Akter,2023 6th International Conference on Electrical Information and Communication Technology (EICT),########,2023,"Breast cancer is another fastest growing reason for death in women, trailing only lung cancer. Breast cancer mortality can be reduced if it is discovered early and treated. Because conventional breast cancer assessment needs much longer, an automated method needs to be invented for early cancer detection. This research provides a network for detecting breast cancer from ultrasound images which uses deep neural networks and a trainable sub-layer network to select the best features. Because ultrasound images contain noise and artifacts, a bilinear filtering method is considered in the preprocessing step to reduce the noise and artifacts. The DenseNet201 model was used in our proposed network as a deep learning-based technique since It has the potential to disperse feature strength, stimulate feature reuse, and significantly decrease the parameters. The proposed system along with other models were trained and evaluated on a balanced ultrasound dataset divided into benign, malignant, and normal classes. Each modelâ€™s performance was evaluated utilizing several accuracy metrics. Among the models examined, the suggested model outperformed the others, suggesting its potential for accurately diagnosing breast cancer from ultrasound images. The discoveries of the work contribute to medical imaging and emphasize the efficacy of the DenseNet201 model.",Interpolation;Ultrasonic imaging;Transfer learning;Artificial neural networks;Transformers;Breast cancer;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10427740,IEEE Conferences,,,,,,
Detection of Human Pancreatic Cancer using ML and DL Model: A State-of-the-Art Review,M. V. M; M. Subramoniam,2023 Annual International Conference on Emerging Research Areas: International Conference on Intelligent Systems (AICERA/ICIS),########,2023,"Pancreatic cancer is a highly dangerous cancer with a poor prognosis. In radiological imaging, computer-aided pancreatic tumor image segmentation is commonly employed by CAD, diagnosis, and quantitative evaluations such as CT and MRI. Over the years, pancreatic cancer rates in India have been increasing, with 100,000 new cases reported annually. The survival rate for patients with pancreatic cancer can be increased with early detection, effective treatment, and medical care delivery. Various imaging methods are used for detection of pancreatic cancer, and radiologists can utilize computer-aided diagnostic techniques to detect and diagnose abnormalities earlier and more rapidly. Before recommending a biopsy test, the radiologist consults with a computer-aided diagnostic. Several Computer-Aided Diagnosis (CAD) methods have been developed for the early detection of pancreatic cancer using mammography images, with most CAD systems focusing on identifying and detecting nodules. Since treatment for pancreatic cancer is based on the stage of cancer, staging pancreatic cancer at detection is critical. Therefore, this study focuses on evaluating CAD methods for segmenting nodules and identifying various stages of cancer to aid radiologists in assessing the disease.",Solid modeling;Image segmentation;Design automation;Pancreatic cancer;Public healthcare;Medical diagnostic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10420045,IEEE Conferences,,,,,,
A U-Shaped Architecture Based on Attention Mechanism,C. Lu; L. Lin; M. Xu,2023 IEEE 23rd International Conference on Communication Technology (ICCT),########,2023,"With the rapid development of computer and artificial intelligence, image segmentation technology based on deep learning plays an increasingly important role in the medical field. In this paper, we propose the attention mechanism-based U-Net++, a new and more powerful medical image segmentation architecture. Our architecture is essentially an attention-based encoder-decoder network that uses dense skip connection to capture features at different levels, by introducing channel attention and spatial attention, the neural network pays more attention to the pixel regions that are decisive for classification and ignores the insignificant regions, so as to handle the distribution relationship of the feature map channels. In addition, we design a hybrid loss function that integrates the cross-entropy loss, Dice loss, and Focal loss to focus more on the part which is difficult to divide of the medical image dataset. We evaluate the attention-based U-Net++ compared to U-Net and UNet++ architectures in a medical image segmentation task for skin lesion datasets. Our experiments show that the average IoU gain of the U-Net++ networks based on the attention mechanism exceeds that of general U-Net and UNet++.",Image segmentation;Neural networks;Computer architecture;Skin;Lesions;Task analysis;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10419575,IEEE Conferences,,,,,,
Classification Of Retinal Fundus Images Using Convolution Neural Network (CNN),M. Qaddour; Y. B. Touimi; K. Minaoui,2023 IEEE International Conference on Advances in Data-Driven Analytics And Intelligent Systems (ADACIS),########,2023,"Timely and precise identification of retinal diseases holds paramount significance in preventing vision impairment and enhancing clinical outcomes for patients. This research delves into the application of Deep Learning techniques to categorize and forecast seven prevalent eye ailments: â€™opacity,â€™ â€™diabetic retinopathy,â€™ â€™glaucoma,â€™ â€™macular edema,â€™ â€™macular degeneration,â€™ â€™retinal vascular occlusion,â€™ and â€™normal.â€™ Retinal images play an indispensable role in the diagnosis of these ocular conditions, offering invaluable insights to ophthalmologists. Given the retinaâ€™s sensitivity to microvascular alterations associated with various retinal diseases, substantial research efforts have been dedicated to devising early diagnostic approaches grounded in the analysis of medical imagery..",Sensitivity;Visual impairment;Retina;Eye diseases;Reproducibility of results;Reliability;Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10424224,IEEE Conferences,,,,,,
Remote Sensing Single Image Super-Resolution Benchmarking with Transfer Learning Algorithms,E. Thiruppandiaraj; S. Das,"2023 7th International Conference on Electronics, Materials Engineering & Nano-Technology (IEMENTech)",########,2023,"In the context of real-world applications like medical imaging systems, tracking, astronomical imaging, navigation, and remote sensing (RS), there is a pressing need to enhance or upscale images with minimal errors. This is particularly critical for tasks such as target detection, image classification, and land use mapping. However, remote sensing images often suffer from limitations in spatial, spectral, radiometric, and temporal resolution due to complex atmospheric conditions and sensor constraints. Additionally, acquiring these images can be expensive and time-consuming. In this study, we propose a Single Image Super-Resolution (SISR) method to address these challenges by upscaling low-quality remote sensing images to higher resolution, enabling a better understanding of these images. We also discuss the specific challenges in remote sensing super-resolution techniques and review various upscaling approaches, while analyzing the impact of other factors like weather conditions, image capture time, and different scene types on the techniqueâ€™s effectiveness.",Training;Analytical models;Superresolution;Transfer learning;Spatial resolution;Remote sensing;Meteorology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10423556,IEEE Conferences,,,,,,
Deep Learning Based Automated Pneumonia Detection from X-ray Images,S. K. A. S; Rajashree,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",########,2023,"Artificially Intelligent (AI) based systems possess the capability of successfully realizing the insights from frequently available huge datasets. Various health care and medical imaging applications have been significantly benefited for making accurate decisions for improving the health conditions of the patients. Nowadays, medical science area have witnessed enormous advancement with the effective utilization of AI and deep learning (DL) techniques for various applications namely automated patient monitoring, prescribing medications, in-depth analysis of severe disease patterns and performing laser based surgeries based on the observations of the disease patterns occurring in the medical based digital images. Most of the automated medical oriented digital images are generated using MRIs, X-rays and CT scans which makes the processing of such images by numerous modern AI and DL algorithms efficient and highly accurate for critical decision making and medical treatment diagnosis. Conclusively, human based expert system is replaced by various AI and DL based systems which has fast processing and accurate reporting capabilities especially in medical science domain. Thus, our article proposes the utilization of AI and DL based techniques to efficiently propose diagnosis for patients suffering from pneumonia based on the gathered X-ray images. Precisely, our article focuses upon the effective utilization of DL based modified convolutional neural network (CNN) and visual geometry group (VGG-16) for accurately classifying the chest X-ray images into normal and pneumonia categories. Proposed model based on the recognition of the pattern changes in the chest X-ray images have the ability to classify the disease into either a normal or a pneumonia case with an overall training and validation accuracy of 97% respectively.",Deep learning;Pulmonary diseases;Digital images;X-rays;Artificial intelligence;Medical diagnostic imaging;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395716,IEEE Conferences,,,,,,
Ortho Vision: Autoencoder-CNN Fusion Approach for Fracture Detection,S. Abhiram; A. K. K; V. G M; A. T; A. S,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",########,2023,"We propose an advanced framework for automated fracture detection in medical X-ray images., harnessing the power of hybrid deep learning methodologies. Through the fusion of autoencodersâ€ adeptness in feature extraction and a convolutional neural networks (CNN) efficiency in recognizing the complex patterns., the proposed model achieves remarkable accuracy in identifying fractures. By integrating data augmentation and preprocessing strategies., the system demonstrates enhanced robustness in handling variations in image quality and presentation. This innovation holds substantial promise in revolutionizing the field of medical diagnostics by offering a reliable and efficient tool for clinicians to expedite fracture diagnosis., facilitate treatment decisions., and ultimately improve patient care. The amalgamation of diverse techniques not only establishes a robust fracture detection mechanism but also showcases the potential of interdisciplinary approaches. The proposed approach demonstrated excellent accuracy in fracture detection with a 92% percentage., which can greatly improve medical diagnosis and treatment planning.",Analytical models;Technological innovation;Robustness;Convolutional neural networks;Medical diagnosis;Medical diagnostic imaging;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394950,IEEE Conferences,,,,,,
CovDNet: A Hybrid Multi - Level Densely Connected Neural Network for Improvised Analysis and Classification of COVID-19 Using Chest CT-Scans,D. S; P. G D; R. Suguna; V. Duraivelu; C. Jamunadevi; S. G. Sundhar,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",########,2023,"COVID-19, caused by the new coronavirus SARS-Co V-2, has turned into a worldwide health emergency, needing speedy and precise diagnostic techniques. This abstract provides a thorough evaluation of research works focusing on COVID-19 identification using DenseNet, a cutting-edge convolutional neural network architecture. DenseNet is notable for its innovative design, which fosters feature reuse while relieving the vanishing gradient problem and enhancing network information flow. Researchers have created novel ways for COVID-19 identification using medical imaging, such as chest X-rays and CT scans, by harnessing the capabilities of DenseNet. The analyzed studies demonstrate DenseNet's efficiency in recognizing COVID-19-specific patterns and distinguishing them from other lung diseases. These investigations have shown remarkable accuracy, sensitivity, and precision, exceeding established machine learning approaches and even professional radiologists. This research study also highlights the difficulties and constraints experienced while using DenseNet to identify COVID-19. Issues such as dataset quantity, class imbalance, and model decision-making process interpretability are addressed. Furthermore, future research paths and prospective enhancements, such as multi-modal data integration and the development of explainable AI systems, are investigated. The experimental results indicate that the proposed approach achieves high accuracy, sensitivity, and precision in the identification of COVID - 19. The proposed methodology achieved an accuracy of 89.74%, sensitivity of 87.25%, and precision of 79.56%, which outperforms the existing state-of-the-art methods. The proposed approach is robust and can effectively differentiate between COVID-19 positive and negative cases, which is essential for early detection and prompt treatment.",COVID-19;Sensitivity;Computed tomography;Statistics;X-ray imaging;Standards;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394895,IEEE Conferences,,,,,,
Enhancing Brain Tumor Detection through CNN-Based Analysis of MRI Scans,S. S; A. S. M; V. P. T; A. V. Gadagkar; K. K. P,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",########,2023,"Brain tumors constitute a grave medical condition necessitating precise and prompt diagnosis to ensure efficacious treatment. Convolutional Neural Networks (CNNs) exhibit promising potential in accurately detecting brain tumors from medical imagery. This paper introduces a CNN-grounded strategy for detecting brain tumors via Magnetic Resonance Imaging (MRI) scans. The method we propose encompasses preprocessing MRI scans to extricate pertinent features and segment the tumorafflicted region. These processed images subsequently fuel a CNN model, comprising multiple convolutional strata for feature extraction, coupled with pooling strata to curtail spatial dimensions. The derived features then traverse through a fully connected layer for classification. Our method's efficacy is assessed using an accessible repository of MRI scans featuring brain tumors. Our experiments reveal that the CNN model proposed attains a 95% accuracy rate, accompanied by a 93% sensitivity rate, and a 96% specificity rate in brain tumor detection. These findings underscore our method's proficiency in brain tumor detection employing CNNs.",Image segmentation;Sensitivity;Magnetic resonance imaging;Aerospace electronics;Brain modeling;Feature extraction;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395347,IEEE Conferences,,,,,,
Earlier Detection of Pancreatic Cancer Using Neural Network Based Optimization Technique,K. S. C. Mauryan; H. Nishat; U. Arunkumar; P. Megaladevi,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",########,2023,"Medical healthcare systems are being extensively studied, giving computer technology lots of space to innovate. The most important medical research is predicting cancer, which may take numerous forms and affect many body parts. One of the most common fatal diseases is pancreatic cancer, which cannot be cured once identified and is sometimes unexpected since it is placed in the abdomen beyond the stomach. CT and MRI often give CAD, quantitative evaluations, and automated pancreatic cancer segmentation. These cancer classification methods might identify, predict, and assist personalized medicine to cure cancer without malignant invasions. Flying Squirrel optimization segments, extracts, and classifies features. CNN coupled with Frog Leap optimization. The proposed approach uses frog leap optimization to identify picture normalcy and abnormality. The suggested method minimizes errors for correct classification. Segmenting the aberrant picture with an adaptive flying squirrel algorithm determines cancer size and severity. The more efficient CNN-FLFS method predicts pancreatic cancer with 99% accuracy.",Image segmentation;Stomach;Space technology;Pancreatic cancer;Feature extraction;Prediction algorithms;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395597,IEEE Conferences,,,,,,
Identification of Brain Tumors in MR Images Using UNet CNN Model,L. Saipogu; A. S. Reddy; G. Malleswari,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",########,2023,"A benign brain tumor, characterized by the growth of non-cancerous brain cells, has the potential to undergo malignant transformation. Gliomas, the most prevalent primary brain tumors, originate from the development of abnormal glial cells in the brain and spinal cord. Magnetic resonance imaging (MRI), a widely accepted non-invasive imaging method, can detect brain tumors, and offer various tissue contrasts using diverse imaging modalities. Traditionally, the laborious and time-consuming task of manually segmenting and analyzing structural MRI data of brain tumors was the domain of neuroradiologists. However, recent advancements in comprehensive and automated segmentation methods for brain tumors are poised to greatly improve both cancer detection and treatment. BRATS 2015 is a dataset used for the segmentation of brain tumor images, consisting of 200 MRIs depicting high-grade gliomas and 44 MRIs displaying low-grade gliomas. The proposed aims to develop an automated deep learning model for detecting and classifying brain cancers using MRI data. Extensive simulations were carried out on the BRATS dataset to evaluate the enhanced performance in categorizing brain tumors. Thanks to progress in computer-aided design, machine learning, and deep learning techniques, we now have the capability to identify cancerous lesions in medical images.",Deep learning;Image segmentation;Spinal cord;Magnetic resonance imaging;Computational modeling;Training data;Brain cancer;Brain modeling;Task analysis;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395702,IEEE Conferences,,,,,,
Enhancing Biomedical Image Interpretation Through a Hybrid Machine Learning Algorithm,R. Rajkumar; L. N. Vempaty; R. Thiyagarajan; C. K. A; H. Hemane; V. S,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",########,2023,"In contemporary medicine, biomedical image interpretation is essential for disease diagnosis and the selection of appropriate treatments. However, manually scrutinizing these images is time-consuming and may lead to erroneous conclusions. The proposed work offers a novel approach to resolving these issues by utilizing a mixed machine-learning technique to improve the interpretation of biological images. The proposed system accurately evaluates physical images using machine learning techniques, including convolutional neural networks and decision trees. The algorithm aims to integrate the most beneficial aspects of multiple image analysis methods to enhance their overall performance. Using a carefully selected dataset, demonstrate the algorithm's precision and robustness compared to other approaches. The findings imply that the algorithm could substantially alter the interpretation of biological images in clinical and academic settings. This discovery has far-reaching implications, paving the way for improved diagnostic precision and further study of the human body. This hybrid method is a promising step toward automating image processing and paves the way for new research and implementation opportunities in healthcare technology.",Machine learning algorithms;Machine learning;Biology;Robustness;Decision trees;Convolutional neural networks;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395806,IEEE Conferences,,,,,,
Prostate Cancer Classification using Cat Swarm Optimization with Deep Learning on MRI Images,H. V; K. V. Archana; B. D; U. M. S; P. Sasireka; S. G P,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",########,2023,"Prostate Cancer (PC) is a common malignancy amongst men globally, and accurate classification plays a significant part in its diagnoses and treatment planning. Due to its superior soft tissue contrast, Magnetic Resonance Imaging (MRI) has developed as a valuable imaging modality for the assessment of PC. PC classification on MRI is a major task in medical imaging analysis, and deep learning (DL) approaches are employed to accomplish automated and accurate classification. In recent times, Convolutional Neural Network (CNN) structure was selected for the classification of PC. This manuscript introduces an automated Prostate Cancer Classification using Cat Swarm Optimization with Deep Learning (PCC-CSODL) technique on MRI Images. The main purpose of the PCC-CSODL algorithm lies in the effectual and accurate identification of PC. For achieving this, the PCC-CSODL technique involves NASNet feature extractor at the initial stages. In addition, the CSO algorithm is used for the optimum hyperparameter selection of the NASNet model. For classification purposes, attention based long short term memory (ALSTM) model can be used. The results demonstrated the effectiveness of the PCC-CSODL technique in accurately classifying PC on MRI. It has the potential to assist clinicians in making informed decisions and enhancing patient outcomes in PC management.",Deep learning;Magnetic resonance imaging;Feature extraction;Classification algorithms;Convolutional neural networks;Prostate cancer;Particle swarm optimization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395331,IEEE Conferences,,,,,,
Melanoma Unveiled: Harnessing Convolutional Neural Network ResNet 50 for Precise Segmentation and Detection,M. Mahaboob; S. Ramalingam; S. K; S. K. S; U. T; U. S. A,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",########,2023,"Tumors classified as sarcomas within the realm of skin diseases manifest within the connective, supportive, and enveloping tissues of bodily structures. Due to their limited occurrence within the body and their extensive array, they exhibit heterogeneity when scrutinized through microscopic visuals. These growths can often be mistaken for other conditions like fibroadenoma of the breast, lymphadenopathy, and thyroid nodules. Such misdiagnoses significantly impede the effective medical care and treatment of patients. Many existing models put forth for assessing these tumors tend to overlook the diversity and scale of the data. Hence, our proposition involves an innovative approach utilizing a machine learning-based Convolutional Neural Network RESNET 50 techniques, incorporating an innovative technique for preparing data, which facilitates the extraction of features and subsequent classification. The results affirm that machine learning approaches can effectively enhance the automation of decision-making during the assessment of SDTs. Dermatological disorders encompass ailments that impact the body's largest organ, the skin. Manifesting an array of indications, these conditions may involve symptoms like irritation, inflammation, skin blemishes, and sores. Notable skin diseases comprise hives, dermatitis, psoriasis, eczema, vitiligo, rosacea, and acne. The available treatments for skin disorders differ based on the nature and extent of the ailment, encompassing choices such as topical ointments, oral remedies, light-based interventions, and adjustments in daily habits. In the event of suspecting a skin issue, it's essential to consult a qualified healthcare professional for accurate diagnosis and appropriate medical guidance.",Visualization;Microscopy;Medical services;Melanoma;Feature extraction;Skin;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395375,IEEE Conferences,,,,,,
Detection Of Bronchial Tuberculosis Using ASFF-Yolov5S Model,W. Li; S. Xu; H. Peng; W. Liang,2023 9th International Conference on Systems and Informatics (ICSAI),########,2023,"On CT images, irreversible damage caused by bronchial tuberculosis can be seen, and early observation and intervention can provide assistance for the patientâ€™s prognosis. So in this article, it is proposed to use depth learning to detect CT images of patients. However, in the learning process of deep learning classification models, as each level continues to expand, the focus points of each layerâ€™s features will be covered and the connection will become weaker, leading to the interruption of the connection between levels and the deterioration of the learning effect. In this article, we use an adaptive spatial feature fusion mechanism, which can automatically learn features at different spatial scales and fuse them together, thereby improving the modelâ€™s generalization ability and increasing the connectivity of hierarchical relationships in the judgment of bronchial tuberculosis, making the judgment more accurate. Specifically, based on the YOLO V5 model, various levels of information extracted from the Neck section are utilized to enhance features for prediction through an adaptive spatial feature fusion mechanism. The model proposed in this article outperforms commonly used object detection models in terms of performance and accuracy. In addition, the proposed framework may serve as a preliminary screening tool for the differential diagnosis of bronchial tuberculosis.",YOLO;Adaptation models;Tuberculosis;Fuses;Predictive models;Feature extraction;Neck,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10423315,IEEE Conferences,,,,,,
Breaking the Boundaries of Oncological Diagnosis: A Holistic Framework for Multi-Cancer Identification and Comprehensive Diagnostic Reporting Utilizing Advanced AI and NLP,S. S. V C; R. Ponraj; N. E. T; K. K.,2023 International Conference on Recent Advances in Science and Engineering Technology (ICRASET),########,2023,"Innovative deep learning models for cancer classification, including VGG-19, DenseNet201, MobileNetV3, ResNet50V2, YOLOv5, and GPT-2, have completely changed the way that doctors diagnose cancer. This study introduces a multi-modal technique for an accurate and speedy cancer diagnosis. With the aid of cutting-edge technology, this system combines object identification (YOLOv5), natural language processing (GPT-2), and picture classification models (VGG-19, DenseNet201, MobileNetV3, ResNet50V2) to provide scientists and medical professionals with a flexible toolkit. It generates thorough reports with tumour images and spots malignant irregularities. Diagnostic accuracy is improved by real-time application, which benefits laboratories by reducing turnaround times and medical practitioners by providing an important decision support tool. Reports with tumour photos enhance the capacity to comprehend results. This study is important because it has the potential to improve cancer detection by using cutting-edge algorithms and models. This research also promises to improve patient care, prediction, and therapy. The practical method presented in this paper ushers in a new age in medical diagnostics by empowering laboratories and doctors.",YOLO;Deep learning;Laboratories;Real-time systems;Medical diagnostic imaging;Cancer;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10419980,IEEE Conferences,,,,,,
Automatic Classification and Localization of Lung Diseases from Chest X-Ray Images using Deep Learning,M. H. Imran; P. C. Shill,2023 International Conference on Recent Advances in Science and Engineering Technology (ICRASET),########,2023,"Chest diseases have become an ongoing issue in recent years, having an enormous effect on individuals throughout nations worldwide. If these diseases are not detected in time, they can be fatal and cause death. Chest radiography (CXR) is a cost-effective method of identifying and diagnosing diseases. But identifying chest anomalies from CXR images is time-consuming and needs professional radiologists. Automatic biomedical image segmentation helps speed up disease detection and diagnosis. So, a fully automated system is necessary, and there is a lot of work regarding this. Rapid advancements in deep learning have produced ground-breaking outcomes in this area. Modern networks like U-Net and SegNet, however, frequently perform poorly in difficult domains. This paper proposes a fully automated system for chest disease detection along with a segmentation module that have combined the classification and segmentation tasks into a single model and have developed a modified new architecture that is based on Nested UNet architecture. It has also a lightweight design and is less likely to overfit. And finally, the proposed model has obtained an excellent chest disease detection performance with an accuracy of 92.86%.",Deep learning;Image segmentation;Transfer learning;Lung;Task analysis;X-ray imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10420300,IEEE Conferences,,,,,,
Deep Learning Empowering Diagnosis of Thyroid Nodule Malignancy through Ultrasound Imaging,M. F. Shakeel; M. Hasan Khan; Y. U. Khan,2023 International Conference on Recent Advances in Science and Engineering Technology (ICRASET),########,2023,"Thyroid nodules are often identified and categorized according to their likelihood of developing into cancer, and they can be deadly if not identified accurately. This work widens the categorization to six different classifications from the previous research's binary classification as benign or malignant. The study's proposal of a very effective deep learning-based method, which provides a more thorough knowledge of the malignant potential of thyroid nodules with exceptional precision, addressed the shortcomings of the existing classification methods. The basic model for the suggested technique is EfficientNet-B1, which is well-known for its strong feature extraction skills. The model's performance was further enhanced using regularization approaches, and assessments of the model on isolated data have shown amazing performance with 100% accuracy, sensitivity, and specificity. These exceptional results illustrate the model's amazing capacity to correctly categorize thyroid tumors into several classifications. Further testing of the model's performance against a reference model with a 60% dropout rate produced almost identical results, validating the proposed model's consistency for the unique categorization of thyroid nodules. By overcoming dataset constraints and imbalances, this work proves the superiority of the suggested deep learning-based classification algorithm for thyroid nodules. This model performs better than previous methods, making it a trustworthy tool for physicians in the identification and categorization of thyroid tumors.",Deep learning;Data augmentation;Data models;Classification algorithms;Thyroid;Cancer;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10420029,IEEE Conferences,,,,,,
Leveraging Deep Learning Techniques for Profiling and Categorizing Lung and Pancreatic Tumors,A. Jeelani; C. Vaishnawi; R. K. Yadav,2023 International Conference on Recent Advances in Science and Engineering Technology (ICRASET),########,2023,"The study provides a unique strategy for ""Leveraging Deep Learning Techniques for Profiling and Categorizing Lung and Pancreatic Tumors."" The suggested technique incorporates Convolutional Neural Networks (CNNs) for image analysis, Recurrent Neural Networks (RNNs) for temporal modeling, and Transfer Learning to fine-tune pre-trained models. It aims to improve tumor classification precision by tackling the particular challenges given by pancreatic and lung cancers. The outcomes of our investigation show that the proposed strategy outperforms existing methods in numerous key areas. F1 was 0.85, specificity was 0.87, recall was 0.82, accuracy was 0.85, precision was 0.88, and area under the curve was 0.92, according to the results. These values routinely exceed six original conventional approaches, including CNNs, RNNs, Transfer Learning, Autoencoders, Support Vector Machines (SVMs), and Reinforcement Learning. The confusion matrix measurements further underscore the superiority of the suggested strategy with more true positives and true negatives, and fewer false positives and false negatives. In conclusion, the suggested strategy improves the accuracy and objectivity of lung and pancreatic tumor profiling by harnessing the power of deep learning and a multi-faceted approach. It offers a more tailored, efficient, and data-driven approach to the arduous issue of tumor classification. The data clearly illustrate its superiority over traditional approaches, making it a potential innovation in the field of cancer.",Deep learning;Support vector machines;Recurrent neural networks;Transfer learning;Lung;Reinforcement learning;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10420133,IEEE Conferences,,,,,,
Attention-Guided Residual Network for Skin Lesion Classification Using Deep Reinforcement Learning,R. Prasanna Kumar; K. Venkatraman; C. Jawahar; B. Harish; S. Bharathraj; K. Mukesh,2023 International Conference on Integrated Intelligence and Communication Systems (ICIICS),########,2023,"The increasing incidence of skin cancers emphasizes the critical need for early detection, making effective therapeutic intervention possible. Although existing skin lesion segmentation methodologies offer some solutions, they are often hindered by inconsistent annotations present in the datasets. Accurate delineation of lesion boundaries is fundamental for diagnosing various skin conditions. In this research, a skin lesion segmentation method is introduced using an attention-guided reinforcement learning algorithm, framed within the context of a Markov Decision Process. This approach mirrors the method doctors use when identifying areas of concern on the skin. To improve classification accuracy, a self-attention mechanism is integrated into the network. This addition refines the focus on vital regions within the images, ensuring more precise segmentation. With deep reinforcement learning, the agent is trained to segment these regions, employing a series of actions that are continuously variable. This continuous action space is exploited by the segmentation model, which utilizes the deep deterministic policy gradient algorithm, allowing for iterative refinement from broader segmentation to detailed delineation. The proposed method is tested on benchmark dataset, ISIC 2017, and achieved an accuracy of 97.10% for naevus, 96.33% for melanoma, and 95.13% for seborrheic keratosis cases. Additionally, the enhanced approach with attention module, outperformed several contemporary skin lesion segmentation techniques in terms of performance.",Deep learning;Image segmentation;Visualization;Reinforcement learning;Skin;Classification algorithms;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421742,IEEE Conferences,,,,,,
Comparative Analysis of Deep Learning Architecture with Ensemble Learning in Cranial and Mediolateral View Images,H. S. Sushma; K. Sooda,2023 International Conference on Integrated Intelligence and Communication Systems (ICIICS),########,2023,"The study explores the field of anomaly detection employing advanced methods. Mammography analysis for the identification of anomaly is examined in terms of the effectiveness of ensemble learning when combined with the powerful EfficientNet-B7 deep learning architecture. This study attempts to enhance anomaly identification accuracy by utilizing the abilities of these methods. Comparative experimental results show significant enhancements corresponding to the proposed ensemble learning model. The integrated technique outperforms the standalone EfficientNetB7 modelâ€™s 95.81% and 94.89% training and validation accuracies while attaining 98.86% and 98.29% with the proposed approach, respectively and the memory consumption has increased by approximately 18.28% with the proposed approach and 22.84% without the proposed approach. With better generalization and reduced overfitting characteristics, the ensemble model consistently shows higher accuracy over training and validation datasets. These results demonstrate the possibility of ensemble learning to improve the stability and accuracy of detection of anomalies. Selecting the right model and considering ensemble learning into account while analyzing medical images are crucial choices that could impact the diagnostic systemâ€™s reliability, effectiveness, and accuracy.",Training;Computational modeling;Memory management;Random access memory;Data models;Ensemble learning;Anomaly detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421073,IEEE Conferences,,,,,,
Enhancing Respiratory Disease Diagnosis through Deep Learning: A CNN-Based Approach for Image Classification,Laxmibai; V. Patil,2023 International Conference on Integrated Intelligence and Communication Systems (ICIICS),########,2023,"Early and accurate identification is crucial for the efficient treatment of respiratory diseases. Leveraging the capabilities of Deep-Learning (DL), this work presents a Convolutional Neural Network (CNN) model for the detection of respiratory diseases utilizing chest X-ray images. We begin by discussing the significant impact of respiratory diseases on human health and the critical need for timely diagnosis. Our proposed CNN model is meticulously designed to classify these X-ray images, distinguishing between normal lung conditions and pneumonia cases. The results showcase an accuracy of 93.62% without normalization and 95.03% with normalization in this crucial diagnostic task. Furthermore, this work lays the groundwork for future enhancements, including image enhancement techniques, advanced feature extraction, and classification models, with the overarching goal of advancing respiratory disease diagnosis and patient care. Through this research, we contribute to the ongoing efforts to combat respiratory diseases, empowering healthcare practitioners with efficient and accurate diagnostic tools.",Pulmonary diseases;Lung;Feature extraction;Trajectory;Convolutional neural networks;X-ray imaging;Image enhancement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421028,IEEE Conferences,,,,,,
Challenges and Opportunities in Integrating Machine Learning with Medical Imaging: A Comprehensive Review,M. Chaudhary; H. Agrawal,2023 Second International Conference on Informatics (ICI),########,2023,"In recent years, there has been a significant increase in the usage of electronic medical records and diagnostic imaging, which coincides with the enormous success that machine learning algorithms have had in performing image identification jobs in recent years. This article is an introduction to machine learning techniques as they are applied to medical image processing, with a specific focus on convolution neural networks as well as the clinical aspects of the issue. In this era of medical big data, one of the benefits of machine learning is that critical hierarchal linkages within the data may be discovered algorithmically rather than having to painstakingly hand-craft features. It saves a significant lot of time and effort. We cover major research areas and applications in medical image classification, localization, detection, segmentation, and registration sectors. In conclusion, we will discuss research challenges, emerging trends, and probable future possibilities.",Training;Machine learning algorithms;Medical services;Machine learning;Radiation therapy;Medical diagnostic imaging;Next generation networking,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421032,IEEE Conferences,,,,,,
Analysis of EfficientNet Family Models by Retraining for Tuberculosis Detection from Chest X-Ray Images,R. D. Bhosale; D. M. Yadav,2023 Second International Conference on Informatics (ICI),########,2023,"This article presents a comprehensive study on the retraining of neural networks from the EfficientNet family, focusing on the detection of Tuberculosis (TB) through analysis of Chest X-Ray images. Leveraging the Shenzen and Montgomery Chest X-ray Dataset, impressive performance is showcased by the retrained models. Notably, the EfficientNet-B7 model emerges as the most effective, achieving remarkable rates of accuracy, sensitivity, specificity, and F1 score of 98.5%, 98.5%, 99%, and 98% respectively. The potential of EfficientNet architectures in medical image analysis is underscored by the results, particularly in the context of TB detection. A contribution is made to the growing body of knowledge concerning the application of deep learning techniques to enhance disease diagnosis through non-invasive imaging methods, thereby paving the way for improved early detection and patient care.",Sensitivity;Image analysis;Tuberculosis;Neural networks;Medical diagnosis;X-ray imaging;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10420853,IEEE Conferences,,,,,,
Enhanced CNN Security based on Adversarial FGSM Attack Learning: Medical Image Classification,L. Khriji; S. Messaoud; S. Bouaafia; A. C. Ammari; M. Machhout,"2023 20th International Multi-Conference on Systems, Signals & Devices (SSD)",########,2023,"Convolutional Neural Networks (CNNs) have grown in popularity for clinical image processing applications like as Covid and cancer detection. A new study, however, shows that hostile attacks with modest, unnoticeable disruptions can damage deep healthcare learning systems. This creates safety issues about using these technologies in healthcare situations. In this study, we will look at the approaches used to fight adversarial attacks on medical imaging. Next, we intend to investigate the resilience of pre-trained CNN architectures, as well as LeNet5 and MobileNetV1 models against Fast Gradient Sign Method (FGSM) attacks in a medical healthcare application-based chest X-ray dataset. We discover that pre-trained CNN models are much more sensitive to antagonistic assaults than other models, due to key feature discrepancies between them and regular models. Finally, we propose to improve the CNNâ€™ models security by investigating adversarial training. According to the numerical results, models with lower computational complexity and restricted layers are much more safe against malicious attacks than bigger models which are commonly utilized in medical healthcare systems.",Adaptation models;Computational modeling;Medical services;Numerical models;Convolutional neural networks;Biomedical imaging;Immune system,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411241,IEEE Conferences,,,,,,
A diagnostic system for classifying and segmenting breast cancer based on ultrasound images,H. Ghabrim; C. Essid; H. Sakli,"2023 20th International Multi-Conference on Systems, Signals & Devices (SSD)",########,2023,"Breast cancer is the most common type of cancer among women worldwide. Ultrasound is a type of imaging widely used in the diagnosis and examination of many soft tissues, including abnormalities of the breast because it offers the advantages of being real-time, portable, low-cost, and non-invasive. Ultrasounds suffer from high variability as well as speckle noise, which reduces image quality; therefore, it may be difficult for doctors to detect a cancerous cell. In this paper, we use image enhancement filters to improve image quality then we develop the U-Net model to segment ultrasound images. The accuracy reached was 0.97 and the dice coefficient was 0.95. Furthermore, we classify images as malignant or benign using various traditional techniques such as k nearest neighbors (KNN), Random Forest, Decision Tree and support vector machine (SVM). The highest rate is achieved by KNN. After the image enhancement, KNN obtained an accuracy of 0.86.",Support vector machines;Image segmentation;Ultrasonic imaging;Medical services;Breast cancer;Task analysis;Random forests,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411248,IEEE Conferences,,,,,,
Diagnosis of lung cancer based on CT scans using Vision Transformers,T. Gulsoy; E. B. Kablan,2023 14th International Conference on Electrical and Electronics Engineering (ELECO),########,2023,"Lung cancer remains a major global health problem that requires early and accurate diagnosis to improve patient outcomes. Assessment by specialists is time-consuming, tedious and leads to diagnostic inconsistencies. This has led to the development of computer-aided diagnosis systems for lung cancer diagnosis. Traditionally, proposed systems have used a transfer learning approach with CNN-based models. It has also been observed that cross-validation is usually not applied in these studies. On the other hand, cross-validation contributes to more reliable results by enabling the developed models to generalise to various image samples. In recent years, Vision Transformer (ViT) models have shown remarkable success in various computer vision tasks, including image classification. In this paper, we present our pioneering approach to lung cancer classification using various Vision Transformer-based models. However, we recognised the potential of these models to capture fine-grained patterns in medical images. To ensure the robustness and reliability of our models, we also introduced 5-fold cross-validation as a key component of our methodology. The proposed system yielded the highest results with 99.69% accuracy, 99.62% precision and 98.80% recall. This research represents an important step in utilising the latest computer vision techniques to improve lung cancer diagnosis, ultimately contributing to better patient care and outcomes.",Computer vision;Computational modeling;Transfer learning;Lung cancer;Transformers;Task analysis;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416046,IEEE Conferences,,,,,,
Fast Multi-Modal Multi-Instance Support Vector Machine for Fine-grained Chest X-ray Recognition,H. Seo; H. Wang,2023 IEEE International Conference on Data Mining (ICDM),########,2023,"Chest X-ray (CXR) analysis plays an important role in patient treatment. As such, a multitude of machine learning models have been applied to CXR datasets attempting automated analysis. However, each patient has a differing number of images per angle, and multi-modal learning should deal with the missing data for specific angles and times. Furthermore, the large dimensionality of multi-modal imaging data with the shapes inconsistent across the dataset introduces the challenges in training. In light of these issues, we propose the Fast Multi-Modal Support Vector Machine (FMMSVM) which incorporates modality-specific factorization to deal with missing CXRs in the specific angle. Our model is able to adjust the fine-grained details in feature extraction and we provide an efficient optimization algorithm scalable to a large number of features. In our experiments, FMMSVM shows clearly improved classification performance.",Support vector machines;Training;Scalability;Feature extraction;Data mining;X-ray imaging;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415737,IEEE Conferences,,,,,,
Enhancing Respiratory Diseases Detection Using GAN Augmentation and Transfer Learning,Z. Benhaili; Y. Abouqora; Y. Balouki; L. Moumoun,2023 7th IEEE Congress on Information Science and Technology (CiSt),########,2023,"Pneumonia is a potentially fatal bacterial disease that affects one or both lungs in humans and is frequently caused by the bacteria Streptococcus pneumonia. Chest X-rays provide an effective way to diagnose and detect this disease, it supports professionals to speed up detection and avoid deadly consequences, especially in vulnerable groups. Over the last few years, deep learning, particularly Convolutional neural network models, has improved medical image interpretation. Where an extensive quantity of labelled data is necessary to train a successful deep learning model, obtaining enough annotated images with balanced datasets is frequently problematic. In this work, we demonstrate how the suggested Generative Adversarial Network (GAN) can be efficiently used to supplement data and enhance disease detection precision in chest X-rays images. Later, we compare between several classification models using synthetic data, demonstrating that our GAN-based augmentation method can be a solution to the previous problems.",Microorganisms;Pulmonary diseases;Transfer learning;Generative adversarial networks;Transformers;X-ray imaging;Mixers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409986,IEEE Conferences,,,,,,
Pneumonia Classification and Localization in Enhanced X-ray Images with Transfer Learning and CNN Architectures,A. A. Aouragh; M. Bahaj,2023 7th IEEE Congress on Information Science and Technology (CiSt),########,2023,"Pneumonia is a pulmonary inflammation that can be caused by a variety of germs: bacteria, viruses, or fungi. Worldwide, pneumonia is a common disease responsible for an estimated $15 \%$ of deaths in kids under 5 years old. In general, the risk of pulmonary infection increases steadily after the age of 40 and is particularly high among individuals over 65. Despite the progress made in treating pulmonary infections, pneumonia remains a major cause of hospitalization and requires particular attention concerning efficient diagnosis and treatment. This study is built on a dataset made up of 5863 JPEG X-ray chest images. Through different convolutional neural network architectures based on transfer learning associated with data augmentation and various image enhancements such as Noise 2 void denoising and histogram equalization. We have implemented several models for pneumonia detection and localization; most of these networks are extremely promising in terms of performance, and most of the time they accurately exceed $98 \%$ in all metrics, which can simplify and speed up medical diagnosis, reduce pressure on hospitals, and save a lot of lives.",Location awareness;Pulmonary diseases;Transfer learning;Lung;Transform coding;Convolutional neural networks;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409997,IEEE Conferences,,,,,,
An Efficient Attention-Based Network for Screening Major Depressive Disorder with sMRI,X. Qu; Y. Xiong; K. Zhai; X. Yang; J. Yang,2023 29th International Conference on Mechatronics and Machine Vision in Practice (M2VIP),########,2023,"Major Depressive Disorder (MDD) is a prevalent psychiatric disorder that adversely affects the quality of life of those affected. Artificial intelligence (AI) has emerged as a promising tool for computer-aided diagnosis of MDD based on magnetic resonance imaging (MRI) technology. In this study, we propose a novel deep learning framework that utilizes the SlowFast network to analyze structural MRI (sMRI) for screening MDD. We incorporate a multi-scale feature fusion mechanism that integrates attentional mechanisms with path information to enhance the channel attentional features of gray matter images. Furthermore, we explore the correlation between regional features of sMRI slices and the spatial information our classification model focuses on. We evaluated our proposed method on MDD datasets generated from multiple research centers with varying scanning parameters and scanner field strengths, achieving a classification accuracy of 93.12%. Our method outperforms existing sMRI-based unimodal methods and provides clinicians with a reliable method for MDD diagnosis. During the investigation of regional spatial dependency, an impressive classification accuracy of 94.38% was achieved. By introducing the attention mechanism in the multi-scale feature fusion process, we can effectively focus on channel features, achieve more accurate MDD classification, and provide a more precise and reliable method for sMRI-based MDD diagnosis,",Deep learning;Correlation;Magnetic resonance imaging;Grey matter;Depression;Reliability;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10413424,IEEE Conferences,,,,,,
Segmentation of Thyroid Nodules Based on Hyperspectral Images,J. Wang; C. Tao; J. Du; B. Hu; Z. Zhang,2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),########,2023,"Thyroid nodules are a common thyroid disease, and early detection and treatment have a significant impact on the prognosis of patients. Hyperspectral image technology has significant advantages in medical image analysis due to its high-resolution and multi band characteristics, especially in the recognition and classification of thyroid nodules, which has important application value. But currently, there is no very effective method for segmenting hyperspectral images of thyroid nodules. Deep learning algorithms have shown superior performance in recent years in computer vision and pattern recognition tasks, including medical image analysis. This paper proposes the use of VGG model for segmentation of hyperspectral images of thyroid nodule tissue. Hyperspectral images can obtain spatial and spectral information of organizations, and VGG models can segment images through deep networks. Using hyperspectral and VGG, we found that thyroid nodule tissue can be well distinguished at the pixel level.",Deep learning;Image segmentation;Classification algorithms;Prognostics and health management;Thyroid;Hyperspectral imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10408843,IEEE Conferences,,,,,,
Recognition of Medical Images of Eye Diseases in a Deep Learning Perspective,Z. Fu; M. Xu; Y. Bai; J. Zhang; J. Feng; S. Li,2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),########,2023,"By studying and learning various deep learning models, the most suitable model for identifying diseased eyes, normal eyes, and highly myopic eyes in medical images is selected, and effective identification and prediction of eye medical images are performed. Medical images are obtained through Alibaba Cloud's dataset and analyzed through image quality enhancement and feature extraction. By comparing the test data between models, the deep learning model DenseNet121 is finally used for model training. It can accurately identify different types of eye medical images and make disease predictions based on the images.",Deep learning;Training;Analytical models;Image recognition;Data models;Biomedical imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10408760,IEEE Conferences,,,,,,
ITR-Net: A Hybrid Deep Learning Architecture for Precise and Efficient Medical Image Registration,Y. Yan; L. Su; C. Zhou; Y. Huang; J. Li; R. Li; H. Hassan; B. Huang,2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),########,2023,"This research proposes a weakly supervised-based learning registration network called Improved Transformer Registration Net (ITR-Net) to improve medical image registration accuracy. Firstly, we improved the transformer module by incorporating patch embedding and a feed-forward layer. These enhancements enable the transformer module to focus on local features in close proximity while establishing connections between distant voxels. Secondly, we embedded this module within U-Net, utilizing a CNN structure to extract image features more precisely and enhance matching accuracy. Then we evaluated the performance of our approach using three-phase CT imaging data of kidneys and lungs. The results demonstrate that our method surpasses traditional and pure CNN-based registration algorithms in terms of both registration accuracy and efficiency.",Image registration;Computed tomography;Lung;Transformers;Feature extraction;Kidney;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409014,IEEE Conferences,,,,,,
An Efficient Deep Learning Model for Intraoperative Tissue Classification in Gynecological Cancer,R. Mishra; M. Kumar; B. S; A. K. Sharma; C. Raja; T. T. Moharekar,2023 9th International Conference on Smart Structures and Systems (ICSSS),31-Jan-24,2023,"Ovarian Cancer is a formidable adversary in the realm of women's health, often eluding early detection and posing significant challenges for effective treatment. Leveraging the potential of artificial intelligence (AI) and machine learning (ML) to improve the accuracy and effectiveness of diagnosis has become imperative in the fight against this silent killer. This research presents an innovative approach known as CNNGWO, which combines the power of Convolutional Neural Networks (CNNs) with the optimization capabilities of Grey Wolf Optimization (GWO) to enhance the diagnostic accuracy of Ovarian Cancer. In this work, we explore the fusion of CNNs, a deep learning architecture renowned for its proficiency in image analysis, and GWO, a nature-inspired optimization technique inspired by the social behavior of grey wolves. The CNN component is employed for the automatic extraction of intricate patterns and features from histopathological images, providing a foundation for precise cancer detection. GWO plays a pivotal role in fine-tuning the CNN model's hyperparameters, optimizing its performance, and enhancing its predictive accuracy. Through rigorous experimentation and validation, we demonstrate the effectiveness of the CNNGWO framework in Ovarian Cancer diagnosis. The integration of these cutting-edge technologies yields remarkable results, with the model consistently outperforming traditional methods with an accuracy of 0.98. Accuracy rates, precision, recall, and F1-scores all exhibit substantial improvements, signifying the potential of CNNGWO as a valuable tool in early cancer detection.",Neural networks;Predictive models;Feature extraction;Cancer detection;Convolutional neural networks;Optimization;Ovarian cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10407080,IEEE Conferences,,,,,,
A Deep Evaluation of Digital Image based Bone Cancer Prediction using Modified Machine Learning Strategy,T. Bapu B R; V. J; S. R; T. S. J; V. S; S. C,2023 9th International Conference on Smart Structures and Systems (ICSSS),31-Jan-24,2023,"Every single person, regardless of their age or stage in life, is at risk of developing the fatal disease known as cancer. There is a greater likelihood that cancer will affect more than one-third of the population at some point throughout their lifespan. In particular, bone cancer is a significant cause for concern in terms of health since it frequently leads to the patient's passing away. By examining the pictures obtained from X-ray, MRI, or CT scans, it is possible to identify the presence of bone cancer. The manual process requires a significant amount of effort and time-consuming expertise, and it also requires specific knowledge. As a result, the development of an automated method to differentiate between healthy bone and bone that has advanced cancer is of the utmost importance. The texture of malignant bone in the affected region is distinct from that of healthy bone in that portion of the body. It is important to note that the sample contains a number of images that exhibit morphological characteristics that are characteristic of both malignant and healthy bone. The early identification of this malignancy, including bone cancer, may be accomplished with the highest possible degree of precision through the use of medical imaging correlation in conjunction with image processing and machine learning techniques. This study proposes a strategy for identifying bone cancer by utilizing data acquired from actual clinical trials. The method is comprised of a number of phases that have the potential to increase the accuracy of disease prediction. The proposed model is designed based on Modified Machine Learning Strategy, called Elevated Learning based Bone Cancer Prediction (ELBCP), in which it evaluates the input image and predicts the bone cancer with high precision as well as the model is cross-validated with the conventional learning based classification model called Support Vector Machine (SVM) to prove the efficiency and performance of the proposed scheme.",Support vector machines;Magnetic resonance imaging;Machine learning;Predictive models;Bones;Feature extraction;Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10407146,IEEE Conferences,,,,,,
Unleashing the Potential of Artificial Intelligence and Deep Learning in Pneumonia Detection Systems,P. V. S; V. Aiyyasamy; S. D; R. Jayadurga; V. A. Kandaswamy; P. Sundara Bala Murugan,2023 9th International Conference on Smart Structures and Systems (ICSSS),31-Jan-24,2023,"Bacteria, viruses, and fungi are all examples of microorganisms that have the potential to infect the lungs and cause pneumonia. This infection causes the alveoli of the lungs to become inflamed, which makes it difficult for people to breathe. The severity of the condition ranges from mild to severe. Blood tests, pulse oximetry, and chest X-rays are three of the diagnostic procedures that are utilized. Artificial intelligence (AI) and deep learning have been utilized for image analysis in a variety of medical sectors throughout the course of the past few years. They are pretty helpful when it comes to extracting complicated information from the photographs that are loaded into the system. When it comes to the detection and classification of lung disorders, deep learning is very necessary. This research provides a comprehensive analysis of the medical image pneumonia identification strategies that are utilized in deep learning. The goal of this study is to introduce a novel approach called Artificial Intelligence based Learning for Pneumonia Detection (AILPD), in which it is used to explain its implementation in a system for detecting the pneumonia illness, to present a review of recent advancements in the field, and to provide ideas for future possibilities along similar lines. The purpose of this research is to evaluate the effectiveness of the proposed technique by contrasting the proposed algorithm with the traditional Convolutional Neural Network (CNN) approach. Additionally, this research offers a full review of the deep learning technologies that are utilized in the prevention and treatment of pneumonia.",Deep learning;Pulmonary diseases;Lung;Prediction algorithms;Classification algorithms;Convolutional neural networks;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10407052,IEEE Conferences,,,,,,
Standard Training Dataset vs. Different Testing Dataset to Compare Deep Learning Architectures Models in Diagnosing COVID-19,K. A. Saeed; W. T. Kahwachi,2023 9th International Conference on Smart Structures and Systems (ICSSS),31-Jan-24,2023,"The Coronavirus-2 disease epidemic has spurred global interest in adopting architectures for deep learning, especially the deep convolutional neural network -DCNN, often developing high-performance deep CNN models that require access to additional data for training. Unfortunately, such data is frequently unavailable. In this paper, we present a novel standardized dataset designed for training deep CNN models. This dataset encompasses chest CT scan images, with both COVID-19 and normal cases, by two distinct input sample sizes. Additionally, we employ this dataset to evaluate the effectiveness of recent DCNN techniques in diagnosing new Coronavirus disease, comparing it with the SARS-CoV-2 dataset. Notably, we observe variations in the behavior of DCNN models when using different datasets for training and testing, emphasizing the modelâ€™s dependency on dataset characteristics. Furthermore, our findings indicate that the proposed approach exhibits exceptional performance in detecting Coronavirus disease. Consequently, this approach holds promise for integration into medical diagnosis systems reliant on image analysis, offering potential advantages for resource-constrained computing devices, thus mitigating the need for expensive GPUs. In the forthcoming study, we propose applying this approach to evaluate the effectiveness of various classification techniques suggested by other studies to generate substantial CT imaging datasets for COVID-19 research.",COVID-19;Training;Performance evaluation;Computed tomography;Convolutional neural networks;Standards;Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10407120,IEEE Conferences,,,,,,
Deep Learning-Enhanced MRI for Brain Tumor Detection and Characterization,J. J; S. Raghul; N. A. Kumar; K. Ravindra Desai; V. Chourasia; A. K. Agrawal,2023 9th International Conference on Smart Structures and Systems (ICSSS),31-Jan-24,2023,"Brain tumors are a critical healthcare concern, demanding accurate and timely diagnosis for effective treatment planning. Magnetic Resonance Imaging (MRI) is a valuable tool for non-invasive brain tumor assessment. This study presents a novel approach, Deep Learning-Enhanced MRI (DE-MRI), which combines the power of AutoEncoder and Residual Neural Network (ResNet) architectures for improved brain tumor detection and characterization. In the DE-MRI framework, an AutoEncoder is employed to extract meaningful features from MRI images, reducing data dimensionality while preserving critical information. These features are then fed into a ResNet-based neural network, enabling high-level feature learning and classification. This two-stage architecture enhances the model's ability to differentiate between normal brain tissue and tumor regions, as well as categorize tumors into various types and grades. Whenever compared to conventional MRI-based tumor identification methods, the suggested DE-MRI methodology shows promising results in terms of accuracy, sensitivity, and specificity. Additionally, it offers the advantage of reduced manual intervention and faster diagnosis, potentially leading to more timely and tailored treatment plans for patients with brain tumors. The AutoEncoder-ResNet hybrid model distinguishes itself by achieving the highest accuracy of 0.96 and the most minimal loss of 1.46. Overall, DE-MRI, with its innovative combination of AutoEncoder and ResNet, represents a promising advancement in the field of medical imaging for brain tumor diagnosis and characterization, contributing to improved patient outcomes and healthcare efficiency.",Magnetic resonance imaging;Medical services;Sensitivity and specificity;Feature extraction;Brain modeling;Biological neural networks;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10407487,IEEE Conferences,,,,,,
Multi-Scale Swin Transformer for COVID-19 Classification in Chest X-Rays,X. Wu; H. Zhang; Z. Gong; Y. Zhao; L. Yang; J. Chang,2023 5th International Conference on Artificial Intelligence and Computer Applications (ICAICA),31-Jan-24,2023,"The COVID-19 pandemic, induced by the SARS-CoV-2 virus, has underscored the importance of rapid and precise diagnostic tools. While RT-PCR tests remain the gold standard for COVID-19 detection, their limitations highlight the value of imaging modalities, particularly Chest X-Rays (CXR). Accurate interpretation of CXR images, however, is challenging due to their lower resolution and the nuanced presentations of the disease. To address this, we introduce a novel approach centered around a multi-scale Swin Transformer tailored for adept COVID-19 classification in CXR images. Our methodology emphasizes three core components: Multi-scale Image Partitioning, Hierarchical Transformer Layers, and Adaptive Window Merging. These facets ensure the proficiency of our model in capturing the intricate representations of COVID-19, offering both enhanced accuracy and interpretability. Such innovations pave the way for improved diagnostic tools in the fight against this global health crisis.",COVID-19;Visualization;Computational modeling;Transformers;Feature extraction;X-ray imaging;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10405665,IEEE Conferences,,,,,,
2D Cardiac MRI Classification with 10-Fold Cross Validation on Deep Learning Model,D. Irmawati; O. Wahyunggoro; I. Soesanti,"2023 IEEE 7th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)",31-Jan-24,2023,"Deep learning is still being explored at the moment, particularly in medicine. This tool has the ability to train on very large and complicated datasets. This capacity greatly aids researchers and medical professionals in their efforts to analyze cardiac Magnetic Resonance Imaging (MRI) more accurately and quickly. In the present study, we suggest a brand-new system for categorizing cardiac abnormalities. Sunnybrook Cardiac Data (SCD) training data was used to train and test the LeNet5 deep learning model. Hyperparameter tuning is applied before training. The model was tested using 10-folds cross validation, with performance results of 99 % accuracy. The novelty of the proposed method for classifying cardiac disease using cardiac MRI Images with the LeNet5 model.",Deep learning;Training;Magnetic resonance imaging;Training data;Predictive models;Tuning;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10405227,IEEE Conferences,,,,,,
Machine Learning Algorithms for Classifying Disorder in Brain and Lung Biomedical Images,P. Pitale; N. Chaudhari; D. Ghatge; K. Ingole; A. H. Gade; V. S. Choubey,2023 1st DMIHER International Conference on Artificial Intelligence in Education and Industry 4.0 (IDICAIEI),31-Jan-24,2023,"Biomedical imaging and its research guide tremendous impact on the medical field. Different types of biomedical imaging help doctors to diagnose and detect diseases in advance. The early diagnosis increases the life span of humans. The machine learning algorithms are incorporated with the biomedical images to make the Computer-Aided Diagnosis (CAD) system. The CAD will reduce the human efforts and the false negatives of the diagnosis. Any damage that happens to the cells is known as abnormality. The abnormalities can be found with the help of computer-aided systems. Brain tumor classification is a complex problem due to the size, shape, and intensity variation, the dataset consists of glioma, meningioma, and pituitary tumor images. The features from the deep architecture and wavelet scattering methods are explored for brain tumor classification. These features are classified by using a Support Vector Machine with a cubic kernel. The proposed method shows a promising accuracy while considering the state-of-the-art methods",COVID-19;Computed tomography;Magnetic resonance imaging;Lung;Scattering;Medical services;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10406985,IEEE Conferences,,,,,,
Exploring the Potential of Convolution Neural Network Based Image Classification,R. Sharma; S. Bashir; V. N. Tiwary; S. Kumar,2023 1st DMIHER International Conference on Artificial Intelligence in Education and Industry 4.0 (IDICAIEI),31-Jan-24,2023,"This research examines the utilization of Convolutional Neural Networks (CNNs) in image classification, shedding light on recent progress in machine learning algorithms. Recognized as a unique component of deep learning, CNNs have earned significant acclaim in a myriad of visual tasks, attributed to their capacity for hierarchical pattern recognition and extraction of stable features. In this investigation, an enhanced CNN model is introduced, which employs cutting-edge preprocessing and augmentation strategies to elevate image readability, thereby streamlining the learning mechanism. key machine learning principles were employed, equipping the CNN model to identify and distinguish intricate structures within images with amplified precision. The model's efficiency is assessed using multiple benchmark datasets, achieving optimistic outcomes concerning precision, recall, and F1-score. The paper also touches upon inherent obstacles such as overfitting and computational expenses linked with CNNs, suggesting remedies to boost scalability and efficiency. The observations suggest that with meticulous planning and execution, CNNs can serve as a potent instrument for automated image classification, paving the way for a wide array of applications from medical imaging to self-driving vehicles. This research augments the expanding literature in machine learning, offering perspectives that could inform the design of more effective and efficient image classification systems.",Deep learning;Training;Computational modeling;Data models;Convolutional neural networks;Task analysis;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10406528,IEEE Conferences,,,,,,
Decoding the Hidden: Direct Image Classification Using Coded Aperture Imaging,J. O. Munoz; E. M. Rutter; R. F. Marcia,2023 IEEE 9th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP),31-Jan-24,2023,"Coded aperture imaging has emerged as a solution to enhance light sensitivity and enable imaging in challenging conditions. However, the computational expense of image reconstruction poses limitations in processing efficiency. To address this, we propose a direct classification method using convolutional neural networks. By leveraging raw coded measurements, our approach eliminates the need for explicit image reconstruction, reducing computational overhead. We evaluate the effectiveness of this approach compared to traditional methods on the MNIST and CIFAR10 datasets. Our results demonstrate that direct image classification using raw coded measurements achieves comparable performance to traditional methods while reducing computational overhead and enabling real-time processing. These findings highlight the potential of machine learning in enhancing the decoding process and improving the overall performance of coded aperture imaging systems.",Sensitivity;Source coding;Imaging;Apertures;Decoding;Image reconstruction;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10403419,IEEE Conferences,,,,,,
Detection of Vocal Cord Disease Based on Local Contour Features of Laparoscopy Images,Y. Fan; R. Yao; J. Liu; X. Yuan,"2023 Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)",31-Jan-24,2023,"Vocal cord disease is usually manifested as throat discomfort, dysphonia and other symptoms. If not treated in time, it may lead to dyspnea, canceration and other serious consequences. Laryngoscope is a common examination method for detecting vocal cord diseases. It is urgent to design an intelligent method to automatically detect vocal cord diseases from laryngoscope images using computer technology, so as to reduce the workload of laryngologists and the dependence on a large number of professional laryngologists. In laryngoscopic images, the symptoms of vocal cord diseases are concentrated on the internal contour of the vocal cord, usually characterized by vegetations, sausage-like masses and small protrusions. The existing research on laryngoscope image classification has paid little attention to the role of vocal cord contour in the diagnosis of vocal cord diseases, and failed to effectively focus on the local fine-grained features contained in the internal vocal cord contour. In this paper, we study the detection of vocal cord diseases based on laryngoscopic images, and propose a new classification method of laryngoscopic images, that is, local contour feature (LCF) based on local vocal cord contour feature. LCF includes four stages: image segmentation to obtain the overall vocal cord contour, internal vocal cord contour segmentation to obtain the internal contour curve by comparing the changes of adjacent pixel values, extracting the potential features in the vocal cord internal contour, taking the tangent slope angle of each point on the contour as the potential features, and classification module. We carried out experiments on data sets constructed from real laryngoscope images. The experimental results show that the proposed method LCF can effectively improve the detection performance of vocal cord diseases, with an accuracy of 97.2%.",Laparoscopes;Image segmentation;Design methodology;Vegetation mapping;Feature extraction;Diseases;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10412770,IEEE Conferences,,,,,,
MedPipe: End-to-End Joint Search of Data Augmentation and Neural Architecture for 3D Medical Image Classification,X. He; X. Chu,2023 IEEE International Conference on Medical Artificial Intelligence (MedAI),31-Jan-24,2023,"Data augmentation plays a crucial role in deep learning-based medical imaging analysis, but manually designing tailored data augmentation strategies for each dataset is impractical. Although automatic data augmentation (ADA) techniques have been explored, they often focus solely on data augmentation without considering the importance of neural architecture. Similarly, neural architecture search (NAS) methods mainly concentrate on optimizing the neural architecture while overlooking the impact of data augmentation. However, both data augmentation and neural architecture are interrelated and should be considered together. The joint optimization of data augmentation and neural architecture can lead to improved model performance by harnessing the complementary effects of customized data augmentation strategies and compatible neural architectures. Despite this, the seamless integration of data augmentation and neural architecture search remains under-explored. To address this research gap, we propose MedPipe, an approach that enables end-to-end joint search of data augmentation and neural architecture. We introduce a compact data augmentation search space and unify data augmentation and neural architecture into a cohesive network. This allows simultaneous exploration, optimizing their synergy for enhanced performance. Experimental evaluation on nine medical datasets highlights the necessity of the joint search for data augmentation and neural architecture, demonstrating the superior performance of our approach. Our work opens up possibilities for future applications in diverse medical domains.",Image segmentation;Three-dimensional displays;Computer architecture;Data augmentation;Optimization;Biomedical imaging;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10403239,IEEE Conferences,,,,,,
Uncertainty-Aware Deep Learning for Segmenting Ultrasound Images of Breast Tumours,A. A. Munia; I. Hossain; S. M. Jalali; P. Tabarisaadi; A. Rahman; S. Nahavandi,"2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",29-Jan-24,2023,"Precise image segmentation is one of the dominant factors in disease diagnosis. A typical application is the segmentation of breast ultrasound images, allowing radiologists to suggest what to do next. After emerging deep learning technology especially convolutional neural networks (CNNs), the image segmentation model achieved state-of-the-art performance in various medical applications such as cancer detection and classification, lung node segmentation, cell segmentation and so on. However, despite these successes, a big question arises: to what extent is the model certain about the predicted result? Generally, most deep learning models focus on high accuracy but not on uncertainty of predicted results, which is not enough to make a critical real-life decision such as a disease diagnosis, where a wrong decision can be life-threatening. Hence for making a crucial decision, it is essential that the predicted result will provide not only accuracy but also estimate model uncertainty. Our contribution to this research is to build a system that predicts pixel-wise semantic segmentation and provides uncertainty estimation of the predicted results. It is achieved by adding a dropout layer during training and using Monte Carlo dropout in inference. We evaluate our model with the breast ultrasound image dataset (BUSI) and compare the results with a few other state-of-the-art methods where our method outperforms others in terms of IoU.",Deep learning;Training;Image segmentation;Uncertainty;Ultrasonic imaging;Predictive models;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394411,IEEE Conferences,,,,,,
Initial Analysis of Multiple Retinal Diseases Classification with Fuzzy Medical Image Retrieval,V. Uher; J. NowakovÃ¡; P. KrÃ¶mer,"2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",29-Jan-24,2023,"Medical image retrieval is a highly discussed topic, and it includes an efficient classification of diagnoses based on the similarity search in large databases of medical images. It is very important for early and correct diagnosis and treatment. In this paper, we focus on detecting four diagnoses of treatable retinal diseases in optical coherence tomography (OCT) images. The fuzzy medical image retrieval model (FMIR) is applied to transfer images to fuzzy signatures organized in Fuzzy S-tree, as it was previously successfully used for breast cancer detection and COVID-19 chest X-ray detection. The paper examines and compares the performance of the FMIR method on 4-class and binary classification models built on an OCT dataset and compares the impact of two metrics, Euclidean and Hamming fuzzy distances. The experiments show a clear dominance of Hamming fuzzy distance. The best accuracy is achieved for binary classification (61.16 - 93.8%), while the performance of the 4-class model is worse (51.7%). The distribution of signature space and classification performance are analyzed in detail.",Measurement;Visualization;Optical coherence tomography;Computational modeling;Image retrieval;Retina;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394609,IEEE Conferences,,,,,,
Self-Supervised Multimodal Fusion Network for Knee Osteoarthritis Severity Grading,W. Wu; K. Hu; W. Yue; W. Li; M. Simic; C. Li; W. Xiang; Z. Wang,2023 International Conference on Digital Image Computing: Techniques and Applications (DICTA),29-Jan-24,2023,"Knee osteoarthritis (OA) is a highly prevalent form of arthritis and a leading cause of physical disability, given the growing aging population. To assist the knee OA assessment, there is a demanding interest in computer-aided grading algorithms. Existing grading methods generally require resource-intensive annotated datasets for supervised training. Moreover, they only consider unimodal data, whilst multimodal medical images are rarely utilised to formulate better knee OA patterns. Therefore, in this study, a novel Self-supervised Multimodal Fusion Network (S-MFN) is proposed for multimodal unsupervised knee OA grading with X-ray and magnetic resonance imaging (MRI) modalities. Specifically, S-MFN involves two modality-specific streams to obtain knee OA representations from the two corresponding modalities. A modality-aware information exchange mechanism is devised to interactively formulate cross-modal patterns in a multi-scale manner regarding the scales of feature maps. To this end, a multimodal contrastive learning is introduced in a self-supervised manner through modality-specific and cross-modal modelling. Comprehensive experimental results on the widely used dataset, Osteoarthritis Initiative (OAI), demonstrate the effectiveness of the proposed method.",Analytical models;Magnetic resonance imaging;Computational modeling;Self-supervised learning;Osteoarthritis;X-ray imaging;Information exchange,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10410966,IEEE Conferences,,,,,,
On the Correlations between Performance of Deep Networks and Its Robustness to Common Image Perturbations in Medical Image Interpretation,C. F. Chong; X. Fang; X. Yang; W. Luo; Y. Wang,2023 International Conference on Digital Image Computing: Techniques and Applications (DICTA),29-Jan-24,2023,"The robustness of medical image interpretation deep learning models to common image perturbations is crucial, as the medical images in clinical applications may be from different institutions and contain various perturbations that did not appear in training data, decreasing the interpretation performance. In this paper, we investigate the correlations of the robustness of 28 ImageNet models under 6 image perturbation types over 10 severity levels on the CheXpert chest X-ray (CXR) classification dataset. The results demonstrate that: (1) If a model has a higher ImageNet accuracy, after fine-tuning it on CheXpert for CXR classification, it tends to be more robust on perturbed CXRs. (2) If a model has a higher CXR classification performance after fine-tuning on CheXpert, it is not necessarily more robust on perturbed CXRs, depending on the severity levels of the perturbations. Under stronger perturbations, lower CXR performance models tend to be more robust instead. (3) The model architectures may be a key factor to the robustness. For instance, no matter how large the models are, EfficientNet and EfficientNetV2 models tend to be more robust, while ResNet models tend to be more vulnerable. Our work can help select or design robust models for medical image interpretation to improve the capability for clinical applications.",Correlation;Perturbation methods;Computational modeling;Computer architecture;Robustness;Biomedical imaging;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10410918,IEEE Conferences,,,,,,
Nephrolithiasis Taxonomy: A Multifaceted Exploration of Renal Calculi,V. Lakshmi; K. Sivachandra; S. Abhishek; T. Anjali,"2023 2nd International Conference on Automation, Computing and Renewable Systems (ICACRS)",26-Jan-24,2023,"Nephrolithiasis is the medical term used to define the urological disorders that occur in human beings. Kidney disorders are an area of critical concern, so early diagnosis and treatment are vital in saving the patientâ€™s life. The variety and complexity of kidney problems make it very challenging to diagnose with accuracy. Most such disorders occur without any symptoms, which makes them much more difficult to detect at an earlier stage. In order to save the patientâ€™s life, early identification and therapy are essential. This research aims to create a diagnostic system that can precisely and accurately classify kidney computed tomography scan images into four categories: stones, cysts, tumors, and normal states. The traditional methods of identification are very challenging and time-consuming, especially using CT scan images. In this study, we developed a customized Convolutional Neural Network model that accurately classifies the CT scan images into different categories. This research study introduces an innovative approach that addresses the diversity and complexities of nephrolithiasis. The customized CNN model used in this research gives an accuracy of 99.2%, a precision of 98.9%, and a recall of 98.6%, proving it is very efficient and highly reliable for classifying medical data.",Computed tomography;Computational modeling;Data models;Convolutional neural networks;Medical diagnostic imaging;Kidney stones;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10404959,IEEE Conferences,,,,,,
Disease Classification in X-Ray Images using Convolutional Neural Networks,T. J. Allen Matthew; K. Shanish Roshan; K. Paul Jerome Immanuel; S. Mithun; T. J. Jebaseeli,"2023 2nd International Conference on Automation, Computing and Renewable Systems (ICACRS)",26-Jan-24,2023,"Convolutional Neural Networks (CNNs) have emerged as a revolutionary tool in the domain of disease categorization, fundamentally changing the landscape of medical imaging diagnostics. This study deeply investigates the application of CNNs to automatically and with high precision classify diseases using medical imagery, including X-rays, MRIs, and CT scans. The utilization of CNNs in this research takes advantage of their proficiency in recognizing intricate patterns and features within medical images, thereby simplifying the disease diagnosis process. The incorporation of CNNs in disease classification represents a promising advancement in the healthcare sector. Through extensive training on extensive datasets, these neural networks can discern intricate patterns and subtle irregularities in medical images, resulting in improved precision and efficiency in disease diagnosis. By automating the categorization process, healthcare providers can minimize the likelihood of human errors and enhance the speed of diagnosis, ultimately leading to improved patient outcomes. This study emphasizes the potential of CNNs to reshape disease classification, offering a glimpse into a healthcare future where advanced technology plays a pivotal role in ensuring accurate and timely disease identification. It also underscores the ongoing necessity for research and development in the field of medical image analysis, as CNNs continue to reshape the healthcare landscape with their ability to revolutionize disease diagnostics.",Telemedicine;Medical services;X-rays;Medical diagnosis;Medical diagnostic imaging;Research and development;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10405339,IEEE Conferences,,,,,,
Enhancing Shift-Invariance for Accurate Brain MRI Skull-Stripping using Adaptive Polyphase Pooling in Modified U-Net,A. Padalia; T. Shah; P. Pujari; A. Karande,"2023 2nd International Conference on Automation, Computing and Renewable Systems (ICACRS)",26-Jan-24,2023,"Image segmentation is an essential aspect of image processing, where regions of interest (ROI) are identified by employing feature descriptors like edges, color, and texture. Medical images often contain unwanted segments, necessitating preprocessing steps to extract valuable information. In the realm of medical image segmentation, skull-stripping in 3D volumetric Brain MRI images holds significant importance. This study delves into supervised learning techniques to extract crucial brain features from non-brain tissues. The approach involves a modified U-net that operates on 2D slices of the brain MRI, yielding a skull-stripped image with superior accuracy compared to existing methods. The architecture incorporates Adaptive Polyphase Pooling layers, enhancing the architectureâ€™s robustness and the modelâ€™s generalization. These Adaptive Polyphase Pooling layers ensure shift-invariance in the predicted output, maintaining consistency even if the single view of the Brain MRI shifts along the two axes in the plane.",Image segmentation;Adaptive systems;Three-dimensional displays;Magnetic resonance imaging;Supervised learning;Feature extraction;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10404359,IEEE Conferences,,,,,,
Machine Learning based Computer Aided Diagnosis Models for Thyroid Nodule Detection and Classification: A Comprehensive Survey,G. N. Sujini; S. Balakrishna,"2023 2nd International Conference on Automation, Computing and Renewable Systems (ICACRS)",26-Jan-24,2023,"In recent years, thyroid nodules have become a common illness, and their incidence surges with age. Since most of the nodules are benign/perform insignificantly, timely and accurate detection of thyroid nodules are essential to minimize the patient risks and medical expenses. Computer aided diagnosis (CAD) models were presently employed in a diagnoses of thyroid nodule. Besides, several medical imaging modalities like ultrasound, computed tomography (CT), and SPECT are widely used to detect thyroid diseases. Numerous deep learning (DL) and machine learning (ML) models are being utilized in the design of CAD models to detect and classify the thyroid diseases. From this perspective, research focuses on the survey of automated CAD models to detect and classify thyroid nodules by the use of DL techniques. Besides, a set of different methods employed in the detection of thyroid disease along with their objectives are briefed. In addition, the reviewed methods are investigated based on the methodology and imaging modality used. Finally, the reviewed methods are summarized based on different aspects along with a brief experimental results analysis.",Surveys;Solid modeling;Ultrasonic imaging;Computational modeling;Surges;Thyroid;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10404375,IEEE Conferences,,,,,,
ResNet and ResNeXt-Powered Kidney Tumor Detection: A Robust Approach on a Subset of the KAUH Dataset,S. P. Praveen; S. R. Sidharth; T. K. Priya; Y. S. Kavuri; S. M. Sindhura; S. Donepudi,"2023 2nd International Conference on Automation, Computing and Renewable Systems (ICACRS)",26-Jan-24,2023,"Renal disorders, especially neoplasms, provide a global health risk. This study analyses 2,170 photographs from the 8,000-image KAUH dataset to identify kidney tumours. This study uses deep convolutional neural networks (CNNs) to identify kidney tumours by kind and stage. This project aims to create a reliable kidney tumour detection model.This study fuses two resilient CNN architectures, ResNet and ResNeXt, to improve the detection model. ResNeXt architecture solves gradient-related problems in deep learning models. The ResNeXt architecture allows multi-path networks with a cardinality choice. ResNeXt is ideal for feature extraction and learning due to its high scalability and gradient flow.Our research shows that our model is resilient, achieving 94% accuracy on a subset of 2,170 pictures from the KAUH dataset. This research uses ResNet-pretrained machine learning algorithms and ResNeXtâ€™s gradient flow characteristics to improve healthcare outcomes.",Renewable energy sources;Scalability;Computer architecture;Convolutional neural networks;Reliability;Kidney;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10404405,IEEE Conferences,,,,,,
Deep Learning for Accurate Chest Disease Classification: A CNN-Based Approach for Lung Cancer Subtypes and Normal Cells,B. Sumithra; G. Vallathan; M. Raman Kumar; K. Govindharaju,"2023 International Conference on System, Computation, Automation and Networking (ICSCAN)",26-Jan-24,2023,"This exploration paper presents a profound learning approach for the grouping of chest sicknesses, zeroing in on cellular breakdown in the lungs subtypes (Adenocarcinoma, Enormous cell carcinoma, Squamous cell carcinoma) and typical cells. Utilizing Convolutional Brain Organizations (CNNs), we prepared the model on a dataset of nonlittle cell cellular breakdown in the lungs (NSCLC) pictures got from different sources. The CNN showed promising execution, precisely separating between different chest sickness classes. Assessment measurements, including exactness, accuracy, review, F1-score, and AUC-ROC, affirmed the model's viability and speculation. Moreover, interpretability procedures, for example, Graduate CAM were utilized to imagine important districts adding to the grouping choices. Cooperation with clinical specialists guaranteed arrangement with clinical perceptions, making this study an important commitment to simulated intelligence helped medical services and early infection recognition.",Squamous cell carcinoma;Electric breakdown;Lung;Lung cancer;Organizations;Medical services;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394855,IEEE Conferences,,,,,,
The Art of YOLOv8 Algorithm in Cancer Diagnosis using Medical Imaging,N. Palanivel; D. S; L. P. G; S. B; S. M. M,"2023 International Conference on System, Computation, Automation and Networking (ICSCAN)",26-Jan-24,2023,"Cancer continues to be a global health challenge, demanding innovative solutions to improve early detection and treatment outcomes. This research project harnesses the power of deep learning in the field of medical imaging to investigate the applicability of the YOLOv8 (You Only Look Once version 8) algorithm for diagnosing various cancer types, such as Acute Lymphoblastic Leukemia, Cervical, Lung, Colon, Oral, and Skin cancers. The YOLOv8 algorithm, renowned for its real-time object detection prowess, represents a promising candidate for automating the identification and classification of cancerous regions within medical images. This study encompasses a comprehensive methodology, starting with the collection and preprocessing of diverse and well-annotated medical image datasets. The YOLOv8 algorithm is then fine-tuned and trained on these datasets, capitalizing on its object detection capabilities to discern cancerous lesions. The model's performance undergoes a comprehensive evaluation using established metrics, guaranteeing its dependability and precision within a clinical setting. The findings of this study have the potential to offer insightful information on YOLOv8. By bridging the gap between cutting-edge deep learning technology and clinical practice, this research project seeks to advance the field of medical imaging and provide a foundation for more precise, efficient, and accessible cancer detection methods. Ultimately, the goal is to enhance the early diagnosis of cancer, offering new possibilities for timely intervention and improved patient outcomes.",Deep learning;YOLO;Art;Medical services;Transforms;Cancer detection;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395046,IEEE Conferences,,,,,,
Deep Learning Model to Detect and Classify Bone Fracture in X-Ray Images,I. M; V. I; A. J; P. P; R. J,"2023 International Conference on System, Computation, Automation and Networking (ICSCAN)",26-Jan-24,2023,"The skeletal system of the human body is very important and bones are vital to it. A support and facilitate body movement. To detect fractures in bones, doctors use X-ray images, but this manual technique is laborious and prone to mistakes. Therefore, an automated approach for diagnosing bone fractures is required. Power electronic device modeling is a frequent use of deep neural networks, or DNNs. The proposed system, provides a deep neural network model using ResNet 50 to distinguish between bones that are healthy and damaged. But when faced with little data, our deep learning model has a tendency to over fit. The data augmentation techniques is used to increase the data set in order to overcome this problem. This stratergy uses 5-fold cross- validation and our model is able to discriminate objects with 92.44% classification accuracy.",Deep learning;Hospitals;Biological system modeling;Artificial neural networks;Manuals;Bones;Data augmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394986,IEEE Conferences,,,,,,
An Improved Fusion Model from GoogLeNet and AlexNet to Predict Breast Cancer using Deep Learning,S. R; N. M; V. B; R. N; S. E,"2023 International Conference on System, Computation, Automation and Networking (ICSCAN)",26-Jan-24,2023,"Despite the fact that breast cancer is one of the most common cancers to kill women due to its high mortality and ease of recurrence, an early and precise diagnosis can significantly improve the chances of a successful recovery. It is very important to diagnose the disease at beginning level to avoid any complications. Traditional early diagnosis, however, is inaccurate and depends on human experience. Since prediction accuracy and efficiency may be improved, numerous researchers have suggested several machine learning techniques. Deep convolutional neural networks like GoogLeNet and AlexNet models are chosen for the diagnosis of breast cancer because they are excellent at extracting deep characteristics from images and performing fantastic image categorization. Additionally, the GoogLeNet model and AlexNet model is integrated to form a fused model which is enhanced to make better accuracy, more specifically aimed at medically related digital images. During the diagnosis our aim is to achieve the higher level accuracy, with reduced computing weight. The enhanced model, which is more focused on breast cancer pathological sections and increases the performance, by combining the properties of two different types of network structures. The integrated proposed model achieved 97.5% Precision, 97.3% Recall, 98.3% Specificity, 98.3% Accuracy, and 98.1% F1-Score. The improved model may more precisely identify Breast cancer, reduce the likelihood of inaccurate diagnosis and delayed diagnosis due to physician personal reasons, and assist healthcare professionals in providing patient care and monitoring, making the entire evaluation and therapy process more sophisticated and effective.",Training;Adaptation models;Pathology;Computational modeling;Medical treatment;Predictive models;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395210,IEEE Conferences,,,,,,
Unsupervised Medical Image Denoising Using CycleGAN: Improving Low-Dose CT Image Quality,M. Sowjanya; M. Laxmi; B. Sreelatha; V. G,"2023 International Conference on System, Computation, Automation and Networking (ICSCAN)",26-Jan-24,2023,"Medical imaging plays a significant role in clinical diagnosis, but the use of low radiation doses in CT imaging can lead to poor image resolution and precise diagnosis. In this investigation, a novel method has been suggested to enhance low-dose CT (LDCT) images using an unsupervised deep learning model based on CycleGAN. The CycleGAN architecture enables us to perform image-to-image translation without the need for paired data, making it suitable for medical image denoising where matching high-dose CT (HDCT) images are often unavailable. Our model employs two generators for LDCT-to-HDCT and HDCT-to-LDCT translation, along with two discriminators to differentiate real and generated images in both domains. Extensive experiments demonstrate that our approach significantly improves the quality of LDCT images, yielding highfidelity HDCT-like images. This technique holds great promise for enhancing medical image quality and ultimately benefiting patient care.",Image quality;Measurement;PSNR;Image resolution;Computed tomography;Noise reduction;Generators,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10395736,IEEE Conferences,,,,,,
Thyroid Nodule Detection and Classification Using Deep Learning,S. Gowda; T. D; V. B. Sheth; V. R; G. N; H. P,"2023 International Conference on Computational Intelligence for Information, Security and Communication Applications (CIISCA)",26-Jan-24,2023,"Cells within the thyroid gland can grow abnormally, leading to the formation of radiographically identifiable lumps termed nodules. The latter is a little mass that has developed at the thyroid gland's level and necessitatesa medical examination in order to accurately diagnose the nodule's source and decide any potential treatments. Malignant nodules can be found using the Fine Needle Aspiration Biopsy (FNAB) technique. Yet, FNAB is an expensive, intrusive procedure that significantly increases patient anxiety. As a substitute, thyroid nodule ultrasound imaging is performed to classify the nodules. Even for experienced radiologists, accurately diagnosing thyroid nodules by ultrasonography is challenging due to the varied appearances of both cancerous and non-cancerous nodules. This paper suggests deep learning-based system that is capable of detecting and classifying thyroid nodules that may be able to offer radiologists unbiased assistance.",Neural networks;Information security;Ultrasonography;Needles;Diagnostic radiography;Medical diagnostic imaging;Thyroid,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10403628,IEEE Conferences,,,,,,
Convolutional Neural Network for Pleural Effusion Classification,B. C. R. S. Furriel; A. J. M. Paulo; G. A. S. Ribeiro; P. V. Santos; E. P. Reis; G. S. Mendes; J. Q. Paiva; R. M. Loureiro; L. Rittner; M. R. C. Reis,2023 IEEE Latin American Conference on Computational Intelligence (LA-CCI),26-Jan-24,2023,"This paper presents an approach for classifying pleural effusion using convolutional neural networks. The study utilizes a dataset of chest X-rays from the Chest X-Ray 14 database, comprising both normal and pleural effusion images. To enhance the modelâ€™s robustness and generalization, the Data Augmentation technique is employed for each class. The obtained results are highly promising. In Experiment I, the model achieves remarkable training and test success rates of 98.21% and 97.70%, respectively. In Experiment II, where an extensive data augmentation technique is applied, the model yields training, validation, and test rates of 78.66%, 87%, and 91%, respectively. These outcomes indicate the potential of the proposed classification model in facilitating the automated detection of pleural effusion and other lung diseases. This research makes a significant contribution to the advancement of computer-aided medical diagnosis, particularly by leveraging convolutional neural networks for chest X-ray image analysis.",Training;Data augmentation;Feature extraction;Convolutional neural networks;Task analysis;X-ray imaging;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409349,IEEE Conferences,,,,,,
A System For Detecting Brain Tumors Through the Use of Deep Learning and Image Classification with Improved Accuracy,M. Prakram; K. Rawal; A. Singh,2023 6th International Conference on Contemporary Computing and Informatics (IC3I),26-Jan-24,2023,"The success of treatment strategies and patient outcomes depends on early detection and accurate diagnosis of brain tumours. Deep learning and other image classification approaches have showed great promise in recent years for detecting tumours in medical imaging. This study details a method for automatically and accurately detecting brain tumours through the use of deep learning and picture classification. The proposed method employs a collection of brain images, such as MRI scans, to extract characteristics and classify the images into categories using convolutional neural networks (CNNs). The CNN model learns diagnostic features by being trained on a large collection of labelled data. The model's parameters are optimised through iterative training until they perform admirably at spotting cancer. The effectiveness of the system is evaluated across a number of dimensions, including sensitivity, specificity, precision, and accuracy. The outcomes demonstrate that the deep learning-based approach outperforms conventional image processing techniques in detecting brain tumours. The system's precision in classifying brain images enables early detection, which in turn aids in the rapid response and treatment planning required in such cases.",Deep learning;Training;Computer architecture;Brain modeling;Convolutional neural networks;Tumors;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10397805,IEEE Conferences,,,,,,
An Efficient Breast Cancer Disease Prediction Method using Deep Learning,D. Sharma; R. Kumar; A. Jain,2023 6th International Conference on Contemporary Computing and Informatics (IC3I),26-Jan-24,2023,"Breast Cancer is considerable concern among health issues in women. It is one of the higher mortality factors when compared to other cancers. Early prediction of cancer patients helps doctors to give a better diagnosis and prognosis of the disease. The study's goal is to assess how well proposed CNN-based models perform at classifying patients' tumour types as either malignant or benign depending on whether they are cancerous or non-cancerous. The goal of the study is to identify the key factors, such as the number of convolutional layers, the calibre of the training data, and the dependent variable, that influence the model's performance during training. The study makes use of the Kaggle-available Breast Cancer Histopathological data set. It is commonly used to assess CNN-based models in the health care sector. The classification performances of the models are analysed, and the models' training efficacy is assessed. The work demonstrates that robust feature representation and precise patient predictions can be achieved by utilising deep learning techniques, notably CNN models. The metrics for estimation performance were 97.42% accuracy, 97.39% Precision, and 97.45% Recall, respectively. The study's findings support the idea that using deep learning techniques can help doctors make accurate diagnoses, choose the best course of therapy, and monitor patients' prognoses. In comparison to conventional procedures, it substantially offers clinicians a solution. According to the study, managing and interpreting healthcare data may be done much more effectively when machine learning and deep learning techniques are used.",Deep learning;Training;Instruments;Medical treatment;Predictive models;Breast cancer;Prognostics and health management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10397634,IEEE Conferences,,,,,,
An Extended Deep Learning and Feature Pyramid Based Network for Screening CT-Scans Images,D. Pandey; K. Pandey,2023 6th International Conference on Contemporary Computing and Informatics (IC3I),26-Jan-24,2023,"Deep convolution neural Network is basically using a methodology to teach computer systems in the same way as humans learn by using examples and through experiences. It is a method of solving exhaustive image processing problems by using a heuristic-based approach. The objective of this research is to develop an extended deep learning-based hybrid method to detect abnormal symptoms using CT-scan Images. The detection of abnormal in any infected people is the most challenging issue due to less medical facility and time required to confirm its correctness based on the test conducted. Many AI-based approaches are suggested by researchers but most of the work is not integrated as a working model because of its correctness and accuracy level. The proposed work is implemented using Python and TensorFlow library. For experimentation, the CT-scan dataset is used with two different classes like normal or abnormal in nature. The proposed hybrid approach takes an input of CT-SCAN images from the selected data sets, converts it into grayscale, and then into. tiff format to get a more specific feature suitable for preprocessing step of the deep convolution network. Feature pyramid network has also been used on the selected images which provides better results in terms of low-level features in the deeper convolution networks. This hybrid methodology is helping us to get better results in comparison to state of art available. The implemented extended deep CNN network can accurately recognize varieties of real test cases CT-scans images. The learning accuracy of the hybrid method of RESNET101V2 with Feature pyramid network (FPN) and preprocessed dataset is up to 98.68 percent and it is maximizing the standard available state of art technique by a good margin in terms of result accuracy.",Deep learning;Art;Convolution;Computational modeling;Neural networks;Feature extraction;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398119,IEEE Conferences,,,,,,
Artificial Intelligence-Based Learning Approaches for Medical Imaging,Y. Kumar; S. Wadhwa; J. Rani; Kussum,2023 6th International Conference on Contemporary Computing and Informatics (IC3I),26-Jan-24,2023,"Artificial Intelligence is the most debated and discussed topic in the field of the health sector, especially in the research of medical imaging. It has the power to process many medical images and uncover those diseases that are not appreciated by the naked eye. AI-based models such as machine and deep learning can recognize patterns that aid medical diagnoses. In fact, these learning models have outperformed human capabilities in most applications, such as computer-aided diagnosis/detection, prediction of diseases, segmenting images, image generation, etc. Hence, this paper discusses the role and significance of AI and its techniques, such as machine and deep learning models, in medical imaging. Besides this, the works of the researchers who have used these techniques to process medical images are also presented along with their challenges that still persist.",Deep learning;Computational modeling;Medical services;Predictive models;Pattern recognition;Medical diagnostic imaging;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398104,IEEE Conferences,,,,,,
High Quality Segmentation Using Deep Learning Centered Detection And Correction of Cardiac MR Motion Artefacts Throughout Reconstruction,U. Rawat; V. Batra; R. K. Sharma; M. Kulandhaivel; C. Mukuntharaj; D. Dongre,2023 6th International Conference on Contemporary Computing and Informatics (IC3I),26-Jan-24,2023,"Deep learning techniques have been effectively used for a variety of purposes to separate anatomical features in healthcare imaging. Yet, the caliber of the picture being split has a significant impact on this outcome. The large quantity of clinical photographs with significant picture artefacts caused by organ motion, patient mobility, and/or picture acquisition-related difficulties is a frequently ignored topic inside the medical imaging evaluation field. This study compares various methods for concurrently compensating for artefacts and segregating the heart cavity. We address the effects of picture image noise on cardiac MR segments. The technique is based on our most current joint artefact detecting and reconstructing approach, which creates high-quality MR pictures from k-space the use of a By requiring a data integrity term, the associated logistic regression effectively transforms the artefacts correction problem into an under-sampled image enhancement challenge. In this research, we suggest coupling this with a recognition networks in edge architecture. Our instruction enhances three distinct tasks: Segmentation process, artefact restoration, and identification of artifacts are the first two. Using ventricular MR k-space data that has been artificially damaged with untreated rebuilt pictures; we train the reconstructing networks to autonomously repair formation-related aberrations. We demonstrate a high level of classification performance and clearly good pixel density in the absence of manufactured motion artifacts to use a training dataset comprising 500 2D+time cine MR observations from of the UK Biobank set of data. In comparison to several image correcting structures, we demonstrate improved efficiency.",Deep learning;Training;Image segmentation;Motion segmentation;Transforms;Medical services;Maintenance engineering,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10397682,IEEE Conferences,,,,,,
Computer-Aided Diagnosis for Breast Cancer Classification Using Deep Learning,Rishu; L. Singh; P. K. Shukla; K. Jindal,2023 6th International Conference on Contemporary Computing and Informatics (IC3I),26-Jan-24,2023,"In today's world, breast Cancer is one of the most deadly diseases among women, although early detection dramatically improves survival rate. CNN Models are the category of DL architecture that was developed to improve accuracy in breast cancer categorization. Compared to traditional methods, CNN has demonstrated better classification efficiency and tumor detection in medical imaging. Based on deep CNN architectures This research provides a new breast cancer classification system. This research provides a new breast cancer classification system based on deep CNN architectures. To develop a technique for early and accurate breast cancer detection in order to avoid wasteful therapy (error 1) due to False Positive and late treatment (error 2) owing to False Negative is the main aim of this research methodology. The primary goal of this work is to prevent error 2, for which we employ Resnet-152.",Deep learning;Medical treatment;Computer architecture;Breast cancer;Computer aided diagnosis;Informatics;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10397674,IEEE Conferences,,,,,,
Cutting-Edge Developments in Deep Learning Applications for Breast Cancer Detection: A Comprehensive Overview,P. Chakrabarti; S. Bala Krishnan; S. Choudhary,2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS),25-Jan-24,2023,"Breast cancer is a common tumor that affects women all over the world. Today, ductal carcinoma is the most prevalent form of breast cancer. People all around the world are really concerned about their health due to this sort of breast cancer. Predictive analytics are crucial for quickly identifying, diagnosing, and developing treatment regimens for persons with ductal carcinoma. The aim of this study is to identify research areas that haven't been sufficiently explored in the existing ductal carcinoma prediction analytic literature. The objective of this study is to review all previous studies on predictive analytics and ductal carcinoma. We intend to identify the present boundaries and gaps in this field of study by carefully examining the prior research. We'll also make recommendations for potential future research areas based on what we've discovered in order to close any gaps and advance the discipline. We will primarily concentrate on carefully selecting the appropriate prediction models, identifying the appropriate components, and assessing the accuracy of these models in predicting outcomes such as recurrence, metastasis, and survival. We will also discuss how crucial it is to investigate ductal carcinoma using an interdisciplinary approach, which entails collaborating with authorities in the domains of oncology, radiology, genetics, and data science. The main goal of the study is to give an in-depth look at the current state of research on predictive analytics for ductal cancer while pointing out possible directions for future research.",Predictive models;Radiology;Data science;Oncology;Genetics;Breast cancer;Mammography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389840,IEEE Conferences,,,,,,
Cataract Disease Identification Using Transformer and Convolution Neural Network: A Novel Framework,D. Kumar; B. Bakariya; C. Verma; Z. Illes,2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS),25-Jan-24,2023,"The significance of disease detection approaches based on deep learning (DL) in medical research, driven by artificial intelligence (AI), is gaining considerable attention. However, research in this domain encounters challenges in achieving the desired level of progress. These challenges stem from the diverse range of health diseases and the unique regional characteristics associated with many of these disease types. Among the diseases affecting the eyes, cataracts, a frequently encountered eye condition, can lead to visual impairments. Detecting cataracts accurately and in a timely manner is crucial for effective risk management and preventing the potential progression toward blindness. This paper introduces a deep neural network that utilizes convolutional neural network (CNN) models, namely VGG16 and ResNet50, and a Vision Transformer (ViT) based approach. These models are specifically designed for automatic cataract detection in eye images. Additionally, media noise filtering, implemented as median filtering, is employed as a preprocessing technique to reduce noise and enhance overall image quality. In addition, methods of data augmentation are utilised to combat the problem of overfitting. These methods involve expanding the size of the dataset prior to the training of the model. Based on the results of the experimental study, it is evident that the ViT method outperforms existing cataract detection approaches, demonstrating an impressive accuracy of 70%.",Cataracts;Adaptation models;Filtering;Transformers;Convolutional neural networks;Diseases;Residual neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10390523,IEEE Conferences,,,,,,
An Optimized CNN and Transfer Learning Approach for Pneumonia Detection,S. Verma; D. Ganesh Gopal; P. Kumar Sharma,2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS),25-Jan-24,2023,"The most prevalent lung condition affecting people worldwide is pneumonia. Diagnosing pneumonia only from a chest X-ray (CXR) might be challenging. The study aims to simplify pneumonia infection detection for experts and novices alike. We suggest a deep learning (DL) approach for identifying pneumonia using transfer learning (TL). A residual network previously trained on ImageNet is used in the proposed method to recover image features, which is then fed into a CNN classifier for prediction. The performance of the suggested model displays the ability to diagnose pneumonia, showing that the ResNet152V2 model could effectively distinguish between normal and pneumonia X-rays, reducing the burden of radiologists. Using residual learning (ResNet152V2), a model that can determine whether or not a person has pneumonia is trained. Here, the outputs of five different models are compared. The model is executed or trained using GPU through the Google colab. Compared to the CPU's performance, the GPU can considerably speed up the detecting process.",Adaptation models;Pulmonary diseases;Transfer learning;Graphics processing units;X-rays;X-ray imaging;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10390240,IEEE Conferences,,,,,,
"Recent Advances, Challenges, and Applications of Deep Learning in Healthcare Systems for Medical Diagnosis and Treatment",R. C. Patra; G. Saritha; G. A. Raghuwanshi; P. Parthiban; V. Ashok,2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS),25-Jan-24,2023,"In the realm of healthcare, the integration of Deep Learning (DL) stands as a potent force, propelling advancements in medical diagnosis and treatment. This paper navigates recent strides, persistent hurdles, and emerging applications within this synergy. DL basics are elucidated initially, demonstrating neural networks' role in data analysis. Progressing further, we unveil DL's robust applications in medical diagnosis, particularly via Convolutional Neural Networks, revolutionizing image-based disease detection. Yet, this frontier is not devoid of challenges, encompassing data privacy concerns and model interpretability. In tandem, DL empowers personalized treatment by predicting disease trajectories and expediting drug discovery. Expanding horizons, DL streamlines healthcare management through resource optimization and digitized records. In conclusion, this synthesis of DL and healthcare signifies a transformative trajectory, promising refined diagnostics and patient-centric treatments, though not devoid of ethical and technical intricacies.",Deep learning;Data privacy;Medical services;Weaving;Trajectory;Medical diagnosis;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10390124,IEEE Conferences,,,,,,
Unlocking Insights: Diseases Recognition in the Human Eye Using CNN and Random Forest,A. Suryavanshi; V. Kukreja; V. Jain; A. Bhattacherjee,2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS),25-Jan-24,2023,"An integral sign of both physical and mental health, pain is a complicated and multidimensional component of the human experience. Since it influences diagnosis, therapies, and overall patient care, timely and accurate pain evaluation is critical to the medical field. Innovative methods of measuring pain are necessary, though, because pain is subjective and some populations find it difficult to communicate verbally. To address this need, the project aims to transform pain detection by using facial expression analysis. A thorough framework that utilizes cutting-edge technologies, such as deep learning, Random Forest categorization, neural networks based on convolution (CNNs), and painstaking picture preprocessing, is presented by the researchers. Carefully crafted, the system accommodates the various, nuanced ways that pain manifests itself at varying degrees of intensity. The Convolutional Neural Networks, also referred to as CNNs are the main component of the method and are used to extract features. The classification phase of the system is where the real action happens. Here, Random Forest-a potent ensemble learning technique-is used to classify facial expressions into distinct pain severity categories. To produce precise and trustworthy classifications, this step makes use of the extensive feature set that CNNs have extracted. The assessment validates the pain detection system's outstanding performance. The model's accuracy and generalization skills across a wide range of pain intensity levels are demonstrated by the precision, recall, and F1-Score measures. Beyond its direct clinical uses, the work advances computer vision and AI-driven solutions for complicated human interactions in a larger context. With the ongoing advancements in technology, the automated pain detection system presents a ray of hope for bettering patient care, enabling timely treatments, and expanding the comprehension of how people express themselves when they are in pain.",Deep learning;Pain;Medical treatment;Forestry;Transforms;Feature extraction;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10390306,IEEE Conferences,,,,,,
Segmentation-Based Classification Deep Learning Model for Breast Cancer Detection using Mammogram images,A. Sinha; M. Pandey; M. N. B. J. Naskar; S. S. Rautaray,2023 IEEE 3rd Mysore Sub Section International Conference (MysuruCon),24-Jan-24,2023,"Breast cancer has emerged as a leading cause of mortality, responsible for an extensive number of deaths in recent years. The current imaging-based diagnostic methods adopted for the detection of breast cancer, such as mammography, has shown inadequate effectiveness in clinical environments due to their tendency for significant mistake rates. This paper introduces an effective methodology that uses segmentation based on deep learning classifiers for classification in order to perform an automated, productive, and precise diagnosis of breast cancer. In order to enhance the best model combination, a hybrid approach was employed, integrating deep learning segmentation models with VGG-19 classification models. The performance of the proposed methodology was evaluated using several statistical metrics, such as accuracy, precision, recall, f1-score, and receiver operating characteristics (ROC), along with cross-entropy loss function. The proposed methodology showed outstanding results compared to other segmentation-based classification model techniques. The research concluded that the UNet segmentation method, along with an improved VGG-19 classifier, had an improvement of 2.25% as compared to the VGG-19 (Pre-Trained) model.",Deep learning;Image segmentation;Magnetic resonance imaging;Receivers;Breast cancer;Mammography;Standards,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10397015,IEEE Conferences,,,,,,
A Transfer Learning-based Pre-trained VGG16 Model for Skin Disease Classification,G. Singh; K. Guleria; S. Sharma,2023 IEEE 3rd Mysore Sub Section International Conference (MysuruCon),24-Jan-24,2023,"Skin disorders pose a significant global health risk, impacting millions of individuals and placing a substantial burden on healthcare systems. The accuracy and speed of diagnosis are crucial for effectively managing various conditions. Deep learning models have demonstrated exceptional performance in diverse medical imaging applications, including the categorization of skin diseases. In recent years, the VGG16 deep learning architecture has gained prominence for its ability to extract meaningful features from images. In this study, a VGG16 model has been leveraged to early diagnose skin diseases. This approach involves collecting an extensive dataset comprising images of different skin disorders sourced from an open-source repository ""Kaggle"". Further, the VGG16 model is then fine-tuned on this collected dataset to learn the distinguishing patterns and characteristics associated with different skin conditions. The evaluation of the model's effectiveness has been done using standard metrics such as precision, recall, F1-score, and accuracy. These metrics assess the model's analytical capabilities in distinguishing between various skin disorders. The proposed deep learning model achieves remarkable accuracy of 90.1%, proving its proficiency in diagnosing a wide range of skin diseases, including those that appear similar. Furthermore, precision, recall, and F1-score have been identified as 0.867, 0.942, and 0.891, respectively. This research contributes to the evolution of computer-aided disease detection, potentially leading to enhanced healthcare outcomes by facilitating early detection and treatment of skin disorders. Nonetheless, continuous refinements and validation on larger, more diverse datasets are imperative to further enhance the model's accuracy and ability to generalize across various conditions.",Deep learning;Training;Computational modeling;Medical services;Feature extraction;Skin;Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396942,IEEE Conferences,,,,,,
A Comprehensive Review on Anomaly Detection in Images: Challenges and Future Research Directions,S. Kumari; C. Prabha,2023 IEEE North Karnataka Subsection Flagship International Conference (NKCon),24-Jan-24,2023,"Identifying irregularities in data, or ""anomalies,"" is essential in several fields, like medical imaging, intrusion detection (ID), fraud detection (FD), etc. A brief review of various approaches and methods presented by numerous researchers is presented in this paper. These approaches range from transformer models to image filters, autoencoders to low-rank representation, vision transformers to isolation forests, and convolutional autoencoders to deep neural networks. The primary goal of these methods is to capture the anomaly in the images. The significance of anomaly detection in numerous other applications viz. healthcare, cybersecurity, and industrial control systems are also highlighted in the paper and it also provides a brief insight into a variety of machine-learning methods to accurately detect anomalies.",Industrial control;Intrusion detection;Medical services;Machine learning;Transformers;Fraud;Anomaly detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396507,IEEE Conferences,,,,,,
Kidney Tumor Segmentation and Grade Identification in CT Images,R. D. Brehar; D. -A. Mitrea; S. Nedevschi; T. Moisoiu; F. I. Elec; M. A. Socaciu,2023 IEEE 19th International Conference on Intelligent Computer Communication and Processing (ICCP),23-Jan-24,2023,"Kidney tumor grade identification by means of feature based classification combined with semantic segmentation of Computed Tomography (CT) images targeting tumor region extraction are the two main contributions of this paper. For all the patients involved in the study three phases of the CT examinations are considered: native, arterial and venous phase. Medical specialists have annotated the images marking the regions in which tumors reside for each of the three phases. On each type of phase deep learning based segmentation has been applied in order to identify the tumor area. The differentiation among various tumor grades by means of deep learning is extremely challenging due to the small amount of available image data and due to the high similarity between different tumor grades. Hence feature based classification is applied to segmented regions in order to distinguish among the four tumor grades. The accuracy of the recognition varies from 60% up to 90% depending on the tumor grade, with first grade tumors being more difficult to be recognized while fourth grade tumors are correctly identified for most patients.",Deep learning;Image segmentation;Image recognition;Ultrasonic imaging;Computed tomography;Computer architecture;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398718,IEEE Conferences,,,,,,
Optimizing Pneumonia Detection Model based on CNVM (CNN-SVM),Y. Lin; T. Li; H. Xie; T. T. Toe; L. Lei,2023 2nd International Conference on Health Big Data and Intelligent Healthcare (ICHIH),23-Jan-24,2023,"To improve the precision of the diagnosis of pneumonia, this study presents a unique method that combines Convolutional Neural Networks (CNN) with Support Vector Machines (SVM). Using sophisticated convolution and pooling techniques, the CNN is used to extract complex features from X-ray images, turning the image data into high-dimensional feature vectors. This makes it possible to analyze structural details precisely. These feature vectors are then fed into an SVM classifier, which uses its robust classification abilities to differentiate between cases of pneumonia and non-pneumonia. The SVM builds a solid classification limit after extensive training on a variety of datasets, producing extremely precise diagnoses. In comparison to traditional methods, experimental results show a considerable improvement in pneumonia detection. The combined CNN-SVM technique demonstrates improved sensitivity and specificity, which significantly lowers the rate of misdiagnosis. The model also performs exceptionally well across diverse pneumonia types and severity levels, highlighting its adaptability and dependability. In conclusion, this study offers a novel method for diagnosing pneumonia by combining CNN and SVM. It delivers a considerable improvement in diagnostic precision and has enormous potential to assist clinical judgment in instances of pneumonia.",Support vector machines;Adaptation models;Pulmonary diseases;Feature extraction;Turning;Convolutional neural networks;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396157,IEEE Conferences,,,,,,
An Investigation into the Correlation Between MRI Preprocessing and Performance of Alzheimer's Disease Classification CNN Model,H. S. Cho; J. -C. Jeong; H. B. Hong,2023 14th International Conference on Information and Communication Technology Convergence (ICTC),23-Jan-24,2023,"An irreversible degenerative neurological disease, Alzheimer's disease (AD) affects a large proportion of the elderly population. Due to the fact that there is no perfect treatment method yet for Alzheimer's disease, medical imaging such as MRI is currently the best method to diagnose mild cognitive impairment or early AD, as well as to respond early and treat it. Additionally, with the advancement of deep learning technology, research on AD reading automation through MRI data is receiving a great deal of attention. Numerous results have been published as a result of this research. Preprocessing of MRI data is a basic part of the automatic reading technology that uses MRI data. The purpose of this study is to compare the performance of MRI data preprocessing in the automatic AD reading technology using MRI with the results of the study.",Deep learning;Training;Neurological diseases;Analytical models;Three-dimensional displays;Magnetic resonance imaging;Sociology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10393407,IEEE Conferences,,,,,,
Structural Anomaly Detection in Advanced Manufacturing Execution Systems,M. R. Subhan; M. J. A. Shanto; L. A. C. Ahakonye; D. -S. Kim; T. Jun,2023 14th International Conference on Information and Communication Technology Convergence (ICTC),23-Jan-24,2023,"Anomaly detection is crucial to ensuring the robustness and reliability of operational systems. Early identification of anomalies and deviations from standard patterns prevents potential failures, enhances decision-making processes, and optimizes overall system performance. This paper investigates image-based structural anomaly detection for manufacturing execution systems utilizing an optimized VGG16 convolutional neural network to identify structural anomalies. The optimized VGG16 model significantly performs binary classification on the test data to distinguish normal and anomalous instances. A comparative analysis with another classifier demonstrates that the optimized VGG16 is notable with high anomaly detection accuracy and has the potential to improve system reliability significantly. Experimental findings on publicly available image-based anomaly datasets demonstrate the efficiency and effectiveness of the proposed technique in identifying anomalies in management execution systems.",Productivity;System performance;Decision making;Solids;Robustness;Manufacturing;Information and communication technology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10393367,IEEE Conferences,,,,,,
Federated Learning in Prediction of Dementia Stage: An Experimental Study,B. Eom; M. Zubair; D. -H. Park; H. Kim; Y. -H. Suh; S. Lim; C. Park,2023 14th International Conference on Information and Communication Technology Convergence (ICTC),23-Jan-24,2023,"Federated Learning(FL) has emerged as the optimal approach for training machine learning models when dealing with data containing sensitive information, making data sharing impractical. Particularly in contexts where privacy is a primary concern, such as medical applications, Federated Learning demonstrates its efficacy as a solution. Motivated by this, we have conducted comprehensive experiments using a medical image dataset. One of the key objectives of these experiments is to evaluate the influence of Non-IID data which is frequently encountered in Federated Learning, especially within the medial field. We present our exploration of Federated Learning in classification of OASIS medical images, along with the results obtained from various experiments.",Training;Degradation;Privacy;Biomedical equipment;Federated learning;Medical services;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10392680,IEEE Conferences,,,,,,
Brain Tumor Detection & Classification into Different Categories using Deep Learning Model,U. Y. Tambe; A. Shanthini,2023 International Conference on Advanced Computing Technologies and Applications (ICACTA),23-Jan-24,2023,"This studyâ€™s objective is to create a Convolutional Neural Network (CNN) based system for identifying and categorizing brain cancers from MRI scans. The proposed methodâ€™s accuracy was assessed and contrasted with other classification techniques. The difficulty of identifying and classifying tumors is a result of the perplexing nature of images produced by imaging methods like MRI. In this study, a deep learning model for classifying various tumor types into 4 categories/types - glioma, meningioma, pituitary and no-tumor is proposed. To determine the most effective course of treatment, a special model with hyperparameter adjustment was built. The suggested methodâ€™s effectiveness in accurately classifying the tumors visible in the MRI pictures was demonstrated after it was applied to a sizable dataset of MRI images with an accuracy of 95.94%.",Deep learning;Computers;Magnetic resonance imaging;Computational modeling;Brain cancer;Medical services;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10393430,IEEE Conferences,,,,,,
From Pixels to Decisions: A Comprehensive Review of Brain Tumor Classification Techniques and Their Clinical Implications,A. Dubey; H. Dangi; V. Chaurasia; J. S. Yadav; L. Kumre,"2023 IEEE 3rd International Conference on Applied Electromagnetics, Signal Processing, & Communication (AESPC)",22-Jan-24,2023,"With the increasing prevalence of medical imaging, the need for efficient and accurate image analysis techniques has grown significantly. Artificial Intelligence has evolved as an effective tool to support medical image analysis, especially in the detection and classification of various diseases, including brain tumors. This paper presents an overview of latest improvements in deep learning-based brain tumor detection and classification. The state-of-art techniques of brain tumor classification have been discussed with details of base technology, benchmark datasets and evaluation metrics for providing insights into the performance of different approaches. The integration of deep learning in medical image analysis holds great promise in enhancing diagnostic accuracy and expediting treatment planning for improved treatment routine.",Measurement;Magnetic resonance imaging;Planning;Artificial intelligence;Medical diagnostic imaging;Tumors;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10390304,IEEE Conferences,,,,,,
Enhancing Monkeypox Skin Lesion Detection: A Fusion Approach of VGG 16 and Xception Features with SVM Classifier,D. Muduli; A. V. S. C. Naidu; K. V. Durga; K. Rahul; M. J. Kumar; S. K. Sharma,"2023 IEEE 3rd International Conference on Applied Electromagnetics, Signal Processing, & Communication (AESPC)",22-Jan-24,2023,"Monkeypox, a viral ailment resembling smallpox, can be identified through transfer learning, which involves utilizing pre-trained deep learning models to recognize patterns in medical images and facilitate early detection. This study assesses the efficacy of pre-trained convolutional neural network (CNN) models such as VGG 16, VGG 19, InceptionV3, and Xception as feature extractors. The study combines non-handcrafted features from these models, creating a final feature matrix that is inputted into various conventional machine learning classifiers. Testing was conducted on a publicly available dataset of monkeypox skin images, with the best performance achieved by VGG 16 + Xcepetion + SVM, exhibiting an accuracy of 97.14%, a sensitivity of 93.75%, and a specificity of 100%. This research highlights the potential of deep learning in medical image analysis and its potential to aid clinicians in the early detection of monkeypox.",Deep learning;Support vector machines;Feature extraction;Skin;Lesions;Convolutional neural networks;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10390185,IEEE Conferences,,,,,,
Advancements in Melanoma Skin Cancer Detection Using Deep Learning: A Comprehensive Review,K. Kachare; N. Bhagat; P. Raundale,"2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA)",22-Jan-24,2023,"In the past few years skin cancer has become the most common cancer worldwide, and come to know that Melanoma is the most prevalent form of cancer. Also, it's difficult to detect Melanoma in its early stage even for a skilled dermatologist. Recently detecting Melanoma has been viable due to Deep Learning. In this study, we surveyed the effectiveness of various deep-learning algorithms for Melanoma skin cancer detection. We have studied various papers and evaluated different methods on the basis of accuracy, sensitivity, specificity, preprocessing techniques used, image segmentation, image classification, and feature extraction. The findings of this study will shed important light on how well various deep learning algorithms perform at melanoma skin cancer detection, which may assist direct the creation of more efficient diagnostic tools and enhance patient outcomes.",Deep learning;Image segmentation;Systematics;Melanoma;Feature extraction;Skin;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10392174,IEEE Conferences,,,,,,
Cancer Classification Revolution: Employing Advanced Deep CNNs for Multi-Class Detection of Breast Irregularities,J. Nalifabegam; C. Ganeshbabu; N. Askarali; A. Natarajan; P. Maheshwari,"2023 Third International Conference on Smart Technologies, Communication and Robotics (STCR)",22-Jan-24,2023,"early detection plays a pivotal role in reducing breast cancer mortality rates significantly. Detecting breast cancer at an early stage can increase positive outcomes by up to 8%. While radiologists analyze breast images using techniques such as mammograms, X-rays, and MRIs, accurately identifying features like micro calcifications, lumps, and masses remains challenging, leading to high false positives and false negatives. Recent advancements in deep learning and image processing offer hope for more advanced tools in early breast cancer diagnosis. This study focuses on constructing a Deep Convolutional Neural Network (CNN) capable of recognizing and categorizing various breast abnormalities, including lumps, asymmetries, calcifications, and carcinomas. Unlike previous research that primarily differentiated between benign and malignant cancers, this approach allows for more specialized disease treatment. The methodology involves transfer learning using a pre-trained model (ResNet50), fine-tuning it with the available dataset, and developing an enhanced deep learning model emphasizing learning speed. The proposed deep learning model achieved an impressive 88% accuracy in classifying masses, calcifications, carcinomas, and asymmetrical mammograms. This progress holds significant promise for improving the overall accuracy of breast cancer diagnosis and subsequent treatment decisions. The study aligns with ongoing efforts to enhance early detection and treatment, potentially saving lives through more efficient medical procedures.",Deep learning;Adaptation models;Image processing;Transfer learning;X-rays;Breast cancer;Mammography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396886,IEEE Conferences,,,,,,
A Novel Transfer Learning Approach for Eye Tumour Detection using Pre-Trained CNN Models,G. Prabaharan; V. Rajasekar; V. Sarveshwaran,"2023 Third International Conference on Smart Technologies, Communication and Robotics (STCR)",22-Jan-24,2023,"According to a recent study, using retinal imaging to identify the tumour in the eye is difficult. When an image is analyzed, the affected region of the lesion impacted retinal picture is not discovered in the conventional system due to uneven illumination, low contrast, and blurring, which are symptoms of an eye disease. The proposed methodology was created to identify the eye tumour in the earlier stage with enhanced image quality and increased accuracy. The conventional system has a few drawbacks such as lower accuracy and more computational time to analyze the image. Convolutional neural networks (CNN) have recently demonstrated its result of their capacity to learn various category image denotions that aid in image classification in various sectors. These networks have a tremendous ability to extract the proper features from input images with training on millions of images, which produces accurate categorization. The deep learning-based transfer learning approach is suggested for earlier detection of eye tumours. The five pre-trained models such as AlexNet, ResNet, DensNet, VGG 19 and Inception model were used for efficient eye tumour classification. The performance analysis of the proposed approach has shown that higher accuracy of 98.63% and this model shows enhanced performance in better classification.",Training;Transfer learning;Lighting;Imaging;Retina;Feature extraction;Performance analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396907,IEEE Conferences,,,,,,
Enhancing Scientific Image Classification through Multimodal Learning: Insights from Chest X-Ray and Atomic Force Microscopy Datasets,D. C. Meshnick; N. Shahini; D. Ganguly; Y. Wu; R. H. French; V. Chaudhary,2023 IEEE International Conference on Big Data (BigData),22-Jan-24,2023,"In this study, we conduct a detailed evaluation of machine learning and multimodal learning approaches in two distinct areas: a standard medical imaging benchmark and a novel material sciences benchmark. We utilize the CheXpert chest x-ray dataset for medical imaging and introduce a newly created Fluoropolymer Atomic Force Microscopy (AFM) dataset for material sciences. Both datasets are enhanced with additional images and binary metadata, encoded as one-hot vectors. We tested both pretrained and non-pretrained Convolutional Neural Network (CNN) models, such as ResNet50, ResNet101, DenseNet121, InceptionV3, and Xception, across different combinations of image and metadata inputs. Our results reveal that integrating multimodal data, including simple binary metadata, significantly enhances classification accuracy compared to conventional unimodal approaches or advanced MADDi models. This indicates the efficacy of multimodal learning in enriching data representation and boosting image classification performance. Notably, Xception models showed exceptional performance in CheXpert tests, and most models improved crystal structure predictions in AFM datasets. These insights set a new benchmark for performance and underscore the potential of multimodal learning in data-intensive applied science research.",Atomic force microscopy;Force;Metadata;Predictive models;Benchmark testing;Convolutional neural networks;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386478,IEEE Conferences,,,,,,
Data Poisoning Attacks over Diabetic Retinopathy Images Classification,F. Martinelli; F. Mercaldo; M. Di Giammarco; A. Santone,2023 IEEE International Conference on Big Data (BigData),22-Jan-24,2023,"Data poisoning represents a set of techniques aimed at perturbing data for training machine learning models, affecting performance. Such intentional attacks are widespread in many applications involving deep learning algorithms and are aimed to provide misclassifications. In this paper, data poisoning on retinal images for the diabetic retinopathy binary classification (health and sick) is presented and evaluated. The presented attacks are almost imperceptible perturbations of the images that nevertheless decrement the metrics of the trained models. Once exposed to the data poisoning distortions on these images, a possible countermeasure to enhance the security from these attacks is shown. In this way, the robustness and vulnerabilities of the network are highlighted and the best result is also analyzed through the use of heatmaps, for the qualitative point of view. The paper aims to focus on the effects of data poisoning in deep learning model testing phase and to discuss possible countermeasures.",Deep learning;Training;Measurement;Diabetic retinopathy;Perturbation methods;Retina;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386903,IEEE Conferences,,,,,,
Is one label all you need? Single positive multi-label training in medical image analysis,H. Schneider; P. Priya; D. Biesner; R. Kador; Y. C. Layer; M. Theis; S. Nowak; A. M. Sprinkart; U. I. Attenberger; R. Sifa,2023 IEEE International Conference on Big Data (BigData),22-Jan-24,2023,"Deep Learning is proving its immense potential in medical image processing. However, noisy label data can weaken the generalization ability of the model and cause significant performance degradation. This issue can be more prevalent in multi-label classification tasks, since their annotation is more challenging. With too many labels, human annotators may have difficulties mentioning all possible classes, which leads to an increased number of false negative labels. Single Positive Multi-Label (SPML) training deals with the most severe version of this problem, in which for each sample only one positive label is available. All other labels are not observed, i.e. not confirmed as positive or negative. While SPML has already achieved good results for the multi-label detection of objects, its impact on the extremely pertinent medical imaging use case has not yet been explored. In this work, we therefore investigate the performance of state-of-the-art SPML loss functions in the analysis of chest X-rays, both on the public CheXpert and on an in-house data set of the University Hospital Bonn. In addition, we propose our new SPML loss functions, the Generalized Assume Negative and Implicit Weighting Assume Negative loss, which increase the mean average precision by up to 4.6% compared to the popular binary cross-entropy loss.",Training;Degradation;Deep learning;Image analysis;Hospitals;Data models;Noise measurement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386758,IEEE Conferences,,,,,,
Deployment and Application of Deep Learning Models under Computational Constraints,J. Li; W. Wang,2023 IEEE International Conference on Big Data (BigData),22-Jan-24,2023,"Deep learning is a computationally demanding field. So the GPU used has a very important impact on the effectiveness of deep learning. In addition, deep learning requires huge amounts of memory. High throughput means better performance, faster iterations, and more efficient completion of experiments. Therefore, the choice of GPU basically determines the success and experience of deep learning. This paper analyzes four commonly used computer deep learning model structures, VGG, ResNet50, MobileNet, InceptionV3, and their memory requirement. At the same time, this paper also analyzes the relationship between computing power and memory of several mainstream graphics cards produced by Nvidia which are often used for deep learning, including GeForce, Tesla, and etc. Therefore, in the case of limited medical conditions, such as the inability to purchase expensive graphics cards, or do not have enough memory graphics cards, an existing suitable computer model can be found to help medical personnel to perform gland image segmentation, so as to achieve a more efficient, fast and economical scheme.",Deep learning;Graphics;Analytical models;Image segmentation;Medical conditions;Computational modeling;Memory management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386270,IEEE Conferences,,,,,,
Multi-Instance Bias Suppression for Enhanced Generalization in Breast Cancer Diagnosis : Harnessing Histopathological Big Data Insights,S. A. Shah; X. Zeng; A. Ahmed; S. Parvez; R. Xi; M. Hou,2023 IEEE International Conference on Big Data (BigData),22-Jan-24,2023,"The automated diagnosis of breast cancer through Whole Slide Images (WSI) is a critical endeavour to combat the threat it poses to womenâ€™s health. However, traditional deep learning algorithms strongly rely on Independent and Identically Distributed (I.I.D) and then encounter challenges related to multi-instance bias when analyzing multiple tissue sections from the same patient, limiting their generalization capability. To address this, this study introduces Multi-Instance Bias Suppression (MIBS), a novel approach leveraging adversarial training to mitigate patient-specific overfitting. MIBS employs an instance-level discriminator to guide feature generation, disentangling instance-specific cues from broader diagnostic patterns. Through competitive adversarial training, MIBS enhances feature generalization, effectively addressing overfitting and boosting cross-patient accuracy. Validated on the BreakHis dataset, MIBS effectively tackles multi-instance bias-induced overfitting. By bridging the gap between cutting-edge deep learning techniques and the challenges posed by large-scale medical image data, MIBS advances the accuracy and applicability of breast cancer diagnosis. Our approach addresses the multi-instance bias challenge and integrates seamlessly with big data, propelling medical image analysis to new heights of efficiency and precision.",Deep learning;Training;Image analysis;Limiting;Training data;Big Data;Propulsion,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386103,IEEE Conferences,,,,,,
Deep Learning-based Covid and Pneumonia Classification,M. M. Ali; V. Ranjan; A. Farid; M. Raj,2023 Second International Conference On Smart Technologies For Smart Nation (SmartTechCon),19-Jan-24,2023,"Internal images of the body may be captured using X-ray technology, which helps doctors diagnose and treat a wide range of medical issues. Recent studies have focused on optimizing the usage of deep learning algorithms to boost medical imaging's accuracy and productivity. Deep learning AI makes use of massive datasets to train computer algorithms to recognize patterns, which are then used to make predictions or classifications about fresh data. This method has shown promise in facilitating quicker and more accurate detection of conditions including lung cancer and bone fractures using X-ray images. Patient outcomes may improve with the use of deep learning algorithms in medical imaging. Healthcare systems may be strengthened, and the effects of health crises lessened by facilitating faster and more accurate diagnosis. Investing in initiatives that promote the use of deep learning in medical imaging will help get us there. A bespoke deep learning model was able to classify 70% of a dataset evenly between the healthy, COVID, and pneumonia patients, yielding an astonishing 97.96% accuracy. Applying deep learning algorithms to the analysis of X-ray images has shown great potential for improving the efficiency and precision of medical diagnoses. Better health outcomes may result from this enhanced ability to identify abnormalities and illnesses such as lung cancer and bone fractures. The healthcare industry stands to benefit greatly from adopting programs that encourage the integration of deep learning in medical imaging and might even save lives by doing so.",Deep learning;COVID-19;Pulmonary diseases;Lung cancer;Medical services;Prediction algorithms;Bones,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391490,IEEE Conferences,,,,,,
Exploring Transfer Learning approaches for thorax disease diagnosis,D. Pantola; D. Vatsa; M. Gupta,2023 Second International Conference On Smart Technologies For Smart Nation (SmartTechCon),19-Jan-24,2023,"Thorax diseases occur due to the malfunctioning of organs within the thoracic cavity like heart, lungs, and esophagus. Chest X-ray imaging is a standard diagnostic mechanism which is routinely used for the management of a variety of thorax diseases. Nevertheless, the analysis of these images can pose difficulties, particularly in cases where multiple pathology may be present. In past decade, deep learning techniques have been extensively utilized in automating early disease diagnosis. In this paper, we present a comparative study of transfer learning approaches on multiple chest x-ray datasets highlighting the strengths and limitations of each approach. Models accuracy comparison shows that VGG16 (98%) and XceptionNet (97%) performed best on Kaggle dataset and ResNet50 (94.61%) and DenseNet (95%) gave best results on NIH dataset. The presented study shows the power of transfer learning in generating efficient automatic disease detection systems by achieving accuracy similar to that of expert radiologists.",Pathology;Transfer learning;Neural networks;Thorax;Medical diagnosis;X-ray imaging;Standards,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391323,IEEE Conferences,,,,,,
Enhanced Brain Tumor Diagnosis through Multimodal Fusion and 3D Convolutional Neural Networks,C. M. B. M J; E. Kannan; A. D. S; G. Thangarasu; A. Begum,2023 Second International Conference On Smart Technologies For Smart Nation (SmartTechCon),19-Jan-24,2023,"Brain tumor detection has enhanced using a Multi-CNNs with Multimodal Information Fusion method has been proposed in this work. The proposed method leverages multiple modalities, like T1-weighted, T2-weighted, and FLAIR images, to create a fused representation of the input data. A deep neural network architecture using 3D Convolutional Neural Networks (3D-CNNs) is constructed to capture spatial and volumetric information present in medical images. The method also incorporates a real normalization layer and a weighted loss function to improve the learning process and prioritize accurate detection and localization of tumor lesions. Evaluating our method on a Kaggle dataset of 253 images (98 without tumor, 155 with tumor), we achieve superior results compared to existing approaches. Our proposed method attains an accuracy of 98.9%, sensitivity of 99.2%, specificity of 99.1%, recall of 98.7%, precision of 99.3%, and F1 score of 98.9%. Leveraging multimodal fusion, 3D-CNNs, and additional enhancements, our study contributes to more reliable brain tumor diagnosis, benefiting patients and healthcare providers.",Training;Three-dimensional displays;Sensitivity;Brain modeling;Stability analysis;Convolutional neural networks;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391443,IEEE Conferences,,,,,,
Vision Transformer Model for Efficient Stroke Detection in Neuroimaging,O. Katar; O. Yildirim; Y. Eroglu,2023 4th International Informatics and Software Engineering Conference (IISEC),19-Jan-24,2023,"A brain stroke occurs when blood flow to a part of the brain is disrupted, potentially caused by a blocked or ruptured blood vessel. Deprived of oxygen and nutrients, brain cells can start dying within minutes, leading to irreversible damage. Early diagnosis and treatment are crucial to minimize brain damage and improve recovery chances. Clinical assessments and imaging techniques like Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) scans are commonly used for rapid detection, but manual analysis has limitations, including delays and subjectivity. AI-based models offer a faster and more consistent approach for stroke diagnosis, enhancing accuracy. In this study, an explainable Vision Transformer (ViT) model is proposed for stroke classification and localization from brain CT images. The model is validated on a dataset of 6,651 samples. To address an unbalanced dataset, two training scenarios were employed. Scenario-1 directly used the unbalanced dataset, while Scenario-2 equalized sample numbers through data augmentation. In the test phase, Scenario-1 achieved 97.25% accuracy, 98.46% precision, 96.00% recall, 98.50% specificity, and a 97.22% F-1 score. In contrast, Scenario-2 achieved even higher performance with 98.75% accuracy, 99.49% precision, 98.00% recall, 99.50% specificity, and a 98.74% F-1 score. Analyzing the softmax ratios in the model predictions revealed that Scenario-2, with synthetic images in the training set, produced more reliable results. The study also used the Grad-CAM algorithm to visualize the areas of focus in the modelsâ€™ predictions, showcasing their superior localization capabilities. This proposed model is well-suited for clinical use due to its high accuracy rates and robust localization abilities, potentially improving stroke diagnosis and treatment outcomes.",Training;Location awareness;Computed tomography;Magnetic resonance imaging;Computational modeling;Predictive models;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391051,IEEE Conferences,,,,,,
Deep Transfer Learning to Predict Alzheimer Disease Status,A. Benyahia; A. Benammar; I. E. Araar,2023 4th International Informatics and Software Engineering Conference (IISEC),19-Jan-24,2023,"Alzheimer disease (AD) is a significant global health concern, and early diagnosis is crucial for effective intervention. In this study, we explore the use of deep transfer learning convolutional neural networks to classify AD status based on brain magnetic resonance imaging (MRI) data. Specifically, we employ two pre-trained CNN architectures: VGG19 and DenseNet201 to extract features from MRI scans of the Kaggle AD dataset. The dataset used in this study includes a diverse collection of brain MRI scans, accurately classifying individuals into different categories, including those with AD at various stages such as mild, very mild, and moderate, as well as non-AD subjects. We employ these pre-trained CNN architectures to extract features from the Kaggle AD dataset. We employed the SMOTE technique to address the datasetâ€™s class imbalance problem. We applied deep transfer learning to customize these pre-trained models for the AD classification task. Our results display the effectiveness of transfer learning in AD classification, achieving the following accuracies: VGG19 (0.93) and DenseNet201 (0.96), remarkably, the DenseNet201 model emerged as the top performer, achieving an accuracy of 0.96.",Magnetic resonance imaging;Transfer learning;Predictive models;Feature extraction;Brain modeling;Robustness;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391043,IEEE Conferences,,,,,,
A Transformer-Based Deep Learning Architecture for Accurate Intracranial Hemorrhage Detection and Classification,A. ElZemity; M. ElFdaly; S. Abdelfattah; A. Abdelwahab; M. Ramadan; S. Zakzouk; A. Ameen; R. Elkhishen; M. S. Darweesh,"2023 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)",19-Jan-24,2023,"Intracranial Hemorrhage (ICH) is a critical medical condition characterized by bleeding within the skull or brain, necessitating rapid and precise diagnosis for optimal treatment and enhanced patient outcomes. This paper introduces an innovative deep learning architecture, specifically the Swin Transformer, for the detection and classification of ICH. The proposed model achieves a remarkable log loss of 0.04372, outperforming traditional convolutional neural network-based approaches. Furthermore, it encompasses a comprehensive desktop application tailored for healthcare professionals, facilitating streamlined ICH assessment. This pioneering approach not only represents a significant advancement in medical imaging but also carries the potential to revolutionize the landscape of ICH diagnosis. The paper aims to bridge the gap between cutting-edge technology and practical healthcare applications, offering invaluable insights that resonate with healthcare practitioners. It is believed that the proposed research findings provide a lucid perspective, empowering healthcare professionals with a powerful tool to enhance their diagnostic capabilities in the critical realm of ICH detection and classification.",Technological innovation;Deep architecture;Computer architecture;Streaming media;Transformers;Convolutional neural networks;Hemorrhaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391388,IEEE Conferences,,,,,,
Exploiting Uncertainty: A Transformer-Based Multi-Disease Diagnostic Approach to Chest X-RAYS,D. Songfeng; C. Xingwu,2023 20th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP),18-Jan-24,2023,"In the field of medical image analysis, there is significant interest in developing a unified deep-learning framework for the automated detection of multiple diseases. However, the presence of vague annotations in multi-disease datasets hinders models from fully utilizing the available data. To address this, we propose a framework that considers both certain and uncertain labels to better model label dependencies. The methodology consists of three main steps: extracting visual features from medical images using convolutional neural networks, generating sample-specific semantic label embeddings, and utilizing a Transformer model to capture the complex associations between the visual features and semantic label embeddings. Our experiments on chest X-rays illustrate that this comprehensive approach outperforms methods that solely rely on definite labels.",Measurement;Visualization;Uncertainty;Semantics;X-rays;Media;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10387037,IEEE Conferences,,,,,,
Diffusion-Enhanced Magnified Histopathological Images for Robust Breast Cancer Classification,Y. Chengxiao; Z. Xiaoyang; A. Ahmed; M. H. Tunio; F. Shuhuan,2023 20th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP),18-Jan-24,2023,"Histopathological Image Analysis (HIA) is essential for breast cancer diagnosis, where classifier accuracy and precision are crucial for patient care. While developing a breast cancer classifier, researchers faced data limitations and the need for superior histopathological image discrimination. Complex cancerous tissues require nuanced and detailed analysis, but diverse and representative datasets are scarce. This study proposes a naive solution using diffusion techniques to propose a robust cancer classification method (Diffusion-enhanced BCCNet) with an efficient image enhancement mechanism. The three public Breast Cancer datasets (BreakHist, BHI Breast Histopathology Images, and BACH) have shown significant progress, demonstrating the potential of our proposed methodology to address data availability issues.",Image analysis;Medical conditions;Histopathology;Information processing;Computer architecture;Media;Generative adversarial networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10387068,IEEE Conferences,,,,,,
Domain Contrastive Learning for Multi-Site Alzheimer's Disease Classification,L. D. Fiasam; Y. Rao; C. Sey; E. S. E. B. Aggrey; S. L. Kodjiku; J. A. Browne; J. M. Danso; C. C. Ukwuoma; E. S. A. Gyarteng,2023 20th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP),18-Jan-24,2023,"In the field of medical imaging, accurately classifying Alzheimer's Disease (AD) poses a substantial challenge, primarily due to the inherent variability in imaging data across different clinical sites. This paper introduces a novel approach, Domain Contrastive Learning (DCL), to address this challenge by enhancing the robustness and transferability of AD classification models using multi-site Magnetic Resonance Imaging (MRI) data. DCL leverages a unique contrastive learning technique within an autoencoder's latent space, enabling the extraction of domain-invariant features essential for accurate diagnosis. We rigorously evaluate the proposed network on brain images from three distinct imaging sites, employing a Domain Generalization (DG) training protocol to ensure robustness and generalizability across varying data sources. Our model is evaluated on three binary classification tasks: AD vs. Normal Control (NC), AD vs. Mild Cognitive Impairment (MCI), and MCI vs. NC. Results demonstrate that DCL outperforms existing methods, especially in AD vs. NC classification, achieving higher accuracy, sensitivity, and specificity with 0.51%, 1.54% and 1.68% increments respectively. This underscores DCL's potential as a robust and generalizable method for enhanced AD diagnosis, marking a significant step forward in medical imaging challenges in a multi-site context.",Training;Protocols;Wavelet domain;Magnetic resonance imaging;Self-supervised learning;Brain modeling;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10387055,IEEE Conferences,,,,,,
Motion Artifact Correction in MRI using GAN-Based Channel Attention Transformer,T. -H. Tsai; Y. -H. Lin; T. -H. Lin,2023 IEEE Biomedical Circuits and Systems Conference (BioCAS),18-Jan-24,2023,"Magnetic resonance imaging (MRI) is a widely used medical imaging technique that produces precise and detailed anatomical as well as functional data of the human body. However, motion artifacts caused by patient movement during the scan can degrade image quality, making diagnosis more difficult. In this paper, we propose a new approach to motion artifact correction in MRI based on Transformer architecture, a deep learning model that has achieved state-of-the-art results in natural language processing and computer vision tasks. Our model processes motion-corrupted MRI images hierarchically, utilizing channel self-attention mechanism to capture motion-blurred patterns. It is challenging to obtain the paired motion-corrupted and clear images simultaneously in real-world situations. To assess the effectiveness of our approach, we trained and tested the artifact correction model using both real and synthetic motion artifact MRI datasets. Overall, our proposed approach provides a promising direction for motion artifact correction in MRI and has the potential to improve the accuracy and reliability of MRI-based diagnoses.",Deep learning;Magnetic resonance imaging;Computational modeling;Computer network reliability;Computer architecture;Transformer cores;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389083,IEEE Conferences,,,,,,
Brain Tumor Segmentation from MR Images using Customized U-net for a Smaller Dataset,R. Imtiaz; M. W. Mirza; A. Siddiq; M. Farooq-i-Azam; I. R. Khan; S. Rahardja,2023 IEEE Biomedical Circuits and Systems Conference (BioCAS),18-Jan-24,2023,"In medical image analysis, deep learning has emerged as a powerful tool for solving complex tasks such as segmentation. This research presents an original approach using a customized U-net model for automatic brain tumor segmentation in Magnetic resonance imaging scans. The model's performance is thoroughly assessed using metrics like Mean Dice Similarity Coefficient, Sensitivity, Specificity, and Accuracy. The study evaluates the model's proficiency on 50 test images from in-house and established BRaTS 2020 datasets. The proposed U-net model showcases its superiority by achieving high Dice Similarity Coefficient scores, indicating a solid alignment with reference segmentations. It also demonstrates impressive sensitivity and specificity values, signifying its capacity to capture tumor regions and true negatives accurately. The results demonstrate that the proposed model effectively segments tumors on a relatively smaller dataset.",Image segmentation;Adaptation models;Solid modeling;Sensitivity;Sensitivity and specificity;Brain modeling;Solids,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389092,IEEE Conferences,,,,,,
Analysis of Bones Abnormality Detection and Segmentation,M. -A. Ismail; Y. Mohammed; N. Gamal; M. Hatem; S. Ibrahim; E. H. Abbas; M. Mohsen; M. Nasser; H. M. Ebied,2023 Eleventh International Conference on Intelligent Computing and Information Systems (ICICIS),18-Jan-24,2023,"Numerous musculoskeletal disorders can be caused by bone diseases, which are widespread. Musculoskeletal conditions are estimated to affect 1.71 billion people worldwide. In addition to musculoskeletal fractures, knee osteoarthritis, fractures, and femoral neck injuries are all relatively prevalent bone illnesses. Missed fractures, on the other hand, are a frequent prognostic failure in accidents and emergencies. As a result, patientsâ€™ treatment and care are complicated and delayed. Deep Learning (DL) and Artificial Intelligence (AI) are currently garnering a lot of interest for their potential to help physicians identify bone fractures. Studies in traumatology and orthopedics have demonstrated the utility and potential of DL in the radiograph-based diagnosis of fractures and illnesses. In this paper, different classification techniques have been compared to classify the bone. The methods used for detection and segmentation of wrist bones are Inception-v3, VGG16, Densenet-121, and Resnet-50while for elbow bones, Resnet-50, Inception-v3, Densenet-121, and Densenet-169 are used. Two datasets are used in the experiments.: Kaggle dataset and MURA dataset. The experimental results show that the Densenet-169 and Inception-v3 performed better than the other models using MURA dataset and Kaggle dataset respectively.",Wrist;Musculoskeletal system;Image segmentation;Medical services;Bones;Neck;Elbow,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391116,IEEE Conferences,,,,,,
Implicit Feature Augmentation with Feature Transfer For Class-Imbalanced Medical Image Classification,M. Yu; H. Zheng; X. Li; J. Gao; X. Fu; Z. Liu; R. Yu,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"The class imbalance problem, which is prevalent in medical image datasets, seriously affects the diagnostic effectiveness of deep learning-based network models. To alleviate this problem, data re-sampling and loss re-weighting techniques are often used to reshape the decision boundary of the classifier. However, these techniques still lead to biased decision boundary due to the lack of sufficient and diversified samples in the tail classes of medical image datasets. In this paper, we propose an implicit feature augmentation with feature transfer (FT-IFA) method which solves the class imbalance problem by expanding the feature space of tail classes to reshape the decision boundary of the classifier. FT-IFA utilizes prototype similarity to transfer the rich transformation information from the head classes to tail classes on the basis of the balanced feature space, enriching the intra-class diversity of tail classes. Experimental results on two class-imbalanced medical image datasets show that FT-IFA outperforms the current state-of-the-art methods and effectively solves the class imbalance problem.",Head;Prototypes;Tail;Bioinformatics;Medical diagnostic imaging;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385442,IEEE Conferences,,,,,,
Patch-based CNN Models for Bone Marrow Edema Detection Using MRI,A. Gomes; T. Pereira; F. Silva; P. Franco; D. C. Carvalho; S. C. Dias; H. P. Oliveira,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"Bone marrow edema (BME) or bone marrow lesion is the term attributed to an observed signal change within the bone marrow in magnetic resonance imaging (MRI). BME can be originated from multiple mechanisms, with pain being the main symptom. The presence of BME is an unspecific but sensitive sign with a wide differential diagnosis, that may act as a guide that leads to a systematic and correct interpretation of the magnetic resonance examination. An automatic approach for BME detection and quantification aims to reduce the overload of clinicians, decreasing human error and accelerating the time to the correct diagnosis. In this work, the bone region on the MRI slice was split into several patches and a CNN-based model was trained to detect BME in each patch from the MRI slice. The learning model developed achieved an AUC of 0.853 Â± 0.056, showing that the CNN-based model can be used to detect BME in the MRI and confirming the patch strategy implemented to deal with the small data size and allowing the neural network to learn the specific information related with the classification task by reducing the region of the image to be considered. A learning model that can help clinicians with BME identification will decrease the time and the error for the diagnosis, and represent the first step for a more objective assessment of the BME.",Systematics;Pain;Magnetic resonance imaging;Biological system modeling;Neural networks;Bones;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385930,IEEE Conferences,,,,,,
SDATNet: Self-Distillation Adversarial Training Network for AD classification,T. Song; G. Cao; X. Xiong; G. Kang,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"Alzheimerâ€™s disease (AD) is a neurodegenerative brain disorder of unknown etiology that has a significant impact on the lives of patients and their families. Currently, there are no drugs or treatments that can cure AD, but early diagnosis and intervention can mitigate the effects of the disease on patients. Of various brain imaging tools, structural magnetic resonance imaging (sMRI) has been most intensively studied for AD diagnosis as it provides imaging biomarkers of neuronal loss. However, the accuracy of visual inspection by doctors is limited. Therefore, computer-aided diagnosis (CAD) based on sMRI has important clinical significance. Previous research has employed traditional machine learning and deep learning methods for AD image classification. However, these methods face challenges due to the scarcity, high noise, and high redundancy of medical data. Recent studies demonstrate the effectiveness of self-distillation in enhancing the model robustness, but the lack of additional knowledge limits such improvement. To address the issues of high redundancy in medical data and the lack of additional knowledge in self-distillation architectures, we propose the Self-Distillation Adversarial Training Network (SDATNet), which integrates self-distillation and adversarial training into a single framework. We utilize adversarial training methods to simulate noise on medical images, thereby supplementing additional information. Through self-distillation, we achieve cross-scale information interaction, enabling the extraction of discriminative features and effectively improving model performance. Experimental validation on the ADNI dataset demonstrates that our model outperforms other methods on publicly available datasets, achieving an accuracy of 92.77% and specificity of 92.19%. Our ablation experiments and visualization results further validate the reliability and superiority of the model.",Training;Knowledge engineering;Solid modeling;Visualization;Biological system modeling;Termination of employment;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385444,IEEE Conferences,,,,,,
Enhancing Classification Tasks through Domain Adaptation Strategies,A. Chaddad; Y. Wu,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"Domain adaptation (DA) is a technique that uses the knowledge from similar data sets to enhance the generalizability of a model, which proves to be effective in addressing the issue of limited training data. However, due to the complexity of medical data, most advances have occurred in the natural domain and not in the medical field. Furthermore, the majority of advancements are derived from conventional DA datasets, which may introduce result bias. This article presents a comprehensive new analysis of four widely used DA algorithms, tested on more realistic medical data sets to assess the potential of these DA techniques. The examination covers an evaluation of their model performance, discrepancies in data distribution, and the interpretability of the models. For example, the Deep subdomain adaptation network (DSAN) achieves a high level of accuracy on the COVID-19 dataset (89.9%) using Resnet34. Furthermore, the interpretability of the DSANâ€™s prediction using the skin cancer dataset is implausible. In conclusion, we offer perspectives on the outcomes obtained. Our codes are available at https://github.com/AIPMLab/Domain_Adaptation.",COVID-19;Adaptation models;Codes;Training data;Data models;Complexity theory;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385628,IEEE Conferences,,,,,,
Semantic-guided Unknown-aware Rare Disease Diagnosis System,Y. Luo; Y. Yuan,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"The significant challenge posed by rare disease diagnoses has recently motivated researchers to explore computer-aided solutions. While deep learning approaches have shown potential in developing automatic diagnosis systems, their effectiveness diminishes when addressing rare diseases with limited data. Moreover, existing diagnostic models typically fail to detect unknown diseases, making them inappropriate for real-world applications. In this paper, we address the above challenges by proposing a Semantic-guided unknown-aware Rare Disease Diagnosis (SRDD) model. SRDD aims to tackle the performance degradation of classification models in low-data regimes, as well as their inability to distinguish unknown diseases. Specifically, we propose a Semantic-guided Saliency Discovery (SSD) module to explore semantic saliency information within images by aligning image regions with the semantic knowledge embedded within category labels. The image content is subsequently decomposed into semantically related information (SRI) and image instance template (IIT). Then, we design a Reciprocal samples Synthetic Strategy (RSS) to create known and unknown reciprocal points using SRI and IIT. This facilitates a compact feature space for known classes while preserving space for unknown data, promoting accurate known disease diagnosis and unknown disease detection. We validate SRDD on the public skin disease dataset SD-260. SRDD achieves state-of-the-art performance in both known disease classification and unknown disease identification.",Degradation;Deep learning;Computational modeling;Biological system modeling;Semantics;Feature extraction;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385449,IEEE Conferences,,,,,,
Lung Cancer Detection via Federated Learning,L. Caroprese; T. Ruga; E. Vocaturo; E. Zumpano,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"Lung cancer is one of the most common cancers worldwide. In 2020, there were an estimated 2.2 million new cases of lung cancer. It is often diagnosed at an advanced stage because in the early stages, it does not present particularly abnormal symptoms, such as cough and chest pain, which are often underestimated. Typically, it is detected through pronounced symptoms that lead to more in-depth diagnostic examinations or through other routine tests that reveal its presence almost by chance. The tools used for detection include chest X-rays, CT scans, and PET scans. Analyzing these allows the detection of abnormal formations in the early stages. From this perspective, artificial intelligence has set the goal, in recent years, of providing tools for image analysis capable of automatically, rigorously, and above all, promptly providing an initial classification of the detected formations. In particular, in the literature, it is possible to find various solutions that make use of Federated Learning. This method involves training deep learning models on expansive datasets distributed across multiple data centers. Significantly, it safeguards privacy by obviating the need to transmit sensitive patient data. The objective of this paper is to examine existing state-of-the-art solutions, emphasizing workflows and essential strategies. It considers the datasets employed and architectural decisions. The analysis extends to common issues encountered in the studied works, with a specific focus on challenges typical of the medical domain. The paper concludes by exploring potential future solutions to address these issues.",Training;Deep learning;Data privacy;Image analysis;Federated learning;Pain;Lung cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385806,IEEE Conferences,,,,,,
Using Deep Learning to differentiate between Parkinson's Disease and Multiple System Atrophy based on PET and MRI images,H. Tan; B. Luo; C. Cong,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"Accurate early diagnosis of Parkinson's disease (PD) is challenging due to symptom overlap with atypical parkinsonian syndromes, such as multiple system atrophy (MSA), underscoring the importance of developing methods for precise differentiation to guide timely therapeutic interventions and enhance patient survival. In this study, a deep learning approach was employed to differentiate between PD and MSA based on MR images and PET/CT images. A total of 153 patients (119 with PD and 34 with MSA) were included and five-fold cross-validation was employed for evaluation. Image analysis in the experiment included co-registration of MRI and PET images, automatically segmenting the putamen and caudate nucleus regions in MRI images using the nnU-Net network. Subsequently, the regions of interest were extracted from the PET images based on the segmented MRI images, and then trained with Convolutional Neural Networks (CNNs) for classification. In the experiment, the nnU-Net achieved a Dice score of 84.92% in the task of putamen and caudate nucleus region segmentation. Furthermore, the CNNs could perfectly distinguish PD from MSA (accuracy = 89.71%, AUC = 0.87). In conclusion, a method for classifying PD and MSA based on MR and PET images was successfully developed, and significant potential was demonstrated in clinical applications.",Deep learning;Atrophy;Image segmentation;Parkinson's disease;Magnetic resonance imaging;Convolutional neural networks;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385520,IEEE Conferences,,,,,,
Spinal Lesions Classification and Localization with ACAT-Net from X-ray Images,H. Liu; X. Mai; J. Han; S. Zhu; J. Nie; W. Wan; P. Zhang; S. Wang; W. Yu; X. Tang; C. Xue; C. Yang; Q. Feng; Y. Chen,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"X-ray images play an important role in the diagnosis of spinal diseases because of their convenient collection and easy observation. But it is time-consuming and challenging for radiologists to examine the differences between the vertebrae to diagnose abnormalities and locate lesions. Many existing methods try to extract the global features of radiographs and do not make full use of adjacent vertebrae variations. In this paper, we propose a novel Axial-aware neural network with Consecutive Attention Transformer (CAT), namely ACAT-Net, which takes advantage of the convolutional neural network and transformer as a new deep learning framework. A deep convolutional network extracts features of anteroposterior and lateral X-ray images that may have abnormalities in them. The consecutive attention transformer block is then used to focus on the morphological differences of axial adjacent vertebrae on the spines. The ingenious structure we designed can significantly reduce the amount of network parameters. Extensive experiments on clinical and public datasets show that our method is remarkably superior to other existing approaches in the spine X-ray image analysis.",Location awareness;Image analysis;Spine;Neural networks;Network architecture;Transformers;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385629,IEEE Conferences,,,,,,
Multitask learning approach for lung nodule segmentation and classification in CT images,L. Fernandes; H. P. Oliveira,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"Amongst the different types of cancer, lung cancer is the one with the highest mortality rate and consequently, there is an urgent need to develop early detection methods to improve the survival probabilities of the patients. Due to the millions of deaths that are caused annually by cancer, there is large interest int the scientific community to developed deep learning models that can be employed in computer aided diagnostic tools.Currently, in the literature, there are several works in the Radiomics field that try to develop new solutions by employing learning models for lung nodule classification. However, in these types of application, it is usually required to extract the lung nodule from the input images, while using a segmentation mask made by a radiologist. This means that in a clinical scenario, to be able to employ the developed learning models, it is required first to manually segment the lung nodule. Considering the fact that several patients are attended daily in the hospital with suspicion of lung cancer, the segmentation of each lung nodule would become a tiresome task. Furthermore, the available algorithms for automatic lung nodule segmentation are not efficient enough to be used in a real application.In response to the current limitations of the state of the art, the proposed work attempts to evaluate a multitasking approach where both the segmentation and the classification task are executed in parallel. As a baseline, we also study a sequential approach where first we employ DL models to segment the lung nodule, corp the lung nodule from the input image and then finally, we classify the cropped nodule. Our results show that the multitasking approach is better than to sequentially execute the segmentation and classification task for lung nodule classification. For instances, while the multitasking approach was able to achieve an AUC of 84.49% in the classification task, the sequential approach was only able to achieve an AUC of 72.43%. These results show that the proposed multitasking approach can become a viable alternative to the classification and segmentation of lung nodules.",Deep learning;Image segmentation;Hospitals;Computational modeling;Computed tomography;Lung;Lung cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385868,IEEE Conferences,,,,,,
PFCA-Net: a post-fusion based cross-attention model for predicting PCa Gleason Group using multiparametric MRI,C. Xinyu; J. Yan; F. Yin; W. Peiyan; S. Wenbo; X. Hanshuo; W. Xinglong; X. Guoping,2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),18-Jan-24,2023,"Prostate cancer (PCa) is a malignancy originating from epithelial cells within the prostate gland. The gold standard for diagnosing PCa is typically based on the Gleason score. However, the inherent variability in biopsy sampling and its potential discordance with radical prostatectomy outcomes can result in the misclassification of the International Society of Urological Pathology (ISUP) Gleason Group (GG). Furthermore, the employment of prostate-specific antigen (PSA) for screening, guiding biopsy decisions, and the reduction in PSA biopsy thresholds has led to a notable increase in unwarranted biopsies among patients with PCa. Consequently, developing an efficient and accurate method to predict ISUP GG is imperative. Currently, most studies focus on ISUP GG binary classification for specific GG. In this study, we leverage multiparametric Magnetic Resonance Imaging (mpMRI) images, encompassing T2-weighted imaging (T2WI), diffusion-weighted imaging (DWI), and apparent diffusion coefficient (ADC), to introduce a post-fusion based cross-attention model named PFCA-Net. The model is designed to predict ISUP GG categories, specifically GG 0/1, GG 2, GG 3, and GG 4/5. We validate our approach using three distinct deep learning classification models and a mpMRI pre-fusion classification model. Our proposed approach demonstrates exceptional performance, attaining an accuracy (ACC) of 0.9692 and an area under the curve (AUC) of 0.9986, over a dataset comprising 107 PCa patients and encompassing a total of 927 MRI images. This study emphasizes the practical value of mpMRI in distinguishing various ISUP GG categories and expediting the evaluation of PCa GG by clinicians.",Deep learning;Pathology;Magnetic resonance imaging;Biopsy;Employment;Glands;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385606,IEEE Conferences,,,,,,
A Novel Attention-based Explainable Deep Learning Framework Towards Medical Image Classification,G. W. Muoka; D. Yi; C. C. Ukwuoma; M. D. Martin; A. A. Aydin; M. A. Al-Antari,2023 7th International Symposium on Innovative Approaches in Smart Technologies (ISAS),17-Jan-24,2023,"Deep learning applications for medical image classification have shown remarkable promise, particularly incorporating attention-based neural networks. This is particularly relevant in medical imaging, where the integration of Artificial Intelligence assists with various imaging tasks, including classification, segmentation, and detection. Deep learning is revolutionizing medical research and playing a significant role in advancing personalized clinical treatment. However, the lack of interpretability in these models presents a significant obstacle to their adoption in clinical practice. Therefore, there is a growing need for a comprehensive understanding of artificial intelligence systems and their internal mechanisms, capabilities, and limitations, which is the focus of the field of explainable AI. This study proposes a novel attention-based explainable deep learning framework for medical image classification tasks, including Covid-19, breast cancer (BreakHis), lung cancer (LC2500), and Retinal optical coherence tomography (OCT). The proposed framework recorded overall accuracies of 98% (Covid-19 Radiography), 95% (BreakHis), 99.8% (LC2500), and 95%(OCT). For visual analysis of the outcomes, we employ and use the LIME, SHAP, and ELI-5 to analyze the achieved results. The studyâ€™s primary goal is to bridge the gap between the high performance achieved by attention-based models and the necessity for transparency and interpretability in medical image diagnostics.",Deep learning;COVID-19;Analytical models;Lung cancer;Predictive models;Retina;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391289,IEEE Conferences,,,,,,
Artificial Intelligence Computer-Aided Diagnosis to automatically predict the Pediatric Wrist Trauma using Medical X-ray Images,E. M. Erzen; E. BÃœtÃœn; M. A. Al-Antari; R. A. A. Saleh; D. Addo,2023 7th International Symposium on Innovative Approaches in Smart Technologies (ISAS),17-Jan-24,2023,"Pediatric wrist trauma (PWT) is a common injury that occurs in hospital emergency departments, with fractures being among the most frequent cases. The traditional diagnostic process for these injuries involves the collaboration of radiologists and surgeons. However, recent advancements in deep learning-based computer vision algorithms have shown potential in automating and expediting this diagnosis. In this paper, we propose an automatic end-to-end computer-aided diagnosis (CAD) system that can simultaneously detect and classify various types of pediatric wrist injuries, including bone anomalies, bone lesions, foreign bodies, fractures, metallic artifacts, periosteal reactions, pronator signs, soft tissue abnormalities, textual elements, and the axis. To train and evaluate the proposed CAD system, we utilize the public GRAZPEDWRI-DX dataset. The design of our CAD system involves careful steps such as medical X-ray data collection, data labeling, preprocessing, prediction AI model optimization, and model evaluation. The core component used for the detection and classification task in our system is YOLOv8, the advanced and state-of-the-art version of YOLO for object detection. Experimental results demonstrate that our proposed CAD system achieves promising evaluation results in terms of precision (77.80%), recall (54.60%), mAP@50 (59.10%), and mAP50@95 (37.20%). These encouraging evaluation results can serve as a future direction to provide practical solutions for improving the capabilities of the healthcare system in rapidly and accurately predicting pediatric wrist injuries.",Wrist;YOLO;Solid modeling;Computational modeling;Training data;Predictive models;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391582,IEEE Conferences,,,,,,
BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification,Y. Chen; F. Liu; H. Wang; C. Wang; Y. Liu; Y. Tian; G. Carneiro,2023 IEEE/CVF International Conference on Computer Vision (ICCV),15-Jan-24,2023,"Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-Ray (CXR) classifiers have been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current multi-class noisy-label learning methods cannot be easily adapted. In this paper, we propose a new method designed for noisy multi-label CXR learning, which detects and smoothly re-labels noisy samples from the dataset to be used in the training of common multi-label classifiers. The proposed method optimises a bag of multi-label descriptors (BoMD) to promote their similarity with the semantic descriptors produced by language models from multi-label image annotations. Our experiments on noisy multi-label training sets and clean testing sets show that our model has state-of-the-art accuracy and robustness in many CXR multi-label classification benchmarks, including a new benchmark that we propose to systematically assess noisy multi-label methods. Code is available at https://github.com/cyh-0/BoMD.",Training;Adaptation models;Semantics;Manuals;Benchmark testing;Radiology;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377332,IEEE Conferences,,,,,,
A Collaborative Fusion of Vision Transformers and Convolutional Neural Networks in Classifying Cervical Vertebrae Maturation Stages,S. F. Atici; H. Pan; M. H. Elnagar; V. Allareddy; R. Ansari; O. Suhaym; A. E. Cetin,"2023 30th IEEE International Conference on Electronics, Circuits and Systems (ICECS)",10-Jan-24,2023,"We present MultiMixer, an innovative deep-learning approach for automating the classification of Cervical Vertebrae Maturation (CVM) stages. Our method is a four-channel network combining three deep Convolutional Neural Networks (CNNs) and a Vision Transformer (ViT) in a parallel architecture. The combination leverages the CNNsâ€™ proficiency in extracting local features and ViTâ€™s ability to capture global dependencies. Each module is trained independently with distinct initialization parameters. Our approach utilizes a set of directional filters to highlight cervical vertebrae edges in X-ray images and the outputs of the directional filters are fed as input to the CNNs ResNet18, DenseNet, and PyramidNets. and the ViT. The subnetwork outputs are fused through a fully connected layer. For training, we annotated 1018 cephalometric radiographs, categorized by gender and classified by the CVM stages. We employed various training techniques, including image patches and adjustable directional edge enhancers, alongside data augmentation methods to mitigate overfitting. In our experiments, MultiMixer achieved an accuracy of 82.35% for female patients and 77.88% for male patients. Notably, our approach outperformed Vision Transformers and other previously studied network models when estimating the CVM stages within our dataset.",Training;Image edge detection;Transformers;Feature extraction;Parallel architectures;Convolutional neural networks;Integrated circuit modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10382871,IEEE Conferences,,,,,,
Optimizing CNNs for Facial Paralysis Detection: A Hyperparameter Tuning Approach,S. N. Himawan; A. Suheryadi; M. Mustamiin,2023 Eighth International Conference on Informatics and Computing (ICIC),09-Jan-24,2023,"Paralysis is still a major problem not only in Indonesia but also in the world. World Stroke Organization data shows the prevalence of stroke every year there are 13.7 million new cases of stroke, and around 5.5 million of them result in death. One of the symptoms of this disease is paralysis of the face where one side of the face will look lower and the person is unable to smile because the mouth or eyes appear to droop. This is due to damage to the nerves that carry signals from the brain to the facial muscles, or vice versa from the brain to the face. Early detection needs to be done to reduce the larger impact and action can be taken as soon as possible, thus accelerating healing. It is important to detect and classify the level of facial paralysis experienced by a person. In this study, we present a novel approach for the detection of facial paralysis using Convolutional Neural Networks (CNNs) and hyperparameter tuning techniques. The hyperparameter tuning process optimizes model performance by adjusting learning rates, number of dense layers, and size of dense layers. Our model has achieved its highest level of accuracy, reaching 87%. Our findings reveal the potential for accurate and early detection of facial paralysis, offering a promising tool for clinical assessment.",Systematics;Neurons;Mouth;Organizations;Facial muscles;Paralysis;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10382112,IEEE Conferences,,,,,,
An Enhanced Strategy of Detecting Neurological Disorders from Magnetic Resonance Images Using Deep Learning,S. Siddique; S. Shultana; S. Frizell,2023 IEEE Sixth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),08-Jan-24,2023,"In neuroimaging techniques, deep learning technologies are used to analyze brain functionalities and extract beneficial features. Specifically, magnetic resonance images (MRI) and computed tomography (CT) scans are utilized to classify neurological diseases with various static and deep learning approaches. This paper describes the architecture of the deep learning system for detecting neurological disorders from MRI images. The system has been implemented with multiple layers of deep convolutional neural networks. An optimization method, grey wolf optimization, is used for tuning the hyperparameters. Other existing models for medical image classification are compared with the system we designed, and our system outperforms all for this particular dataset. The system can successfully detect the six most common neurological disorders, including Cerebral Aneurysm, Alzheimer's disease, Parkinson's disease, brain stroke, and schizophrenia.",Deep learning;Neurological diseases;Parkinson's disease;Magnetic resonance imaging;Computed tomography;Transfer learning;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10381564,IEEE Conferences,,,,,,
Application of a hybrid EfficientNet-SVM model to medical image classification,I. Bouslihim; W. Cherif; M. Kissi,2023 14th International Conference on Intelligent Systems: Theories and Applications (SITA),03-Jan-24,2023,"Within the realm of medical science, imaging stands out as the most potent diagnostic and therapeutic tool. Convolutional Neural Networks (CNN) have revolutionized this field due to their ability to extract complex features and learn intricate patterns from raw image data. However, due to the complexity of medical image data and the growing demand for greater precision, CNNs struggle to operate at their best within realistic computing timeframes. In response to these challenges, the hybridization of CNN with classification algorithms such as SVM has ameliorated its performance. However, additional improvements are required to bring it into compliance with the requirements of medical image analysis. This study introduces an innovative methodology that leverages a contemporary metaheuristic algorithm to augment the efficiency and efficacy of CNN-SVM within medical image analysis tasks, employing an EfficientNet architecture. Brain Tumor Database (BTD); an MRI-associated dataset was used in this research. In the suggested approach, automatic feature extraction from images is done using CNN, and the final classification is done with Support Vector Machines (SVM). The evaluation of the suggested model entails two categories of statistical assessment metrics: Accuracy and F-measure. The overall accuracy achieved by the proposed model stands at 95.45% for the Brain Tumor Database (BTD).",Support vector machines;Databases;Computational modeling;Computer architecture;Brain modeling;Feature extraction;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10373755,IEEE Conferences,,,,,,
Calcified Tissue Detection using Deep Learning Models in Intravascular Ultrasound images,P. Arora; P. Singh; A. Girdhar,"2023 First International Conference on Advances in Electrical, Electronics and Computational Intelligence (ICAEECI)",03-Jan-24,2023,"Cardiovascular disease is the world's biggest cause of mortality. Heart disease is thought to have calcification as a significant contributing component. Currently, intravascular ultrasonography (IVUS) pictures are used to visually check for the presence of calcification by medical professionals. The goal of the study is to identify dense calcification using IVUS images taken at a 40MHz frequency. To detect these tissue frames, the features were extracted using deep learning models namely, AlexNet, GoogLeNet, SqueezeNet architectures. The IVUS pullbacks of four real patients were used in the tests. According to experimental findings, AlexNet, a deep learning architecture that has been trained, performs better than the other two models. The classification accuracy of the AlexNet (98.10%) model significantly outperforms that of two other previously trained models, GoogLeNet (89.93%) and SqueezeNet (92.80%). Future research into other feature selection techniques like ReliefF, PSO, ACO, and others, together with other models like Vision Transformers, could enhance the diagnostic process' overall accuracy.",Deep learning;Heart;Computational modeling;Ultrasonography;Computer architecture;Feature extraction;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10370885,IEEE Conferences,,,,,,
Improved U-Net Framework for Accurate Lumen Segmentation in Cross-Sectional Ultrasound Images of the Carotid Artery,P. Jonnala; S. R. Guntur,"2023 First International Conference on Advances in Electrical, Electronics and Computational Intelligence (ICAEECI)",03-Jan-24,2023,"Cardiovascular disease (CVD) is among the leading causes and a severe threat to human life of death. The presence of atherosclerotic plaque in the common carotid artery (CCA) is prevalent and has a poor prognosis. Lumen segmentation of the carotid artery in transversal ultrasound images alongside segmenting the different layers and boundaries of the carotid artery in the longitudinal view are considered to be critical in detecting atherosclerosis. Segmentation is performed to identify the lumen region in case of transversal carotid artery images and plaque region in case of longitudinal carotid artery ultrasound images, which allows for the early prediction of carotid artery disease known as atherosclerosis. The segmentation of the plaque using ultrasonic imaging is critical for aiding in the diagnosis and classification of CCA. In the case of transversal carotid artery ultrasound images, lumen boundaries, and carotid wall are taken into consideration to understand the plaque morphology. Deep learning techniques were used to evaluate carotid artery segmentation and atherosclerotic plaque. In this study, semantic segmentation of CCA transversal images is performed using the three types of Convolutional Neural Network (CNN) architectures, U-Net, SegNet and improved U-Net. To evaluate the proposed model, the architecture model is trained on 2165 transversal carotid artery ultrasound images. The presented model has undergone assessment utilizing the ADAM optimizer, achieving a Dice coefficient of 87.03% and a Jaccard Index of 79.92%. Various existing models performed using different metrics for a wide range of hyperparameter values are compared with the proposed model. This system aids the development and evaluation of carotid artery segmentation and atherosclerotic plaque by segmenting the lumen region in the first place by implementing deep learning techniques.",Deep learning;Image segmentation;Ultrasonic imaging;Semantic segmentation;Atherosclerosis;Lumen;Stroke (medical condition),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10370939,IEEE Conferences,,,,,,
Deep Learning Methods for Segmenting and Classifying Diabetic Retinopathy,D. Nagasudha; N. Senthamarai,"2023 First International Conference on Advances in Electrical, Electronics and Computational Intelligence (ICAEECI)",03-Jan-24,2023,"The most common form of diabetic retinopathy, or DR, which impairs vision on the retina of the eye, is diabetes. This causes blindness if not detected at the early stage because DR is not an irreversible process. This disease occurs between age group of 24-75 years. When timely treatement is given approximately we can save 90% people from their vision loss. Detection of the disease becomes difficult as there are only few symptoms so early diagnosis is required to avoid vision loss. Detection and classification of the disease is a tedious task. In the Existing systems the accuracy of the disease is calculated using only small dataset. In the proposed system we will take a large dataset where patients family medical history, diet, low and high quality images will be included. The approach also adds some extra preprocessing techniques like artifact removal, Data augmentation, and feature extraction for classification of the disease. Architectures such as ResNet, InceptionNet, or DenseNet have shown promising results in DR prediction tasks. Ensemble methods involve combining multiple models to make predictions, which often leads to improved accuracy. The bigger the dataset we take the higher the accuracy rate we achieve. The suggested methods build on existing research by utilizing deep learning algorithms and by substituting auto encoders for the dimensionality reduction PCA methodology in order to improve accuracy. Finding enough labeled data for deep learning models to train on might be difficult in the case of diabetic retinopathy. Training and deploying deep learning models require significant computational power and resources, which can be expensive and impractical for some medical facilities.",Deep learning;Diabetic retinopathy;Computational modeling;Blindness;Prediction algorithms;Data models;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10370922,IEEE Conferences,,,,,,
An Experiment to Develop an Enhanced Medical Image Security by using Deep Learning Assisted Crypto Policy,R. G; V. Sujatha; B. Raja; M. TamilSelvi; A. Balaji; A. K. Kumar,"2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",03-Jan-24,2023,"Ensuring a high level of safety and security is imperative when transmitting medical images via open access channels. Medical images hold immense importance in various applications, particularly in real-time contexts like telemedicine. Due to the unique characteristics of medical imaging data, determining the appropriate techniques for safeguarding confidential images against unauthorized access becomes challenging. Existing encryption methods primarily cater to textual data, leaving a gap in effectively securing multimedia data such as images. Using DNN leaves these systems open to attack from adversarial samples, which are pictures with slight changes that can trick the model into producing inaccurate predictions. Utilizing the MNIST database, this research demonstrates that Deep Learning algorithms may be used to encrypted data without compromising accuracy. This finding holds significance as it offers a potential solution for preserving data security while benefiting from advanced machine learning capabilities. In addition, the research goes farther by training a model using encrypted information to categories X-ray coronary angiography images, making it a more difficult scenario. The suggested DNN model's remarkable accuracy score of 0.983 highlights its ability to make predictions with an exceptional level of correctness.",Deep learning;Training;Open Access;Telemedicine;Learning (artificial intelligence);Predictive models;Prediction algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10369813,IEEE Conferences,,,,,,
A Novel Approach for Pathology Detection using CNN based Image Registration Techniques,S. S. Pandi; V. R. Chiranjeevi; B. Kalpana; J. Preethi,"2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",03-Jan-24,2023,"Automatic detection of pathology in images can help in reducing the workload of pathologists and speed up the diagnosis. Advancements in medical imaging technologies have enabled high-quality visualization of tissue structures for anatomical and pathological examinations. This paper is aimed at developing fast and reliable systems for pathology localization towards improving diagnostic accuracy. To develop a robust and accurate image registration algorithm for pathology localization in MRI brain images. A two-stage unsupervised end-end deep learning model called Rigid Transform and B-spline based Convolutional Neural Network (RBCNN ) is proposed for registration of MRI images for brain tumor localization. This paper exploits the rigid transform in the first stage, to extract the rigid transform parameters for pre-registration, resulting in a coarse alignment of the images. Then the B-spline transform is used in the second stage to align the image further to obtain the optimal alignment. This paper exhibits promising results with low computation time and robustness compared to state-of-the-art methods. RBCNN can be integrated with existing image segmentation algorithms to segment the tumors. Further, it can be used as a backbone in tumor detection networks.",Location awareness;Deep learning;Pathology;Image registration;Magnetic resonance imaging;Transforms;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10368886,IEEE Conferences,,,,,,
A survey on deep Learning's Effectiveness in Detecting Brain Tumors,P. J. Devi; C. P. D. Cyril; C. M. Babu,"2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",03-Jan-24,2023,"Recognizing brain tumors is a crucial part of medical imaging. Deep learning algorithms have become effective tools for examining medical images. The present article seeks to give a thorough summary of current developments in deep learning-based brain tumor diagnosis. The investigation starts out by outlining the basic ideas of brain tumour identification and emphasizing its importance in tumor field of medicine. The survey emphasizes the necessity for larger and more varied datasets, the efficiency of automatic feature learning, as well as the significance of pre-processing and data augmentation in enhancing model performance. This investigation draws to a close with recommendations for future research areas and a focus on how deep learning has the ability to revolutionise the way that brain tumour detection is done while also enhancing patient outcomes.",Deep learning;Surveys;Representation learning;Training data;Medical services;Brain modeling;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10369684,IEEE Conferences,,,,,,
Cardiac Arrhythmia Class Segmentation Using the Deep 2D CNN Technique,J. D; A. Kamuganti; D. N. Gorle; A. Eragadinla; K. P. Gundu,"2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",03-Jan-24,2023,"In this article, we gave a strong technique to arrhythmia class division on electrocardiograms (ECGs) that utilizes profound two-layered convolutional brain organizations (CNNs), which have of late exhibited uncommon execution in the space of item identification and example recognization. As input for the CNN model, each ECG beat was converted into a two-dimensional grayscale image. Numerous deep-learning techniques are used to optimize the proposed CNN semantic segmentation model. Convolutional neural network (CNN)--based deep learning techniques have been effective in resolving a variety of issues in medical imaging, including image segmentation. Additionally, we contrasted our suggested model with AlexNet and VGGNet, two well-known CNN models. The evaluation employed the MIT-BIH arrhythmia database. At the evaluation, 10-fold cross-validation was carried out with each ECG recording serving as test data to precisely validate our CNN model. Our experimental results have effectively supported the claim that high accuracy may be reached without manually preprocessing the ECG signals with noise filtering, feature extraction, or feature reduction. This was done using the converted ECG images and the suggested CNN model.",Arrhythmia;Semantic segmentation;Organizations;Learning (artificial intelligence);Electrocardiography;Feature extraction;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10369974,IEEE Conferences,,,,,,
Fine-Tuned EfficientNetB0 Model for Brain Tumor Multiclassification,R. Pillai; N. Sharma; R. Gupta,"2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",03-Jan-24,2023,"Brain tumors are caused mainly by DNA abnormalities, which result in aberrant cell division, prolonged cell lifespans, and the creation of aggregative masses inside the brain. Such tumors prevent the brain from operating at its best, sometimes resulting in long-term problems. These tumors' genesis (primary or secondary) and malignancy (benign or malignant) are used to classify them. While some might not be immediately lethal, their growth or contact with healthy brain tissue might cause damage. Investigations into various approaches, such as conventional medical imaging and artificial intelligence (AI), mainly using deep learning (DL) and transfer learning (TL), is prompted by efforts to identify and categorize brain tumors. Implementing the EfficientNetB0 model, which is famous for its adaptability and efficacy, is an excellent demonstration of this strategy. This work uses a well-curated dataset of 3264 photos from various classes to closely analyze the use of the EfficientNetB0 TL model to identify and classify brain tumors from MRI scans. The model's adaptability and accuracy are meticulously examined using a dual-pronged strategy incorporating transfer learning and fine-tuning, yielding an astounding accuracy of 98.5%. Accuracy and loss charts illustrate the model's learning development in vivid detail, and a well-defined confusion matrix provides extensive insights into categorization effectiveness across various tumor classifications. The achievement of a 98.5% accuracy rate highlights the potential paradigm change made possible by AI and TL in the diagnosis and classification of brain tumors. This accomplishment confirms the efficacy of such sophisticated procedures and promises to speed up diagnosis, enable prompt therapies, and improve patient prognoses. The combination of AI and TL heralds a new era of precision medicine, ready to reshape the field of brain tumor care as research in this area grows.",Adaptation models;Technological innovation;Magnetic resonance imaging;Transfer learning;DNA;Transforms;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10369755,IEEE Conferences,,,,,,
Transfer Learning for Accurate Classification of Breast Cancer in Medical Imaging,R. Sangeetha; R. P. Shukla; S. Vats; P. Vishwakarma; J. Logeshwaran,"2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",03-Jan-24,2023,"Transfer learning has recently been developed as a powerful technique for accurate classification of medical images. It is predominantly used in deep learning models to facilitate training of models on small data sets. It is based on the process of leveraging the knowledge gained from prior related tasks and transferring it to a new task. This technique can be used to improve the accuracy of classification models trained on medical images, specifically for the classification of breast cancer. Such models are able to provide an improved accuracy of cancer classification compared with those trained in a standard fashion. Additionally, transfer learning models demonstrate the ability to increase computational efficiency, reduce over fitting, and construct useful representations from data with fewer annotations. This technique is particularly useful for medical imaging due to the expense and difficulty in acquiring large annotated datasets for training purposes. This paper explores the use of transfer learning for accurate classification of breast cancer in medical imaging, and its potential applications in the diagnosis of this disease..",Training;Deep learning;Computational modeling;Transfer learning;Breast cancer;Data models;Knowledge management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10368665,IEEE Conferences,,,,,,
Automatic Classification of Alzheimer's Using Brain MRI Data and the ResNet152V2 Architecture,P. Shourie; V. Anand; S. Gupta,"2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",03-Jan-24,2023,"Alzheimer's disease is a neurological condition that typically affects elderly people and causes memory loss and cognitive deterioration. Effective intervention and therapy for Alzheimer's depend on an early and correct diagnosis. In this article, the model presents a ResNet152V2 architecture-based deep learning method for the cataloging of Alzheimer's disease using brain MRI data. The classification model is built on the ResNet152V2 architecture, which is renowned for its depth and potent feature extraction capabilities. The architecture is a good contender for identifying small brain anomalies suggestive of Alzheimer's disease since it can extract complicated patterns and characteristics from medical imaging. A collection of brain MRI images from healthy people and those with Alzheimer's disease is being pre-processed for investigation. To increase the robustness and precision of the model, the dataset has been expanded and normalized. The findings highlight how important deep learning is for supporting early diagnosis and intervention, providing a noninvasive and effective way to identify those who are at risk of developing Alzheimer's disease.",Deep learning;Magnetic resonance imaging;Medical treatment;Feature extraction;Brain modeling;Data models;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10369054,IEEE Conferences,,,,,,
Comparative Assessment of deep learning methods for Prediction of Uterine Fibroid,B. Shamini P; B. Jaison,"2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",03-Jan-24,2023,"Today, convolutional networks, a type of deep learning algorithm, are frequently used in medical science. Automatic disease detection, in particular, is of great importance in the medical field. For the detection and classification of tumors, it is necessary to segment the affected region and the tumor. In general, medical imaging techniques are used to diagnose the tumor, such as magnetic resonance imaging (MR), ultrasound (US) and computed tomography (CT). Our study surveysdeep learning's applications for image classification, object detection, segmentation. Accurate segmentation of the affected region, tumors, and spine from MR images remains challenging, however it is still challenging to achieve because of 1) Wide differences in the size and shape of tumors amongst individuals; 2) the lack of contrast between tissues and organs that are next to each other; and 3) the undetermined number of tumors. Neural networks are able to capture the region and tumors [2]. The survey concludes with current findings and an analysis of open research challenges and directions.",Deep learning;Image segmentation;Ultrasonic imaging;Three-dimensional displays;Computed tomography;Convolutional neural networks;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10368805,IEEE Conferences,,,,,,
A Novel Approach to Detect COVID using DenseNet Architecture,S. S. Pandi; K. Deepak Kumar; A. Senthilselvi; D. R. Ramani,"2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",03-Jan-24,2023,"Evolution of machine learning and deep learning approaches, and novel anatomical and functional imaging modalities have resulted in several computer-aided diagnosis and detection systems. These systems are centered around pathology localization, detection and classification of abnormalities. Conventional medical image analysis approaches such as classification and segmentation are tailored to the problem of detecting or classifying abnormalities in pathology images. Machine learning and deep learning models are trained on a large number of pathology images in the form of labeled image datasets, so that they generalize well with unseen data. A binary classifier model which is trained on CT images which are quality enhanced with optimal filters is proposed for COVID19 detection from chest images. This paper is based on DenseNet architecture featuring dense connections, to propagate and concatenate feature maps across dense layers for collective learning. This paper captures diverse manifestations of COVID19 specific infections from CT images, improving the detection accuracy. Further, explainable analysis of this paper provides insights on the morphologies of the infection patterns for extensive analysis.",Location awareness;COVID-19;Deep learning;Pathology;Image segmentation;Computed tomography;Computational modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10368717,IEEE Conferences,,,,,,
Application of CT Radiomics Method Based on Machine Learning Classifier in Hepatic Vascular Bifurcation Detection,H. Chang; Z. Diao; Z. Wang; Q. Qi,"2023 16th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",02-Jan-24,2023,"Hepatic vascular tumors tend to occur at the bifurcation points of hepatic vessels, where the rupture of the vessel wallsâ€™ weaker sections can lead to life-threatening bleeding. As a result, quick and accurate detection of hepatic vascular bifurcations is essential to aid physicians in diagnosing hepatic vascular tumors and creating surgical plans. However, current bifurcation detection techniques face challenges in clinical detection, low detection efficiency, compromised detection accuracy, and insufficient data security due to individual factors such as vascular deformation and data noise. This study aims to address the limitations of current bifurcation detection techniques by combining radiomics feature engineering technology and existing machine learning methods to propose a classification pipeline that is suitable for hepatic vascular bifurcation detection. Multiple feature selection algorithms are employed to obtain optimal features, which are then used to evaluate the performance of different models in bifurcation detection. Experimental results demonstrate that the K-Nearest Neighbors (KNN) classifier achieves the best performance in hepatic vascular bifurcation detection, with the test setâ€™s average accuracy, precision, recall, F1-score, and area under the curve (AUC) being 0.964, 0.942, 0.968, 0.954, and 0.988, respectively. These findings indicate that the proposed pipeline has significant practical value in the early diagnosis of hepatic vascular diseases.",Image segmentation;Pipelines;Surgery;Signal processing algorithms;Machine learning;Bifurcation;Signal processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10373273,IEEE Conferences,,,,,,
Image Visibility Graph Analysis of the Eye Disease Images,D. Wu; Z. Ma; X. Yang; G. Jiang; C. Yan; L. Ma,"2023 16th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",02-Jan-24,2023,"Visibility graph algorithm is a widely used time series complex network transformation algorithm. It has been mainly applied to one-dimensional or multidimensional time series, while many real-life objects, such as images, cannot be represented by sequences. In this paper, we focus on two-dimensional images and propose a novel method of image visibility graph network (IVGN) with redefined node-degree features. We use the staining images of corneal ulcer of eye as the research object and demonstrate the application of IVGN to medical images. Our method consists of the following steps: (1) We construct a novel IVGN by treating each pixel as a node and connecting the visible pixels. We compute the average node degree of the IVGN for ocular pathology image analysis. (2) Based on the IVGN, we extract the node-degree features of the visible images and map them to a new degree topology, which is used for image enhancement. (3) By doing so, we build a gray co-occurrence matrix based on the degree topology and extract its four parameters, showing the role of the degree topology in image texture extraction. (4) As a result, we use the proposed contrast-based descriptors to achieve effective detection of eye staining images with different categories and severity levels.",Pathology;Time series analysis;Signal processing algorithms;Surfaces;Signal processing;Feature extraction;Topology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10373331,IEEE Conferences,,,,,,
Convolutional Neural Networks for Nail Disease Detection: A Promising Approach in Dermatology,A. Ajmal; W. Ahmad; S. M. Adnan,2023 18th International Conference on Emerging Technologies (ICET),01-Jan-24,2023,"The aim of this study is to use Deep Learning (DL) techniques to classify and detect human nail diseases. Early and reliable detection is crucial in aiding timely interventions and suitable treatment for nail diseases and their profound impact on a person's well-being. Using a diverse dataset of nail images, a CNN model was developed and trained to achieve the study's objectives. For the model to be able to accurately detect and classify different diseases, the dataset was carefully collected to include various types of nail diseases. In order to improve the model's performance and robustness, the nail images were preprocessed, and data augmentation techniques were applied. The model's assessment encompassed 8 distinct nail diseases, resulting in an impressive accuracy rate of 98.44%. Additional evaluation metrics such as precision, recall, and F1-score were also computed, yielding values of 99.22%, 98.44%, and 99.02% respectively. The achieved outcomes were compared with state-of-the-art (SOTA) techniques, affirming the superiority of the proposed model. This study emphasizes the potential benefits of DL techniques in enhancing healthcare practices, enhancing dermatological diagnostics, and improving the overall well-being of patients suffering from nail diseases.",Deep learning;Computational modeling;Nails;Sociology;Medical services;Data augmentation;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10374663,IEEE Conferences,,,,,,
Breast Lesion Classification using Radiomics-derived Regions of Interest on Mammograms,S. A. Gujar; X. Lei; S. Y. Cen; D. H. Hwang; B. Varghese,2023 19th International Symposium on Medical Information Processing and Analysis (SIPAIM),01-Jan-24,2023,"The objective of the study is to design and validate an end-to-end workflow that combines automatic segmentation and CNN Transfer Learning-based classification for breast cancer diagnosis using mammograms. A radiomics-based skewness parametric map was used to isolate regions of interest (ROI) on the mammogram containing the breast lesion. Subsequently, a CNN model, ResNet50 and AlexNet, was implemented to extract spatial-level features from these ROIs and classify them as malignant or benign. The CBIS-DDSM dataset comprising 1580 mammograms was used for training, testing, and validating the breast cancer prediction models. ResNet50 achieved the best performance of classification testing accuracy of 0.72 and an Area Under Curve (AUC) = 0.71, 95% CI: 0.66-0.75, while AlexNet achieved a testing accuracy of 0.64 and an AUC of 0.61. When comparing the performance of our classification models to those of human-annotated ROIs, our results are similar to or better than those reported in the literature. We have demonstrated an end-to-end automatic solution for Breast cancer detection and classification using mammographic data.",Training;Neural networks;Predictive models;Feature extraction;Mammography;Breast cancer;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10373475,IEEE Conferences,,,,,,
An Ensemble Approach for Uterine Pathology Classification in MRI Imaging Using Deep Learning,T. G. Singh; B. Karthik; M. Wahengbam,2023 International Conference on Sustainable Communication Networks and Application (ICSCNA),01-Jan-24,2023,"Early detection and efficient treatment of a variety of gynecological disorders depend critically on the categorization of uterine pathology from Magnetic Resonance imaging (MRI). It dives into deep learning to suggest an ensemble method for improved uterine disease classification that taps into the combined power of many models. It intends to enhance accuracy, reduce false positives, and strengthen the generalization capabilities of the classification system by integrating the strengths of convolutional neural networks (CNN), recurrent neural networks (RNN), and a transfer learning model (ResNet). Combine the predictions from each of these distinct models using a variety of ensemble approaches, such as majority voting, stacking, and boosting. Its effectiveness was assessed utilizing performance assessment measures such as accuracy, precision, recall and F1-Score. The outcomes showed that ensemble strategies, notably stacking, outperformed individual models in several ways. The ensemble technique has great promise for developing the area of medical imaging and might revolutionize the detection of uterine disease by improving accuracy and dependability. To establish the practical application of the technology, more validation studies and clinical evaluations are necessary.",Deep learning;Pathology;Recurrent neural networks;Magnetic resonance imaging;Stacking;Medical services;Ensemble learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10370073,IEEE Conferences,,,,,,
Lung Cancer Diagnosis and Classification Using Hybrid Neural Network Techniques,M. Yasmin; A. Andrew Roobert,2023 International Conference on Sustainable Communication Networks and Application (ICSCNA),01-Jan-24,2023,"Lung cancer remains a formidable global health challenge, with early and accurate diagnosis being a critical determinant of patient outcomes. In response to this pressing need, this research presents a novel approach for lung cancer diagnosis and classification utilizing hybrid neural network techniques. By harnessing the synergistic power of convolutional neural networks (CNNs) and recurrent neural networks (RNNs), we have developed an integrated model capable of effectively processing diverse data sources to enhance diagnostic precision. The first component, CNNs, is employed for feature extraction from medical imaging data, such as X-ray images and computed tomography (CT) scans. These networks excel in capturing intricate patterns and subtle abnormalities in medical images, contributing to the early detection of lung cancer. The second component, RNNs, leverages sequential patient data, including clinical histories, genetic information, and longitudinal records, providing valuable contextual information for comprehensive analysis. The hybrid neural network model facilitates a holistic approach to lung cancer diagnosis and classification, as it unifies the strengths of both data-driven imaging analysis and sequential data interpretation. Experimental evaluations of this approach reveal its potential to achieve superior accuracy, outperforming traditional methods, and thus allowing for timely and precise cancer diagnosis. This advancement holds significant promise in improving the prognosis and management of lung cancer patients, potentially leading to more personalized treatment strategies and ultimately reducing the burden of this devastating disease. As such, this research contributes to the ongoing efforts to enhance lung cancer diagnosis, making strides towards improved patient care and survival rates.",Deep learning;Recurrent neural networks;Lung cancer;Lung;Medical services;Data models;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10370424,IEEE Conferences,,,,,,
Enhanced Grey Wolf Algorithm with Improved Deep Belief Network for Chest X-Ray Image Classification Model,T. Kumar; R. Ponnusamy,2023 International Conference on Sustainable Communication Networks and Application (ICSCNA),01-Jan-24,2023,"Clinical images play a major part in the initial recognition of ailments. Chest Computer Tomography (CT) images using radiological models and X-rays proved to recognize the initial phases of pulmonary-based ailments. X-ray analysis is time taking and needs highly educated experts to indulge. However, the usage of Machine Learning (ML)-related techniques could foster efficiency, support medics in disease diagnosis, accelerate time to diagnosis, and reduce the burdened healthcare system. Deep Learning (DL) methods were used on fundus and Chest X-Ray (CXR) images for diagnosing diseases. In this study, a Modified Grey Wolf Optimizer with Bilinear Deep Belief Network (MGWO-BDBN) technique for Medical X-Ray Image Classification is presented. The presented MGWO-BDBN technique exploits the Median Filtering (MF) process for enhancing the medical X-ray image quality. Besides, feature extracting method, Inception v3 is employed to produce feature vectors. Moreover, the BDBN method is applied as a classification technique for allotting suitable class labels to the CXR images. Finally, the MGWO method is used as a hyperparameter tuning strategy for allotting suitable hyperparameter values to the BDBN model. The MGWO method is resulted by the utilization of the Levy flight concept with the traditional GWO method. The investigational assessment of the MGWO-BDBN model is examined by implementing a sequence of simulations and the outputs are examined under diverse factors. A relational study pointed out the augmented outputs of the MGWO-BDBN model exhibiting an outcome of 98.67% related to present models.",Image recognition;Sensitivity;Computational modeling;X-rays;Feature extraction;Reliability;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10370409,IEEE Conferences,,,,,,
Detection of Retinal Diseases Using Advanced Deep Learning Algorithms,N. Ashok; K. Gangadhara Rao,2023 International Conference on Sustainable Communication Networks and Application (ICSCNA),01-Jan-24,2023,"Retinal diseases are the most dangerous diseases that can cause vision loss if not detected early. Diagnosing retinal diseases accurately and timely to reduce permanent vision loss is essential. In this study, Fine-tuned Classification with transfer learning (FTC-TL) is introduced, which focuses on the classification of retinal diseases such as Age-Related Macular Degeneration (AMD), Diabetic Macular Edema (DME), Diabetic Retinopathy (DR), Drusen, and Choroidal Neovascularization (CNV) from the two essential datasets like Optical Coherence Tomography (OCT) and Fundus images. The study discusses the key challenges associated with the classification of retinal diseases, such as the need for extensive and diverse datasets, the interpretability of deep learning models, and the integration of clinical knowledge into the algorithms. The efficacy of various deep learning architectures, such as VGG19 with Recurrent Neural Networks (RNNs) and hybrid models, in disease classification, is investigated. The significance of data preprocessing techniques such as image augmentation and normalization is also discussed In addition, the challenges and potential solutions for dealing with imbalanced datasets, a common issue in medical image classification, are discussed A comparative analysis of cutting-edge deep learning models highlights their sensitivity, specificity, and accuracy performance for classification models. Furthermore, the incorporation of explainable AI techniques, which allow clinicians to understand and trust deep learning (DL) model predictions, is discussed",Deep learning;Analytical models;Diabetic retinopathy;Recurrent neural networks;Transfer learning;Retina;Prediction algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10370475,IEEE Conferences,,,,,,
Automating MRI-Based Ovarian Cancer Diagnosis with a DCNN*,A. Jenefa; V. Ebenezer; E. B. Edwin; J. J. Rajan; P. K. Stanley; M. R. Thanka,2023 International Conference on Sustainable Communication Networks and Application (ICSCNA),01-Jan-24,2023,"Early identification and prompt intervention in ovarian cancer cases can significantly enhance the prognosis for patients. In response to this need, we crafted an automated analysis tool for MRI scans through a deep learning methodology. We sourced MRI scans from a healthcare facility and segmented them into training, validation, and testing segments. We adopted a DCNN strategy for our deep learning model and honed it with the training data. The validation segment facilitated model fine-tuning, while the testing segment measured its precision. The model attained AUC-ROC values of 0.95 in validation and 0.94 in testing, showcasing its robustness. Additionally, the model exhibited a commendable sensitivity (0.92) and specificity (0.89) in the test data. These results point towards our deep learning tool's capability to identify ovarian cancer reliably in MRI scans, potentially elevating the precision and speed of ovarian cancer diagnosis. Nevertheless, more extensive evaluations with broader datasets and real-world clinical scenarios are essential.",Deep learning;Training;Visualization;Sensitivity;Magnetic resonance imaging;Training data;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10370390,IEEE Conferences,,,,,,
An Ensemble Deep Learning Approach for Enhanced Classification of Pituitary Tumors,S. D. Muhammad; Z. Kobti,2023 IEEE Symposium Series on Computational Intelligence (SSCI),01-Jan-24,2023,"Tumor detection has emerged as a significant aspect of neuro-oncology and neuroradiology, with critical importance in improving patient survival rates. Tumors, whether benign (non-cancerous) or malignant (cancerous), can result in severe morbidity, and their accurate detection is very important for treatment. In recent years, medical imaging modalities such as Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) have been extensively utilized for non-invasive tumor detection. These imaging techniques provide in-depth information about the tumor's location, size, and morphology, which is pivotal for diagnosing and planning therapeutic interventions. However, the manual interpretation of these imaging modalities is time-intensive and susceptible to human inaccuracies. Moreover, the subtle features of tumors can be easily missed in the manual assessment. Hereby, we propose an ensemble deep learning approach to classify pituitary tumors, based on the weighted average technique that incorporates three base deep learning models: ResNet 152, DenseNet 201, and VGG 16. Moreover, we implement the Segment Anything Model (SAM) to perform segmentation to our dataset and then execute the ensemble model to classify pituitary tumors from normal/healthy brain images. We compare our proposed approach using segmented data and non-segmented data, finding that the segmented data outperforms the non-segmented data by a margin of 1.77%.",Deep learning;Image segmentation;Magnetic resonance imaging;Computed tomography;Computational modeling;Manuals;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10371824,IEEE Conferences,,,,,,
Prediction of Spinal Abnormalities in Neuroradiology Images Applying Deep Transfer Learning,A. A. Andrews Interiano; M. Alejandro MartÃ­nez Palma; K. M. Reyes Leiva,2023 IEEE International Conference on Machine Learning and Applied Network Technologies (ICMLANT),01-Jan-24,2023,"The vertebral column is the structure that provides support, protection, and facilitates human mobility. There are vertebral column deformities such as scoliosis and spondylolisthesis that can cause discomfort in patients. Therefore, early detection of these deformities through radiological images is of great importance to prevent their progression. Nowadays, there are systems that use artificial intelligence to efficiently classify these images. In this case, deep transfer learning with pre-trained models was employed to classify radiographic images into three different classes (no scoliosis, scoliosis, and spondylolisthesis). The research followed a quantitative approach, had an exploratory scope, and an experimental design. The methodology implemented was iterative through four increments. The first increment focused on testing and fine-tuning the MobileNetV3-Small, MobileNetV3-Large, and EfficientNetV2B0 models. In the second increment, tests and fine-tuning were conducted on the EfficientNetV2B1, EfficientNetV2B2, and InceptionResNetV2 models. In the third increment, a database of medical images from Honduran patients was created. In the fourth, predictions were made using the best-performing models. As a result, it was found that in the first increment, an average accuracy of 97.01% was achieved, and in the second increment, an average accuracy of 98.01% was reached for the evaluated models. The creation of a local database for future research lines was consolidated, and the classification algorithm was validated through predictions of radiographic images of Honduras patients.",Scoliosis;Transfer learning;X-rays;Predictive models;Neurosurgery;Classification algorithms;Diagnostic radiography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10372991,IEEE Conferences,,,,,,
Improved Classification of Alzheimerâ€™s Disease With Convolutional Neural Networks,S. Muhammed; J. Upadhya; S. Poudel; M. Hasan; K. Donthula; J. Vargas; J. Ranganathan; K. Poudel,2023 IEEE Signal Processing in Medicine and Biology Symposium (SPMB),########,2023,"This work focuses on classifying MRI images using machine learning models to identify Alzheimerâ€™s disease (AD), the most common form of dementia, at an early stage [1]. It is now possible to identify and forecast the onset of AD by analyzing brain scans collected through Magnetic Resonance Imaging (MRI) and using artificial intelligence (AI) technologies, classifying patients as either at risk or not. The main goal is to make precise predictions. Improved prediction and detection tools for radiologists, physicians, and caregivers will be made available as a result of the studyâ€™s determination of the likelihood that individuals would develop AD and proper categorization of them. Using machine learning methods such as Convolutional Neural Network (CNN), Support Vector Machine (SVM), and fastaiâ€”a user-friendly deep learning library and frameworkâ€”we developed models using a dataset of 6400 MRI images from the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) 4 class for the early detection and classification of AD [2]. The models achieve an impressive accuracy rate of 84.4%, with the addition of a pre-trained Visual Geometry Group (VGG) layer to the sequential model, outperforming other algorithms. This study demonstrates a viable method for classifying and diagnosing Alzheimerâ€™s disease early on, utilizing MRI images and machine learning models. The acquired accuracy rates show the modelsâ€™ potential to assist doctors and other caregivers in AD diagnosis and management, thereby maximizing overall effectiveness.",Support vector machines;Machine learning algorithms;Magnetic resonance imaging;Computational modeling;Signal processing algorithms;Machine learning;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10372725,IEEE Conferences,,,,,,
SEFA: A Shared Encoder and Feature Adaptation Framework for Breast MRI Segmentation,H. Xue; K. Zhong; X. Wu; Y. Gao; Y. Wu; L. Zhang; G. Qian; P. Wang,2023 International Annual Conference on Complex Systems and Intelligent Science (CSIS-IAC),########,2023,"Significant progress has been made in developing accurate automatic segmentation systems for various biomedical applications using convolutional networks (CNNs). However, these systems often lose effectiveness when they encounter a domain shift caused by variations in imaging protocols. Traditional supervised transfer learning approaches are not ideal because manually annotating new data for every testing domain is impractical, while unsupervised domain adaptation is a challenging subject in the field of biomedical image analysis. In this paper, we introduce a new framework for unsupervised domain adaptation called Shared Encoder and Feature Adaptation (SEFA). This framework focuses on cross-sequences adaptation and is more resilient to disparities in input data and does not require any annotations on the test domain. Specifically, our segmenter model is based on a compact fully convolutional network designed for breast mask prediction. First, the shared encoder is designed to minimize the distribution disparity between the source and target domains. Then, a domain discriminator is set up to discriminate the feature space of both domains. Our model is optimized by using unpaired TSE/DCE sequences in an unsupervised manner. This eliminates the need for labeling additional medical datasets. Using our unsupervised approach, we achieve segmentation accuracies that are comparable to those obtained through supervised training. These accuracies are measured by metrics such as Case/Global Dice Score and average symmetric surface distance (ASD), which are close to the best possible performance.",Training;Image segmentation;Protocols;Magnetic resonance imaging;Transfer learning;Breast;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10363963,IEEE Conferences,,,,,,
Lung cancer detection and classification using CNN and image segmentation,D. Hrizi; K. Tbarki; S. Elasmi,2023 IEEE Tenth International Conference on Communications and Networking (ComNet),########,2023,"Lung cancer is one of the most prevalent cancers in the world. It is a particular illness that gets out of hand. and creates aberrant lung cell growth. Deoxyribonucleic acid (DNA) mutation caused by numerous genetic reasons causes these cells to behave differently from other normal cells. However, cancer-related mortality can be decreased with early diagnosis and treatment of patients. In return the use of convolutional neural networks (CNN) in the field of medical imaging diagnosis is widespread, however these networks have drawbacks, including slow training speed and poor diagnostic accuracy due to the size of the input matrix to the algorithm. This article proposes a CNN optimization method based on reducing the computational cost of the CNN. The process optimizes the initial CNN parameters due to a strong segmentation step that came before the classification step, which results in condensed regions at the start of CNN. The model was developed and applied to the analysis and diagnosis of medical imaging data for lung cancer on the basis of this optimization approach. Experimental results on 1190 images show that the suggested method performs more diagnostically than CNN when applied directly to pre-segmented images. The suggested method achieves a diagnostic accuracy of 97.06%, and it only takes 0.2481 seconds to diagnose 1190 digital tomography of the human head image datasets",Training;Image segmentation;Head;Computational modeling;Lung cancer;DNA;Tomography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10366739,IEEE Conferences,,,,,,
Chest X-Ray Feature Pyramid Sum Model with Diseased Area Data Augmentation Method,C. Kim; G. Kim; S. Yang; H. Kim; S. Lee; H. Cho,2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),########,2023,"Deep learning has shown considerable promise in medical image analysis, but significant challenges remain. These stem from the inherent complexities of medical images, such as varying sizes of lesions within the same image and the potential coexistence of multiple diseases. To address these issues, we propose a novel model combining TResNet with Feature Pyramid Network (FPN). This model adeptly handles multi-label classification, demonstrating robust performance across a range of lesion sizes. Furthermore, most medical images follow a long-tail distribution, presenting class imbalance problems, where the occurrence of one lesion often correlates with the presence of others. Considering these correlations, we introduced a strategy for dealing with the class imbalance issue by augmenting minority classes using bounding box information of the disease. Our proposed approach offers a novel solution for handling the unique challenges in deep learning-based medical image analysis, paving the way for more precise interpretations of complex medical images. The performance of mAP in 26 disease classes has been improved from 32.76% to 33.37% in a single model, and 35.11% in ensemble model.",Image analysis;Correlation;Data augmentation;Feature extraction;Data models;Lesions;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350374,IEEE Conferences,,,,,,
Advanced Augmentation and Ensemble Approaches for Classifying Long-Tailed Multi-Label Chest X-Rays,T. -H. Nguyen-Mau; T. -L. Huynh; T. -D. Le; H. -D. Nguyen; M. -T. Tran,2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),########,2023,"Chest radiography is a common medical diagnostic procedure, often resulting in a long-tailed distribution of clinical findings. This challenges standard deep learning methods, which tend to favor more common classes and might miss less frequent but equally important ""tail"" classes. Chest X-ray diagnoses represent a multi-label problem due to the potential for multiple simultaneous diseases in patients. In this paper, we propose straightforward yet highly effective techniques to address the long-tailed imbalance in chest X-ray datasets. We specifically utilize EfficientNetV2 and ConvNeXt as our primary architectures, allowing the image sizes to influence architectural decisions. To counter dataset imbalance, we employ various basic and advanced augmentations. Mosaic augmentation is applied, and we alter the method of obtaining the label to manage this multilabel classification problem. We leverage the Binary Focal Cross-Entropy loss function and deploy several ensemble strategies to boost performance. These include Stratified K-Fold cross-validation and Test Time Augmentation. Our proposed method demonstrated its effectiveness during the Development and Testing phases of the CXR-LT: MultiLabel Long-Tailed Classification on Chest X-Rays competition. Our approach yields substantial results with an mAP of 0.354, securing a position within the top five.",Uncertainty;Transfer learning;Tail;Self-supervised learning;Medical diagnosis;Task analysis;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350415,IEEE Conferences,,,,,,
Causality-Driven One-Shot Learning for Prostate Cancer Grading from MRI,G. Carloni; E. Pachetti; S. Colantonio,2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),########,2023,"In this paper, we present a novel method for the automatic classification of medical images that learns and leverages weak causal signals in the image. Our framework consists of a convolutional neural network backbone and a causality-extractor module which extracts cause-effect relationships between feature maps that can inform the model on the appearance of a feature in one place of the image, given the presence of another feature within some other place of the image. To evaluate the effectiveness of our approach in low-data scenarios, we train our causality-driven architecture in a One-shot learning scheme where we propose a new meta-learning procedure which entails meta-training and meta-testing tasks that are designed using related classes but at different levels of granularity. We conduct binary and multi-class classification experiments on a publicly available dataset of prostate MRI images. To validate the effectiveness of the proposed causality-driven module, we perform an ablation study and conduct qualitative assessments using class activation maps to highlight regions strongly influencing the networkâ€™s decision-making process. Our findings show that causal relationships among features play a crucial role in enhancing the modelâ€™s ability to discern relevant information and yielding more reliable and interpretable predictions. This would make it a promising approach for medical image classification tasks.",Metalearning;Magnetic resonance imaging;Decision making;Predictive models;Feature extraction;Reliability;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350430,IEEE Conferences,,,,,,
Using Large Text To Image Models with Structured Prompts for Skin Disease Identification: A Case Study,S. Rajapaksa; J. M. Uwabeza Vianney; R. Castro; F. Khalvati; S. Aich,2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),########,2023,"This paper investigates the potential usage of large text-to-image (LTI) models for the automated diagnosis of a few skin conditions with rarity or a severe lack of annotated datasets. As the input to the LTI model, we provide the targeted instantiation of a generic but succinct prompt structure designed upon careful observations of the conditional narratives from the standard medical textbooks. In this regard, we pave the path to utilizing accessible textbook descriptions for automated diagnosis of conditions with data scarcity through the lens of LTI models. Experiments show the efficacy of the proposed framework, including much better localization of the infected regions. Moreover, it has the immense possibility for generalization across the medical sub-domains to mitigate the data scarcity issue, and debias automated diagnostics from the all-pervasive racial biases.",Linear systems;Location awareness;Computer vision;Conferences;Skin;Data models;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350482,IEEE Conferences,,,,,,
Robust MSFM Learning Network for Classification and Weakly Supervised Localization,K. Kumar; B. Pailla; K. Tadepalli; S. Roy,2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),########,2023,"Robust classification and localization of bone fractures are beneficial to avoid misdiagnosis or underdiagnosis. However, state-of-the-art classification methods aim to improve accuracy which lacks reliability, and tackled localization problems in a supervised manner with much-annotated data that leads to high costs. In this paper, we propose a multistage feature map (MSFM) learning network to predict the class of the image and the area of interest without annotated bounded box. MSFM consists of three stages to predict the representation with different objectives and aims to improve the accuracy and reliability of classification. The weakly supervised MSFM model localizes the region of interest (ROI) by taking representation from all the stages supervised by image-level labels only. We also introduced a feature augmentation technique to enforce the model to consider other discriminative regions. End-to-end training of MSFM is performed jointly at all stages. Based on the comprehensive experiments, our approach achieves state-of-the-art results on the standard MURA dataset, which includes the elbow, finger, forearm, humerus, shoulder, wrist, hand, and bone tumor dataset. Code: github.com/MAXNORM8650/MSFM.",Location awareness;Wrist;Training;Computer network reliability;Computational modeling;Shoulder;Bones,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350827,IEEE Conferences,,,,,,
An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports,E. Dack; L. Brigato; M. McMurray; M. Fontanellaz; T. Frauenfelder; H. Hoppe; A. Exadaktylos; T. Geiser; M. Funke-Chambour; A. Christel; L. Ebner; S. Mougiakakou,2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),########,2023,"The pandemic resulted in vast repositories of unstructured data, including radiology reports, due to increased medical examinations. Previous research on automated diagnosis of COVID-19 primarily focuses on X-ray images, despite their lower precision compared to computed tomography (CT) scans. In this work, we leverage unstructured data from a hospital and harness the fine-grained details offered by CT scans to perform zero-shot multi-label classification based on contrastive visual language learning. In collaboration with human experts, we investigate the effectiveness of multiple zero-shot models that aid radiologists in detecting pulmonary embolisms and identifying intricate lung details like ground glass opacities and consolidations. Our empirical analysis provides an overview of the possible solutions to target such fine-grained tasks, so far overlooked in the medical multimodal pretraining literature. Our investigation promises future advancements in the medical image analysis community by addressing some challenges associated with unstructured data and fine-grained multi-label classification.",COVID-19;Hospitals;Pandemics;Computed tomography;Pulmonary diseases;Lung;Glass,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350919,IEEE Conferences,,,,,,
A Comparative Study of Vision Transformer Encoders and Few-shot Learning for Medical Image Classification,M. Nurgazin; N. A. Tu,2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),########,2023,"Recently, computer vision has been significantly impacted by Vision Transformer (ViT) networks. These deep models have also succeeded in medical image classification. However, most existing deep learning-based methods primarily rely on a lot of labeled data to train reliable classifiers for accurate prediction. This requirement might be impractical in the medical field, where the data is limited and manual annotation is expensive. Therefore, this study explores the application of ViT in few-shot learning scenarios for medical image analysis, addressing the challenges posed by limited data availability. We evaluate various ViT models alongside few-shot learning algorithms (i.e., ProtoNet, MatchingNet, and Reptile), perform cross-domain experiments, and analyze the impact of data augmentation techniques. Our findings indicate that when combined with ProtoNets, ViT architectures outperform CNN-based counterparts and achieve competitive performance against state-of-the-art approaches on benchmark datasets. Cross-domain experiments further reveal the effectiveness of ViT models in few-shot medical image classification.",Computer vision;Computer architecture;Manuals;Benchmark testing;Transformers;Reliability;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350402,IEEE Conferences,,,,,,
Automatic Medical Report Generation via Latent Space Conditioning and Transformers,C. Adornetto; A. Guzzo; A. Vasile,"2023 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)",########,2023,"This paper presents a comprehensive exploration of integrating artificial intelligence (AI) in the healthcare sector, focusing on the development and implementation of a novel framework called VAE-GPT. Our architecture combines Variational Autoencoder (VAE) and Generative Pre-trained Transformer (GPT), to generate high-quality medical reports. The VAE component enables the model to learn a latent space representation of the images, capturing the underlying patterns and structures. The GPT component leverages the power of transformer-based language models to generate coherent and contextually relevant text. Additionally, a novel metric, Medical Embeddings Attention Distance (MEAD), is proposed in order to capture the semantic similarity between the generated and training medical reports, taking into account the importance of specific words determined by the attention module. Experiments on real dataset demonstrate that our framework achieves state-of-the-art comparable performances in generating accurate and informative medical reports.",Training;Measurement;Semantics;Focusing;Medical services;Computer architecture;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10361320,IEEE Conferences,,,,,,
Federated Learning for Medical Images Analysis: A Meta Survey,A. Raza; A. Guzzo; G. Fortino,"2023 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)",########,2023,"Machine learning and deep learning have demonstrated significant promise for many kinds of medical imaging applications, including segmentation, classification, and detection. The quantity of data needed for developing effective models for medical images, however, is substantial and can be restricted by privacy restrictions like HIPAA and GDPR. Federated learning has gained popularity in the field of medical imaging as a privacy-focused approach. This meta-analysis, which focuses on current work, extensively investigates the use of federated learning in medical image processing. On images of the liver, stomach, colon, prostate, breast, and lungs, we examine methods employed throughout the previous five years. The paper explores techniques for classifying medical images, addresses data diversity and privacy concerns, and examines how federated learning is impacted by non-uniform(non-IID) data distribution. The Benefits and challenges of using federated learning for medical imaging are also discussed.",Deep learning;Data privacy;Image analysis;Federated learning;Lung;Liver;Breast,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10361373,IEEE Conferences,,,,,,
Classification of Skin Lesions using Deep Learning Models with Transfer Learning Techniques,P. Mekala; S. B; G. Vishnu; G. J. Sai; P. Subash,2023 IEEE International Conference on Recent Advances in Systems Science and Engineering (RASSE),########,2023,"One of the most powerful strategies for improving skin cancer patient's chances of survival is finding the disease at its earliest possible stage for diagnosis. It is an essential step to find out the type of cancer in order to ensure efficient treatment machine learning and deep learning have seen explosive growth in the past few years, particularly for the detection and categorization of even the deadliest diseases such as cancer. The use of deep convolutional neural networks (DCNN) has also increased significantly due to their revolutionary impact on computer vision and medical image processing. In this work, we are using three different Deep learning models for skin lesions classification. The main goal of this work is to classify skin lesions in RGB images using deep learning methods such as Densely connected Convolutional Neural Networks, Residual Neural Networks, and Convolutional Neural Networks. The existing methods are costly and time-consuming. This has been a problem for many patients who are in need of critical medical attention. But, due to the emergence of deep learning techniques, this problem could be overcome at a better cost than naive methods. This project aims to Classify an image using Neural networks. Image pre-processing is done on an image. After implementing three different deep learning models to classify the skin lesions from the images, we select the most accurate model and use it moving forward. The model classifies the skin lesion type from the test case images and will come to know which type of skin cancer the patient is suffering from. These kinds of use cases are closely related to the training data. Generalizability can be achieved by carefully picking the right data, collecting more of such data, and using it for training.",Deep learning;Training;Transfer learning;Training data;Skin;Lesions;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10363548,IEEE Conferences,,,,,,
Improving Kidney Tumor Classification With Multi-Modal Medical Images Recovered Partially by Conditional CycleGAN,S. Pavarut; W. Preedanan; I. Kumazawa; K. Suzuki; M. Kobayashi; H. Tanaka; J. Ishioka; Y. Matsuoka; Y. Fuji,IEEE Access,########,2023,"The accurate classification of kidney tumors necessitates the utilization of various diagnostic techniques. Within the domain of medical imaging tests, the integration of multi-modal medical imaging represents an innovative approach to enhance diagnostic precision. However, the integration of multi-modal medical imaging faces a critical challenge: the insufficiency of correspondingly paired data across modalities, resulting in a paucity of training samples for neural networks. To mitigate this limitation, generative artificial intelligence, specifically generative models capable of generating additional data, thereby addressing the gap in the multi-modal medical imaging record. In our work, our primary objective is to improve the classification results that outperform an existing method of using single-modal medical images. To achieve this, we harness the wealth of information of multi-modal data derived from Contrast-Enhanced Computed Tomography (CECT) and Magnetic Resonance Imaging (MRI), along with their respective subtypes, to determine which specific modality or pair contributes the most effectively to classification. Our work introduces the comprehensive comparison between the different multi-modal fusion techniques, in which the Area Under the Curve (AUC) serves as the benchmark for performance evaluation. Moreover, this work tackles the problem of the unavailability of kidney tumor data by partially recovering from the available data using Conditional CycleGAN, which is part of the image-to-image translation that maps between two different image domains. Through the employment of multi-modal fusion techniques and our proposed recovery of missing data, our research has yielded superior classification results than single-modal classification approaches.",Tumors;Kidney;Magnetic resonance imaging;Generators;Computed tomography;Medical diagnostic imaging;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10367977,IEEE Journals,,,,,,
Deep Learning-Based Diagnostic Tool for Brain Tumours,A. Singh; M. Rakhra; D. Singh; A. Sharma; R. Kumar,2023 3rd International Conference on Innovative Sustainable Computational Technologies (CISCT),########,2023,"In this work, we provide the results of our experiments on Brain Tumour Classification and explore their implications. MRI, short for magnetic resonance imaging, is a crucial piece of medical imaging technology that creates high-resolution pictures of the inside of a human body by combining radio waves, a strong magnetic field, and a computer. In this study, we employ a CNN-based Deep learning model to categorise cases of brain tumours. Common applications for CNNs (Convolutional Neural Networks) include video and image analysis. It processes images by first ingesting them and then convoluting them into smaller and smaller pieces. The convolutional layer analyses the input picture using a series of filters designed to identify elements like edges, curves, and patterns. Here, we constructed the CNN using data obtained from Kaggle. It contains roughly 3300 MRI scans of the brain, split into 4 groups (no tumours, meningioma tumours, glioma tumours, and pituitary tumours) and a total of 384 testing pictures. We used Keras to construct the model. The accuracy of the suggested project is quite high, coming in at 97.142%. The Confusion matrix is used to evaluate the precision. It is a data table for measuring the efficacy of an ML system. This provides the results of a comparison between the actual values in a dataset and the model's projected values in an easy-to-understand format.",Image analysis;Shape;Magnetic resonance imaging;Impedance matching;Image edge detection;Brain modeling;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351280,IEEE Conferences,,,,,,
Comparative Analysis of Deep Learning-Based Brain Tumor Prediction Models Using MRI Scan,S. M. P. Gangadharan; M. Dharani; N. Thapliyal; N. Yamsani; J. Singh; P. Singh,2023 3rd International Conference on Innovative Sustainable Computational Technologies (CISCT),########,2023,"This research examines using MRI images for brain tumour identification in deep learning-based tumour prediction models. Four well-known deep learning architecturesâ€™ accuracy and generalization capacities, including VGG16, Inception V3, ResNet-152, and ResNet-50, were assessed. A large dataset of MRI images that included both tumour and non-tumor patients was used to train and test the models. The comparison studyâ€™ findings revealed encouraging performance across all models. With exceptional training accuracy of 99.97% and testing accuracy of 96.55%, VGG16 emerged as the best performance. It had exceptional recall (95.25%), accuracy (99.95%), and a shallow false positive rate. While retaining respectable precision and recall levels, ResNet-152 and Inception V3 demonstrated competitive accuracy rates of 93.34% and 94.23%, respectively. The overall performance study showed that all models performed admirably in detecting brain cancers, with high true positive rates and low false positive rates. The results highlight the potential for these models to serve as useful instruments for early tumour identification, assisting medical practitioners in providing prompt and precise diagnosis. The research also emphasizes the clinical importance of the models' generalization ability, confirming their suitability for processing as-yet-untested MRI data. The study also recommends topics for further investigation, such as improving model architectures, adding multimodal imaging data, and carrying out rigorous clinical validation. The robustness and practical application of the models might be further improved by working with radiologists and other healthcare professionals. These deep learning-based tumour prediction models ultimately represent a promising development in medical imaging, opening the door for enhanced patient care and brain tumour detection.",Deep learning;Analytical models;Magnetic resonance imaging;Computational modeling;Medical services;Computer architecture;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351227,IEEE Conferences,,,,,,
Role of Artificial Intelligence in Thyroid Disorder,D. Guleria; V. K. Garg,2023 3rd International Conference on Innovative Sustainable Computational Technologies (CISCT),########,2023,"In clinical research, personalised medicine, and medical diagnostics, Artificial Intelligence (AI) is transforming healthcare and providing new tools. It is evident that using artificial intelligence technologies can help doctors and laboratory medicine professionals improve test prescription, test interpretation, decision-making, process optimization, and assay design. AI make use of mathematical models to carry out functions that need human like intellect. The diagnosis and risk assessment of thyroid disorders has both shown promise in the use of AI-based technologies. AI algorithms can provide more accurate and efficient assessments by considering multiple things into consideration at once by analysing a patient's medical history, symptoms, and laboratory results. This helps diagnose thyroid disorders very precisely. Thyroid disease, particularly hypothyroidism, affects people of all ages and is projected to continue to rise with increasing age. Because the Thyroid stimulating hormone (TSH) reference range must be adjusted for age, body fat and other characteristics such as multidrug therapy and overall well-being, clinical diagnosis of hypothyroidism is difficult. To treat hypothyroidism, someone needs vigilance, tackling cardiovascular disease, and tailored therapy. Therapies of subclinical hypothyroidism (SCH) in the old population must be done with caution, led by growing age, and the severity of SCH. TSH level of 10 m U/l appears to be the suitable beginning; however, it should be reconsidered on a frequent basis, and the Levothyroxine (LT4) dose should be adapted to the participant's well-being and the existence of dyslipidaemia and other metabolism abnormalities.",Terminology;Telemedicine;Sociology;Medical treatment;Risk management;Artificial intelligence;Statistics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351217,IEEE Conferences,,,,,,
Medical Image Denoising and Brain Tumor Detection Using CNN and U-Net,B. D. Shivahare; S. K. Gupta; A. N. Katiyar; P. S; P. Singh; M. Diwakar,2023 3rd International Conference on Innovative Sustainable Computational Technologies (CISCT),########,2023,"The process of medical imaging, which incorporates images produced by X-rays, ultrasound imaging, angiography, etc., is intricate. During the imaging process, image noise is also captured, some of which are very corrosive, causing a disturbance that impairs the quality of the image. The challenge of removing destructive Gaussian additive white noise from medical images while maintaining the fine features is addressed by the suggested approach. The concept of method noise is combined with a deep learning-based framework for a convolutional neural network (CNN), using method noise as a post-processing operation, to create the proposed technique. Brain tumor detection over noisy images is much difficult with better accuracy. Hence in this paper we proposed a method for denoising as well as for brain tumor detection using CNN and U-Net. By explicitly adding Gaussian additive white noise at various noise variance values (= 10, 15, 20, 25, 30), distorted images are produced. The generated denoised images' aesthetic appeal and quantitative metrics, such as peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM), are next assessed. The results are also contrasted with those of other non-traditional methods to evaluate the effectiveness of the suggested methodology further. The findings of the critical examination demonstrate the proposed methodology's admirable effectiveness in degaussing the medical images that had been tainted by Gaussian noise. This method can be applied in a variety of real-world contexts in the realm of medical image processing.",PSNR;Computed tomography;Noise reduction;X-rays;Additive white noise;Convolutional neural networks;Biomedical image processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351338,IEEE Conferences,,,,,,
Automated Routine Colon Cancer Nuclei Classification Using Black Widow Optimization with Deep Learning Model,K. Ibrahim; Z. Abed; M. A. Alkhafaji; A. H. Alawadi; S. Sheel,2023 6th International Conference on Engineering Technology and its Applications (IICETA),########,2023,"Precise and efficient classification of histological cell nuclei is of great significance because of its promising application in the domain of medical image analysis. It facilitates the physician to explore various factors and better understand the treatment of cancer. Due to cellular heterogeneity detection and classification of cell nuclei in histopathology images of tissue stained with the typical haematoxylin and eosin stain becomes a tedious process. The deep learning approach has been demonstrated to produce remarkable outcomes on histopathology images in different fields. Therefore, this study designs an Automated Routine Colon Cancer Nuclei Classification Using Black Widow Optimization with Deep Learning (ARCCNC-BWODL) model. The presented ARCCNC-BWODL algorithm focuses majorly on the identification and classification of RCC cell nuclei. The presented algorithm applies improved Faster SqueezeNet model to make feature vectors. Besides, the hyperparameter tuning of the Faster SqueezeNet approach is performed via the BWO system. To classify the nuclei effectively, long short term memory is used. The simulation outcome of the ARCCNC-BWODL model was tested on medical imaging database and the outcomes exhibited the enhancements of the ARCCNC-BWODL over other DL techniques.",Deep learning;Histopathology;Databases;Classification algorithms;Colon;Tuning;Optimization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351377,IEEE Conferences,,,,,,
Optimal Elman Neural Network for Pancreatic Cancer Classification Using Computed Tomography Images,A. H. Shnawa; G. Mohammed; M. R. Hadi; K. Ibrahim; M. M. Adnan; W. Hameed,2023 6th International Conference on Engineering Technology and its Applications (IICETA),########,2023,"Pancreatic cancer is a deadly form of tumor and estimations are reduced in the present scenarios. Automatic pancreatic cancer classification by the use of computer-aided diagnosis (CAD) technique becomes essential for tracking, predicting, and classifying the presence of pancreatic cancer. Artificial intelligence (AI) techniques assist in medical decision-making with the consideration of huge amounts of medical imaging data. The latest improvements in deep learning (DL) algorithms assist in the effective design of pancreatic cancer classification models. In this view, this paper establishes an end-to-end pancreatic cancer classification using a metaheuristic and deep transfer learning (ETEPCC-MDTL) algorithm. The proposed ETEPCC-MDTL algorithm aims to precisely determine the presence of pancreatic tumors on computed tomography (CT) images. To accomplish this, the ETEPCC-MDTL method initially applies bilateral filtering (BF) based pre-processing and Shannon's entropy-based image segmentation technique. In addition, Neural Architectural Search Net (NASNet) model is employed for feature extraction purposes with cat swarm optimizer (CSO) as a hyperparameter optimization algorithm. Moreover, glowworm swarm optimization (GSO) with Elman neural network (ENN) is exploited for the classification process. The experimental validation of the ETEPCC-MDTL algorithm is tested under benchmark medical image databases.",Solid modeling;Image databases;Computed tomography;Computational modeling;Neural networks;Pancreatic cancer;Benchmark testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351360,IEEE Conferences,,,,,,
Automated Osteosarcoma Detection and Classification Using Advanced Deep Learning with Remora Optimization Algorithm,M. K. Obaid; H. A. Abed; S. B. Abdullah; H. M. Al-Jawahry; S. Majed; A. R. Hassan,2023 6th International Conference on Engineering Technology and its Applications (IICETA),########,2023,"Osteosarcoma is considered a primary malignant bone tumor which affects young people and adults. Manual osteosarcoma identification is time consuming and necessitates expert knowledge. Since earlier detection of osteosarcoma reduces death rate, computer aided diagnosis (CAD) models can be developed to examine the medical images for decision making. The latest breakthroughs in machine learning (ML) and deep learning (DL) methods find to be useful for enhancing detection performance and diagnostic time. In this view, this article introduces an Automated Osteosarcoma Detection and Classification using metaheuristics with Advanced Deep Learning (AODC-MADL) algorithm. The proposed AODC-MADL model makes use of recent DL models with hyperparameter optimization algorithms to detect and classify osteosarcoma. For achieving that, the presented AODC-MADL algorithm pre-processes the input images to optimize the image quality. Moreover, the Dense-EfficientNet architecture is utilized to form a set of feature vectors. Besides, attention based bidirectional recurrent neural network (ABiRNN) model receives the feature vectors and performs classification process. At last, the remora optimization algorithm (ROA) is used to optimally choose the hyperparameters related to the ABiRNN model. A series of experiments have been conducted to depict the outstanding performance of the AODC-MADL algorithm. The experimental outcome emphasized that the AODC-MADL algorithm has obtained higher performance over other approaches.",Deep learning;Image quality;Solid modeling;Recurrent neural networks;Metaheuristics;Hyperparameter optimization;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351357,IEEE Conferences,,,,,,
Improved Deep Learning with Metaheuristics Driven COVID-19 Diagnosis on Chest X-Ray Images,M. A. Alkhafaij; A. S. Oleiwi; R. R. Ali; E. Ali; M. Al-Tahee; M. Almusawi,2023 6th International Conference on Engineering Technology and its Applications (IICETA),########,2023,"Recently, Artificial Intelligence (AI) has undergone significant changes in part of medical image processing. Earlier diagnosis using chest X-ray (CXR) images proved that a key solution in fighting COVID19. Several computer-aided diagnostic (CAD) techniques have been used for assisting radiologists and provide a secondary recommendation for the same. CXR has become the first screening method to play a considerable role in the analysis of COVID19 contamination. Formerly, various traditional machine learning (ML) and deep learning (DL) systems were utilized for automatic diagnosis of chest radiography X-ray images. One method widely applied with DL is transfer learning (TL) which re-uses the data learned from trained modules like resolving one problem and applying it to the same problem. This study presents an Improved Deep Learning with Metaheuristics based disease recognition and classification (IDLM-D2C) for COVID19 on CXR images. The presented IDLM-D2C approach concentrates on the precise detection and classification of COVID19 on CXR images. Initially, the IDLM-D2C algorithm implements the Weiner filter (WF) as a noise elimination step. In addition, the IDLM-D2C method exploits seagull optimization algorithm (SGO) with an improved Faster SqueezeNet model for feature extraction. For COVID19 detection, deep learning modified neural network (DLMNN) model was used with chimp optimization algorithm as hyperparameter optimizer. The experimental validation of the IDLM-D2C method is tested on benchmark medical image dataset. The comparison analysis highlighted the improvements of the IDLM-D2C method over other approaches.",COVID-19;Deep learning;Solid modeling;Metaheuristics;Transfer learning;Benchmark testing;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351403,IEEE Conferences,,,,,,
Improved Qubit Neural Network Based Computer Aided Detection Model for COVID-19 on Chest Radiographs,W. H. Madhloom Kurdi; Z. Abed Almoussawi; B. Mohammed Khaleel; Z. N. Abdulhussain; K. AL-Attabi; R. Khalid,2023 6th International Conference on Engineering Technology and its Applications (IICETA),########,2023,"The significance of the advanced computation model for medical diagnoses has been gradually recognized with the growth in e-health. Specifically, in the pandemic COVID19, the advanced computational modelling role is crucial plays in understanding complex clinical data in future years. Computed tomography (CT) and Chest X-ray (CXR) images were presented for the diagnosis of COVID19. The deep learning (DL) method has proven better and more efficient in several medical imaging and computer vision applications. With the increase in the COVID outbreak, researcher workers are making use of the DL method to identify coronavirus infection. This study develops a hybrid squirrel search algorithm with improved deep learning-based detection (HSSA-IDLD) system for the diagnoses of COVID-19. The major intention of the HSSA-IDLD technique lies in the recognition and discrimination against COVID-19 in healthcare images. Initially, the medical images are processed using a contrast enhancement approach. For feature extraction, an improved EfficietNet model is utilized in this study. In the presented HSSA-IDLD technique, the HSSA is derived as a hyperparameter optimizer. Moreover, Qubit neural network (QBNN) approach was exploited for the recognition and classification of COVID-19. A widespread simulation process was carried out to demonstrate the enhanced performance of the HSSA-IDLD algorithm. An extensive comparison study reported improvements in the HSSA-IDLD technique compared to recent DL models.",COVID-19;Radiography;Image recognition;Computational modeling;Computed tomography;Qubit;Neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351200,IEEE Conferences,,,,,,
Machine Learning Techniques to Classify Brain Tumor,T. M. Bahya; N. Hussein,2023 6th International Conference on Engineering Technology and its Applications (IICETA),########,2023,"Automated identification of defects in medical imaging, such as brain tumors in Magnetic Resonance Imaging (MRI), is essential for diagnostic applications. This research paper focuses on diagnosing normal and abnormal brain tumors, including meningioma, glioma, and pituitary tumors. To address the challenge of large data volumes, automated methods using machine learning algorithms are developed. Gradient Boosting (GB), Adaptive Boosting (ADA), and Support Vector Machine (SVM) algorithms are employed to enhance accuracy and reduce diagnosis time. The study utilizes a publicly available dataset and applies feature extraction techniques, including Fast Fourier Transform (FFT), K-means clustering, and Tamura texture analysis. Additionally, dataset dimensionality reduction techniques are employed. The reported accuracy rates for normal and abnormal cases are 92.6% for GB, 86% for ADA, and 82% for SVM. For specific tumor types, the accuracy rates are 67.9% for GB, 65.3% for SVM, and 59.6% for ADA. Based on superior performance, GB is recommended as the algorithm of choice. This research paper demonstrates the effectiveness of these techniques in automating brain tumor detection and improving diagnostic accuracy.â€",Support vector machines;Machine learning algorithms;Magnetic resonance imaging;Refining;Boosting;Feature extraction;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351260,IEEE Conferences,,,,,,
"Transfer Learning with Pre-trained CNNs for MRI Brain Tumor Multi-Classification: A Comparative Study of VGG16, VGG19, and Inception Models",A. A. Siti Nurfarahin; D. Rudzidatul Akmam; M. N. Norliza,2023 IEEE 2nd National Biomedical Engineering Conference (NBEC),########,2023,"In the field of medical imaging and diagnosis, the advancement of machine learning techniques has brought about significant progress. The classification of brain images, particularly in the context of brain tumors, has evolved from traditional methods to more sophisticated approaches like deep learning, specifically Convolutional Neural Networks (CNNs). The features that CNN extracts were significantly impacted by the size of the training dataset. When the training dataset is small, the CNN often overfits. Therefore, Deep CNNs (DCNN) with transfer learning have been created. This study used data augmentation and transfer learning techniques to examine the potential for categorization of brain MR images by pre-trained DCNN VGG-19, VGG-16, and Inception V3 models. Accuracy, recall, precision, and F1 score validation on the test set revealed that the pre-trained Inception V3 model with transfer learning performed the best. In this work, VGG-16 model with Adam optimizer achieved the highest average accuracy with 99% and high accuracy of 91% for multi-class classification using InceptionV3 model.",Training;Deep learning;Magnetic resonance imaging;Transfer learning;Brain modeling;Feature extraction;Internet,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10352589,IEEE Conferences,,,,,,
AI Clinical Decision Support System (AI-CDSS) for Cardiovascular Diseases,P. D. K; M. S. Abirami,2023 International Conference on Computer Science and Emerging Technologies (CSET),########,2023,"The AI-CDSS (Artificial Intelligence Clinical Decision Support System) is a powerful tool designed to assist healthcare professionals in making informed and evidence-based decisions in patient care. It leverages artificial intelligence algorithms and data analysis techniques to provide personalized recommendations and insights. This system explores the features and benefits of the AI-CDSS, including patient data analysis, diagnostics and treatment recommendations, drug interaction and adverse event detection, predictive analytics, real-time monitoring and alerts, and continuous learning and improvement. The model also discusses the applications of AI-driven decision-making systems in healthcare, focusing on areas such as cancer diagnosis and treatment, chronic disease management, medication optimization, surgical decision support, infectious disease outbreak management, radiology and medical imaging analysis, mental health support, and clinical trials and research. Additionally, the paper highlights existing methodologies, such as deep learning models like CNNs and RNNs, that have shown potential in cardiovascular disease prediction. However, it emphasizes the need for rigorous validation, evaluation, and consideration of ethical and regulatory aspects before deploying AI models in clinical practice. Ultimately, the integration of AI-driven decision-making systems has the potential to revolutionize chronic disease management, improve patient consequences, and enhance the quality of care provided to individuals with longterm health conditions.",Deep learning;Decision support systems;Data analysis;Medical services;Predictive models;Feature extraction;Cardiovascular diseases;Artificial intelligence;Medical diagnostic imaging;Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346885,IEEE Conferences,,,,,,
A Survey on Decentralization and Virtualization of Medical Trials: An approach through Ensemble learning models and Convolutional Neural Networks,D. Chakravarty; A. M. Thomas; V. Vivek,2023 International Conference on Computer Science and Emerging Technologies (CSET),########,2023,"- In sectors such as medicine where accuracy and time are the most prioritized factors the use of technology and its tools have led to huge advancements and contributed to saving millions of lives. Technology has been bifurcated into various categories to suit multiple requirements of the medical sector. The use of Machine Learning along with well-designed models and sister technologies such as Computer Vision has opened up a whole new dimension in medical sciences. Machine learning deals with the building of algorithms that can predict using fed patterns which further specializes in Deep Learning. Deep learning's success in a variety of pattern recognition applications has sparked excitement and raised expectations that deep learning, or Artificial Intelligence (AI), could revolutionize the health care that we know today. Right from diagnosis to treatment it can revolutionize the way we construct and operate the medical sector. Over the last decade, Convolutional Neural Networks have achieved breakthroughs in a range of pattern recognition domains. CNN's are generally employed to solve complex image-driven pattern recognition tasks. They have proven to be a great tool in image recognition and classification and are tuned according to the needs of the user. Ensemble learning, as a research area, strives to bring data fusion, data modeling, and data mining together in a single framework. Ensemble learning combines the informative knowledge gained from the previous outcomes to produce knowledge, discovery, and improved predictive performance using adaptive voting methods. In this study, the methodologies of numerous models of image recognition and classification, as well as ensemble learning from multiple authors, are evaluated. In addition, this study shall discuss any feasible enhancements that could be implemented to the existing models as well as how the ideologies of the models could be used for efficient diagnosis in the field of medicine.",Deep learning;Surveys;Adaptation models;Image recognition;Computational modeling;Data models;Ensemble learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346978,IEEE Conferences,,,,,,
Detection and Classification of Brain Tumour Using EfficientNet and Transfer Learning Techniques,A. Kukreti; G. P. M S; M. Ram; P. K. Naik,2023 International Conference on Computer Science and Emerging Technologies (CSET),########,2023,"Medical imaging plays a crucial role in the diagnosis and categorization of brain tumours. Recently, deep learning methods, specifically Convolutional Neural Networks (CNNs), were effectively utilised to this issue. In this research, we propose employing a Convolutional Neural Network (CNN) architecture based on EfficientNet for the identification and classification of brain tumours. Preprocessing, feature extraction, and classification are the three key components of our proposed model. During the initial stages of preparation, we apply skull stripping and intensity normalization techniques to remove unwanted noise and enhance image contrast. Next, we employ transfer learning with the EfficientNet architecture to extract relevant features from the preprocessed images. Finally, we use a fully connected layer with a softmax activation function to classify the extracted features into four categories such as healthy brain, glioma, meningioma, and pituitary tumor. Brain Tumour Segmentation (BraTS) is a dataset used for research into brain tumours, and we use it in order to evaluate our suggested approach, which contains 3D MRI scans of brain tumors. The experimental results demonstrate that our proposed model achieves high accuracy, sensitivity, and specificity for tumor detection and classification. Specifically, our model achieves an overall accuracy of 97.2 %, sensitivity of 96.8%, and specificity of 97.4%. These results outperform numerous state of the art methods, demonstrating the effectiveness of our proposed CNN architecture for brain tumor detection and classification. Overall, our study presents a promising approach for improving the accuracy and efficiency of brain tumor diagnosis through deep learning techniques.",Deep learning;Three-dimensional displays;Transfer learning;Sensitivity and specificity;Brain modeling;Feature extraction;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346858,IEEE Conferences,,,,,,
"Comprehensive Study on High Resolution Aerial Image Classification to highlight Issues, potential Application, and Future Scope",N. Bhatnagar; S. Srivastava; S. Vats,2023 4th IEEE Global Conference for Advancement in Technology (GCAT),########,2023,"In several fields, including remote sensing, medical imaging, surveillance, and object recognition, high-resolution picture categorization is vital. The availability of high-resolution images has presented fresh difficulties and chances for the advancement of advanced categorization methods. This paper provides an overview of the techniques, challenges, and applications along with the work done by several researchers in past years related to high-resolution image classification. When the target is aerial image classification then Feature Extraction, SVM, CNN, FCN are some of the available techniques. No single source is detailing the work so the purpose of writing the paper is to present a thorough analysis of the study so new researchers and beginners of this field may get perfect motivation to start their research work",Support vector machines;Image resolution;Surveillance;Writing;Feature extraction;Object recognition;Remote sensing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10353405,IEEE Conferences,,,,,,
"Machine Learning and Deep Learning Transforming Healthcare: An Extensive Exploration of Applications, Algorithms, and Prospects",L. A. A. Gracious; R. M. Jasmine; E. Pooja; T. P. Anish; G. Johncy; R. S. Subramanian,2023 4th IEEE Global Conference for Advancement in Technology (GCAT),########,2023,"The potential for machine learning and deep learning to revolutionize the healthcare industry is examined in this thorough survey study. These cutting-edge technologies have the potential to completely transform healthcare by providing accurate diagnoses, customizing drugs for each patient, and ultimately enhancing patient outcomes. The study offers a thorough investigation of a number of applications, such as clinical decision support systems, electronic health record analysis, illness diagnosis and prediction, personalized medicine, and drug development. In this article, we focus on the essential methodologies, obstacles, and opportunities related to the use of machine learning and deep learning in healthcare. This source aims to be a helpful resource for researchers, medical practitioners, and decision-makers who are looking to maximize the benefits of modern technologies to improve the provision of healthcare services. Additionally, we would like to contribute to a better and more effective healthcare environment by bridging the technology and healthcare barrier.",Deep learning;Surveys;Drugs;Decision support systems;Ethics;Data privacy;Precision medicine,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10353476,IEEE Conferences,,,,,,
GAN and CNN Model for Generation of Synthetic Medical Images and Classification,V. Kulkarni; P. Kosamkar; R. Shinde,2023 4th IEEE Global Conference for Advancement in Technology (GCAT),########,2023,"for any machine learning or deep learning model, data is the most essential ingredient. Data augmentation has become the need of the hour as the required amount of data is not available in many fields and real world scenarios. In such situations, synthetic data comes into the picture. Synthetic data generation techniques have made it possible to create a huge amount of data from really small data. It has been possible to make realistic artificial data that resembles real data. Finding a particular dataset that has required features is a difficult task. Medical data is rare and expensive. It is nearly impossible to get real data with clinical trials in the required amount. In this paper, we will be discussing, synthetic medical image generation with Generative Adversarial Networks (GANs). We used Deep Convolutional GAN for the generation of medical images. We were able to generate 400 synthetic images of retinal fundus from a tiny dataset of 20 images. We calculated the FID score to evaluate the generated synthetic images and we were able to keep it as low as 38.31. We applied the generated synthetic images and real images on CNN model for a comparative study on accuracies achieved with both the images. We were able to get the accuracy of 60% with synthetic images out of dataset consisting of only 20 images.",Training;Deep learning;Image synthesis;Generative adversarial networks;Retina;Data models;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10353443,IEEE Conferences,,,,,,
Definition and Classification of Heart Diseases From Data Obtained with Echocardiography Doppler,C. FiÃ§Ä±cÄ±; O. KoÃ§ak; Z. Telatar,2023 Medical Technologies Congress (TIPTEKNO),########,2023,"The usage areas of artificial intelligence applications are increasing rapidly. Studies on the use of artificial intelligence in medical imaging, especially in the evaluation process of the outputs of devices such as Computed tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Mammography, Ultrasound (USG) and X-Ray, indicates that the efficiency of use of devices can be increased and these outputs can be evaluated more efficiently. At this point, artificial intelligence applications provide convenience to people using these devices and help reduce their workload. The aim of this study is to develop a classification of aortic stenosis by using a deep learning approach known as Convolutional Neural Networks (CNN) from Doppler echocardiography images. Promising results were obtained in discrimination of aortic valve diseases.",Echocardiography;Magnetic resonance imaging;Computed tomography;Valves;Doppler effect;Convolutional neural networks;Positron emission tomography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10359232,IEEE Conferences,,,,,,
A Hybrid Intelligence Approach for Circulating Tumor Cell Enumeration in Digital Pathology by Using CNN and Weak Annotations,L. Tong; Y. Wan,IEEE Access,########,2023,"Counting the number of Circulating Tumor Cells (CTCs) for cancer screenings is currently done by cytopathologists with a heavy time and energy cost. AI, especially deep learning, has shown great potential in medical imaging domains. The aim of this paper is to develop a novel hybrid intelligence approach to automatically enumerate CTCs by combining cytopathologist expertise with the efficiency of deep learning convolutional neural networks (CNNs). This hybrid intelligence approach includes three major components: CNN based CTC detection/localization using weak annotations, CNN based CTC segmentation, and a classifier to ultimately determine CTCs. A support vector machine (SVM) was investigated for classification efficiency. The B-scale transform was also introduced to find the maximum sphericality of a given region. The SVM classifier was implemented to use a three-element vector as its input, including the B-scale (size), texture, and area values from the detection and segmentation results. We collected 466 fluoroscopic images for CTC detection/localization, 473 images for CTC segmentation and another 198 images with 323 CTCs as an independent data set for CTC enumeration. Precision and recall for CTC detection are 0.98 and 0.92, which is comparable with the state-of-the-art results that needed much larger and stricter training data sets. The counting error on an independent testing set was 2-3% and 9% (with/without B-scale) and performs much better than previous thresholding approaches with 30% of counting error rates. Recent publications prove facilitation of other types of research in object localization and segmentation are necessary.",Deep learning;Annotations;Training;Testing;Image segmentation;Cancer;Tumors;Cells (biology);Pathology;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10363183,IEEE Journals,,,,,,
Classification of Companion Animalsâ€™ Ocular Diseases: Domain Adversarial Learning for Imbalanced Data,M. G. Nam; S. -Y. Dong,IEEE Access,########,2023,"In contrast to the widespread implementation of computer-aided diagnosis of human diseases, the limited availability of veterinary image datasets has hindered its application in animals. Additionally, while most medical imaging data are captured in clinical settings, such as optical coherence tomography and fundus photography, diagnosis based on digital camera or smartphone images can be more beneficial for pet owners. This study specifically focuses on achieving generalization between screening environments, aiming to accurately diagnose diseases using casual images obtained by pet owners, despite the majority of training images being captured with specialized equipment in hospitals. Given these challenges and the significant role of computer-aided diagnosis in veterinary science, this study aims to develop a practical deep-learning framework for classifying ocular surface disease images in companion animals. The dataset used in this study consists of diverse ocular disease images of canines and felines obtained through slit lamps and digital cameras. The proposed approach includes two layers of labels for multitask learning and a gradient reversal layer based on normalized feature maps. We achieved 84.7% and 65.4% accuracy for the total dataset of canine and feline, respectively. For the camera domain in particular, canines and felines reached 86.2% and 73.2% accuracy, respectively.",Diseases;Cameras;Biomedical imaging;Feature extraction;Cats;Cornea;Convolutional neural networks;Computer aided diagnosis;Eyes;Diseases;Adversarial machine learning;Visual impairment;Ophthalmology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10365157,IEEE Journals,,,,,,
Multimodal Convolutional Deep Belief Networks for Stroke Classification with Fourier Transform,M. Roder; N. Gomes; A. Yoshida; J. P. Papa; F. Costen,"2023 36th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)",########,2023,"Several studies have investigated the vast potential of deep learning techniques in addressing a wide range of applications, from recommendation systems and service-based analysis to medical diagnosis. However, even with the remarkable results achieved in some computer vision tasks, there is still a vast scope for exploration. Over the past decade, various studies focused on developing automated medical systems to support diagnosis. Nevertheless, detecting cerebrovascular accidents remains a challenging task. In this regard, one way to improve these approaches is to incorporate information fusion techniques in deep learning architectures. This paper proposes a novel approach to enhance stroke classification by combining multimodal data from Fourier transform with Convolutional Deep Belief Networks. As the main result, the proposed approach achieved state-of-the-art results with an accuracy of 99.94%, demonstrating its effectiveness and potential for future applications.",Graphics;Deep learning;Solid modeling;Computer vision;Fourier transforms;Data models;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10347165,IEEE Conferences,,,,,,
A Systematic Study on Enhanced Deep Learning Based Methodologies for Detection and Classification of Early Stage Cancers,O. H. Kesav; R. G. K,"2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",########,2023,"Cancer is considered as a principal basis of death worldwide, with brain and lung cancers being particularly challenging to detect and classify in their early stages. Early detection and classification of cancer are crucial for effective treatment and improved patient outcomes. However, traditional diagnostic methods for these cancers often fail to detect them in their early stages due to their complex nature and lack of specific symptoms. Deep learning-based methodologies have shown great promise in the detection and classification of various diseases, including cancers. However, the accuracy of existing deep learning models for early-stage cancer detection and classification still needs improvement, particularly for cancers such as brain and lung cancers. There are several challenges associated with developing accurate methodologies. First, the available data for early-stage cancers is often limited, making it challenging to develop accurate models. Second, these cancers can have complex and heterogenous features, making it difficult to develop models that can accurately detect and classify them. Third, the interpretability of the features learned by deep learning models is limited, making it challenging to gain insights into the biological mechanisms underlying early-stage cancer development. Therefore, the problem statement for this research is to develop enhanced deep learning-based methodologies that can accurately detect and classify early-stage cancers, particularly brain and lung cancers, by addressing the challenges associated with existing methods. The proposed research will incorporate advanced techniques such as transfer learning, attention mechanisms, and adversarial training to progress the accuracy of cancer detection and categorization. Furthermore, the research will investigate the interpretability of the features learned by the proposed models to gain insights into the biological mechanisms underlying early-stage cancer development. Ultimately, the proposed research aims to improve patient outcomes by enabling earlier detection and treatment of brain and lung cancers, which can significantly improve survival rates and quality of life.",Deep learning;Measurement;Training;Systematics;Biological system modeling;Transfer learning;Lung cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346973,IEEE Conferences,,,,,,
Modified EfficientNetB3 Deep Learning Model to Classify Colour Fundus Images of Eye Diseases,R. Sharma; J. Gangrade; S. Gangrade; A. Mishra; G. Kumar; V. Kumar Gunjan,"2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",########,2023,"The only way to prevent blindness from eye problems is by early detection and prompt treatment. Although colour fundus photography (CFP) is useful for fundus inspection, there is a need for computer-assisted automated diagnosis tools due to the similarities between the early symptoms of many eye disorders. The suggested approach uses cutting-edge deep learning model to categorize images into several disease categories by learning distinguishing features from the input images. The high-resolution fundus photos from individuals with diabetic retinopathy (DR), glaucoma, cataract, and healthy eyes make up most of the dataset used for this research. The experimental findings show that the suggested system achieve 97% accuracy with modified efficientNetB3 model and surpasses current approaches for categorizing eye diseases. This approach may help doctors diagnose and treat eye conditions earlier, leading to better patient outcomes.",Deep learning;Photography;Image color analysis;Computational modeling;Operating systems;Transfer learning;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346769,IEEE Conferences,,,,,,
Brain Tumor Detection Using Convolutional Neural Network,S. Uke; M. Kaulwar; Y. Kawtikwar; S. Khadse; A. Khandare,"2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",########,2023,"Tumor is the abnormal development of cells in the body. The fairly significant overgrowth of brain cells is known as a brain tumor and classification manually takes time and is only possible at a few diagnostic facilities. Therefore, it is necessary to create a system that can identify the type of brain tumor based solely on the input MR images. To evaluate these malignancies, a common imaging approach is magnetic resonance imaging (MRI), however, manual segmentation cannot be done in a timely manner due to the volume of data generated by MRI, restricting the application of exact quantitative measurements in clinical practice. This study presents a novel hybrid methodology for the classification of brain cancers based on the analysis of magnetic resonance imaging (MRI) data. The methodology compares the performance of different classifiers, namely Convolutional Neural Networks (CNNs), K-nearest neighbor, Support Vector Machine and Logistic Regression in predicting the presence of brain tumors. The CNN model is specifically employed to get features from the MRI data and apply them to the classifiers. The results of the study show that the different CNN models, along with KNN, SVM, LR, and Long Short-Term Memory Networks were taken for tumor classification. The achieved accuracies were 82.35%, 78.43%, 61.26% and 74.28% respectively. Among these algorithms, KNN demonstrated the highest accuracy, indicating its effectiveness in accurately classifying brain tumors based on the MRI data. By integrating these classifiers and utilizing the CNN model for feature extraction, this hybrid methodology presents a promising method for classifying brain tumors from MRI data. The findings highlight the potential of combining deep learning techniques like CNNs with traditional machine learning algorithms to increase the reliability of diagnosing brain tumors and contribute to the field of medical image analysis.",Support vector machines;Magnetic resonance imaging;Volume measurement;Manuals;Brain modeling;Data models;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346764,IEEE Conferences,,,,,,
Alzheimer's Disease Classification Using Deep Learning,A. Ftoutou; N. Majdoub; T. Ladhari,2023 IEEE International Conference on Artificial Intelligence & Green Energy (ICAIGE),########,2023,"Alzheimer's disease is a neurological condition that primarily impacts older person's memory and is incurable. Worldwide, Alzheimer's disease primarily impacts adults over 65. This condition requires an early diagnosis in order to be accurately detected. Due to the high number of individuals who come with the illness, manual determination by medical professionals is risky and laborious. There is a require for superior precision in early conclusion approaches despite the fact that a variety of strategies have been used to diagnose and categorise Alzheimer's disease. Such representations are capable of being learned from data by deep learning techniques. In the proposed study, transfer learning with ResNet-50 and Fastai will be used to accomplish multilayer categorization of Alzheimer's illness, i.e., Mild Demendia, Moderate Demendia, Non Demendia, and Very Mild Demendia. This technique results in high projected accuracy, a major improvement over past studies and ample evidence of the effectiveness of the suggested strategies.",Deep learning;Transfer learning;Manuals;Nonhomogeneous media;Alzheimer's disease;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346502,IEEE Conferences,,,,,,
Advanced Bone Fracture Detection Through Deep Learning with Radiological Imaging,I. Wali; H. Mokaddem; A. Bouzid; A. Kessentini; N. Masmoudi,"2023 IEEE International Conference on Design, Test and Technology of Integrated Systems (DTTIS)",########,2023,"The project consists of designing an embedded platform for the identification of bone fractures. This platform uses deep learning to analyze images provided by the doctor. The system performs necessary image processing to identify the presence or not of the fracture. For this, a deep learning model is trained to provide the identification results. Results show an increasing accuracy with a lower loss.",Deep learning;Image processing;Imaging;Medical services;Bones,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10348328,IEEE Conferences,,,,,,
Federated Learning-assisted Self-supervised CNN for Monkeypox Diagnosis,N. Jahan; G. Bajwa; T. Akilan,2023 IEEE Western New York Image and Signal Processing Workshop (WNYISPW),########,2023,"Monkeypox (Mpox) is a contagious viral illness that affects both humans and animals, and its early diagnosis is critical for the effective management and prevention of this disease. This work proposes a federated learning-assisted self-supervised convolutional neural network (CNN) for Mpox identification from skin lesion images. The demand for domain experts for ground truth annotations is reduced with the help of self-supervised learning, as it can process unlabeled data. The self-supervision is driven by a framework called simple contrastive learning for representations (SimCLR), which extracts discriminative patterns of Mpox from the skin lesion images. Federated learning (FL), on the other hand, enables a privacy-preserved collaborative training approach to build the Mpox classifier on vast diverse datasets from several healthcare institutions, remotely. Thorough experimental study on a publicly available benchmark dataset, the proposed approach achieves competitive performances.",Training;Surveys;Federated learning;Self-supervised learning;Signal processing;Skin;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10349384,IEEE Conferences,,,,,,
A Deep Texture Metric for PET Image Quantification and Image Synthesis,R. L. Smith; R. R. John; I. Ackerley; K. Al-Battat; E. Alsyed; E. Spezi; K. Wells; N. Morley; C. Marshall,"2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)",########,2023,"Fluorodeoxyglucose (FDG) Positron Emission Tomography (PET) serves as a cornerstone in functional imaging. PET imaging reveals metabolic abnormalities before morphological alterations occur. The standardized uptake value is a common metric to semi-quantify PET images. Visual scoring of PET image heterogeneity has demonstrated correlations with tumour characteristics. Quantification of this spatial variation in FDG distribution via its shape and texture has proven to be a fertile area of research under the umbrella term ""Radiomics."" Deep learning with convolutional neural networks (CNN) serves as large feature extractors with proven ability in tumour type classification in PET imaging. Deep learning in PET imaging has also been utilized for image enhancement, reconstruction, segmentation and image synthesis. In this work, we quantify the feature distribution of neural activations of each layer in a CNN following propagation through the network of a PET image. The Gramian matrix of the activation functions quantifies the neural representation of the CNN and thus serves as a measure of visual perception. Image synthesis using gradient descent on the target PET image Gramian matrix allows visualization of the texture. This metric is tested in PET imaging data of a recently developed texture phantom and a single primary tumour from a non-small-cell lung cancer (NSCLC) patient. It is demonstrated that the summation of the Gramian decreases with decreasing homogeneity. The visual appearance of the synthesized PET images closely resembles that ground truth PET images, both in the texture phantom and clinical image.",Deep learning;Visualization;Image synthesis;Shape;Imaging phantoms;Feature extraction;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10338021,IEEE Conferences,,,,,,
Automatic Labeling of Glycolytic Volumes In PET Using Deep Texture Analysis,R. R. John; I. Ackerley; R. L. Smith; J. Scuffham; A. Robinson; V. Prakash; M. Shastry; M. Halling-Brown; E. Lewis; P. Strouhal; K. Wells,"2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)",########,2023,"Positron Emission Tomography and Computed Tomography (PET-CT) is a vital imaging technique for accurate cancer diagnosis, staging, and treatment planning, offering complementary morphological and anatomical information. A 5-layer 3D convolutional deep learning texture model was employed to identify glycolytic regions in PET-CT data, achieving an average sensitivity and specificity of 96.1% and 99.4%, respectively, for binary classification targeting primary tumor patches. Using a dataset of PET-CT data from 486 esophageal patients, we analyzed network activations across each layer for characteristic activation patterns of four glycolytic uptake classes: primary tumor, bladder, liver, and myocardium. PCA analysis of the activations was performed to isolate uncorrelated features learned during training and reveal unique feature clusters in PCA space, demonstrating that glycolytic regions with high SUV values exhibit distinct textures learnable by deep learning architectures. This information was used to prune low activation probability nodes in the network, resulting in a more efficient deployable network with slightly improved classification performance. A comprehensive quantitative evaluation of redundant filters in the network, examining filter combinations that result in positive (tumor-present) and negative (tumor-absent) predictions across multiple patients will be presented, alongside preliminary results on the use of AI for automatic staging.",Training;Solid modeling;Three-dimensional displays;Semiconductor detectors;Sensitivity and specificity;Information filters;Positron emission tomography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10338391,IEEE Conferences,,,,,,
Comparison of Extracting Tissue- and Image-based Characteristic Features for Machine Learning Prediction of Colorectal Polyp Malignancy,L. Kuo; M. J. Pomeroy; Y. Gao; W. Cao; S. Chang; L. Li; P. J. Pickhardt; Z. Liang,"2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)",########,2023,"Detection and classification of precursor polyp via computed tomography colonography (CTC) has shown its significant clinical impact on management of colorectal cancer (CRC). In this study, we compared the classification performance of five machine learning (ML) models and one deep learning (DL) model. The five ML models utilized the Haralick feature (HF), Haralick measure (HM), extended Haralick feature (eHF), and extended Haralick measure (eHM), and one Affine tissue elastic feature with Karhunen-LoÃ¨ve (KL) Transformation on virtual monoenergetic images (Affine_KL VMI) that included 10 energies and were trained with a Random Forest classifier. The DL model was a 17-layer 3D convolutional neural network (3D_CNN) model including 3 instance normalization layers. The performance of the six models was evaluated using the area under the receiver operating characteristic curve (AUC).The AUC values for the six models ranged from 0.870 to 0.997 for 100 polyp masses with a diameter larger than 30mm, 0.711 to 0.985 for 182 polyp masses with a diameter between 10 to 30mm, 0.601 to 0.890 for 357 polyp masses with a diameter less than 10 mm, respectively. The Affine_KL VMI model consistently outperformed the other models, achieving the highest AUC values across all datasets, ranging from 0.890 to 0.997. This indicates that the Affine_KL VMI ML model is a promising approach for detecting and classifying precursor polyps in CTC images and may have significant clinical impact on the management of CRC.",Solid modeling;Semiconductor device measurement;Microwave integrated circuits;Three-dimensional displays;Semiconductor detectors;Computational modeling;Energy measurement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10337972,IEEE Conferences,,,,,,
Vision Transformer-based Sinogram Enlarging for Reducing Artifacts in Sparse-view Micro-CT,T. Okamoto; H. Haneishi,"2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)",########,2023,"Micro-computed tomography (micro-CT) provides three-dimensional (3D) morphological structures at the micrometer scale. Although this modality is expected to contribute to histopathology for analyzing the 3D microstructures of tissue specimens, micro-CT imaging for tissue specimens requires a long scan time. Sparse-view CT, which reduces the number of projections, has great promise for speeding up the scanning process; however, analytical reconstruction from fewer projections causes severe streak artifacts on tomographic images. Convolutional neural network (CNN) based artifact reduction methods have been proposed and outperformed conventional filter processing and compressed sensing approaches. However, CNNs are inefficient for capturing long-range characteristics. Vision Transformer, as an alternative to CNN, has recently appeared, and Swin Transformer achieved state-of-the-art performance on image classification tasks on benchmark datasets. SwinIR based on the Swin Transformer also outperformed the performance of CNN-based networks for image super-resolution tasks. In this paper, we proposed an artifact reduction method for sparse-view micro-CT. We developed a SwinIR-based deep learning network for vertically enlarging sparse-view sinograms to estimate full-view sinograms. Experimental results showed that the proposed method achieved the best performance with the lowest total error among the comparison methods.",Microwave integrated circuits;Micrometers;Three-dimensional displays;Semiconductor detectors;Superresolution;Transformers;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10338258,IEEE Conferences,,,,,,
Deep learning-based Radionuclide Identification for high temperature applications,M. Park; S. Kim; C. Park; S. Kim; C. Kim; J. . -Y. Yeom,"2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)",########,2023,"We propose a deep learning-based radionuclide identification algorithm capable of accurately identifying nuclear isotopes even with temperature fluctuations. To detect radiation at high temperatures, we fabricated a radiation detector using a ruggedized PMT and Ce:GPS scintillator, which demonstrates promising performance in high temperature conditions. Our proposed deep learning architecture comprises five hidden layers and one fully connected layer. The accuracy of the proposed radionuclide identification algorithm was verified using radiation sources of 137Cs, 57Co, and 133Ba. The trained deep learning model could identify all classes with a high accuracy of almost 100% from spectra obtained at 25 â„ƒ, 50 â„ƒ, 100 â„ƒ, and 150 â„ƒ. Furthermore, the model demonstrated a relatively high classification performance of 96% and 85.2% in identifying radionuclides from spectra obtained at 75 â„ƒ and 125 â„ƒ, respectively, which were not included in the training data. This finding validates that our suggested deep learning model can accurately identify nuclides, regardless of temperature fluctuations.",Deep learning;Semiconductor device modeling;Microwave integrated circuits;Fluctuations;Scintillators;Semiconductor detectors;Radiation detectors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10338128,IEEE Conferences,,,,,,
Explainable AI and Transfer Learning in the Classification of PET Cardiac Perfusion Polar Maps,S. M. Hosseini; R. M. Moghaddam; J. Schultz; A. Saraste; R. KlÃ©n; J. Teuho,"2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)",########,2023,"This study explores the application of transfer learning and explainable artificial intelligence for the classification of cardiac perfusion polar maps. The dataset consisted of 138 polar maps from 15O-H2O positron emission tomography (PET) myocardial perfusion imaging (MPI). A test accuracy of 0.869 was achieved with partially fine-tuned VGG19. Gradient-weighted class activation mapping (Grad-CAM) provided insight into the model's decision-making process. The results showed that while some predictions were based on clinically relevant features, others were based on irrelevant features, rendering them clinically invalid. These findings emphasize the need for further investigation into the validation of deep learning predictions for clinical use.",Deep learning;Microwave integrated circuits;Semiconductor detectors;Transfer learning;Decision making;Myocardium;Rendering (computer graphics),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10338595,IEEE Conferences,,,,,,
Transforming Healthcare: Raabin White Blood Cell Classification with Deep Vision Transformer,R. Rubin; S. M. Anzar; A. Panthakkan; W. Mansoor,2023 6th International Conference on Signal Processing and Information Security (ICSPIS),########,2023,"In the realm of computer vision, the application of deep learning techniques has brought about significant transformations, particularly in the domain of image classification. This study delves into an in-depth exploration of the Raabin White Blood Cell (WBC) dataset, leveraging the power of the Deep Vision Transformer (ViT), an advanced deep learning model, to advance the classification of white blood cells. Although the Raabin WBC dataset may be relatively lesser-known, its inherent value lies in its specialized focus on white blood cell images. To harness the potential of this dataset, we meticulously curated it for compatibility with DeepViT, followed by rigorous training and fine-tuning. Our efforts yielded exceptional results, with an impressive accuracy rate of 97%, thus surpassing previous benchmarks. This remarkable performance underscores the datasetâ€™s significance in the context of medical image analysis, highlighting its potential for various applications, particularly in the field of medical research and diagnosis. Furthermore, this study serves as a compelling testament to the adaptability and efficacy of DeepViT when applied to specialized datasets. These groundbreaking results accentuate the importance of exploring lesser-known datasets in conjunction with advanced deep learning models, as they hold the promise of delivering exceptional performance in specific domains. This work paves the way for further research and applications in medical image analysis, setting a benchmark for future white blood cell classification studies.",White blood cells;Deep learning;Training;Measurement;Benchmark testing;Signal processing;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10344258,IEEE Conferences,,,,,,
Beyond Labels: Visual Representations for Bone Marrow Cell Morphology Recognition,S. Fazeli; A. Samiei; T. D. Lee; M. Sarrafzadeh,2023 IEEE 11th International Conference on Healthcare Informatics (ICHI),########,2023,"Analyzing and inspecting bone marrow cell cytomorphology is a critical but highly complex and time-consuming component of hematopathology diagnosis. Recent advancements in artificial intelligence have paved the way for the application of deep learning algorithms to complex medical tasks. Nevertheless, there are many challenges in applying effective learning algorithms to medical image analysis, such as the lack of sufficient and reliably annotated training datasets and the highly class-imbalanced nature of most medical data. Here, we improve on the state-of-the-art methodologies of bone marrow cell recognition by deviating from sole reliance on labeled data and leveraging self-supervision in training our learning models. We investigate our approachâ€™s effectiveness in identifying bone marrow cell types. Our experiments demonstrate significant performance improvements in conducting different bone marrow cell recognition tasks compared to the current state-of-the-art methodologies1.",Training;Representation learning;Visualization;Pipelines;Morphology;Self-supervised learning;Bones,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10337224,IEEE Conferences,,,,,,
Deep Learning Networks for Breast Lesion Classification in Ultrasound Images: A Comparative Study,M. R. Ferreira; H. R. Torres; B. Oliveira; A. R. V. F. de AraÃºjo; P. Morais; P. Novais; J. L. VilaÃ§a,2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),########,2023,"Accurate lesion classification as benign or malignant in breast ultrasound (BUS) images is a critical task that requires experienced radiologists and has many challenges, such as poor image quality, artifacts, and high lesion variability. Thus, automatic lesion classification may aid professionals in breast cancer diagnosis. In this scope, computer-aided diagnosis systems have been proposed to assist in medical image interpretation, outperforming the intra and inter-observer variability. Recently, such systems using convolutional neural networks have demonstrated impressive results in medical image classification tasks. However, the lack of public benchmarks and a standardized evaluation method hampers the performance comparison of networks. This work is a benchmark for lesion classification in BUS images comparing six state-of-the-art networks: GoogLeNet, InceptionV3, ResNet, DenseNet, MobileNetV2, and EfficientNet. For each network, five input data variations that include segmentation information were tested to compare their impact on the final performance. The methods were trained on a multi-center BUS dataset (BUSI and UDIAT) and evaluated using the following metrics: precision, sensitivity, F1-score, accuracy, and area under the curve (AUC). Overall, the lesion with a thin border of background provides the best performance. For this input data, EfficientNet obtained the best results: an accuracy of 97.65% and an AUC of 96.30%.Clinical Relevanceâ€” This study showed the potential of deep neural networks to be used in clinical practice for breast lesion classification, also suggesting the best model choices.",Image quality;Image segmentation;Ultrasonic imaging;Sensitivity;Benchmark testing;Filling;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340293,IEEE Conferences,,,,,,
Multi-Level Swin Transformer Enabled Automatic Segmentation and Classification of Breast Metastases,A. Masood; U. Naseem; J. Kim,2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),########,2023,"Detection of metastatic breast cancer lesions is a challenging task in breast cancer treatment. The recent advancements in deep learning gained attention owing to its robustness, particularly in addressing automated segmentation and classification issues in medical images. In this paper, we proposed a modified Swin Transformer model (mST) integrated with a novel Multi-Level Adaptive Feature Fusion (MLAFF) Module. We constructed a modified Swin Transformer network comprising of a Local Transferable MSA (LT-MSA) and a Global Transferable MSA (GT-MSA) in addition to a Feed Forward Network (FFN). Our novel Multi-Level Adaptive Feature Fusion (MLAFF) module iteratively combines the features throughout multiple transformers. We utilized a pre-trained deep learning model U-Net and trained it on mammography utilizing Transfer Learning for automated segmentation. The proposed method, mST-MLAFF, is used for breast cancer classification into normal, benign, and malignant classes. Our model outperformed comparison methods based on U-Net and Swin Transformer in breast metastatic lesion segmentation on the seven benchmark datasets, namely INBreast, DDSM, MIAS, CBIS-DDSM, MIMBCD-UI, KAU-BCMD, and Mammographic Masses. Our model achieved 98% Dice-Similarity coefficient (DSC) for segmentation and an average of 94.5% accuracy for classification, whereas U-Net based model achieved 92% DSC and Swin Transformer achieved 93% DSC. Extensive performance evaluation of our model on benchmark datasets shows the potential of our model for breast cancer classification.Clinical relevanceâ€” This research work is focused on assisting the radiologist in the early detection and classification of breast cancer. A single mammography image is analyzed in less than a minute for automated segmentation and classification into malignant and benign classes.",Performance evaluation;Image segmentation;Adaptation models;Biological system modeling;Benchmark testing;Transformers;Delta-sigma modulation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340831,IEEE Conferences,,,,,,
Domain Adaptation and Feature Fusion for the Detection of Abnormalities in X-Ray Forearm Images,L. Alzubaidi; M. A. Fadhel; A. S. Albahri; A. Salhi; A. Gupta; Y. Gu,2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),########,2023,"The main challenge in adopting deep learning models is limited data for training, which can lead to poor generalization and a high risk of overfitting, particularly when detecting forearm abnormalities in X-ray images. Transfer learning from ImageNet is commonly used to address these issues. However, this technique is ineffective for grayscale medical imaging because of a mismatch between the learned features. To mitigate this issue, we propose a domain adaptation deep TL approach that involves training six pre-trained ImageNet models on a large number of X-ray images from various body parts, then fine-tuning the models on a target dataset of forearm X-ray images. Furthermore, the feature fusion technique combines the extracted features with deep neural models to train machine learning classifiers. Gradient-based class activation heat map (Grad CAM) was used to verify the accuracy of our results. This method allows us to see which parts of an image the model uses to make its classification decisions. The statically results and Grad CAM have shown that the proposed TL approach is able to alleviate the domain mismatch problem and is more accurate in their decision-making compared to models that were trained using the ImageNet TL technique, achieving an accuracy of 90.7%, an F1-score of 90.6%, and a Cohen's kappa of 81.3%. These results indicate that the proposed approach effectively improved the performance of the employed models individually and with the fusion technique. It helped to reduce the domain mismatch between the source of TL and the target task.",Training;Support vector machines;Adaptation models;Logistic regression;Biological system modeling;Transfer learning;Gray-scale,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340309,IEEE Conferences,,,,,,
Parkinsonian Tremor Detection with Compact Convolutional Transformer from Bispectrum Representation of tri-Axial Accelerometer Signals*,H. Alfalahi; A. A. Shehhi; C. Lamprou; I. Ziogas; E. Ganiti-Roumeliotou; A. H. Khandoker; L. J. Hadjileontiadis,2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),########,2023,"After the breakthroughs of Transformer networks in Natural Language Processing (NLP) tasks, they have led to exciting progress in visual tasks as well. Nonetheless, there has been a parallel growth in the number of parameters and the amount of training data, which led to the conclusion that Transformers are not suited for small datasets. This paper is the first to convey the feasibility of Compact Convolutional Transformers (CCT) for the prediction of Parkinsonian postural tremor based on the Bispectrum (BS) representation of IMU accelerometer time series. The dataset includes tri-axial accelerometer signals collected unobtrusively in-the-wild while subjects are on a phone call, and labelled by neurologists and signal processing experts. The BS is a noise-immune, higher-order representation that reflects a signalâ€™s deviation from Gaussianity and measures quadratic phase coupling. We performed comparative classification experiments using the CCT, pre-trained CNNs such as VGG-16 and ResNet-50, and the conventional Vision Transformer (ViT). Our model achieves competitive prediction accuracy and F1 score of 96% with only 1.016 M trainable parameters, compared to the ViT with 21.659 M trainable parameters, in a five-fold cross-validation scheme. Our model also outperforms pre-trained CNNs such as VGG-16 and ResNet-50. Furthermore, we show that the performance gains are maintained when training on a larger dataset of BS images. Our effort here is motivated by the hypothesis that data-efficient transformers outperform transfer learning using pre-trained CNNs, paving the way for promising deep learning architecture for small-scale, novel and noisy medical imaging datasets.Clinical relevanceâ€” Novel deep learning model for unobtrusive prediction of Parkinsonian Postural Tremor from Bispectrum image representation of tri-axial accelerometer signals collected in-the-wild.",Accelerometers;Visualization;Convolution;Transfer learning;Training data;Predictive models;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340646,IEEE Conferences,,,,,,
The Influence of Brain MRI Defacing Algorithms on Brain-Age Predictions via 3D Convolutional Neural Networks,R. J. Cali; R. R. Bhatt; S. I. Thomopoulos; S. Gadewar; I. B. Gari; T. Chattopadhyay; N. Jahanshad; P. M. Thompson,2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),########,2023,"In brain imaging research, it is becoming standard practice to remove the face from the individualâ€™s 3D structural MRI scan to ensure data privacy standards are met. Face removal - or â€˜defacingâ€™ - is being advocated for large, multi-site studies where data is transferred across geographically diverse sites. Several methods have been developed to limit the loss of important brain data by accurately and precisely removing non-brain facial tissue. At the same time, deep learning methods such as convolutional neural networks (CNNs) are increasingly being used in medical imaging research for diagnostic classification and prognosis in neurological diseases. These neural networks train predictive models based on patterns in large numbers of images. Because of this, defacing scans could remove informative data. Here, we evaluated 4 popular defacing methods to identify the effects of defacing on â€˜brain ageâ€™ prediction â€“ a common benchmarking task of predicting a subjectâ€™s chronological age from their 3D T1-weighted brain MRI. We compared brain-age calculations using defaced MRIs to those that were directly brain extracted, and those with both brain and face. Significant differences were present when comparing average per-subject error rates between algorithms in both the defaced brain data and the extracted facial tissue. Results also indicated brain age accuracy depends on defacing and the choice of algorithm. In a secondary analysis, we also examined how well comparable CNNs could predict chronological age from the facial region only (the extracted portion of the defaced image), as well as visualize areas of importance in facial tissue for predictive tasks using CNNs. We obtained better performance in age prediction when using the extracted face portion alone than images of the brain, suggesting the need for caution when defacing methods are used in medical image analysis.",Deep learning;Three-dimensional displays;Magnetic resonance imaging;Predictive models;Prediction algorithms;Convolutional neural networks;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340740,IEEE Conferences,,,,,,
The Effectiveness of Self-supervised Pre-training for Multi-modal Endometriosis Classification*â€,D. Butler; H. Wang; Y. Zhang; M. -S. To; G. Condous; M. Leonardi; S. Knox; J. Avery; M. L. Hull; G. Carneiro,2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),########,2023,"Endometriosis is a debilitating condition affecting 5% to 10% of the women worldwide, where early detection and treatment are the best tools to manage the condition. Early detection can be done via surgery, but multi-modal medical imaging is preferable given the simpler and faster process. However, imaging-based endometriosis diagnosis is challenging as 1) there are few capable clinicians; and 2) it is characterised by small lesions unconfined to a specific location. These two issues challenge the development of endometriosis classifiers as the training datasets tend to be small and contain difficult samples, which leads to overfitting. Hence, it is important to consider generalisation techniques to mitigate this problem, particularly self-supervised pre-training methods that have shown outstanding results in computer vision and natural language processing applications. The main goal of this paper is to study the effectiveness of modern self-supervised pre-training techniques to overcome the two issues mentioned above for the classification of endometriosis from multi-modal imaging data. We also introduce a new masking image modelling self-supervised pre-training method that works with 3D multi-modal medical imaging. Furthermore, to the best of our knowledge, this paper presents the first endometriosis classifier, fine-tuned from the pre-trained model above, which works with multi-modal (i.e., T1 and T2) magnetic resonance imaging (MRI) data. Our results show that self-supervised pre-training improves endometriosis classification by as much as 31%, when compared with classifiers trained from scratch.",Training;Solid modeling;Three-dimensional displays;Magnetic resonance imaging;Surgery;Transforms;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340504,IEEE Conferences,,,,,,
Comparison of Anatomical and Diffusion MRI for detecting Parkinsonâ€™s Disease using Deep Convolutional Neural Network,T. Chattopadhyay; A. Singh; E. Laltoo; C. P. Boyle; C. Owens-Walton; Y. -L. Chen; P. Cook; C. McMillan; C. -C. Tsai; J. -J. Wang; Y. -R. Wu; Y. van der Werf; P. M. Thompson,2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),########,2023,"Parkinsonâ€™s disease (PD) is a progressive neurodegenerative disease that affects over 10 million people worldwide. Brain atrophy and microstructural abnormalities tend to be more subtle in PD than in other age-related conditions such as Alzheimerâ€™s disease, so there is interest in how well machine learning methods can detect PD in radiological scans. Deep learning models based on convolutional neural networks (CNNs) can automatically distil diagnostically useful features from raw MRI scans, but most CNN-based deep learning models have only been tested on T1-weighted brain MRI. Here we examine the added value of diffusion-weighted MRI (dMRI) - a variant of MRI, sensitive to microstructural tissue properties - as an additional input in CNN-based models for PD classification. Our evaluations used data from 3 separate cohorts - from Chang Gung University, the University of Pennsylvania, and the PPMI dataset. We trained CNNs on various combinations of these cohorts to find the best predictive model. Although tests on more diverse data are warranted, deep-learned models from dMRI show promise for PD classification.Clinical Relevanceâ€” This study supports the use of diffusion-weighted images as an alternative to anatomical images for AI-based detection of Parkinsonâ€™s disease.",Deep learning;Atrophy;Magnetic resonance imaging;Biological system modeling;Predictive models;Brain modeling;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340792,IEEE Conferences,,,,,,
Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimerâ€™s Disease Detection,N. J. Dhinagar; S. I. Thomopoulos; E. Laltoo; P. M. Thompson,2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),########,2023,"Neuroimaging of large populations is valuable to identify factors that promote or resist brain disease, and to assist diagnosis, subtyping, and prognosis. Data-driven models such as convolutional neural networks (CNNs) have increasingly been applied to brain images to perform diagnostic and prognostic tasks by learning robust features. Vision transformers (ViT) - a new class of deep learning architectures - have emerged in recent years as an alternative to CNNs for several computer vision applications. Here we tested variants of the ViT architecture for a range of desired neuroimaging downstream tasks based on difficulty, in this case for sex and Alzheimer's disease (AD) classification based on 3D brain MRI. In our experiments, two vision transformer architecture variants achieved an AUC of 0.987 for sex and 0.892 for AD classification, respectively. We independently evaluated our models on data from two benchmark AD datasets. We achieved a performance boost of 5% and 9-10% upon fine-tuning vision transformer models pre-trained on synthetic (generated by a latent diffusion model) and real MRI scans, respectively. Our main contributions include testing the effects of different ViT training strategies including pre-training, data augmentation and learning rate warm-ups followed by annealing, as pertaining to the neuroimaging domain. These techniques are essential for training ViT-like models for neuroimaging applications where training data is usually limited. We also analyzed the effect of the amount of training data utilized on the test-time performance of the ViT via data-model scaling curves.Clinical Relevanceâ€” The models evaluated in this work could be trained on neuroimaging data to assist in diagnosis, subtyping and prognosis of Alzheimer's disease.",Neuroimaging;Training;Magnetic resonance imaging;Training data;Computer architecture;Transformers;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341190,IEEE Conferences,,,,,,
Development of Deep Learning Model Base on Modified CNN Architectures for Brain Tumours Early Diagnosis,Z. Indra; Y. Jusman; R. Kurniawan,2023 3rd International Conference on Electronic and Electrical Engineering and Intelligent System (ICE3IS),########,2023,"Brain cancer is classified as a malignant tumours that can spread to other parts of the human body, especially the spine and brain instantaneously. Brain tumour has been considered as the biggest problem for human health due to the high mortality rate it has. However, not all brain tumours are classified as malignant tumours and can be treated properly before they mutate to be malignant. Unfortunately, brain tumours is very difficult to detect early because it often resembles normal tissue. Therefore, this disease is often detected when it has reach a critical phase and difficult to treat. This study aims to develop a Deep Learning model for an automatic classification system in order to help early detection of brain tumours by utilizing the CNN algorithm with several popular architectures such as EfficientNetB0, MobileNetV2, and Xception. In addition, to improve the performance of the developed Deep Learning model, modifications were made to the CNN architecture by adding several additional layers. Based on the results obtained, this study managed to achieve very good performance and surpassed previous research benchmarks with accuracy, precision and recall values above 97%.",Deep learning;Electrical engineering;Architecture;Buildings;Benchmark testing;Brain modeling;Mobile applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335095,IEEE Conferences,,,,,,
Challenges on Deep Learning Models to Improve Magnetic Resonance Imaging Brain Tumor Images Classification: A Review,F. H. A. Anjasmara; I. Soesanti,2023 3rd International Conference on Electronic and Electrical Engineering and Intelligent System (ICE3IS),########,2023,"Brain tumor is one of the mortal cancers in the world. A brain tumor can be diagnosed by invasive or non-invasive approaches. Nowadays, magnetic resonance imaging or MRI is the most non-invasive approach to do anatomical neuroimaging. Brain tumor diagnosis using deep learning models has robust improvement in classification. This review provides the recent research topics in last the five years. There are some developments in deep learning models: augmentation, architecture modification, detection, segmentation, and ensemble. Augmentation helps generating more data to train a model, architecture modification adapts to solve feature extraction and selection, and detection or localization assists feature extractor with segmented images. Those methods sometimes work with image processing on MRI images, to boost feature extraction process. According to literature review, architecture modification is the most popular approach. After all, the main purpose of all models is improving classification accuracy. By classifying many deep learning methods and datasets, both help to analyze challenge and opportunity on brain tumor classification based deep learning methods. In conclusion, the best start to the research in MRI brain tumor classification using deep learning is developing a model. Hopefully, this paper can be a brief summary for novice researchers.",Deep learning;Neuroimaging;Location awareness;Image segmentation;Adaptation models;Magnetic resonance imaging;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335482,IEEE Conferences,,,,,,
Transformer-Based Unsupervised Image Registration Using SSIM and Homography Loss for Steady Camera and Aerial Videos,G. Abdollahinejad; M. Hashemi,2023 31st International Conference on Electrical Engineering (ICEE),########,2023,"Image registration is an essential and initial block in the pipeline of computer vision tasks and systems. It is defined as transforming a moving image into a target image with the minimum difference when aligned. Unlike previous work for general-purpose datasets, aerial images suffer from mechanical shakes, leading to deformed distortion similar to medical volumetric image registration task. Inspired by medical approaches, we use a Transformer-based network with a semi-unlimited receptive field Swin block to produce a general output for each pixel named flow matrix. Flow matrix is utilized instead of regressing parameters of transformation matrix with a fixed degree of freedom that cannot handle the structural difference between images. This leads to introducing a new loss function based on the Structural Similarity Index Measure (SSIM) and embedding Homography transformation as a regularization term. Combining a generalized-designed network and loss function based on problem definition significantly enhanced results.",Electrical engineering;Image registration;Pipelines;Dogs;Transformers;Loss measurement;Indexes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10334862,IEEE Conferences,,,,,,
"Disease Recognition in Medical Images Using CNN-LSTM-GRU Ensemble, a Hybrid Deep Learning",A. Makandar; M. N. Jadhav,2023 7th International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS),########,2023,"This study introduces a pioneering web application development framework poised to revolutionize healthcare management, elevate patient outcomes, and optimize disease diagnosis processes. Leveraging cutting-edge deep learning techniques, pretrained models, and a user-friendly interface, this framework equips medical professionals with a potent toolset for precise and efficient decision-making. By fusing deep learning methodologies such as CNN-LSTM and CNN-GRU, the framework adeptly captures both spatial and temporal nuances in sequential medical data. Its primary focus lies on the accurate diagnosis of prevalent diseases, including lung cancer, pneumonia, and tuberculosis. The integration of ensemble techniques enhances the reliability of disease classification, achieving remarkable accuracy, precision, and recall rates across diverse illness categories. Furthermore, the development of an intuitive user interface ensures that healthcare practitioners can harness the power of this technology without the need for extensive technical expertise. This accessibility empowers clinicians to expedite critical decisions with unprecedented efficiency and accuracy, ultimately improving patient care and clinical outcomes. This study serves as a testament to the transformative potential of state-of-the-art technologies within the medical domain. As the application is scalable and adaptable to accommodate growing medical image datasets, it paves the way for the continuous evolution of more effective and efficient disease detection systems. In an era marked by technological advancement, this innovation marks a significant stride towards the future of healthcare, where advanced deep learning solutions play a pivotal role in transforming patient care and diagnosis.",Deep learning;Visualization;Tuberculosis;Pulmonary diseases;Lung cancer;Feature extraction;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10334080,IEEE Conferences,,,,,,
A Deep Pair Siamese CNN for Multi-Class Classification of Alzheimer Disease.,H. Alaeddine; A. G. Blaiech; M. Jihene,2023 International Conference on Cyberworlds (CW),########,2023,"Alzheimerâ€™s disease is a neurodegenerative disease characterized by a progressive loss memory and certain intellectual (cognitive) functions leading to repercussions in the activities of daily living. Early diagnosis of Alzheimerâ€™s disease is a difficult task for researchers. In clinical research, magnetic resonance imaging (MRI) is used to diagnose Alzheimerâ€™s disease. MRI can detect cortical atrophy and in particular atrophy of the hippocampi. Approaches based on deep convolutional neural network (CNN) and machine learning represent one solution and they are readily available and described to solve various problems related to the analysis of brain image data. High-dimensional classification approaches have been widely used to study magnetic resonance imaging (MRI) data for automatic classification of Alzheimerâ€™s disease (AD). In this work, we proposed a Deep Siamese Convolutional Neural Network model for a Multi-class Classification of Dementia Stages in Alzheimerâ€™s Disease. The experiments are carried out on the OASIS database accessible free of charge to the public. We compared our model with the best models and found that the proposed model outperforms the best models in terms of different performance.",Atrophy;Databases;Magnetic resonance imaging;Machine learning;Brain modeling;Convolutional neural networks;Alzheimer's disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10337609,IEEE Conferences,,,,,,
Detecting Gastro-Intestinal Cancer from Wireless Capsule Endoscopy Images using Efficient Net Model,S. Geetha; V. Sharmila; S. Sasikala; S. A. A. Balamurugan; N. M. Balamurugan,2023 14th International Conference on Information & Communication Technology and System (ICTS),########,2023,"Recently polyps and ulcers have become a fatal form of gastrointestinal problem, and several researchers have looked into algorithms to diagnose gastrointestinal cancer lesions. This research work investigates the application of CNN to detect Gastro-Intestinal (GI) cancer from wireless capsule endoscopy (WCE) GI images. Five distinct CNN architectures EfficientNet, LeNet, GoogleNet, MobileNet-V2, and ResNet-50 have been studied, with two different setups i) original data with transfer learning ii) original data with data augmentation, and transfer learning. Simulations ran on 37,790 images with five different CNN architectures proved that the EfficientNet model exhibited superior performance of about 99.15% accuracy over all other CNN models, on the augmented dataset. Furthermore, the results from the experiments show that the EfficientNet model significantly outperforms previous methods in terms of classification accuracy.",Wireless communication;Endoscopes;Biological system modeling;Transfer learning;Neural networks;Data augmentation;Gastrointestinal tract,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330886,IEEE Conferences,,,,,,
Deep Learning Deployment on Big Data Infrastructure Using Apache Spark (Case Study: COVID-19 Detection Using X-Ray Images),A. Munif; H. Ramadhani,2023 14th International Conference on Information & Communication Technology and System (ICTS),########,2023,"It is possible to use GPU (Graphic Processing Unit) to increase deep learning performance. This requires us to invest in separate GPUs, which can be relatively expensive. However, if we already have big data infrastructures, it is possible to deploy deep learning on top of them. We utilize the BigDL library on the Apache Spark cluster to run deep learning tasks. BigDL is different from traditional deep learning as it implements distributed and parallel processing. This allows for horizontal scaling of workers using BigDL, resulting in faster training times. Simulation testing on the Apache Spark cluster can use deep learning applications with the transfer learning method, leveraging pre-existing models such as InceptionVl. Deep learning can be developed using the BigDL framework. We use a case study of medical image classification for COVID19 detection. Based on the experiments, the deployment model using BigDL on the Apache Spark infrastructure achieved an average accuracy of 92%, and the average running time is 2 hours, 23 minutes, and 28 seconds.",Deep learning;COVID-19;Scalability;Transfer learning;Graphics processing units;Cluster computing;Big Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330842,IEEE Conferences,,,,,,
Performance Analysis of Deep Learning Models over BreakHis Dataset using Up-Sampling and Down-Sampling Techniques for Classification of Breast Cancer,M. Ahirwar; A. Agrawal,2023 9th International Conference on Smart Computing and Communications (ICSCC),########,2023,"Itâ€™s very challenging to determine the actual cause of breast cancer, therefore early diagnosis is crucial to lowering the diseaseâ€™s fatality rate. The likelihood of survival increases by up to 8% with early cancer identification. Even seasoned radiologists, however, struggle to recognize characteristics like micro-classifications, lumps, and masses, which results in significant false positive and false negative rates. In recent years we have seen a lot of development in deep learning and image processing which gives rise to some optimism for the creation of improved applications for the early diagnosis of breast cancer. In my work, the performance of a Convolutional Neural Network (CNN) and Visual Geometry Group 16 (VGG-16) was analyzed on a breast cancer dataset using upsampling and downsampling techniques. Upsampling involves increasing the number of instances in the minority class (in this case, breast cancer cases) by duplicating them or by performing some other data augmentation technique, while downsampling involves reducing the number of instances in the majority class (non-breast cancer cases) by randomly removing some of them. The purpose of these techniques is to address imbalanced datasets, where one class significantly outnumbers the other. The results of the analysis showed that CNN performed better on the balanced datasets achieved through upsampling and downsampling.",Geometry;Deep learning;Visualization;Image processing;Computational modeling;Data augmentation;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10334935,IEEE Conferences,,,,,,
A Hybrid CNN-LSTM Network For Brain Tumor Classification Using Transfer Learning,S. K. Rajeev; M. P. Rajasekaran; K. Ramaraj; G. Vishnuvarthanan; T. Arunprasath; V. Muneeswaran,2023 9th International Conference on Smart Computing and Communications (ICSCC),########,2023,"Brain tumor analysis is a major area in medical imaging that necessitates precise and efficient techniques for early detection and diagnosis. Magnetic resonance imaging (MRI) is a popular diagnostic method for detecting and characterizing brain tumors. However, the accurate and reliable analysis of MRI images to diagnose brain tumors remains a challenging task, even for experienced radiologists. Deep Learning (DL) has shown remarkable success in medical image analysis tasks like tumor segmentation, classification, and detection. In this proposed method the MRI images are skull stripped and then pre-processed using Gaussian wavelet filter and a pre-trained model AlexNet is used for feature extraction. These extracted features are trained and classified into four classes using a hybrid CNN-LSTM deep learning model. The modelâ€™s performance was analyzed for different parameters and it was found that the proposed model has provided better results when compared with the existing models. The validation accuracy obtained by the model is 97.94%.",Deep learning;Analytical models;Magnetic resonance imaging;Transfer learning;Brain modeling;Feature extraction;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335082,IEEE Conferences,,,,,,
Modification Detection Of Medical Images using ORB Feature Extraction Algorithm and Logistic Regression,A. Mohammed Najeebulla; K. Muhammed Arfab; M. Shadi; S. Shahad; T. Swathi,2023 9th International Conference on Smart Computing and Communications (ICSCC),########,2023,"This paper presents an approach for the detection of modifications in DICOM images using the ORB algorithm combined with logistic regression. The goal is to accurately identify whether a DICOM image has been modified. Features are extracted using the ORB algorithm, and a logistic regression classifier is trained on these features. Padding techniques are used for consistent image sizes, and bagging is applied to improve the modelâ€™s performance. Experimental results demonstrate an 85 percent accuracy in detecting modifications. Future work involves exploring deep learning approaches and additional feature extraction algorithms. This approach contributes to maintaining data integrity in medical image repositories and supports image forensics in healthcare applications.",Deep learning;Logistic regression;Image forensics;Data integrity;Medical services;Feature extraction;Classification algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335031,IEEE Conferences,,,,,,
Detection of different progression stages of Alzheimerâ€™s disease using SURF,A. Francis; I. A. Pandian; J. Anitha,2023 9th International Conference on Smart Computing and Communications (ICSCC),########,2023,"The proposed model that utilizes speeded-up robust features (SURF) and multi-layer perceptron (MLP) for the classification of the four stages of dementia is a promising development. The four stages of dementia are cognitively normal, Alzheimerâ€™s disease, mild cognitive impairment convertible, mild cognitive impairment non-convertible. Accurate classification of these stages is crucial for early detection and intervention of Alzheimerâ€™s disease, as early treatment can slow down memory loss. The use of SURF, a widely used feature descriptor in computer vision, for Alzheimerâ€™s disease detection is innovative and shows potential. The MLP, which is commonly used for classification tasks, is also suitable for this problem. However, it is important to validate the modelâ€™s performance on larger datasets to ensure its reliability and effectiveness. This study demonstrates the potential of machine learning and computer vision techniques in assisting with the diagnosis and management of Alzheimerâ€™s disease.",Computational modeling;Magnetic resonance imaging;Feature detection;Machine learning;Feature extraction;Real-time systems;Computational efficiency,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335035,IEEE Conferences,,,,,,
Applying Transfer Learning on 3D Brain Images and an MLOPS Study for Deployment,N. Mathew; C. T. Joseph,2023 9th International Conference on Smart Computing and Communications (ICSCC),########,2023,"Millions of life can be saved worldwide if there is a possibility to identify the brain tumor at an early stage and if it can be integrated cost effectively with the medical imaging system. The MRI images which are generated for other treatments like accidents, pain therapy etc also can be automatically analyzed in detail helping the doctors to identify the initial stages of brain tumor even in unsuspected cases. But it is always challenging to find a perfect algorithm with a high accuracy to detect the brain tumors at an earlier stage. In this study we propose to use the transfer learning combined with ResGANet model for an improved accuracy. The algorithm was evaluated on BRATS-2020 and BRATS-2021 data sets. The final aim of any machine learning project is to develop the product and bring this into solving real world problems very quickly. Normally this is not accomplished at normal hospitals due to the complexity of integration and the lack of deep computer science skills. This study proposes a cost-effective method for deploying this model in google cloud which can be integrated with any hospital system and can be operated by normal technicians with little effort on training.",Training;Machine learning algorithms;Three-dimensional displays;Hospitals;Pain;Magnetic resonance imaging;Transfer learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335014,IEEE Conferences,,,,,,
Vision Transformer in Oral Cancer Detection,P. Agarwal; N. Gupta; Y. Bharadwaj; A. Yadav; P. Mathur,2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),########,2023,"Oral cancer is a malignancy that can manifest in multiple areas of the mouth, including the tongue, lips, oral mucosa, gums, floor of the mouth, and other soft tissues. Despite the development of advanced diagnostic and therapeutic methods, the incidence and prevalence of oral cancer continue to be a cause for concern. This condition can have devastating effects on patients, including significant physical and emotional distress. Detecting oral cancer at an early stage can improve a patient's chances of survival and reduce the need for aggressive treatment. In recent years, machine learning techniques have demonstrated promising results in the detection of oral cancer. This research aims to explore the role of vision transformers in oral cancer classification as benign and malignant. The results obtained from the vision transformer are compared with Machine learning techniques like SVM, extreme Gradient Boosting (XGBoost), and Random Forest. Overall, the results show that ViT outperformed SVM, Random Forest, and XGBoost in oral cancer detection and stage classification with an accuracy of 96.7%. The accuracy, precision, recall, and F1-score of ViT are higher than other methods. The findings of the research show that all four machine learning methods examined can produce high precision rates in detecting oral cancer, with the Vision Transformer technique achieving superior results as vision transformers are designed to capture global context information in images. The proposed system has great potential to advance the precision and efficiency of oral cancer diagnosis, which can lead to more prompt and effective treatment.",Support vector machines;Tongue;Machine vision;Lips;Mouth;Learning (artificial intelligence);Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331857,IEEE Conferences,,,,,,
Brain Tumour Ordering Using Visual Geometry Group Network,S. Balabantaray; M. Rath; T. Samal; M. R. Mishra,2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),########,2023,"A person's life may be significantly impacted negatively by brain tumours as they can impair cognitive function, causing memory loss, trouble concentrating, and personality or behaviour changes. By examining medical imaging data such as magnetic resonance imaging (MRI) scans, classification algorithms can be trained to learn and identify outlines and structures in the pictures that are associated with the existence or nonappearance of a brain tumor and to diagnose it. The general classification challenges in brain tumor classification include tumor heterogeneity, imaging technique limitations, limited sample size, lack of standardization, and incomplete understanding of the underlying biology. In the past, researchers have used various techniques such as traditional machine learning algorithms, deep learning models, and ensemble methods to classify brain tumors. These limitations can often lead to erroneous accuracy at times even after thorough training on a model which may hamper the desired output required for a medical diagnosis. This paper focuses on applying preprocessing methods to the MRI scans of brain tumor in order to establish uniformity among the inputs. Our main objective is to apply a texture descriptor classification which. Thus, our novel approach is focused on an improved algorithm which will prove to be robust to monotonic gray-scale changes caused. The proposed strategy intends to improve the inputs, which will be integrated into a classification algorithm with the efforts to obtain precise results and improved accuracy. The superiority and diversity of the preparation data, as well as the particular parameters and hyperparameters used to train the model, are all factors that affect how well these models function.",Geometry;Training;Visualization;Magnetic resonance imaging;Computational modeling;Biological system modeling;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331783,IEEE Conferences,,,,,,
Brain MRI Classification Using Transfer Learning Techniques,D. Paranjpe; S. Vikram; S. Sonawane; A. Deshpande,2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),########,2023,"Brain tumors must be grouped to be diagnosed and treated correctly. Deep learning methods have recently showed potential in the classification of brain cancers using diagnostic pictures like magnetic resonance imaging (MRI) scans. This work demonstrates the obstacles and future objectives for classifying brain cancers using deep learning. It talks about what has been done recently in this area. This research study discusses how convolutional neural networks (CNNs) and some variations, such as transfer learning and ensemble models, can be used to classify brain tumors. Also, this study compares how well different CNN architectures work on different kinds of brain cancer, like meningiomas, gliomas, and pituitary tumors. Lastly, discusses about what is wrong with and hard about the current deep learning-based approaches and suggest new ways to look into them.",Training;Deep learning;Pathology;Magnetic resonance imaging;Transfer learning;Brain cancer;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331713,IEEE Conferences,,,,,,
Breast Cancer Prediction Using a Deep Learning Algorithm on the Cloud Medical Data,C. Tamilselvi; M. Sindhuja; S. Hemamalini; S. L. Jemina; B. Dhanwanth; M. T A,2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),########,2023,"Recent strides in data generation have given rise to an unprecedented surge in information accumulation, marking the advent of the big data era. Unfortunately, conventional machine learning algorithms find it challenging to adeptly manage the distinctive characteristics presented by big data. This study is centered on the task of breast cancer prediction in the midst of this data-rich landscape. Specifically, we delve into two pivotal data types: gene expression (GE) and DNA methylation (DM).Our objective revolves around unlocking the potential of Deep Learning algorithms, both when applied individually to each dataset and when they come together in synergy. For this purpose, we've elected MATLAB as our primary platform. At the heart of breast cancer prediction, we employ the Convolutional Neural Network (CNN) algorithm, renowned for its proficiency in pattern recognition and feature extraction. Our study is underpinned by a comprehensive analysis that encompasses GE, DM, and the amalgamated GE and DM datasets.Furthermore, we gauge the performance of the CNN algorithm by placing it head-to-head with the Random Forest (RF) algorithm, a widely adopted choice in machine learning for classification tasks. The outcomes of our experiments vividly underscore the superiority of our approach, especially when we harness the potential of the GE dataset. This particular configuration yielded the highest accuracy and the most cost-effective results among all the classifiers considered. These findings serve as a beacon, illuminating the remarkable efficacy of Deep Learning techniques in the domain of breast cancer prediction, particularly when set against the backdrop of big data. This study underscores the remarkable potential of Deep Learning algorithms as they navigate the complex healthcare landscape and promises more accurate, cost-efficient diagnostic tools on the horizon.",Deep learning;Machine learning algorithms;Medical services;Big Data;Germanium;Prediction algorithms;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331863,IEEE Conferences,,,,,,
Alzheimer Disease Detection using Deep Learning,R. Singh; C. Prabha; H. M. Dixit; S. Kumari,2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),########,2023,"Alzheimer's disease represents a significant global health challenge, with accurate diagnosis being a critical factor in effective treatment. MRI has emerged as a potent tool for early detection and monitoring, given its non-invasive nature and the high-quality images it provides. This study introduces an innovative method for detecting Alzheimer's disease, leveraging the fine-tuned EfficientNet-B5 model, which was trained using the Augmented Alzheimer's MRI Dataset V2. The proposed model using Deep CNN has shown acceptable performance. The model is fine-tuned to identify subtle patterns and anomalies within MRI scans linked to Alzheimer's disease. By employing the Augmented Alzheimer's MRI Dataset V2 for training and evaluation, the model's robust adaptability and heightened diagnostic precision are ensured. The proposed system achieved 96.64% accuracy. This outcome underscores both the clinical promise of the method proposed and the effectiveness of employing deep learning in the realm of medical image analysis. Importantly, this method has the potential to enhance early Alzheimer's diagnosis and management, ultimately leading to improved patient outcomes and an enhanced quality of life.",Deep learning;Training;Adaptation models;Image analysis;Magnetic resonance imaging;Alzheimer's disease;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331661,IEEE Conferences,,,,,,
Retinal Vessel Classification and Segmentation Using Advanced Machine Learning Techniques,N. Ragavendran; G. Aravind Swaminathan; Nithin; M. Mariammal; B. Diviyapriya; Euodial,2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),########,2023,"The automated segmentation of blood vessels in retinal fundus images plays a critical role in the field of medical imaging analysis. To create a decision support system to help in the initial diagnosis and treatment of retinal sickness, precise vascular segmentation is regarded to be crucial. Using fuzzy edge detection and learning approaches, this research creates a system that can segment and classify retinal blood vessels. To begin, the proposed RBVSC-FEDLM model employs a contrast enhancement method. In addition to employing GOA to fine-tune the membership functions, a Fuzzy Cluster Network Algorithm for Edge Detection (FCNAED) technique is employed to locate the edges of retinal fundus images. The ORB (Oriented FAST and Rotated BRIEF) feature extractor is used for the extraction of feature vectors. Finally, the use of Auto Encoder (AE) for categorizing retinal images is a novel contribution to the field. This study uses a benchmark dataset to verify the RBVSC-FEDLM model's accuracy, and found that it outperforms state-of-the-art alternatives.",Machine learning algorithms;Image edge detection;Clustering algorithms;Blood vessels;Retina;Feature extraction;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331799,IEEE Conferences,,,,,,
Comparative Evaluation of Transfer Learning Models for Knee Osteoarthritis Severity Classification Using KL Grading Scale,T. Sivakumari; R. Vani,2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),########,2023,"Knee Osteoarthritis (KOA) is a prevalent and debilitating joint condition that affects millions of individuals worldwide. Early and accurate diagnosis of KOA severity is crucial for effective treatment planning and patient management. One widely accepted method for assessing KOA severity is the Kellgren-Lawrence (KL) grading scale, which categorizes KOA into different stages based on radiographic evidence from knee X-ray images. This study addresses the critical need for an efficient and accurate classification system for KOA severity, leveraging the power of deep learning and transfer learning techniques. The primary objective of this research work is to identify the most effective model for classifying KOA severity levels from knee X-ray images using the KL grading scale. This study has encountered significant challenges related to preprocessing of knee images and system computation time. Initially, knee images required intricate preprocessing steps to enhance data quality, and the computational time posed constraints on real-time applications. To overcome these challenges, this study has employed advanced techniques and conducted a comprehensive performance analysis of various deep learning models, including NasNetLarge, AlexNet, Inception V3, DenseNet, Mobile Net, and ResNet. These models have shown significant promise in various image classification tasks, making them compelling candidates for the task of KOA severity classification.",Deep learning;Analytical models;Computational modeling;Transfer learning;Real-time systems;Planning;Time factors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331895,IEEE Conferences,,,,,,
Analysis of Acute Lymphoblastic Leukemia Detection Methods Using Deep Learning,P. K. Talupuri; B. Naseeba; N. P. Challa; A. S. Sathwik,2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),########,2023,"This research work puts forward a comparative study of four prominent deep learning models - ResNet, InceptionNet, MobileNet and EfficientNet â€” for the classification and detection of Acute Lymphoblastic Leukemia (ALL) from microscopic single blood cell images. Leukemia, a critical hematological malignancy, demands accurate and swift diagnosis to facilitate effective treatment. The advent of deep learning has revolutionized medical image analysis, enabling automated and efficient disease detection. In this work, we evaluate the performance of ResNet, InceptionNet, MobileNet, and EfficientNet, all of which have demonstrated exceptional capabilities in various computer vision tasks. The proposed study involves the construction of a dataset containing diverse blood cell images, which then undergoes preprocessing and augmentation to ensure model robustness and generalization. Subsequently, the four deep learning architectures are implemented, pretrained on large-scale image datasets, and fine-tuned on the leukemia dataset. Training, validation, and testing phases are conducted under controlled experimental conditions. The results reveal nuanced differences in the performance of ResNet, InceptionNet, MobileNet, and EfficientNet for leukemia detection and classification. The evaluation metrics provide insights into their strengths and limitations, helping guide selection based on specific application requirements. This study clarifies how various architectures impact model performance in the context of medical image analysis.",Deep learning;Training;Analytical models;Image analysis;Computational modeling;Microprocessors;Computer architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331776,IEEE Conferences,,,,,,
Pneumothorax: Lung Segmentation and Disease Classification Using Deep Neural Networks,D. S. V. Kancherla; P. Mannava; S. Tallapureddy; V. Chintala; K. P; C. Iwendi,2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),########,2023,"Medical imaging is crucial in detecting pneumothorax, a potentially life-threatening condition characterized by air accumulation in the pleural cavity. Chest X-Rays (CXR) and Computed Tomography (CT) scans are vital tools for diagnosing pneumothorax, offering detailed insights into the chest cavity. The accurate delineation of lung regions and the detection of lung diseases pose significant challenges. Variations in patient anatomy, limited training data, and the complexity of lung diseases contribute to the difficulty of the task. Recent approaches, such as advanced U-Net architectures with residual connections and 3D U-Net variants for lung nodule segmentation, are enhancing the precision and efficiency of lung image analysis. This research presents a novel approach that combines Deep Learning (DL) based segmentation and disease detection techniques to enhance accuracy in CXR image analysis. The proposed methodology comprises two core components: A U-Net-inspired segmentation model with residual connections for precise lung region extraction and a Convolutional Neural Network (CNN)for disease detection.",Image segmentation;Image analysis;Pulmonary diseases;Diversity reception;Lung;Computer architecture;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331853,IEEE Conferences,,,,,,
CX-Net: Multi-scale Enhanced Fusion Network for Pneumonia Classification,C. Liu; C. Xu; C. Nie; J. Han; H. Ma; D. Wang,2023 IEEE 3rd International Conference on Computer Systems (ICCS),########,2023,"Due to the blurred nature of lung X-ray images and the variability in lesion density and lesion location, even relatively experienced radiologists can have diagnostic difficulties. Deep Learning (DL) can quickly and accurately determine the type of pneumonia from an X-ray. However, the methods proposed so far will extract many invalid features during extraction, and the pneumonia lesions are location-specific, and the features around the lesions often affect the final classification accuracy. Therefore, this paper proposes a multi-scale local enhancement model (CX-Net) consisting of a multi-scale fusion module, a local channel attention module, and a lung enhancement module to solve this problem. The local channel attention module (LCAM) performs local data evaluation on the features extracted by the encoding network ResNet-50 to assign weights to the feature layer. The Lung Enhancement Module (LEM) enhances the data of possible diseased areas to highlight the specific features of the lung lesion location. The enhanced features are multi-scale superimposed to obtain a matrix with attentional bias, thus avoiding the influence of useless features to a greater extent. In this paper, different public data sets in Kaggle are merged into a binary classification dataset, a three-classification dataset, and a four-classification dataset to test the performance of the model. Finally, it is shown through extensive experiments that the results of the proposed model in this paper outperform the current state-of-the-art methods. The models proposed in the two classifications, three classifications, and four classifications achieved a classification accuracy of 98.34%, 97.62%, and 94.87%.",Deep learning;Image analysis;Pulmonary diseases;Computational modeling;Lung;Feature extraction;Encoding,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335527,IEEE Conferences,,,,,,
"Deep Learning-Based Dermatological Condition Detection: A Systematic Review With Recent Methods, Datasets, Challenges, and Future Directions",S. S. Noronha; M. A. Mehta; D. Garg; K. Kotecha; A. Abraham,IEEE Access,########,2023,"Dermatological conditions are a global health concern affecting all age groups. They encompass various skin issues such as rashes, moles, acne, blisters, hives, nodules, cysts, macules, and papules. Early diagnosis of dermatological conditions is crucial to prevent skin damage and chronic diseases. Recent advancements in artificial intelligence and medical image processing, particularly through deep learning, have significantly improved the precision and efficiency of dermatological disease detection by dermatologists. This paper thoroughly examines deep learning-based methods for detecting dermatological conditions from dermoscopic images. Specifically, it presents study of 22 methods that are used to detect dermatological conditions such as basal cell carcinoma, melanocytic nevus, seborrheic keratosis, psoriasis, benign keratosis, melanoma, acne, cold sore, warts, eczema, hives, shingles, scar, skin tag, inflammatory hyper pigment, cyst, dark circle, blackhead, burn and skin rash. It also covers clinical diagnostic methods for dermatological conditions and the need for computer-aided diagnosis. In this paper, we have also proposed the categorization of deep learning-based dermatological condition detection methods. Moreover, a comprehensive summary of these methods is presented. In addition, this paper summarizes the majority of the datasets available for computer-aided detection of dermatological conditions. Furthermore, it presents enormous challenges and potential future research directions. This survey informs researchers about the latest advancements in deep learning-based detection methods for dermatological conditions and allows dermatologists to stay updated on technological breakthroughs in this area.",Surveys;Skin;Learning systems;Deep learning;Dermatology;Basal cell carcinoma;Artificial intelligence;Convolutional neural networks;Dermatology;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342692,IEEE Journals,,,,,,
Attention-Based Multiscale Deep Neural Network for Diagnosis of Polycystic Ovary Syndrome Using Ovarian Ultrasound Images,S. Rashid; M. Karnati; G. Aggarwal; M. K. Dutta; P. Sikora; R. Burget,2023 15th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT),########,2023,"Polycystic Ovary Syndrome (PCOS) is a hormonal disorder that impacts a significant proportion of women in their reproductive years. It results in irregular menstrual cycles and elevated levels of androgens, known as male hormones. Women with PCOS often have ovaries that develop numerous small fluid-filled sacs called follicles, but they fail to release eggs regularly. While the precise cause of PCOS remains unknown, early identification and weight loss can help mitigate the risk of long-term complications. In this study, a novel attention-based multiscale convolutional neural network (AMCNN) is proposed for the detection of PCOS. The utilization of dilated convolution aids in preserving the multi-scale features with fewer parameters. The integration of multiscale characteristics is achieved by the attention mechanism, which enhances the importance of features within significant channels. The experimental results demonstrate the superior performance of the AMCNN, surpassing other prominent algorithms with an accuracy of 98.79%, proving its effectiveness for medical industrial applications.",Training;Performance evaluation;Ultrasonic imaging;Ultrasonic variables measurement;Magnetic resonance imaging;Manuals;Elasticity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10333275,IEEE Conferences,,,,,,
Classification and Segmentation of Brain Tumor MRI Images Using Convolutional Neural Networks,C. B. Ruiz,2023 IEEE International Conference on Engineering Veracruz (ICEV),########,2023,"The precise identification and segmentation of brain tumors play a crucial role in medical image analysis, enabling accurate diagnosis and effective treatment planning for a global challenge in central nervous system cancer. In this study, a deep learning-based model for multiclass brain tumor classification and binary segmentation in MRI scans is proposed. Based on convolutional neural networks, the model classifies four tumor types, achieving an overall accuracy of 93.56%. The segmentation model was oriented only to the meningioma class and the U -Net architecture was used. The model accurately delineates tumor boundaries, with a high similarity to the annotated masks under medical supervision. The segmentation model achieved an overall accuracy of 98.54 % and a mean intersection over junction of 81.96%, demonstrating its ability to handle complex tumor shapes and sizes. Both models were integrated into a web application capable of predicting new images and displaying the results. Extensive experiments were performed on the model hyperparameters and training data, and the results outperformed state-of-the-art approaches in accuracy.",Image analysis;Biological system modeling;Magnetic resonance imaging;Training data;Predictive models;Brain modeling;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329651,IEEE Conferences,,,,,,
Diagnosis Neuro-Oncology Diseases via Artificial Intelligence and Multi-Sequence MRI Methods,A. Letyagin; E. Amelina; B. Tuchinov; V. Groza; M. Amelin; S. Golushko; E. Pavlovskiy,"2023 IEEE Ural-Siberian Conference on Computational Technologies in Cognitive Science, Genomics and Biomedicine (CSGB)",########,2023,"The study of brain tumor structure and its type-dependent variations is one of the most important research areas in which medical imaging techniques are used. The structural and statistical analysis of these lesions raises various related problems and projects, such as the detection of the neuro oncology diseases, the shape and the segmentation of specific sub-regions (i.e. necrotic part, (non-)enhanced part, edema), the classification of the tumor occurrence and the subsequent treatment up-prognosis. Almost all of these problems are usually solved numerically, particularly with the tendency to use methods related to artificial intelligence (AI), often including deep learning (DL) networks. One of the most complicated, least researched and challenging tasks in this field is the classification of tumor types. This difficulty can be explained by several reasons, the most important of which is the severe limitation of existing open-source datasets that contain clinically confirmed tumor type designations based on radiological examination protocols. Magnetic resonance imaging (MRI) is the most common method for screening, primary detection and non-invasive diagnosis of brain diseases, as well as a source of recommendations for further treatment and observation. In this paper, we extend the previous research works on the robust multi-sequences segmentation and classification methods which allows to consider all available information from MRI scans by the composition of TI, TIC, T2 and T2-FLAIR sequences. It is based on the clinical radiology hypothesis and presents an efficient approach to combining and matching 3D methods to search for areas of comprised the GD-enhancing tumor in order to significantly improve the model's performance of the particular applied numerical problem of brain tumor classification and metastasis segmentation. All investigations performed and results presented are based on the private Siberian brain tumor dataset, including labeled volumetric MRI scans describing a wide variety of tumors and associated clinically relevant ground truth (GT) information.",Image segmentation;Three-dimensional displays;Statistical analysis;Magnetic resonance imaging;Stability analysis;Lesions;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329808,IEEE Conferences,,,,,,
Pap Smear Medical Image Classification Using Deep Learning: A Systematic Review,E. Selvano; G. N. Elwirehardja; B. Pardamean,2023 6th International Conference of Computer and Informatics Engineering (IC2IE),########,2023,"Cervical cancer has been known as one of the most prevalent medical disorders globally and a leading cause of death. Early detection, particularly through Pap tests, plays a vital role in its prevention. Previous studies have leveraged machine learning and deep learning techniques to classify the medical images obtained from Pap tests. In this study, a Systematic Literature Review methodology was used to examine 15 relevant papers that have been filtered from queries to Google Scholar which have gone through 4 stages of filtering that include: identification, screening, eligibility, and inclusion. This study addresses two research questions regarding the datasets and deep learning techniques for classifying pap smear images in recent years. The performance of the models was analyzed and potential areas for improvements are suggested. The findings of this study reveal that the Herlev University Hospital and SIPaKMed datasets are the most used. The methodologies used by researchers range from machine learning techniques, transfer learning using Convolutional Neural Networks, and utilize state-of-the-art models with novel optimizing methodology. While there are exciting opportunities in the field, challenges include model generalization and interpretability.",Deep learning;Systematics;Hospitals;Computational modeling;Bibliographies;Transformers;Convolutional neural networks;Tuning;Image classification;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331248,IEEE Conferences,,,,,,
Pneumonia Detection Using Snapshot Ensemble Model,K. Pyar; S. M. Maung Zaw,2023 27th International Computer Science and Engineering Conference (ICSEC),########,2023,"Pneumonia is a serious and potentially life-threatening disease that predominantly affects the elderly population. Early diagnosis plays a crucial role in saving lives. Medical imaging, particularly the use of deep learning techniques, has shown promise in aiding the detection and classification of pneumonia in patients based on their chest X-rays. This paper presents a novel system for pneumonia detection and classification, utilizing a deep learning snapshot ensemble model. The EfficientNet-B0 technique is employed to address the pneumonia identification problems. The system is trained on the chest X-ray images (pneumonia) Kermany dataset, which is a well-known dataset in the field. The proposed system demonstrates impressive performance, achieving an accuracy of 97% for pneumonia classification.",Deep learning;Pollution;Pulmonary diseases;Computational modeling;Sociology;Ensemble learning;Older adults,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329752,IEEE Conferences,,,,,,
Breast Cancer Subtype Classification Based on PET/CT Bimodal Imaging Feature Fusion,L. Jia; Z. Chen; D. Shao; A. A. Abba,2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI),########,2023,"In this paper, the powerful feature extraction capability of convolutional neural networks is used to extract advanced latent features of breast lesions, and the classification of breast cancer subtype is realized based on the features extracted from PET/CT bimodal images. The DenseNet network model is used to construct the PET/CT imaging feature extraction network. Since the PET/CT bimodal imaging data from breast cancer patients is very limited, the transfer learning method is adopted to solve the problem of insufficient data and prevent overfitting during training. In order to combine PET/CT bimodal imaging features, two feature fusion approaches are proposed, which are offline feature fusion and online feature fusion. For offline feature fusion, principal component analysis is employed to reduce the dimensionality of the deep features extracted by the feature extraction network from individual modalities. The reduced features are then concatenated and forwarded into a classifier for breast cancer subtype classification. For the online feature fusion, the feature maps output by the PET/CT feature extraction networks are directly concatenated and then input into the classification subnetwork for breast cancer subtype classification. The validity is evaluated using clinical data in practice. The PET/CT dual-modality features can improve the accuracy of breast cancer classification, and the experimental results show that the accuracy of online feature fusion is the highest.",Training;Transfer learning;Imaging;Feature extraction;Breast cancer;Pattern recognition;Lesions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10332100,IEEE Conferences,,,,,,
Semi-Supervised Learning for Pupil Segmentation in TCM,N. Niu; H. Peng; Q. Liu; P. Zeng; Q. Yao; L. Wang; Y. Wang; G. Wang; M. Huang; C. Xue,2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI),########,2023,"Accurately locating and segmenting objects in medical images is crucial for providing precise data support. Traditional Chinese medicine diagnosis relies on pupillary features to reflect the condition of internal organs and pathologies. Combining image segmentation technology with pupil features can offer practical tools for medical treatment. However, obtaining and sharing medical image data is challenging, posing a hurdle for supervised learning approaches. Semi-supervised learning, which can use labeled and unlabeled data, can improve accuracy and generalization and is suitable for medical image processing. This method combines deep learning and traditional Chinese medicine to achieve medical image pupil segmentation with limited data, paving the way for disease discrimination. It effectively addresses the scarcity of data and the inadequacy of supervised learning methods for traditional TCM medical images. This approach provides more accurate and practical TCM image analysis and diagnosis tools.",Image segmentation;Pathology;Image analysis;Supervised learning;Medical treatment;Semisupervised learning;Pattern recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331984,IEEE Conferences,,,,,,
A Deep Learning Approach for Varicocele Detection from Ultrasound Images,M. A. Awad; L. Samrraie; A. M. Abdalla; O. AlZoubi; A. Migdady; M. B. Yassein,2023 14th International Conference on Information and Communication Systems (ICICS),########,2023,"Varicocele is a disease characterized by abnormal dilatation of the scrotal venous plexus pampiniformis. It is a common cause of infertility and may cause pain or discomfort in some cases. In this article, we present a new approach for automatic classification of varicocele using Deep Learning convolutional neural networks. The available dataset consists of images converted to different color modes. Each color mode dataset is partitioned, augmented, and trained using a Deep Learning network. Experiments were conducted with all possible combinations of four different pre-trained models and over three color modes to determine the best combination that achieves the highest performance, with and without augmentation. Training and testing were evaluated using various metrics. Analysis of the results demonstrated the effectiveness of the proposed system. Results showed an accuracy of 83.1% with the RGB color mode when using ResNet50, and that the effect of augmentation was small.",Deep learning;Measurement;Training;Ultrasonic imaging;Image color analysis;Pain;Gray-scale,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330493,IEEE Conferences,,,,,,
Predicting Ejection Fraction from Electrocardiogram Signals using a Multi-task Learning Model,G. Zhong; Y. Wang; S. Liu; X. Deng; A. Wang; C. Yang,2023 IEEE 19th International Conference on Body Sensor Networks (BSN),########,2023,"The aim of this study was to investigate the feasibility and effectiveness of using electrocardiogram (ECG) signals to predict ejection fraction (EF) in an extremely imbalanced dataset. We collected ECG signals from 9365 patients and conducted a correlation analysis with EF. After collecting and preprocessing the ECG signal, we developed a deep learning-based multi-task learning model designed to extract features from ECG signals and perform predictions. Our study employed a model based on Transformer, multi-scale convolutional neural networks (CNN), channel attention, and pre-trained ResNet to predict EF (EF > 50 or EF â‰¤ 50) and EF value. The experimental results demonstrate that the proposed model exhibits excellent predictive performance, with an AUC of 0.825 and MAE of 4.855 for predicting EF. It outperforms other models and shows better results in comparison. Our study validates the feasibility and effectiveness of using ECG signals to predict EF and provides strong support for early diagnosis and treatment of cardiovascular diseases.",Electrocardiography;Predictive models;Myocardium;Multitasking;Transformers;Feature extraction;Convolutional neural networks;Cardiovascular diseases;Reliability;Monitoring,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331174,IEEE Conferences,,,,,,
Generative Adversarial Networks for Augmenting Endoscopy Image Datasets of Stomach Precancerous Lesions: A Review,B. MagalhÃ£es; A. Neto; A. Cunha,IEEE Access,########,2023,"Gastric cancer (GC) is still a significant public health issue, among the most common and deadly cancers globally. The identification and characterization of precancerous lesions of the stomach using endoscopy are crucial for determining the risk of cancer and guiding appropriate surveillance. In this scenario, deep learning (DL)-based computer vision methods have the potential to help us classify and identify particular patterns in endoscopic images, leading to a more accurate classification of these types of lesions. The quantity and quality of the data used heavily influence the classification performance of DL networks. However, one of the major setbacks for developing high-performance DL classification models is the typical need for more available data in the medical field. This review explores the use of Generative Adversarial Networks (GANs) and classical data augmentation techniques for improving the classification of precancerous stomach lesions. GANs are DL models that have shown promising results in generating synthetic data, which can be used to augment limited medical datasets. This review discusses recent studies that have implemented GANs and classical data augmentation methods to improve the accuracy of cancerous lesion classification. The results indicate that GANs can effectively increase the datasetâ€™s size, enhance the classification modelsâ€™ performance. In specific applications, such as the augmentation of endoscopic images depicting gastrointestinal polyps and Barrettâ€™s esophagus Adenocarcinoma, our review reveals instances where GANs, including models like Deep Convolutional GANs and conditional GANs, outperform classical data augmentation methods. Furthermore, this review highlights the challenges and limitations of the recent works using GANs and classical data augmentation techniques in medical imaging analysis and proposes directions for future research.",Lesions;Medical diagnostic imaging;Data models;Stomach;Cancer;Solid modeling;Endoscopes;Data augmentation;Deep learning;Generative adversarial networks;Gastroenterology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336816,IEEE Journals,,,,,,
3D Efficient Multi-Task Neural Network for Knee Osteoarthritis Diagnosis Using MRI Scans: Data From the Osteoarthritis Initiative,P. S. Q. Yeoh; S. L. Goh; K. Hasikin; X. Wu; K. W. Lai,IEEE Access,########,2023,"Deep learning, particularly Convolutional Neural Networks, has demonstrated effectiveness in computer-aided diagnosis applications, including knee osteoarthritis analysis. Two of the most common tasks done in medical imaging are segmentation and classification tasks. This research investigates the feasibility of multi-task models for volumetric analysis using Magnetic Resonance Imaging scans in knee osteoarthritis diagnosis, while considering computational efficiency. In order to leverage the correlation between segmentation and classification tasks, two 3D multi-task models, OA_MTL (Osteoarthritis_Multi-Task Learning) and RES_MTL (Residual_Multi-Task Learning) models are developed to simultaneously segment knee structures and classify knee osteoarthritis incidence. The performance of the multi-task models is evaluated against single-task baseline models and other existing convolutional neural network models using a total of eight different performance metrics, while comparing the computational complexity among the models. Experimental results demonstrate that multi-task model leverages the information of segmentation task to improve the classification performance. OA_MTL is a multi-task model that incorporates an encoder-decoder architecture, residual modules, and depthwise separable convolutions for enhanced performance. OA_MTL achieves superior performance for classification tasks with an accuracy score of 0.825, and a comparable segmentation DSC score of 0.915. OA_MTL achieves a favorable trade-off between computational complexity and model performance. The contribution of this work includes an approach that simultaneously performs knee structure segmentation and osteoarthritis classification in 3D MRI, which addresses the need for efficient models in the field of medical imaging, specifically on computationally challenging 3D medical imaging applications.",Task analysis;Three-dimensional displays;Multitasking;Computational modeling;Osteoarthritis;Magnetic resonance imaging;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336807,IEEE Journals,,,,,,
Enhancing Pneumonia Detection using Vision Transformer with Dynamic Mapping Re-Attention Mechanism,M. A. Labbaf Khaniki; M. Mirzaeibonehkhater; M. Manthouri,2023 13th International Conference on Computer and Knowledge Engineering (ICCKE),########,2023,"Pneumonia is a prevalent respiratory infection with potentially life-threatening consequences. In this research, we propose a novel deep learning approach to enhance pneumonia detection using the Vision Transformer (ViT) architecture with a dynamic mapping re-attention mechanism. The re-attention mechanism in the ViT architecture facilitates the flow of information between patches of the input image, enabling contextual understanding through self-attention. This mechanism captures long-range dependencies and enhances the ViT's ability to effectively recognize and interpret images. The dynamic mapping attention mechanism allows the model to dynamically assign attention weights to different regions of the input data based on their relevance to pneumonia detection. By adaptively adjusting the attention weights, the model can capture both local details and global dependencies, enabling more accurate diagnosis. The ViT utilizes the dynamic mapping re-attention mechanism with attention dropout. Here the performance of this ViT compared to the conventional ViT and the ViT incorporating the re-attention mechanism. This comparative analysis offers valuable insights into the effectiveness of the proposed dynamic mapping re-attention mechanism. Additionally, it highlights the added benefits of attention dropout in enhancing the overall performance of the model.",Deep learning;Knowledge engineering;Adaptation models;Image recognition;Pulmonary diseases;Computer architecture;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10326313,IEEE Conferences,,,,,,
Capturing Local and Global Features in Medical Images by Using Ensemble CNN-Transformer,J. M. Kaleybar; H. Saadat; H. Khaloo,2023 13th International Conference on Computer and Knowledge Engineering (ICCKE),########,2023,"This paper introduces a groundbreaking classification model called the Controllable Ensemble Transformer and CNN (CETC) for the analysis of medical images. The CETC model combines the powerful capabilities of convolutional neural networks (CNNs) and transformers to effectively capture both high and low frequency features present in medical images. The model architecture comprises three main components: a transformer classification block (TCB), a transposed-convolutional decoder block (TDB), and a convolutional encoder block (CEB). The CEB is responsible for capturing multi-local features at different scales and draws upon components from VGGNet, ResNet, and MobileNet as backbones. By leveraging this combination, the CEB is able to effectively detect and encode local features. The TDB, on the other hand, consists of sub-decoders that decode and sum the captured features using ensemble coefficients. This enables the model to efficiently integrate the information from multiple scales. Finally, the TCB utilizes the SwT backbone and a specially designed prediction head to capture global features, ensuring a comprehensive understanding of the entire image. The paper provides detailed information on the experimental setup and implementation, including the use of transfer learning, data preprocessing techniques, and training settings. The CETC model is trained and evaluated using two publicly available COVID-19 datasets. Moreover, the model is able to outperform existing state-of-the-art models across a variety of evaluation metrics, which is encouraging. Experimental results clearly demonstrate that the CETC model performs better than other deep learning models, emphasizing its potential for accurate and efficient image analysis in the field of medicine.",COVID-19;Measurement;Training;Analytical models;Transfer learning;Transformers;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10326274,IEEE Conferences,,,,,,
Using Deep Learning for Classification of Lung Cancer on CT Images in Ardabil Province : Classification of Lung Cancer using Xception,M. A. Javad Zadeh Barzaki; J. Abdollahi; M. Negaresh; M. Salimi; H. Zolfaghari; M. Mohammadi; A. Salmani; R. Jannati; F. Amani,2023 13th International Conference on Computer and Knowledge Engineering (ICCKE),########,2023,"In the past, doctors had to physically locate the region they thought could have lung cancer. Several studies have shown that manual segmentation is time-consuming and dependent on the operator and instrument. To aid healthcare providers in making an early diagnosis of lung cancer, an algorithm known as the XCEPTION Based Classification for Lung Cancer (XCEPTION) has been created. XCEPTION enhances the categorization of lung cancer lesions by using a neural network to assist doctors in diagnosing lung cancer. Medical pictures of lung cancer in malignant, benign, and healthy people may all be classified by XCEPTION. Each and every ethical guideline has been followed while doing research for this study. This approach uses a convolutional neural network to categorize lung cancer in order to help medical practitioners identify lung cancer lesions. With precision and sensitivity, XCEPTION can identify incoming medical photos as belonging to cancer patients or healthy people.",Knowledge engineering;Ethics;Sensitivity;Instruments;Neural networks;Lung cancer;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10326262,IEEE Conferences,,,,,,
Retinal Diseases Classification System Using OCT Images Combined with CNN Models,F. Dhaoui; A. Zrelli,"2023 International Symposium on Networks, Computers and Communications (ISNCC)",########,2023,"Optical Coherence Tomography (OCT) has become one of the foremost successful and commonly utilized imaging strategies for non-invasive retinal eye disease assessment. Visual deficiency is analyzed utilizing OCT with one of the taking after two eye maladies categories: diabetic macular edema (DME) or age-related macular degeneration (AMD). The classification of eye retina infections utilizing OCT pictures as of late has become a challenge with the advancement of machine instructing and significant learning strategies. In this paper, a deep learning algorithm is performed. Retinal disease classification playsa crucial role in the early detection and treatment of various eye conditions, such as diabetic macular edema (DME) and age-related macular degeneration (AMD). This paper presents a study on the classification of retinal diseases using Convolutional Neural Networks (CNNs), a powerful deep-learning technique known for its ability to extract meaningful features from images.",Macular degeneration;Visualization;Optical coherence tomography;Medical services;Retina;Feature extraction;Diabetes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323745,IEEE Conferences,,,,,,
Multimodal Analysis of Unbalanced Dermatological Data for Skin Cancer Recognition,P. A. Lyakhov; U. A. Lyakhova; D. I. Kalita,IEEE Access,########,2023,"To date, skin cancer is the most commonly diagnosed form of cancer in humans and is one of the leading causes of death in cancer patients. AI technologies can match and exceed visual analysis methods in accuracy, but they carry the risk of a false negative response when a malignant pigmented lesion can be recognized as benign. Possible ways to improve accuracy and reduce the risk of false negatives are to analyze heterogeneous data, combine different preprocessing methods, and use modified loss functions to eliminate the negative impact of unbalanced dermatological data. The article proposes a multimodal neural network system with a modified cross-entropy loss function that is sensitive to unbalanced heterogeneous dermatological data. The novelty of the proposed system lies in the emerging synergy when using methods to improve the quality of intelligent systems, due to which there is a significant reduction in the number of false negative predictions and an increase in the accuracy of skin cancer recognition. Preliminary cleaning of hair structures on visual data, as well as parallel analysis of heterogeneous dermatological data using a multimodal neural network system sensitive to unbalanced data, were used as methods to improve accuracy. The recognition accuracy for 10 diagnostic categories for the proposed intelligent system was 85.20%. The introduction of weighting factors made it possible to reduce the number of false negative forecasts, as well as increase the accuracy by 1.99-4.28 percentage points compared to the original multimodal systems.",Skin cancer;Visualization;Skin;Neural networks;Lesions;Hair;Melanoma,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10328559,IEEE Journals,,,,,,
MRI Synthesis via CycleGAN-Based Style Transfer,A. Gawande; P. Gandhi; S. Kude; N. Mulla; R. Pawar,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"Medical imaging plays a crucial role in modern medicine by supporting illness diagnosis and therapy. However, there are difficulties in illness diagnosis using medical imaging, such as the costs and infrastructure to conduct an MRI scan, and the strenuous task of correctly interpreting an MRI scan. It is necessary to develop a method for producing MRI images with adjustable contrast levels. A cyclic Generative Adversarial Neural Network called CycleGAN, which performs well with unpaired data and is best suited for medical imagery, is used for the proposed healthcare platform to create artificial MRI scans. The results show that our method converts non-fat saturated MRI images into fat saturated images more effectively than a benchmark pix2pix approach, resulting in a PSNR (peak signal- to-noise ratio) of 12.89, structural similarity of 0.97, and KL divergence of 0.07, using the MRI dataset, a sizeable collection of freely accessible MR brain pictures.",PSNR;Costs;Magnetic resonance imaging;Medical treatment;Benchmark testing;Fats;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308254,IEEE Conferences,,,,,,
Deep Learning for Pulmonary Disease Identification,A. Kukker; S. Dhar; V. Singh; V. Amitabh; S. Likhite,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"The ubiquitous use of machine learning in healthcare including, but not limited to analytics, data collection and processing, differential diagnosis and wearable health tracking has led to vast amounts of research in the domain. With the advent of deep learning, there has been an increasing interest in disease classification using various medical imaging techniques. This paper aims to use deep learning architecture using convolutional neural networks to accurately identify different pulmonary diseases such as pneumonia and COVID-19 from a healthy human beingâ€™s chest x-rays. The model, inherently based on CNN, uses multi-layer variables including convolutional, batch normalization, ReLU, max pooling, fully connected, SoftMax, and classification layers to make up the architecture. The dataset consisting of 3,000 images, varying from healthy lung scans to those of COVID-19 patients and pneumonia patientsâ€™ x-ray scans, aid in the training and testing of the model, which achieves an accuracy of 85%. The work is aimed to be of assistance to healthcare professionals and to be of advantage to them for faster and better decision making.",Deep learning;COVID-19;Recurrent neural networks;Pulmonary diseases;Lung;Medical services;Mathematical models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307502,IEEE Conferences,,,,,,
A Review of Lung Nodules Identification using Machine Learning,S. N. Dwivedi; A. Kaur,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"The identification and classification of lung nodules play a critical role in the early diagnosis and treatment of lung cancer, a leading cause of mortality worldwide. As technology advances, the integration of machine learning algorithms has emerged as a groundbreaking solution to enhance the accuracy and efficiency of lung nodule identification. This review aims to provide an overview of the recent advancements and impactful applications of Machine learning in the field of lung nodule identification. The review begins by elucidating the significance of early lung nodule detection and the challenges associated with traditional methods. It then delves into the immense potential of machine learning, which encompasses various techniques such as deep learning and convolutional neural networks. By leveraging these methods, researchers have achieved remarkable improvements in nodule detection and risk prediction. Moreover, this review highlights the role of curated datasets and various methods of lung nodule classification and segmentation. The utilisation of large-scale datasets, including publicly available datasets and clinical databases, has empowered researchers to develop robust algorithms capable of achieving exceptional performance. Furthermore, the review examines the fact that machine-learning algorithms have demonstrated the ability to identify nodules with remarkable precision. In conclusion, machine learning has emerged as a transformative force in the field of lung nodule identification, offering unprecedented accuracy and efficiency. By harnessing the power of advanced algorithms, healthcare providers can elevate their ability to detect and classify lung nodules, ultimately leading to earlier diagnosis, improved patient outcomes, and a significant reduction in lung cancer-related mortality.",Deep learning;Training;Machine learning algorithms;Lung;Lung cancer;Machine learning;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307157,IEEE Conferences,,,,,,
Brain Tumor Detection using Magnetic Resonance Imaging and Deep Learning Techniques,S. Jaiswal; A. K. Chaudhary; S. Verma,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"Recent advancements in artificial intelligence (AI) have revolutionized various industries, including the medical field. Deep learning algorithms are being used in medical science for diagnosis making it faster and more accurate. This study utilizes deep learning techniques for detection of brain tumor using MRI scans of human brain. The research work utilizes a publicly available database of MRI images of brain tumors including Pituitary, Glioma and Meningioma brain tumors. The image enhancement step consists of noise removal technique using Gaussian Filter, followed by Contrast Limited Adaptive Histogram Equalization (CLAHE) for contrast adjustment and Contour creation for refining edges. For detection different neural network architectures have been used - ResNet 152, Xception, VGG 16, Inception V3 and MobileNet V2, with ImageNet weights to classify the diseases. The models outperform the previously proposed methods and achieves the best accuracy of 98.039% by Xception architecture.",Deep learning;Magnetic resonance imaging;Image edge detection;Transfer learning;Computer architecture;Brain modeling;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10306482,IEEE Conferences,,,,,,
The three dimensional dosimetry imaging for automated eye cancer classification using transfer learning model,M. A. Mohammed; V. A. Mohammed; R. Ramakrishnan; M. A. Mohammed; J. Logeshwaran; M. S,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"Computer vision methods utilizing transfer learning can offer a promising approach for automated eye cancer diagnosis with three-dimensional dosimetry imaging. The latest advances in AI have encouraged researchers to use deep learning models for automated cancer classification. This paper presents a novel approach using three-dimensional dosimetry imaging for automated ocular cancer classification using transfer learning. The approach uses a combination of dataset augmentation methods. The approach is adapted from the popular Inception V3 CNN architecture. Two different feature extraction approaches, namely mean and maximum pooling, are compared while extracting features from the output of the CNN layers. The extracted features are fed to the classifier which uses the SVM with a linear kernel as its decision system. The effectiveness of this method is evaluated on a set of 20 healthy ocular images and 26 suspicious ocular images. The overall classification accuracy achieved with this approach is 94.85%. The results obtained demonstrate that this approach has a promising potential for automated ocular cancer classification with an appropriate transfer learning model.",Support vector machines;Solid modeling;Three-dimensional displays;Transfer learning;Imaging;Feature extraction;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307446,IEEE Conferences,,,,,,
An Optimal Deep Ensemble Model for the Classification of Pneumonia from Chest X-Rays,M. M. Warrier; L. Abraham; R. R,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"Pneumonia is a serious respiratory infection that varies in severity affecting the lungs and resulting in the inflammation of the air sacs in one or both lungs. Monitoring and addressing the burden of pneumonia through effective prevention, early detection, and appropriate treatment remain key priorities in global health efforts. With the improvements in medical imaging technology and the datasetâ€™s availability, deep learning models can analyze medical images such as chest radiographs (Chest X-Rays) and computed tomography scans (CT scans) to aid in diagnosing and classifying pneumonia. In this work, the three best-suitable convolutional neural networks namely-NASNet-Large, ResNet-152V2, and DenseNet-121 have been used for the classification of pneumonia from the Chest X-Rays. To improve further accuracy, the predictions of the three pre-trained models have been combined to generate an ensemble deep-learning model. The performance of the ensemble model was compared with the different pre-trained models. The ensemble model was also evaluated with different optimizers-ADAM, RMSprop, SGD, and NADAM and results have been compared for the best performance. The deep ensemble model with RMSprop provided an accuracy of 91% and outperformed all the other models considered.",Deep learning;Technological innovation;Pulmonary diseases;Computational modeling;Computed tomography;Lung;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10306932,IEEE Conferences,,,,,,
Comparative Survey of Various Intelligent Methods for Breast Cancer Diagnosis and Prognosis,M. Mondal; S. Dasgupta; I. Bhattacharya,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"Cancer remains a major contributor to mortality rates across the globe, causing nearly 10 million fatalities in 2020. Among all cancer types, breast cancer has the highest incidence, around 2.26 million cases reported across the globe by the World Health Organization. In the last few years, breast cancer was diagnosed in approximately 2.3 million people and caused around 685,000 deaths. In this paper, our investigation entails executing a statistical scrutiny of the worldwide proliferation of breast cancer, as well as delving into various machine learning and deep learning techniques for detecting breast cancer, along with their constraints and intricacies. Additionally, we have proposed a hybrid Machine Learning (ML) approach for detecting breast cancer with utmost accuracy, which streamlines the assessment procedure presently employed by radiologists in screening mammograms.",Deep learning;Surveys;Sequential analysis;Training data;Organizations;Streaming media;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307593,IEEE Conferences,,,,,,
Improving Skin Disease Diagnosis with Deep Learning: A Comprehensive Evaluation,P. Sudharshan; D. Jayavarapu; T. Medasani; S. Sivarajan; M. K. Enduri; S. Anamalamudi,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"One of the deadliest maladies is skin disease. Despite it could be challenging to diagnose dermatological problems precisely. In the present study, deep learning will be used to pinpoint the skin problem described earlier. Such a tool would be more efficient than manual procedures, which are time-consuming and call for expert help. This paper's main goal is to provide an in-depth analysis of conceptual review of current advancements in deep learning-based skin disease identification. Although deep learning is gaining popularity, there are still many problems to be solved and much more study to be done. The present manual procedures for diagnosing skin diseases are known to be time-consuming because they rely on professional judgement.",Deep learning;Training;Image recognition;Computational modeling;Machine vision;Manuals;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307670,IEEE Conferences,,,,,,
Review on Brain Tumor Detection using Custom CNN Layers and Transfer Learning on MRI images,A. Saxena; J. Singh,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"Early diagnosis and treatment planning are greatly aided by the early identification of brain tumors. Because of its high resolution, low radiation, and low risk of patient discomfort, magnetic resonance imaging (MRI) is frequently used to diagnose brain tumors. Brain tumor identification is only one area where recent developments in convolutional neural networks (CNNs) have shown exceptional effectiveness. This article summarizes recent progress in detecting brain tumors by utilizing MRI images and bespoke CNN layers with transfer learning. The review kicks off with a discussion of the difficulties of detecting brain cancers, such as the tumors' complexity and heterogeneity and the scarcity of available annotated data. The article proceeds to go into the foundations of CNNs and their applicability to MRI image processing. To improve detection accuracy, we incorporate custom CNN layers that are tailored to capture salient tumor-specific information. The concept of transfer learning, in which CNN models trained on large-scale datasets are repurposed for brain tumor detection, is also discussed at length in the review. Using transfer learning, we can take advantage of what we've learned about general image identification to better train models to spot brain tumors. Fine-tuning, feature extraction, and other transfer learning methods are addressed at length. Recent research using custom CNN layers and transfer learning approaches to detect brain cancers in MRI images is thoroughly analyzed in this study. Among the benefits and drawbacks discussed are the methods' adaptability to small datasets, enhanced detection accuracy, and decreased training time. Also, the significance of using metrics for measuring performance and benchmark datasets for comparing methods fairly is discussed. The analysis concludes with suggestions for future study, such as the combination of functional and diffusion tensor imaging with conventional MRI scans to better detect brain tumors. Furthermore, it stresses the need for interpretability and explainability in CNN-based models to improve confidence and clinical acceptance.",Training;Transfer learning;Brain cancer;Benchmark testing;Brain modeling;Feature extraction;Planning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308244,IEEE Conferences,,,,,,
Image Classification of Covid Chest X-Rays using a Game Theoretical Approach,V. Jain; S. Nagpal; A. Aggarwal; T. Gupta,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"This project's goal is to develop a deep learning algorithm that is capable of correctly classifying images while making use of SHAP (SHapley Additive exPlanations) values to provide explications for the model's predictions. The training of the model will be carried out on a dataset consisting of medical images, such as chest X-rays and MRI scans, which are commonly used for the purpose of diagnosing disease. The usage of SHAP values is meant to attribute the contribution of individual pixels within the input image to the output of the model, delivering significant insights into the process of prediction and the relative significance of distinct characteristics.",Deep learning;Training;Additives;Games;Predictive models;Prediction algorithms;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10306909,IEEE Conferences,,,,,,
Identifying Alzheimerâ€™s Dementia Patients Using MRI Clinical Data,C. Ritul; N. Sampath,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"Alzheimer's disease is a debilitating and progressive neurological disorder that is characterized by a range of symptoms, including memory loss, difficulties with thinking, language, and perception, and a loss of the ability to perform simple daily tasks. Although the exact cause of Alzheimer's disease is unknown, a number of factors are believed to contribute to its development, including the accumulation of amyloid proteins and neurofibrillary tangles in the brain, genetic predisposition, age-related changes in the brain, and lifestyle factors. [1] Given the lack of a cure for Alzheimer's, a model has been proposed to decrease the number of pa-tients with dementia. This model utilizes three blocks of convolutional neural network (CNN) layers, each containing different filter sizes, to improve the accuracy of the model in identifying images based on visual cues. This approach resulted in high loss and validation accuracy, and the model achieved an impressive 98.64% accuracy in training classification and 96% accuracy in testing. When compared with other models, including VGG16, VGG19, and InceptionResNet V2, the proposed model performed better with a higher accuracy rate. InceptionResNet V2 was the closest competitor with an accuracy of 86%, while VGG16 and VGG19 performed similarly, achieving accuracies of 74% and 76% respectively. The accuracy rates suggest that the proposed model is a promising approach to decreasing the number of patients with Alzheimer's disease.",Training;Proteins;Neurological diseases;Visualization;Magnetic resonance imaging;Brain modeling;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308008,IEEE Conferences,,,,,,
Deep Learning based Spectrum predictions for Cognitive Healthcare system,P. Kansal,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"The prevailing pandemic scenario and the constant explosion of wireless devices lead to demand for a large spectrum. There is a crucial need for regular supervision of patient's health in these dire situations of a pandemic. This demands a smart solution for healthcare based on the latest technologies. This research work presents a detailed review of the latest advancements in the context of smart healthcare and medical imaging. This research work also proposes a novel framework for Cognitive healthcare based on Deep Learning. A unique spectrum prediction model has been proposed that combines the Grey model and Long Short Term Memory functionalities and allocates spectrum to the patients according to their risk classification (high, medium, and low). This research work describes the distinct novel applications of Cognitive radio, Deep learning, and the Internet of things in the Smart healthcare and Medical Imaging context. The proposed LSTM-Grey model has also been compared with existing spectrum sensing methodologies based on LSTM and CNN. All simulation results based on detection probability, false alarm probability and Accuracy of prediction indicate that LSTM-Grey model outperforms the existing solutions for healthcare system. The unique solution proposed based on the Grey model and LSTM for spectrum prediction enables transmission of medical data on free spectrum slots for real-time supervision of patient's health. The Deep Learning-based Cognitive Healthcare system is a promising candidate for Smart Healthcare supporting swift diagnosis, tracking, and patient health supervision.",Deep learning;Pandemics;Computational modeling;Simulation;Medical services;Predictive models;Prediction algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307849,IEEE Conferences,,,,,,
Effective Tumor Diagnosis based on Shape and Size of Tumor,M. Rana; M. Bhushan,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"The key aspects of cancer diagnosis include accurate detection of tumor size and shape, as well as efficient classification. Given study provides a brief overview of the recent developments in tumor classification approaches, tumor shape and size detection techniques. In this study, a tumor mask was imposed on the original medical image(s) using a logical AND operator. It also highlights the value of automated tumor size evaluation, which is crucial for classification, prediction, and future treatment. Further, it explores how to quantify tumor size more precisely and consistently using computer-aided methods (machine learning, deep learning, and image analysis). The aim of this study is to generate imposed images for future classification by fusing the tumor mask with image pixels. Then, the imposed images were classified into three categories namely benign, malignant, and malignant metastasis for effective tumor diagnosis. It will improve various aspects of tumor segmentation such as varied size and shape.",Deep learning;Image segmentation;Image analysis;Shape;Metastasis;Medical diagnostic imaging;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308022,IEEE Conferences,,,,,,
Optimization Study of Renal CT Image Classification Using Convolution Neural Networks,A. K; K. V. Nagaraja; T. Singh; B. P. KN,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"Computer tomography (CT), also known as computed tomography, is a medical imaging method that generates detailed and precise horizontal or axial images of targeted regions of the body for diagnostic purposes. In this particular study, the focus was on performing image classification of CT scan images of the Kidney. This organ plays an essential role in detoxification, fluid balance, and maintaining electrolyte levels in the human body. The dataset used for the study contained 12,446 images into four labeled classes - Cyst, Tumor, Stone, and Normal. Convolution Neural Network (CNN) model is built for binary (Normal and Tumor) and multi-image Classification to classify the Renal CT images into four classes. CNN uses ReLU as an activation function for classifying these images. The Model is evaluated and compared based on Accuracy and Loss performance metrics by compiling the CNN model with different deep learning TensorFlow Keras optimizers for Version 2.11.0 for Binary Image Classification at 10,20, and 50 epochs. The CNN model is assembled with Adam Optimizer and Follows the Regularized Leader (FTRL) Optimizer. The Testing and training Accuracy and Losses achieved are evaluated with a learning rate of 0.001 and 0.0001. Adam Optimizer outperforms to be the best Optimizer for Binary and Multi-Image Classification.",Measurement;Training;Convolution;Computed tomography;Neural networks;Optimization;Kidney,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307772,IEEE Conferences,,,,,,
A Comparative Study of Deep Learning Algorithms in Classifying Brain Cancer,U. P. Srivastava,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),########,2023,"Cancer remains a significant global health concern, contributing to a substantial number of fatalities. Specifically, brain cancer (BC) entails the abnormal proliferation of cells within or near the brain. Timely detection plays a pivotal role in effective tumor treatment. Machine learning advancements have significantly aided the medical field in diagnosing brain tumors. Convolutional Neural Network (CNN) has emerged as a widely utilized and effective machine learning technique for image processing and medical imaging. Nonetheless, many existing classification algorithms exhibit inconsistencies. In recent years, deep learning has exhibited promising outcomes in BC classification. This study focuses on the usage of four popular deep learning algorithms, namely AlexNet, GoogLeNet, InceptionNetV3, and ResNet50, utilizing brain Magnetic Resonance Imaging (MRI) slices to classify cancerous tumors into Glioma Tumor (GT), Meningioma Tumor (MT), Pituitary Tumor (PT), and No Tumor (NT) categories. The performance of these algorithms has been thoroughly assessed using metrics such as accuracy, precision, recall, and F1 score. Notably, GoogLeNet demonstrates superior results among the four models employed. Additionally, a comparative analysis against various state-of-the-art classification algorithms highlights the enhanced performance of these deep learning models.",Deep learning;Magnetic resonance imaging;Computational modeling;Brain cancer;Brain modeling;Classification algorithms;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10306832,IEEE Conferences,,,,,,
CT-xCOV: a CT-scan based Explainable Framework for COVid-19 diagnosis,I. Elbouknify; A. Bouhoute; K. Fardousse; I. Berrada; A. Badri,2023 10th International Conference on Wireless Networks and Mobile Communications (WINCOM),########,2023,"In this work, CT-xCOV, an explainable framework for COVID-19 diagnosis using Deep Learning (DL) on CT-scans is developed. CT-xCOV adopts an end-to-end approach from lung segmentation to COVID-19 detection and explanations of the detection modelâ€™s prediction. For lung segmentation, we used the well-known U-Net model. For COVID-19 detection, we compared three different CNN architectures: a standard CNN, ResNet50, and DenseNet121. After the detection, visual and textual explanations are provided. For visual explanations, we applied three different XAI techniques, namely, Grad-Cam, Integrated Gradient (IG), and LIME. Textual explanations are added by computing the percentage of infection by lungs. To assess the performance of the used XAI techniques, we propose a ground-truth-based evaluation method, measuring the similarity between the visualization outputs and the ground-truth infections. The performed experiments show that the applied DL models achieved good results. The U-Net segmentation model achieved a high Dice coefficient (98%). The performance of our proposed classification model (standard CNN) was validated using 5-fold cross-validation (acc of 98.40% and f1-score 98.23%). Lastly, the results of the comparison of XAI techniques show that Grad-Cam gives the best explanations compared to LIME and IG, by achieving a Dice coefficient of 55%, on COVID-19 positive scans, compared to 29% and 24% obtained by IG and LIME respectively. The code and the dataset used in this paper are available in the GitHub repository [1].",COVID-19;Measurement;Visualization;Computational modeling;Wireless networks;Lung;Predictive models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322985,IEEE Conferences,,,,,,
3D CT Scans for Colorectal Cancer Classification using VGG16 and Data Augmentation,K. Hicham; S. Laghmati; A. Tmiri,2023 10th International Conference on Wireless Networks and Mobile Communications (WINCOM),########,2023,"Computed Tomography (CT) scans play a vital role in diagnosing and monitoring various medical conditions. Applying deep learning models for accurate and automated CT scan classification has shown promising results. Among these models, VGG16, a deep convolutional neural network (CNN) architecture, has gained considerable popularity due to its exceptional performance in image recognition tasks. This study adapts VGG16 architecture to classify 3D volumetric CT scans, aiming to detect the presence or absence of polyps in the colon and prevent Colorectal Cancer CRC. The three-dimensional nature of the data poses unique challenges in terms of model complexity and overfitting on limited resources. Accordingly, we employed data augmentation techniques to augment the dataset and improve model generalization. Results suggest that data augmentation did increase the accuracy of VGG16 by +3.3%.",Adaptation models;Three-dimensional displays;Computed tomography;Computational modeling;Wireless networks;Computer architecture;Data augmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322904,IEEE Conferences,,,,,,
Enhanced Deep Learning Explainability for COVID-19 Diagnosis from Chest X-ray Images by Fusing Texture and Shape Features,H. E. Mohamadi; M. E. Hassouni,2023 10th International Conference on Wireless Networks and Mobile Communications (WINCOM),########,2023,"In this paper, we propose a novel method of explainable deep learning for COVID-19 detection in Chest X-ray (CXR) images, employing a Texture-Shape approach. The method extracts texture information from the first convolution layer and shape features from the last convolution layer of Deep Neural Networks, namely CNN, VGG16, and ResNet50. To generate the final explainable map, the extracted texture and shape heatmaps are fused using a guided filter-based method. Our approach is versatile and can be adapted to work with classical explainability methods commonly used in the literature. We validate the efficacy of our method on a well-known COVID-19 database comprising CXR images. We conduct extensive experiments to assess its performance against classical explainability methods such as FEM (Feature Extraction Map) and GradCam (Gradientweighted Class Activation Mapping). For evaluation, we calculate metrics such as the average drop and increase in confidence scores. The obtained results demonstrate that fusing texture and shape information leads to significantly improved explainability compared to conventional methods.",COVID-19;Deep learning;Shape;Convolution;Wireless networks;Feature extraction;Data mining,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322952,IEEE Conferences,,,,,,
Enhancing Breast Masses Detection and Segmentation: A Novel U-Net-Based Approach,N. Bentaher; Y. Kabbadj; M. B. Salah,2023 10th International Conference on Wireless Networks and Mobile Communications (WINCOM),########,2023,"Breast cancer is a frequent form of cancer for women, and globally it is a major cause of death. Early detection is crucial for effective diagnosis and treatment, and can also help to lower mortality rates. In recent years, Convolutional Neural Networks have been applied to mammography to assist radiologists improve their efficiency and precision. U-Net is a fully convolutional neural network and a popular model that uses the encoder-decoder architecture, it is known for its capacity to effectively segment biomedical images. This work aims to present a new model for breast cancer detection and segmentation. Our first purpose was to discover a reliable segmentation model for breast masses in mammography images by combining the ResNet architecture and U-Net. Subsequently, we trained several standard models for classification in normal and abnormal images, such as ResNet101, ResNet152, DenseNet169, MobileNet, InceptionV3, and NasNet. Furthermore, based on the segmentation model that yielded good experimental results, we created our classification model by modifying the output layers in order to generate a prediction label (Normal or Abnormal) instead of generating a mask. Our study contributes to the field of breast cancer detection by employing a novel approach that performs detection and segmentation directly on the entire mammography image, unlike traditional methods that operate on image patches. The main advantage of the proposed approach is to simplify the workflow of breast masses segmentation by eliminating intermediate steps which can introduce errors and increase the complexity of the system. Furthermore, our approach focuses on reducing false positives, which are a common challenge in breast cancer detection. Our classification model reached better results compared to other experimented classification models. We used the INbreast dataset of mammography images to train and evaluate our models. In addition to that we employed GradCAM in order to visualize the prediction results and to have an interpretation of the results.",Training;Image segmentation;Visualization;Biological system modeling;Wireless networks;Predictive models;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322998,IEEE Conferences,,,,,,
Genetic Convolutional Neural Networks Approach for Disease Detection From Chest X-Ray Images,A. Djoudi; A. A. Zaid; Y. Guellouma; H. Cherroun,2023 5th International Conference on Pattern Analysis and Intelligent Systems (PAIS),########,2023,"Advanced technologies for medical imaging, such as chest radiography (CXR), have shown the potential to accurately predict diseases through deep learning models. These models play a significant role in detecting life-threatening lung diseases. However, there are challenges associated with the similarities between the patterns and symptoms of these diseases. These similarities can lead to misinterpretations and fatal errors. In addition, DL models generate a large number of parameters. In this study, we propose an innovative approach to identifying lung diseases using CXR images employing a powerful Genetic Deep Convolutional Neural Network. This model combines a deep CNN model with a Genetic Algorithm (GA), allowing us to extract only the most suitable feature representations from the large-scale of DL model parameters while optimizing the model hyper-parameters simultaneously. The targeted dataset is the COVID-19 radiography dataset. The evaluation shows that our GA approach achieves an accuracy of 77.31%, 78.23%, and 78.87% for three different dataset samples, outperforming traditional methods by 0.51%, 1.56%, and 1.7% respectively. Our findings highlight the potential of combining CNN and GA to enhance lung disease detection. This offers promising implications for improved medical image analysis and diagnosis, showcasing promising prospects in this field.",Radiography;Pulmonary diseases;Transfer learning;Genetics;Feature extraction;Convolutional neural networks;Time factors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322027,IEEE Conferences,,,,,,
Unleashing the Power of EfficientNet-ConvNeXt Concatenation for Brain Tumor Classification,A. Panthakkan; S. M. Anzar; W. Mansoor,2023 15th Biomedical Engineering International Conference (BMEiCON),########,2023,"Brain tumors, impacting a substantial global population annually, necessitate precise detection and classification for timely intervention and effective therapy. Though deep learning models have exhibited potential in medical image interpretation, a demand persists for enhanced accuracy and efficiency. This study introduces an optimized solution for brain tumor detection and classification via a concatenated EfficientNet-ConvNeXt model. This novel approach merges the power of EfficientNet and ConvNeXtâ€”two formidable neural networksâ€”to attain extraordinary precision in categorizing various brain tumor types, namely glioma, meningioma, pituitary tumor, and non-tumor. Experimental evaluations validate the modelâ€™s superiority over standalone architectures and existing deep learning techniques in terms of accuracy, sensitivity, and specificity. Demonstrating robustness against image quality fluctuations and variability in tumor types, the model exhibits strong potential for real-world clinical usage. Implementation of our proposed concatenated EfficientNet-ConvNeXt model resulted in substantial performance elevation, achieving an exceptional 99% predictive accuracy. These findings underscore our approachâ€™s accuracy and efficiency, offering substantial aid to radiologists and clinicians in early-stage brain tumor detection and classification. The modelâ€™s predictive capabilities can considerably influence patient prognosis and therapy planning through enabling early intervention.",Deep learning;Training;Sociology;Medical treatment;Predictive models;Sensitivity and specificity;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322070,IEEE Conferences,,,,,,
Automatic Bone Metastasis Classification: An in-depth Comparison of CNN and Transformer Architectures,M. Afnouch; O. Gaddour; F. Bougourzi; Y. Hentati; A. T. Ahmed; M. Abid,2023 International Conference on Innovations in Intelligent Systems and Applications (INISTA),########,2023,"Automatic classification of bone metastases is a major challenge and is receiving increasing attention from the research community. One of the major challenges is the accurate classification of medical images, especially the distinction between benign and malignant images, which can greatly help physicians in decision-making. Recently, several deep-learning techniques have been proposed for medical image classification. Their performance, however, is influenced by both the dataset and the imaging modality. In this work, we investigate the performance of several state-of-the-art CNN architectures, namely InceptionV3, EfficientNet, ResNext50, and DenseNet161, as well as Transformer architectures, namely ViT and DeiT. We trained and tested these algorithms on a large dataset consisting of CT-scan images. The Transformer algorithms were found to be superior to CNN algorithms in detecting bone metastases. In particular, ViT Tiny achieved the best performance in terms of accuracy and F1-score as compared to other architectures.",Technological innovation;Decision making;Medical services;Bones;Transformers;Classification algorithms;Metastasis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10310593,IEEE Conferences,,,,,,
Explainable CBMIR features extractor module to ease Thoracic aortic aneurysm retrieving preventing potential Acute Aortic Dissection,A. Souid; S. B. Othman; M. Hamroun; H. Sakli; M. N. Abdelkarim,2023 International Conference on Innovations in Intelligent Systems and Applications (INISTA),########,2023,"Medical imaging plays an increasingly crucial role in radiology, facilitating accurate and efficient diagnosis of complex medical cases. One such case that requires attention is aortic aneurysm, which often lacks noticeable symptoms and can result in sudden death if left undetected. In order to address this issue, we have developed a feature extractor block utilizing YOLOv5 and EfficientNet. This block effectively extracts and localizes thoracic aortic enlargement in chest radiography, enabling precise identification of the condition. Our research employs a two-stage training strategy, yielding commendable results for the abnormality and detection modules. The sensitivity and specificity achieved were 0.842 and 0.977, respectively. Additionally, the mean average precision at a threshold of 0.5 reached an average value of 0.884, while TAD (thoracic aortic enlargement) detection achieved a score of 0.994. Other metrics demonstrate favorable outcomes when compared to relevant literature. Consequently, this study represents a significant advancement towards the development of intelligent and efficient image retrieval systems, which have the potential to greatly impact the diagnosis of complex medical cases in the emergency department, specifically those related to TAD.",Training;Radiography;Measurement;Technological innovation;Pain;Aneurysm;Sensitivity and specificity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10310391,IEEE Conferences,,,,,,
Detection of COVID-19 based on CT-Scan and X-ray images with Deep Convolutional Neural Network,E. Abidi; S. Bahroun,2023 International Conference on Innovations in Intelligent Systems and Applications (INISTA),########,2023,"This article presents a novel approach for accurately classifying COVID-19 medical images using deep learning techniques. the proposed approach utilizes a convolutional neural network (CNN) architecture with two inputs segmented CT scan images obtained through deep Learning U-Net for image segmentation, and X-ray images. the CNN effectively classifies the images into three categories COVID-19, non-coronavirus, and pneumonia. the experimental results of the proposed method using the COVID-QU dataset and CT scans showcase its superior performance compared to other state of the art approaches With an impressive accuracy rate of 98% the method demonstrates its ability to accurately detect COVID-19 cases. additionally, the achieved high accuracy, F1-score, and low loss values further emphasize its effectiveness this framework offers a highly accurate solution for detecting COVID-19 infections in CT scans and X-ray images Thereby enhancing the diagnostic capabilities of healthcare professionals in identifying COVID-19 cases.",COVID-19;Deep learning;Image segmentation;Technological innovation;Computed tomography;Pulmonary diseases;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10310366,IEEE Conferences,,,,,,
Enhancing Maternofetal Ultrasound Images Toward Boosting Classification Performance on a Diverse and Comprehensive Data,H. Ghabri; W. Fathallah; H. Sakli; M. N. Abdelkarim,2023 International Conference on Innovations in Intelligent Systems and Applications (INISTA),########,2023,"Accurately classifying maternofetal ultrasound images is crucial for diagnosing fetal abnormalities and ensuring the mother and fetusâ€™s health. This study proposes a novel approach for maternofetal ultrasound image classification using a Convolutional Neural Network (CNN) architecture and image preprocessing techniques. We utilized a pre-trained deep neural network on a large image dataset and applied image preprocessing techniques, such as the CLAHE and Unsharp filters, to enhance the quality of the images. Our findings show that the model achieved an impressive overall accuracy of 99.13% on the test set, demonstrating its effectiveness in accurately classifying maternofetal ultrasound images into their respective standard plans. Moreover, the CLAHE and Unsharp filters significantly improved the accuracy of the model by enhancing the image quality. This study highlights the potential of deep learning models in improving medical imaging diagnosis and offers a promising approach for accurate and efficient maternofetal ultrasound image classification.",Technological innovation;Ultrasonic imaging;Transfer learning;Image preprocessing;Convolutional neural networks;Medical diagnosis;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10310548,IEEE Conferences,,,,,,
"A machine learning model for cancer staging, case study: uveal cancer",F. Hamdaoui; A. Sghaier; A. Bdioui; S. Hmissa,2023 International Conference on Innovations in Intelligent Systems and Applications (INISTA),########,2023,"The fields of medicine and computer science have advanced in parallel, providing revolutionary techniques for the diagnosis and treatment of serious diseases. However, despite the accuracy of medical imaging devices, cancer remains a difficult diagnosis. To address this issue, artificial intelligence is increasingly being used for cancer detection. Our work involves in-depth research on the most effective machine learning methods to apply them in the field of medicine. Our key findings include high accuracy in staging uveal cancers using targeted techniques as GAN, and the development of a web application to assist physicians with prediction tasks.",Technological innovation;Medical services;Machine learning;Streaming media;Generative adversarial networks;Cancer detection;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10310635,IEEE Conferences,,,,,,
Retinal pathologies detection in OCT images based on Bilinear convolutional neural network,Z. Haddad; B. M. Yaya; H. Zgolli; D. SidibÃ©; H. Tabia; N. Khlifa,2023 International Conference on Innovations in Intelligent Systems and Applications (INISTA),########,2023,"Retinal pathologies like choroidal neovascularization (CNV), drusen, and diabetic macular edema (DME) can give rise to microvascular alterations in the retina, ultimately resulting in vision impairment. The manual detection of these diseases poses a significant challenge and necessitates specialized medical expertise. To address this challenge, our study introduces novel deep learning methods for the detection of these ocular pathologies automatically and based on optical coherence tomography (OCT) scans. In our experimental setup, we utilized a dataset comprising 6000 OCT images sourced from the publicly available Kaggle dataset. Through comprehensive evaluations, our study revealed that the implementation of a bilinear convolutional neural network (B-CNN) yielded the highest classification score, surpassing the accuracy achieved by alternative models. Furthermore, when compared to other deep learning networks, our proposed approach showcased superior performance in the early diagnosis of these three ocular diseases.",Deep learning;Training;Pathology;Optical coherence tomography;Visual impairment;Retina;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10310341,IEEE Conferences,,,,,,
Leveraging Convolutional Neural Networks for Malaria Detection from Red Blood Cell Images,D. Petrea; G. -I. Stoleru; A. Iftene,2023 International Conference on Innovations in Intelligent Systems and Applications (INISTA),########,2023,"In recent years, artificial intelligence (AI) has started to be used more and more in the medical field. This paper presents a study focused on malaria classification based on segmented blood cells from images collected from the National Institutes of Health (NIH) database. The research involved the development of a handcrafted convolutional neural network (CNN), as well as experimentation with various fine-tuning approaches using the VGG16 architecture. The conducted experiments have yielded promising results, providing empirical evidence for the potential effectiveness of these techniques in future applications. The augmented CNN achieved an impressive accuracy of 96.51%, while the VGG16 fully trainable model outperformed it with an accuracy of 96.69%. A problem that needs to be analyzed more carefully in the future concerns the explainability of the results so that they can be used with confidence by healthcare professionals.",Technological innovation;Image segmentation;Red blood cells;Databases;Malaria;Neural networks;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10310367,IEEE Conferences,,,,,,
Reduction of Annotation Effort in Medical Image Analysis Based on Self-supervised Learning,K. -H. Chan; Y. -C. Zeng,2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),########,2023,"Self-supervised learning is increasingly valued in medical imaging, with the hope of solving the problem of scarce high-quality labeled medical image datasets. This study proposes a self-supervised learning training framework for single-view and multi-view images, which combines multi-task and semi-supervised loss applied to medical image classification and detection problems. It can save more than 50% of labeled training data and significantly reduce the demand for medical images and labeled data. Meanwhile, the proposed method achieves high performance in the classifications.",Wrist;Heating systems;Training;Tuberculosis;Transfer learning;Training data;Self-supervised learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317280,IEEE Conferences,,,,,,
Deep Learning Model for Automated Detection and Classification of Brain Tumor Disorder,K. Lamba; S. Rani,"2023 IEEE International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics (DISCOVER)",########,2023,"Due to life-threatening condition of brain tumor, its timely identification is required in the healthcare industry to save lives of millions of people suffering from such disorder worldwide. Although, conventional techniques have been adopted by radiologists for diagnosing brain tumor but its asymptotic behaviour at an early stage may result in false predictions by radiologists due to the presence of extremely tiny size tumor that remains undiagnosed and worsen health conditions with the passage of time. To overcome such challenges, there is a requirement of developing computer assisted systems to aid radiologists in decision-making process while diagnosing brain tumor disorder to have better accuracy and efficient outcomes as compared with traditional approaches. Deep neural networks also have ability to extract features from input medical images at large scale, based on which extended version of residual network following 50 layers demonstrated great potential in diagnosing brain tumor along with utilizing support vector machine for classification purpose that distinguished infected images from healthy ones. After providing training to the proposed model, validation is done on the basis of test data and predicted responses after classification task. Thus, proposed model achieved accuracy of 95.5%, 97.28% recall, 86% specificity, 97.46% precision and 97.37% F1-Score which outperforms the existing approaches while diagnosing brain tumor disorder in the healthcare.",Training;Support vector machines;Computational modeling;Medical services;Very large scale integration;Brain modeling;Feature extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10316708,IEEE Conferences,,,,,,
Endoscopic Image Analysis for Gastrointestinal Tract Disease Diagnosis Using Nature Inspired Algorithm With Deep Learning Approach,A. Alruban; E. Alabdulkreem; M. M. Eltahir; A. R. Alharbi; I. Issaoui; A. Sayed,IEEE Access,########,2023,"Endoscopic image analysis has played a pivotal function in the diagnosis and management of gastrointestinal (GI) tract diseases. Gastrointestinal endoscopy is a medical procedure where a flexible tube with an endoscope (camera) is inserted into the GI tract to visualize the inner lining of the colon, esophagus, stomach, and small intestine. The videos and images attained during endoscopy provide valuable data for detecting and monitoring a large number of GI diseases. Computer-assisted automated diagnosis technique helps to achieve accurate diagnoses and provide the patient the relevant medical care. Machine learning (ML) and deep learning (DL) methods have been exploited to endoscopic images for classifying diseases and providing diagnostic support. Convolutional Neural Networks (CNN) and other DL algorithms can learn to discriminate between various kinds of GI lesions based on visual properties. This study presents an Endoscopic Image Analysis for Gastrointestinal Tract Disease Diagnosis using an inspired Algorithm with Deep Learning (EIAGTD-NIADL) technique. The EIAGTD-NIADL technique intends to examine the endoscopic images using nature nature-inspired algorithm with a DL model for gastrointestinal tract disease detection and classification. To pre-process the input endoscopic images, the EIAGTD-NIADL technique uses a bilateral filtering (BF) approach. For feature extraction, the EIAGTD-NIADL technique applies an improved ShuffleNet model. To improve the efficacy of the improved ShuffleNet model, the EIAGTD-NIADL technique uses an improved spotted hyena optimizer (ISHO) algorithm. Finally, the classification process is performed by the use of the stacked long short-term memory (SLSTM) method. The experimental outcomes of the EIAGTD-NIADL system can be confirmed on benchmark medical image datasets. The obtained outcomes demonstrate the promising results of the EIAGTD-NIADL approach over other models.",Feature extraction;Diseases;Classification algorithms;Gastrointestinal tract;Medical diagnostic imaging;Convolutional neural networks;Computational modeling;Endoscopes;Biomedical image processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10319463,IEEE Journals,,,,,,
Artificial Intelligence Analysis for Small Object Detection in Urine Sediment Images,J. Cui; M. Yu; S. Chan; H. Wang,IECON 2023- 49th Annual Conference of the IEEE Industrial Electronics Society,########,2023,"Over recent years, artificial intelligence (AI) methods have reformed the zone of medical imaging. Many AI-based models have been created and improved to related medical image analysis and interpretation. Object detection in medical images is a fundamental task in computer vision. With the continuous development of deep learning technology in recent years, we have been able to achieve good detection performance for conventional objects, but it has still been challenging for small objects due to their small size. In this paper, we propose an artificial intelligence analysis method for object detection in urine sediment images for auxiliary diagnosis. Firstly, we introduce a New Path Aggregation Network (NPANet), which improves the multi-scale fusion section. It can be more beneficial to help detect relatively small cellular objects, thus reducing the rate of missed detection. Secondly, we design an Adaptive-IoU ($AIoU$) loss that can be more easily adopted to achieve better detection accuracy for the model. Finally, the experiments are conducted on our Dataset (named UriSed2K) and the public Urine Microscopic Image Dataset, and achieve the state-of-the-art results. Especially, our detector reaches 61.8% on $AP_{small}$ on the UriSed2K and our work also contributes to the development of medical image processing.",Deep learning;Industrial electronics;Detectors;Object detection;Sediments;Biomedical image processing;Electron microscopy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10312052,IEEE Conferences,,,,,,
Automated Diabetic Foot Ulcer Detection and Classification Using Deep Learning,S. Nagaraju; K. V. Kumar; B. P. Rani; E. L. Lydia; M. K. Ishak; I. Filali; F. K. Karim; S. M. Mostafa,IEEE Access,########,2023,"Diabetic foot ulcers (DFU) are a common and serious complication in individuals with diabetes, and early detection plays a crucial role in effective treatment and prevention of further complications. Automated DFU Detection and Classification using Deep learning (DL) refers to the application of deep learning techniques to automatically detect and classify diabetic foot ulcers from medical images. DL, a subfield of machine learning, has shown promising results in medical imaging analysis, including diabetic foot ulcer detection. The use of deep learning in DFU detection provides various benefits, including the ability to learn complex features, adaptability to different image modalities, and the potential for high accuracy in detection and classification tasks. Therefore, this article introduces a novel sparrow search optimization (SSO) with deep learning enabled diabetic foot ulcer detection and classification (SSODL-DFUDC) technique. The presented SSODL-DFUDC techniqueâ€™s goal lies in identifying and classifying DFU. The proposed technique employs the Inception-ResNet-v2 model for feature vector generation to accomplish this. Since the trial and error manual hyperparameter tuning of the Inception-ResNet-v2 model is a tedious and erroneous process, the SSO algorithm can be used for the optimal hyperparameter selection of the Inception-ResNet-v2 model which in turn enhances the overall DFU classification results. Moreover, the classification of DFU takes place using the stacked sparse autoencoder (SSAE) model. The comprehensive experimental outcomes demonstrate the improved performance of the SSODL-DFUDC system related to existing DL techniques.",Diabetes;Convolutional neural networks;Medical diagnostic imaging;Deep learning;Skin;Task analysis;Optimization;Biomedical image processing;Computer aided diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10315129,IEEE Journals,,,,,,
Machine learning methods and algorithms for predicting congenital heart pathologies,S. B. Rakhmetulayeva; A. K. Bolshibayeva; A. K. Mukasheva; B. M. Ukibassov; Z. O. Zhanabekov; D. Diaz,2023 IEEE 17th International Conference on Application of Information and Communication Technologies (AICT),########,2023,"The aim of this study is to develop machine learning algorithms for predicting pathologies of the cardiovascular system, based on the enrichment and expansion of existing data sets. Various augmentation techniques will be compared and combined augmentation to determine the most appropriate ones for improving and complementing heart images.",Heart;Training;Pathology;Image segmentation;Machine learning algorithms;Echocardiography;Neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10313184,IEEE Conferences,,,,,,
Deep Semantic Segmentation Models for Lung Ultrasound,M. HlibokÃ½; D. Lahunov; S. GecÃ­k; M. Bundzel,2023 World Symposium on Digital Intelligence for Systems and Machines (DISA),########,2023,"This scientific paper explores the application of CNN and ViT models for processing lung ultrasound (LUS) images as part of the APVV project (APVV-20-0232). It provides an understanding of LUS ultrasound processing and the architectures of CNN and ViT models. The study includes an experimental comparison of various CNN and ViT models using a diverse LUS image dataset. Results reveal that CNN models perform well with limited data, while ViT models excel in capturing complex relationships with larger datasets. This research contributes valuable insights to improve LUS ultrasound processing for accurate diagnosis in medical imaging.",Ultrasonic imaging;Semantic segmentation;Lung;Transformers;Data models;Convolutional neural networks;Digital intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308907,IEEE Conferences,,,,,,
TensorFlow vs. PyTorch in Classifying Medical Images â€“ preliminary results,L. Stanescu; G. Dinu,"2023 27th International Conference on System Theory, Control and Computing (ICSTCC)",########,2023,"The paper presents a first series of results regarding the comparison of two deep neural networks designed, trained, and validated with the help of two deep learning frameworks, PyTorch and TensorFlow, in the classification of a large set of breast histopathology images. The original dataset consisted of 162 whole mount slide images of breast cancer specimens scanned at $40\times$. From that, 277,524 patches of size $50 \times 50$ were extracted (198,738 invasive ductal carcinoma negative and 78,786 positive). Invasive Ductal Carcinoma is the most common subtype of all breast cancers. The metrics compared in the training and validation phases were loss, accuracy, precision, recall and specificity. The architecture used for convolutional neural network has 8 layers: 3 convolution layers, 3 max-pooling layers, 1 linear layer and a sigmoid function. The number of epochs is considered a hyperparameter. It defines the number of times the entire data set must be worked through the learning algorithm. In our study we used 5 epochs. For the 8-layer network, PyTorch performed better in accuracy, precision, recall, and loss in both training and validation. In contrast, specificity is lower. Recall or sensitivity is around 73%, and specificity around 93%. Our estimates can be considered and continued since a diagnostic study must have both sensitivity and specificity of at least 70%. The results so far are promising, and we propose to continue the experiments in several directions.",Training;Deep learning;Sensitivity;Histopathology;Convolution;Sensitivity and specificity;Control systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308472,IEEE Conferences,,,,,,
U-Net++DSM: Improved U-Net++ for Brain Tumor Segmentation With Deep Supervision Mechanism,K. Wisaeng,IEEE Access,########,2023,"The segmentation of brain tumors is an important and challenging content in medical image processing. Relying solely on human experts to manually segment large volumes of data can be time-consuming and delay diagnosis. To address this challenge, researchers have set out to develop an algorithm that can automatically determine whether MRI images contain brain tumors and identify their features. This paper proposes the U-Net++DSM, a collaborative approach combining U-Net++ with Deep Supervision Mechanism (DSM) as its backbone. To enhance the segmentation power of U-Net++DSM, medical professionals have trained a dilation operator using fully annotated images. The results of this method demonstrate that the combination of U-Net++DSM and the dilation operator significantly improves segmentation accuracy, especially when the number of fully-labeled images is limited. The results show that the proposed U-Net++DSM outperforms traditional U-Net models by achieving high segmentation performance, surpassing other state-of-the-art models, with a sensitivity of 98.59%, a specificity of 98.64%, an accuracy of 98.64%, and an average Dice score of 98.02% when tested on publicly available databases. Compared to other existing segmentation methods, the U-Net++DSM method has the potential to yield even better brain tumor segmentation results in terms of pixel-based classification and dice similarity performance metrics.",Tumors;Image segmentation;Magnetic resonance imaging;Brain modeling;Medical services;Medical diagnostic imaging;Feature extraction;Brain cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10311588,IEEE Journals,,,,,,
An Approach to Classifying Breast Density Level Using Deep Learning-Based Segmentation Model on Full-Field Digital Mammograms,M. Ãœ. Ã–zÄ±Ã§; A. S. Yilmaz; H. Ä°. Sandiraz; B. H. Estanto,2023 7th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT),########,2023,"Breast density is a structure that determines the ratio of glandular and connective tissue in a woman's breast, directly affecting the assessment of breast cancer risk in radiological images. According to the Breast Imaging and Reporting Data System, it is classified on an international scale as follows: fatty (A), scattered density (B), heterogeneously dense (C), and extremely dense (D). The specified density categories can be examined in four views from a single patient, which are Right Craniocaudal, Right Mediolateral Oblique, Left Craniocaudal, and Left Mediolateral Oblique, using low-dose X-ray-based mammography. One of the major challenges in artificial intelligence-based breast density classification studies is the presence of the pectoral muscle, which affects the results in Right Mediolateral Oblique and Left Mediolateral Oblique views. Therefore, pre-processing methods are used to attempt automatic or semi-automatic removal of pectoral muscle from the images. This study proposes an approach for breast density classification using four-view mammograms from the VinDr-Mammo dataset, with the YOLOv8 segmentation module, while ensuring that the pectoral muscle is not excluded from the image. As a result of the study, successful breast density classification was achieved without removing the pectoral muscle, and the advantages and disadvantages of the system were discussed.",Image segmentation;Connective tissue;Imaging;Breast;Muscles;Mammography;Data systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304947,IEEE Conferences,,,,,,
Deep Learning for Comparative Study of Ovarian Cancer Detection on Histopathological Images,W. O. Falana; A. Serener; S. Serte,2023 7th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT),########,2023,"Ovarian cancer is a serious threat to women's health. It comes from the ovaries, which are important for producing eggs and hormones, such as estrogen and progesterone. Ovarian cancer is usually a consequence of abnormal ovarian cell division, which leads to the creation of malignant tumors. The identification of ovarian cancer in its early stages can be difficult. Nonetheless, having an understanding of the detectable methods at hand is crucial in combating this illness. Deep learning is highly effective in detecting and distinguishing a variety of diseases through medical imaging. This study analyzes how effective deep learning architectures are at identifying cancerous and non-cancerous ovarian histopathological images. The study incorporates three distinguished deep learning models, including DenseNet-201, ResNet-50, and VGG-19. To evaluate models, important performance metrics, such as accuracy, sensitivity, specificity, and AUC, are used comprehensively. The observations show notable discrepancies in the models' abilities, with VGG-19 emerging as the leading structure, presenting exceptional accuracy and sensitivity. ResNet-50 demonstrates exceptional performance, particularly in specificity and AUC. Conversely, DenseNet-201, even though accomplishing a high level of accuracy, shows a need for improvement in specificity. The results show how deep learning can help diagnose cancerous and non-cancerous cells in the ovaries. They also suggest a model to use for classification.",Deep learning;Sensitivity;Microprocessors;Malignant tumors;Computer architecture;Reliability;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304913,IEEE Conferences,,,,,,
Introducing CWDCMFE-MBRC Technique for Breast Cancer Detection: A Comparative Survey of Novel Approaches,K. S. Krishna; P. G. K. Prince,2023 International Conference on Sustainable Emerging Innovations in Engineering and Technology (ICSEIET),########,2023,"Breast cancer poses a significant public health concern for countless women worldwide. Detecting breast cancer in its early stages plays a pivotal role in enhancing patient outcomes and reducing mortality rates. Various imaging techniques have been developed to aid in the early detection of breast cancer., including mammography, ultrasound, MRI, thermography, and x-ray. However, these techniques have their limitations with regard to accuracy, sensitivity, specificity, and precision. In recent years, ways of deep learning have emerged as a promising approach for breast cancer detection. Convolutional neural networks (CNNs) have demonstrated high accuracy in detecting breast cancer from medical images. However, CNNs have certain limitations, one of which is the need for substantial amounts of labeled data. This requirement can present challenges, particularly in the field of medical imaging, where acquiring a large volume of labeled data may be difficult. Moreover, CNNs suffer from a lack of interpretability, posing challenges in comprehending the decision-making process behind the network's predictions. This absence of interpretability makes it arduous to gain insights into how the network arrives at its conclusions. To address these limitations, researchers have developed alternative deep learning methods for breast cancer detection. One such method is the Complex Wavelet Deep Convolutional Multiple Feature Extraction-MBRC (CWDCMFE-MBRC) method. This method brings together the benefits of wavelet transform, deep convolutional neural networks, and multiple feature extraction to achieve high accuracy in breast cancer detection. The CWDCMFE-MBRC method involves several steps. First, the input picture is broken down into sub-bands using the complex wavelet transform. Then, the sub-bands are passed through a deep convolutional neural network to extract high-level features. Next, multiple features are extracted from the output of the neural network using PCA and LDA. Finally, the resulting characteristics are used as input to the MBRC classifier to detect breast cancer. The CWDCMFE-MBRC method has several advantages over CNNs and other traditional imaging techniques. Firstly, it achieves high accuracy in breast cancer detection while requiring less labeled data than CNNs. Additionally, the method provides interpretability through the use of multiple feature extraction techniques, making it easier to understand how the network arrives at its predictions. Deep learning methods, such as CNNs and the CWDCMFE-MBRC method, have shown promise in improving breast cancer detection accuracy. This study aims to conduct a comprehensive survey of breast cancer detection methods, including both traditional imaging techniques and deep learning methods, provide a complete snapshot of the current state of the field.",Deep learning;Surveys;Wavelet transforms;Sensitivity;Ultrasonic imaging;Feature extraction;Breast cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10303314,IEEE Conferences,,,,,,
Image Recognition Using Machine Learning Techniques,K. Chandni; A. P. Singh; A. K. Rai; A. S. Chauhan,2023 International Conference on Sustainable Emerging Innovations in Engineering and Technology (ICSEIET),########,2023,"Image recognition has become a prominent area of research in recent years, and the development of deep learning models has significantly improved the accuracy of image classification tasks. This paper provides an overview of deep learning techniques using two models in image recognition, including deep belief network and convolutional neural network. Additionally, the paper examines some of the applications of image recognition, including object detection, facial recognition, and medical imaging. The research work in this paper provides a comprehensive overview of the state of the art in image recognition in machine learning and highlights some of the key challenges and opportunities for future research. Artificial neural networks are used in the â€œdeep learningâ€ branch of machine learning to mimic how the human brain learns.",Deep learning;Technological innovation;Image recognition;Shape;Feature extraction;Convolutional neural networks;Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10303370,IEEE Conferences,,,,,,
An Intelligent Ovarian Ultrasound Image Generation Algorithm based on Generative Adversarial Networks,H. Xiang; Y. Zhao; H. Huang; K. Miao; X. Dong,2023 IEEE International Ultrasonics Symposium (IUS),########,2023,"A deep learning framework based on generative adversarial networks (GAN) for ovarian ultrasound (US) images synthesis is investigated. This method offers an effective solution for addressing the issue of insufficient and unbalanced data in ovarian disease research. The proposed network, built on the Triple-GAN model, can synthesize a large number of medical ovarian US images which are difficult to distinguish from the real ones. This approach effectively increases the available volume of the ultrasound images of ovarian diseases and facilitates deep learning applications in ovarian ultrasound images. It provides reliable training data replacements. The generated data were validated through professional appraisal, classification method and image metrics. The results demonstrated high credibility and quality. The feasibility of using the generated data in the intelligent classification algorithm of ovarian diseases is verified, which has important practical significance for the research and development of artificial intelligence diagnosis algorithms for various diseases in the future.",Deep learning;Ultrasonic imaging;Image synthesis;Training data;Generative adversarial networks;Classification algorithms;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10306659,IEEE Conferences,,,,,,
Can Data from One Medical Center be Enough to Generalize Lung Ultrasound Pattern Classification? A Multi-Center Domain Generalization Study,U. Khan; E. Torri; A. Smargiassi; R. Inchingolo; L. Demi,2023 IEEE International Ultrasonics Symposium (IUS),########,2023,"Domain shift refers to change of data distribution between training and testing datasets. In case of medical imaging, domain shift is extensive, specifically for multi-center studies. Different medical centers may use different scanners, imaging protocols, subject populations, etc. To mitigate this effect, domain generalization (DG) has been used over the time. In this regard, our focus is to analyze if a pre-trained model can generalize lung ultrasound (LUS) pattern classification among pneumonia patients. Furthermore, if LUS data from one medical center is enough to generalize classification task across different medical centers. Investigated LUS patterns include horizontal artifacts, vertical artifacts, and small to large consolidations. As a proof of concept, data from medical center in Brescia (Italy) is used as source dataset whereas data from medical center in Rome (Italy) is considered as target dataset. ResNet-18 model (pre-trained on ImageNet dataset) is employed. The pre-trained model is trained on the source dataset with transfer learning using linear probing (LP) and linear probing with fine-tuning (LP-FT) approach. Results show that the pre-trained model can generalize much better over the target dataset when it undergoes LP-FT rather than only LP, achieving a mean F1-Score of 63.08%. These findings encourage the use of pre-trained models to generalize across different medical centers for LUS data analysis. Furthermore, they suggest that one medical center as the source dataset may be enough to generalize across other medical centers with state-of-the-art comparable performance for LUS pattern classification.",Training;Analytical models;Ultrasonic imaging;Transfer learning;Pattern classification;Lung;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307947,IEEE Conferences,,,,,,
Vision Transformer and Multiview Classification for Lesion Detection in 3D Cranial Ultrasound,F. Estermann; V. Kaftandjian; P. Guy; P. Quetin; P. Delachartre,2023 IEEE International Ultrasonics Symposium (IUS),########,2023,"With increasing advances in the field of medical brain imaging, we can now assess the presence of punctate white matter lesions (PWML) in the preterm infant. While some studies report a link between these lesions and adverse long-term outcomes, automatic detection of PWML through ultrasound (US) imaging could better assist doctors in diagnosis, at a lower cost than MRI. Many papers focus on MR biomedical image benchmark datasets, but few methods seem to tackle the detection of very small lesions in US images, because it is really challenging due to high class imbalance and low contrast. In this work, we propose a two-phase strategy: 1) Segmentation with a vision transformer to increase the number of detected lesions. 2) Multi-view classification of the lesions predicted in the output mask to reduce the number of false alarms and improve precision. We also compare 3 methods of preprocessing for input data. As a result, our method achieves better performances for PWML detection in US images compared to the best published models, with recall and precision reaching 82% and 60% respectively.",Neuroimaging;Pediatrics;Ultrasonic imaging;Three-dimensional displays;Magnetic resonance imaging;Medical services;Transformers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10306345,IEEE Conferences,,,,,,
Expert-Level Reliability of Automated Skin Ultrasonography Segmentation,A. L. SoÃ³s; M. Vajay; Z. SzekÃ©r; A. HorvÃ¡th; D. Csabai; P. MarosÃ¡n-Vilimszky; G. CsÃ¡ny; N. Kiss; K. Szalai; M. GyÃ¶ngy,2023 IEEE International Ultrasonics Symposium (IUS),########,2023,"The treatment of skin diseases such as skin cancer and chronic inflammatory skin disorders are highly dependent on the clinical assessment of the shape and size of lesions, for which high frequency ultrasound (HFUS) has shown promise. However, the interpretation of ultrasound images can be challenging even for experts. Therefore, automatic segmentation of different skin structures and lesions on HFUS images could prove to be useful. The current work uses a deep neural network (DNN) to automatically segment different skin structures and mark lesions on HFUS images, and evaluates its performance compared to human physician experts. A portable HFUS imaging device with optical image guidance was used to acquire a dataset of 1691 skin ultrasound samples. Two labellers annotated the dataset, and it was split into training and testing subsets. The self-configuring nnU-Net DNN framework was used for segmentation. Performance was evaluated by comparing the DNN segmentation of all skin structures with the labels of a test set, and by comparing the annotations of an attending radiologist and dermatologist with respect to the model and each other using the Dice score and estimation error (standard deviation) of the maximal lesion depth (hereon ""lesion depth error""). The developed DNN accurately segmented the epidermis, dermis, subcutis and lesion, with Dice coefficients of 0.863, 0.922, 0.962, and 0.728, respectively. The agreement between the two human experts in terms of the Dice coefficient and lesion depth error was 0.726, 0.42 mm, respectively. The model provided comparable accuracy, with Dice coefficients of 0.731 (radiologist) and 0.733 (dermatologist), and lesion depth errors of 0.38 mm (radiologist) and 0.19 mm (dermatologist). The results validate the performance of the automated skin segmentation model and highlight the need to compare performance with inter-expert reliability. The use of the machine learning model as an interpretation tool for medical experts may in the future help determine the shape and size of skin lesions, which has a direct impact on their management.",Performance evaluation;Image segmentation;Ultrasonic imaging;Shape;Artificial neural networks;Predictive models;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307251,IEEE Conferences,,,,,,
A Multimodal Adversarial Database: Towards A Comprehensive Assessment of Adversarial Attacks and Defenses on Medical Images,J. Hu; Y. He; W. Zhang; S. Pang; R. Ma; A. Du,2023 IEEE 10th International Conference on Data Science and Advanced Analytics (DSAA),########,2023,"Deep learning models have been widely applied in many fields, including medical image analysis and computer-aided disease diagnosis. However, these models are easily fooled by adversarial attacks from some created adversarial examples which are hardly distinguished by humans. In this paper, we implement a comprehensive assessment of six popular adversarial attacks on four multimodal medical image datasets using two main deep learning-based target models. Moreover, in order to evaluate the capability of defense, two new defense methods are leveraged to cope with medical adversarial attacks. More importantly, we also build and release a big multimodal medical adversarial database (including four medical adversarial datasets) with 712,596 examples to facilitate future research of adversarial attacks and defenses in the multimodal medical image field. Extensive experiments indicate that all-sided adversarial attacks like BIM are still scarce under different evaluation metrics and defenses are not universally successful.",Deep learning;Measurement;Analytical models;Image analysis;Databases;Computational modeling;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10302594,IEEE Conferences,,,,,,
Towards Explainable Artificial Intelligence for Pneumonia and Tuberculosis Classification from Chest X-Ray,G. H. Dagnaw; M. E. Mouthadi,2023 International Conference on Information and Communication Technology for Development for Africa (ICT4DA),########,2023,"The scientific community has shown significant interest in the application of deep learning models for the classification of tuberculosis and pneumonia from Chest X-ray (CXR) images. However, the primary emphasis of the majority of research lies only on enhancing classification accuracy. In this paper, we propose Explainable Artificial Intelligence (XAI) and a lightweight convolutional neural network (CNN) to enhance classification accuracy and explainability. The main contributions of this research are applying Contrast Limited Adaptive Histogram Equalization (CLAHE) to increase the visibility of CXR images, proposing a lightweight CNN, and evaluating it with a visual-based XAI model. We compare the proposed CNN with VGG16, Densnet201, EfficientNEtB0, InceptionV3, and MobileNetV2. We use the score-CAM XAI model to visualize why the model makes a certain decision. The proposed CNN has a classification accuracy of 97.5%, a precision of 97.4%, a 97.4% sensitivity, a 98.6% specificity, and a 97.4% F1 score on the testing set in multi-class classification. The findings indicate that deep learning and XAI can enhance trust in automatic disease detection and classification.",Deep learning;Adaptation models;Visualization;Tuberculosis;Pulmonary diseases;Convolutional neural networks;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10302183,IEEE Conferences,,,,,,
ResMultNet-50: An Automatic Medical Image Diagnosis Approach for Lung Diseases Using Deep Transfer Learning,I. Mwendo; P. Gikunda; A. Maina,2023 International Conference on Information and Communication Technology for Development for Africa (ICT4DA),########,2023,"Lung diseases such as Covid-19, Pneumonia and Tuberculosis remains to be among the leading causes of deaths globally. These diseases present themselves in a similar manner bearing common signs and symptoms such as coughing, fever, fatigue and shortness of breaths. To prevent adverse effects of these diseases and save more lives, early detection and diagnosis of the aforementioned diseases is necessary. This paper proposes a deep transfer learning model: ResMultNet-50 to assist radiologists in their work while adopting inverse class weighting approach to handle class imbalance problem. The proposed approach relied on fine-tuned ResNet-50 for the diagnostic task of detecting the three respiratory diseases from chest x-rays. In the study, a data set comprising of 13,188 chest x-ray images was used and the proposed approach achieved an average accuracy of 96.12%. This model outperformed other deep learning models and transfer learning models used in the previous studies for solving multi-class related problems.",Deep learning;COVID-19;Tuberculosis;Pulmonary diseases;Transfer learning;Fatigue;Information and communication technology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10302230,IEEE Conferences,,,,,,
A Review of Point Sets Parameterization Methods for Curve Fitting,Z. Zhu; L. You; J. Zhang,2023 27th International Conference Information Visualisation (IV),########,2023,"Parametric curve fitting which includes curve approximation and interpolation is a critical topic in many fields, such as medical images, 3D reconstruction and data visualization, etc. To approximate or interpolate point sets with a curve, parameterization of points is usually a key step, which has a great impact on the final result. Many methods that have been proposed to tackle this problem. To our best knowledge, there is not any review paper about point set parameterization for curve fitting. In this paper, we will review the proposed techniques of parameterizing point sets for curve fitting. These methods will be divided into two categories. In the first category, the parameterization of point sets and curve fitting are treated as two separable and successive steps, specifically, the parameters obtained using some techniques in the first step are used for the curve fitting process in the second step. The two steps are solved together in the second category using more complex techniques. Methods in each of the two types will be reviewed in this paper.",Interpolation;Three-dimensional displays;Neural networks;Fitting;Data visualization;Curve fitting;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10303345,IEEE Conferences,,,,,,
Research and implementation of deep network-based breast tumor classification,L. Haotian; W. Shuai,2023 International Conference on Cyber-Physical Social Intelligence (ICCSI),########,2023,"With the development of computer technology, computer-aided diagnosis technology has become one of the important research directions in the field of medicine. Among them, considerable progress has been made in the application of artificial intelligence technology in medical data processing, especially in medical imaging. Currently, breast cancer is one of the most common malignant tumors in women worldwide, accounting for about one-fourth of all malignant tumors, and its incidence has been on the rise in the past decades. If breast cancer can be successfully diagnosed at an early stage, the survival rate of patients can be greatly improved, so early diagnosis and treatment of breast cancer is especially important. However, most of the current diagnostic methods are application-oriented and have not been explored in depth on how to improve the recognition accuracy, so the purpose of this paper is to explore how to improve the accuracy of breast cancer recognition.",Breast tumors;Malignant tumors;Data processing;Breast cancer;Social intelligence;Computer aided diagnosis;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10303888,IEEE Conferences,,,,,,
Integration of AI and SNOMED CT in Chest X-Ray Diagnosis Software System,N. Nguyen-Chi; T. Nguyen-Thanh; Q. Nguyen-Thanh,2023 International Symposium on Electrical and Electronics Engineering (ISEE),########,2023,"In the medical field, the application of artificial intelligence (AI), especially deep learning models of convolutional neural networks (CNN) to disease diagnosis is becoming more and more popular. However, the research and application of AI in disease diagnosis in Vietnam is still limited. Disease prediction is still fragmentary, there is no link between clinical and laboratory results, as well as the problem of disease identification is not synchronized and consistent among studies because labels are described in text that can be confusing, especially in the medical field. Therefore, this research paper applies Systematized Nomenclature of Medicine-Clinical Terms (SNOMED CT) as a set of electronic clinical terminology in labeling input data for machine learning in the field of medical pathology diagnosis as well as making suggestion of the templates for each specific disease. Specifically, the article evaluates the results of a method of diagnosing abnormalities in chest X-ray (XR) images based on the Densenet-121 network architecture and normalizing the forms based on SNOMED CT codes. DICOM medical images are obtained directly and automatically from the X-ray modality through the Picture Archiving and Communication System (PACS) at Telemedicine and Mobile Healthcare lab, Ho Chi Minh City University of Technology. Chest abnormalities are detected, classified and returned to the SNOMECD CT codes corresponding to the major problems in the sample template on workstation using the AI TOOL ASSISTANT software system. A survey of 222 actual abnormally diagnosed chest X-rays images at the international clinic participating in the trial showed that the accurate of the prediction results was 82% when compared with the doctor's conclusions according to these samples.",Codes;Computed tomography;Software systems;Picture archiving and communication systems;Medical diagnosis;Artificial intelligence;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10299854,IEEE Conferences,,,,,,
AI for Automated Thoracic Disease Assessment from X-Ray Imaging: a Review,H. M. Ali; S. M. El-Metwally; M. A. Wahed,2023 5th Novel Intelligent and Leading Emerging Sciences Conference (NILES),########,2023,"With the increasing availability of digital X-ray imaging, artificial intelligence (AI) has emerged as a promising tool for automating the assessment of thoracic diseases. The objective of this study is to systematically review the artificial intelligence (AI) and deep learning methods proposed for the automated assessment of thoracic diseases from chest X-ray images. A thorough search of the relevant literature was conducted, and studies that met the inclusion criteria were critically reviewed. Information on the datasets, model architectures, evaluation metrics, and results was extracted. Convolutional neural networks are prevalent, achieving a state-of-the-art classification performance. Recent studies have explored more complex tasks such as disease localization, segmentation, and report generation. The multitask and multimodal approaches are promising. Challenges related to the data, evaluations, and clinical adoption were identified. This study prevails that there is a significant progress in using deep learning for automated chest X-ray analysis. Further research is needed to validate these models in real-world settings and to facilitate their integration into clinical workflows.",Deep learning;Solid modeling;Analytical models;Recurrent neural networks;Computational modeling;Convolutional neural networks;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296683,IEEE Conferences,,,,,,
Multi-label Ophthalmological Disease Classification Using Vision Transformers,I. Hisham; M. I. Khalil; H. Abbas,2023 5th Novel Intelligent and Leading Emerging Sciences Conference (NILES),########,2023,"Immediate diagnosis and timely treatment of fundus diseases are crucial in preventing permanent vision loss. Fundus imaging, a primary method in retinal imaging, effectively captures and investigates anatomical features and abnormalities in the human eye, and aids in observing and identifying various ophthalmological diseases. These diseases are indicated by variations in or surrounding structures such as the optic disk and the blood vessels. Given the complex nature of these diseases, itâ€™s common for fundus images to show a patient having multiple conditions in one or both eyes. This work aims to develop and suggest various methods for detecting and classifying ophthalmological diseases using multiple deep learning vision transformers. The experiments were conducted and models tested using the ODIR-2019 dataset, which contains fundus images of both left and right eyes. The dataset includes eight categories, and the goal was to implement a transformer based approach for multi-label classification. The study used pre-trained Swin Transformer V2 and Data Efficient Image Transformer (DeiT) in its proposed approaches. After various preprocessing and fine-tuning techniques, the results showed that the transformer approach outperformed state-of-the-art CNN models in terms of accuracy. It achieved an accuracy of 93.1% and AUC of 94.4% for multi-label ophthalmological disease classification.",Deep learning;Training;Optical losses;Neural networks;Transformers;Retina;Optical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296804,IEEE Conferences,,,,,,
Enhancing Breast Cancer Diagnosis with Vision Transformer-based Ultrasound Image Classification,A. Ashraf; A. E. Nagib; H. Mohamed,2023 5th Novel Intelligent and Leading Emerging Sciences Conference (NILES),########,2023,"Around the world, millions of individuals are impacted by the awful disease known as breast cancer. Early detection is essential for more successful therapy. Deep learning techniques provide enormous potential for the detection and categorization of breast cancer in medical images, according to a recent study. This study examined the accuracy with which a Vision Transformer model could distinguish between different types of breast cancer. The model showed a 95% classification accuracy after being trained on a collection of ultrasound pictures. According to the study, radiologists may be able to make more accurate diagnoses since the Vision Transformer model can classify breast cancer more precisely than the CNN model.",Deep learning;Ultrasonic imaging;Computational modeling;Medical treatment;Transformers;Breast cancer;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296582,IEEE Conferences,,,,,,
Data Uncertainty Learning in Breast Cancer Recognition,K. Zhao; T. Luo; K. Chen; L. Zhang,2023 5th International Conference on Data-driven Optimization of Complex Systems (DOCS),31-Oct-23,2023,"Breast cancer has emerged as the most prevalent malignancy, occupying the top position in both incidence and mortality among cancers in women. Distinguishing between benign and malignant breast tumors plays a crucial role in achieving early detection and treatment of breast cancer. As breast biopsy is a critical diagnostic evidence for confirming the malignant tumors of the breast, using its pathological images to diagnose the benign or malignant nature of breast tumors by deep learning models has become an important approach in breast cancer recognition. The noise in the digitization process of biopsy samples' pathological images will lead to decision uncertainty. Nevertheless, most existing deterministic models fail to accurately represent the uncertainty in the data, leading to incorrect breast cancer diagnoses. Misdiagnosis comes at a high cost, as it can result in patients missing the optimal treatment window or receiving unnecessary treatments, etc. To solve this problem, we introduce data uncertainty learning, an effective method for representing noise, into breast cancer recognition for the first time. Then, we propose a data uncertainty learning model for breast cancer recognition task, which learns the mean to recognize breast cancer and variance to represent uncertainty of samples. Specifically, we design a variance regularizer to guide the model adjusting optimization weights of the samples based on the uncertainty and enable the model to focus more on high-quality samples while suppressing the influence of noise, which enhances the performance of breast cancer recognition. Finally, insightful evaluation demonstrates the effectiveness of our model.",Pathology;Uncertainty;Breast tumors;Biological system modeling;Breast cancer;Data models;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294729,IEEE Conferences,,,,,,
Brain Tumor Detection using modified VGG-19 and Inception ResnetV2 models,C. R. Prasad; S. Hussain; B. Srinivas; S. Samala; R. Janapati; S. Yalabaka,2023 IEEE 2nd International Conference on Industrial Electronics: Developments & Applications (ICIDeA),31-Oct-23,2023,"A brain tumor is characterized as an aggregation of abnormal cells within the brain. These tumors can be classified into two categories: malignant and benign. Malignant is cancerous whereas benign is not. Both tumors are very hazardous as they grow rapidly and attack different parts of the cerebrum. Even after extensive research, the cause of the brain tumor is unknown. In this paper, a VGG-19 and an Inception-Resnet V2 model are presented for detecting brain tumor by employing images of MRI scans. The dataset is gathered from Kaggle and preprocessed using Keras Image Data Generator. The VGG-19 model provided an accuracy of 99.71% and the Inception-Resnet V2 provided an accuracy of 99.28%. The proposed models performed well to achieve the task.",Training;Magnetic resonance imaging;Transfer learning;Brain modeling;Data models;Generators;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295200,IEEE Conferences,,,,,,
Deep Learning in Clinically Significant Prostate Cancer Classification via Biparametric MRI Sequences,W. O. Falana; A. Serener; S. Serte,2023 3rd International Conference on Emerging Smart Technologies and Applications (eSmarTA),31-Oct-23,2023,"Accurate detection and classification of clinically significant prostate cancer (csPCa) play a pivotal role in determining optimal treatment strategies. Biparametric MRI (bpMRI) employs diverse imaging sequences to enhance diagnostic accuracy. This paper aims to explore the potential of deep learning frameworks, specifically DenseNet-201 and ResNet-50, in distinguishing between clinically significant (csPCa) and non-significant (non-csPCa) prostate cancer using various imaging sequences from biparametric MRI. The methodology involves an in-depth analysis of axial T2-weighted imaging (T2W), diffusion-weighted imaging (DWI), and apparent diffusion coefficient (ADC) mapping using the aforementioned deep learning frameworks. Our findings demonstrate that among the studied biparametric MRI sequences, axial T2W exhibits the highest accuracy in csPCa classification, as indicated by both classification accuracy and the area under the receiver operating characteristic curve (AUC).",Deep learning;Protocols;Magnetic resonance imaging;Receivers;Behavioral sciences;Prostate cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10293371,IEEE Conferences,,,,,,
Comparative Analysis of Artificial Intelligence for Predicting COVID-19 using Diverse Chest X-ray Images,R. A. A. Saleh; F. Al-Areqi; Z. Al-Huda; M. A. Al-antari,2023 3rd International Conference on Emerging Smart Technologies and Applications (eSmarTA),31-Oct-23,2023,"COVID-19 prediction plays a crucial role in medical decision-making for respiratory health. Accurate and rapid prediction using advanced artificial intelligence (AI) techniques is particularly vital during pandemics. Recent research in the medical field has extensively explored and applied AI-based technologies such as deep learning, attention mechanisms, vision transformers, and explainable AI to various diseases, including COVID-19. However, the limited diversity of medical data poses challenges in accurately evaluating AI models' generalization capabilities for prediction. To address these limitations, we have gathered a comprehensive collection of diverse benchmarks on respiratory diseases, including COVID-19, from various sources. This dataset allows us to assess and compare the performance of AI models under consistent training and testing environments. Our findings aim to inspire researchers in the field and offer valuable insights into the future use of AI techniques in the medical domain. The best classification performance is achieved using ResNet152V2 recording overall accuracy of 96.17%.",COVID-19;Deep learning;Training;Pulmonary diseases;Predictive models;Transformers;Recording,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10293745,IEEE Conferences,,,,,,
Enhancing Classification Performance in Knee Magnetic Resonance Imaging Using Adversarial Data Augmentation,Z. Yan; X. Yang; C. F. Chong; Y. Wang,2023 IEEE 14th International Conference on Software Engineering and Service Science (ICSESS),31-Oct-23,2023,"The utilization of adversarial data augmentation has demonstrated the potential capability to enhance the classification performance in training deep neural networks to perform computer vision tasks. In this paper, we investigate the effectiveness of this approach as a strategy for enlarging a knee Magnetic Resonance Imaging (MRI) dataset by adding adversarial perturbation. Specifically, we use the Fast Gradient Sign Method (FGSM) to perturb a subset of the training dataset as extra training images to re-train a baseline model that was trained under the same configuration as the top-ranked model on the MRNet leaderboard. Particularly, unlike most of the current work, we investigate the impact of two hyperparameters (attack magnitude and the proportion of data to be re-trained) on the performance of area under the ROC curve (AUC), accuracy, sensitivity, and specificity. Additionally, our results show that adversarial data augmentation can further improve the well-trained baseline model's AUC by 0.26%, as well as provide a slight improvement in specificity at the same classification threshold. These findings underscore the potential advantages of adversarial data augmentation as a technique for optimizing the decision boundaries of deep learning models. The code of this work will be available on GitHub after the paper is published.",Training;Sensitivity;Magnetic resonance imaging;Perturbation methods;Sensitivity and specificity;Data augmentation;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10293076,IEEE Conferences,,,,,,
Deep Neural Networks for Brain Tumor Image Segmentation and Detection,R. Chandra; S. Tiwari; S. S. Kumar; S. Agarwal; M. Syafrullah; K. Adiyarta,"2023 10th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",31-Oct-23,2023,"Medical image analysis is a challenging and complex field these days. This discipline focuses especially on the processing of MRI (Magnetic Resonance Imaging) images. It offers multiple methods for locating brain tumors in MRI brain images and compares the precision of all the findings. Convolutional neural networks (CNN) and ResNet architectures are used to train the model. As deep learning models are highly efficient and correctly identify whether the MRI picture of a tumor is healthy or unhealthy. In this work, high-level features are extracted from the input images using the CNN architecture, which has multiple pooling layers. To create the final classification model, fully connected layers are then routed through the extracted characteristics. However, CNN has some drawbacks, and to overcome these issues, a ResNet based architecture has been used. Additionally, U-Net-based MRI brain tumor segmentation algorithms have gained popularity because they significantly improve segmentation accuracy by infusing high-level and low-level feature information via skip connections. The suitability of an attention module called Attention Gate, which was recently developed, for tasks involving the segmentation of brain tumors has also been explored in this work.",Image segmentation;Magnetic resonance imaging;Computer architecture;Logic gates;Feature extraction;Brain modeling;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295631,IEEE Conferences,,,,,,
A Multi-modal Approach to Lung Tumor Detection using Deep Learning,A. Zafar; S. Muneeb; M. Amir; A. Jamil; A. A. Hameed,"2023 IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings)",30-Oct-23,2023,"Lung cancer remains a significant global cause of cancer-related deaths, emphasizing the importance of early detection for improving patient survival rates. This paper introduces an enhanced approach that aims to achieve efficient and precise lung tumor detection and segmentation. The proposed method utilizes a multimodal approach by leveraging both CT and PET scans, enabling improved tumor detection. The methodology incorporates state-of-the-art deep learning architectures, including ResNet, DenseNet, and Inception-v3, for effective tumor classification. Additionally, both immediate fusion (early fusion) and late fusion techniques are applied to integrate data from multiple modalities. The performance of the classification models is evaluated using metrics such as precision, F1 score, accuracy, and sensitivity. The experimental results demonstrate the effectiveness of the proposed approach in accurately segmenting lung tumors. The findings contribute to the existing knowledge in the field of tumor segmentation and medical image analysis, providing valuable insights into the benefits of multimodal fusion and deep learning techniques for lung cancer diagnosis and treatment planning.",Deep learning;Image segmentation;Sensitivity;Image analysis;Lung cancer;Planning;Internet of Things,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10291022,IEEE Conferences,,,,,,
Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach,A. Purnama Muharram; H. Puteri Haryono; A. Haji Juma; I. Puspasari; N. Priya Utama,2023 IEEE International Conference on Data and Software Engineering (ICoDSE),27-Oct-23,2023,"Reading and interpreting chest X-ray images is one of the most radiologist's routines. However, it still can be challenging, even for the most experienced ones. Therefore, we proposed a multi-modal deep learning-based automated chest X-ray report generator system designed to assist radiologists in their work. The basic idea of the proposed system is by utilizing multi binary-classification models for detecting multi abnormalities, with each model responsible for detecting one abnormality, in a single image. In this study, we limited the radiology abnormalities detection to only cardiomegaly, lung effusion, and consolidation. The system generates a radiology report by performing the following three steps: image preprocessing, utilizing deep learning models to detect abnormalities, and producing a report. The aim of the image pre-processing step is to standardize the input by scaling it to 128x128 pixels and slicing it into three segments, which covers the upper, lower, and middle parts of the lung. After pre-processing, each corresponding model classifies the image, resulting in a 0 (zero) for no abnormality detected and a 1 (one) for the presence of an abnormality. The prediction outputs of each model are then concatenated to form a â€˜result codeâ€˜. The â€˜result codeâ€™ is used to construct a report by selecting the appropriate pre-determined sentence for each detected abnormality in the report generation step. The proposed system is expected to reduce the workload of radiologists and increase the accuracy of chest X-ray diagnosis.",Deep learning;System performance;Software algorithms;Lung;Radiology;Predictive models;Generators,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10291842,IEEE Conferences,,,,,,
Detection of Tuberculosis Disease Using Deep Learning Techniques,G. Tummalapalli; J. Kousik; M. Rajasekhar; M. Rajesh; K. Dinesh; K. R. Rao,2023 IEEE International Conference on Data and Software Engineering (ICoDSE),27-Oct-23,2023,"The stealthy Mycobacterium tuberculosis breeds an infectious scourge known as Tuberculosis (TB). It is a bacterial infection related chronic lung disease. An early and accurate diagnosis of TB is essential. chest X-ray images are used to detect this disease. However, manual detection takes longer, which occasionally leads to errors. In this article, we proposed a deep learning approach for TB classification using chest X-rays. Some deep learning algorithms search through a hypothesis space in search of an appropriate hypothesis that will produce accurate predictions for a certain disease scenario. To detect tuberculosis in chest X-rays, we use Res Net a deep learning architecture recognized for its capacity to make use of feature reuse and overcome vanishing gradient issues. In order to provide a more exact hypothesis for the prediction of tuberculosis using the Res Net, a deep learning technique, this proposed model is developed on top of prior research. We were able to reach 99% accuracy with the classifier that was based on deep learning.",Deep learning;Microorganisms;Tuberculosis;Pulmonary diseases;Predictive models;Prediction algorithms;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10291401,IEEE Conferences,,,,,,
Deep Learning for the Investigation of Lung and Pancreatic Tumors,M. Mahadevi; M. Marimuthu; I. G. A. Poornima; K. Vijayalakshmi; D. Vidyabharathi; G. S. Priya,"2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",26-Oct-23,2023,"Among the worst malignancies in the world are lung and pancreatic, and effective treatment depends on early identification and precise diagnosis. A powerful technique for analyzing medical images, deep learning, a category of machine learning, has the potential to enhance the identification and characterization of lung and tumors of the pancreas. In this research, this study examines current developments in classification of images, segmentation, and treatment outcome prediction methods for studying lung and pancreatic cancers. Gaussian Mixture model and EM techniques are employed during preprocessing. The model for classification is then used to determine the disease prediction after segmenting the pancreatic and lung tumors. In order to help radiologists discover malignancies early, traditional machine learning methods have demonstrated encouraging results in properly recognizing lung tumors from CT images. This entails defining tumor edges from medical pictures while segmenting pancreatic tumors. Finally, deep learning offers useful methods for early identification, cancer segmentation and outcome prediction, holding significant potential for research into lung and pancreatic malignancies. To guarantee the dependability, accessibility, and generality. of models developed using deep learning in clinical practice, there are some issues that must be resolved.",Deep learning;Training;Image segmentation;Lung;Lung cancer;Pancreatic cancer;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290585,IEEE Conferences,,,,,,
A Machine Learning-based Multi-Phase Medical Image Classification for Internet of Medical Things,D. Poornima; S. L. Jany Shabu; T. S. Aswin; V. Sruthi; K. D. Sruthi,"2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",26-Oct-23,2023,"The Internet of Medical Things (IoMT) has revolutionised healthcare by allowing for remote monitoring and diagnosis of patients through medical devices. Medical image classification is essential for IoMT applications, as it enables early detection of diseases and personalised treatments. However, single-phase classification models face challenges distinguishing between similar conditions and differentiating them from normal cases. To address these challenges, this research proposes a Machine Learning-based Multi-Phase Medical Image Classification (ML-MPMIC) model that combines image segmentation and classification techniques to achieve higher accuracy in disease diagnosis. The proposed model is evaluated through simulation analysis and parameter optimisation. The proposed ML-MPMIC approach has high accuracy, precision, F score, and Low Root Mean Square Error (RMSE) values and its potential to improve healthcare outcomes in IoMT applications",Image segmentation;Medical devices;Internet of Medical Things;Medical diagnosis;Root mean square;Medical diagnostic imaging;Remote monitoring,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290628,IEEE Conferences,,,,,,
Stroke Classification with Microwave Signals using Explainable Wavelet Convolutional Neural Network,S. Hasan; A. Zamani; A. Brankovic; K. Bialkowski; A. M. Abbosh,IEEE Journal of Biomedical and Health Informatics,,2023,"Stroke is one of the leading causes of death and disability. To address this challenge, microwave imaging has been proposed as a portable medical imaging modality. However, accurate stroke classification using microwave signals is still an open challenge. In addition, identified features of microwave signals used for stroke classification need to be linked back to the original data. This work attempts to address these issues by proposing a wavelet convolutional neural network (CNN), which combines multiresolution analysis and CNN to learn distinctive patterns in the scalogram for accurate classification. A game theoretic approach is used to explain the model and indicate distinctive features for discriminating stroke types. The proposed algorithm is tested in simulation and experiments. Different types of noise and manufacturing tolerances are modeled using data collected from healthy human trials and added to the simulation data to bridge the gap between the simulation and real-life data. The achieved classification accuracy using the proposed method ranges from 81.7% for 3D simulations to 95.7% for lab experiments using simple head phantoms. Obtained explanations using the method indicate the relevance of wavelet coefficients on frequencies 0.95-1.45 GHz and the time slot of 1.3 to 1.7 ns for distinguishing ischemic from hemorrhagic strokes.",Head;Microwave imaging;Hemorrhaging;Time-frequency analysis;Dielectrics;Feature extraction;Wavelet transforms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10293252,IEEE Early Access Articles,,,,,,
Design of an Automatic Subtype Classifier of Breast Carcinoma Using XResNet with Cyclic Scheduling,V. Harshey; A. P. Singh Pharwaha,"2023 9th International Conference on Control, Decision and Information Technologies (CoDIT)",24-Oct-23,2023,"This paper reports the design of an automatic system to classify breast cancer subtypes using an XResNet deep learning model. The prior solutions to this problem suffer from low accuracy or cost of training. Our designed system effectively classifies the breast cancer histopathology patches cropped from whole slide images while being affordable and accurate. The reported system effectively detects cancer's localized or invasive nature. The patient's prognosis and treatment schedule depend on the multi-classification of the images. The presented system achieves state-of-the-art accuracy on the multiclass BACH dataset. On the BACH dataset, we achieved the highest accuracy of 89.95% and balanced accuracy of 87.8% in only 30 epochs matching the state-of-the-art systems. Comparisons of the designed system with recent works establish the model's efficacy, making it worthy for the state-of-the-art breast cancer diagnosis routine. The designed system can be a valuable tool for medical professional for improving the survival of patients.",Training;Deep learning;Schedules;Histopathology;Medical services;Breast cancer;Prognostics and health management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10284127,IEEE Conferences,,,,,,
Automated Multimodal Brain Tumor Classification Using a YOLOv7 Approach,B. S. S; N. B. Muppalaneni,"2023 9th International Conference on Control, Decision and Information Technologies (CoDIT)",24-Oct-23,2023,"Automated brain tumor classification is one among the most complicated and popularly used applications of medical imaging. Manual diagnosis of brain tumors is complex and inefficient. Therefore, identifying the appropriate type of tumor at an early stage plays a significant role in choosing the exact treatment plan. Therefore, a new method based on the YOLOv7 architecture was suggested to extract features, select features, identify tumors, and classify them from multimodal brain images. The brain tumor is categorised using the classifier as glioma, meningioma, non-tumor and pituitary tumors to increase confidence in tumor detection problems. YOLOv7 method is assessed utilizing the brain MRI dataset which includes Br35H dataset, Figshare dataset, and SARTAJ dataset, consisting of 7023 MRI images. Overall effectiveness of the suggested technique is assessed in terms of precision, specificity, sensitivity, F1-score, and accuracy as 98.73%, 99.69%, 99.81 %, 98.56% and 98.73 % respectively. The classification outcomes demonstrates that described method can be useful to help clinicians make quick and accurate decisions related to the diagnosis of brain tumors.",Brain;Image segmentation;Sensitivity;Magnetic resonance imaging;Lung;Manuals;Sensitivity and specificity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10284388,IEEE Conferences,,,,,,
Automatic Segmentation and Classification of Skin Cancer Cells using Thresholding and Deep Learning based Techniques,G. Rani; S. Khandelwal; N. Kundu; V. S. Dhaka,2023 International Conference on Emerging Trends in Networks and Computer Communications (ETNCC),24-Oct-23,2023,"The proposed manuscript derives its origin from the fact that annually about 57 million people die due to skin cancer. Skin biopsy is the most widely practiced method to detect skin cancer. Fear of bleeding, and infection raises the concern of patients to agree for skin biopsy. Also, its' high cost, and requirement of specific infrastructure is a limitation in its use. The proven potential of artificial intelligence motivated us to resolve the above-mentioned challenges and propose an accurate diagnosis method for skin cancer. In this research, we apply different preprocessing methods for removing rulers and hair from the images in the dataset. Also, we employ different segmentation techniques, such as color based segmentation, thresholding, edge detection, and region-based segmentation to segment skin cancer images. Next, we compare the performance of these techniques by implementing them on the same testing dataset. We use PSNR, SSIM, entropy, and brisque score to evaluate the quality of segmented images. The mean entropy of 0.52 bits/pixels, mean PSNR of 38.75dB, mean SSIM of 0.82 and brisque score of 119.09 on the testing dataset prove the efficacy of the proposed model. Further, finetuning the VGG-16 model for classifying the testing dataset comprising 617 images improve the performance and reported 98% accuracy. The high accuracy and good quality of segmented images justify the performance of the model.",Hair;Image segmentation;Image edge detection;Biopsy;Medical services;Market research;Skin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10284963,IEEE Conferences,,,,,,
Lightweight Histological Tumor Classification Using a Joint Sparsity-Quantization Aware Training Framework,D. Aboutahoun; R. Zewail; K. Kimura; M. I. Soliman,IEEE Access,########,2023,"Cancer decision-making is a complex process that can be exacerbated by the limited availability of oncological expertise. This is particularly true in rural areas and settings with fewer resources. Recently, there has been an interest in the potential of artificial intelligence in reliable computer-aided diagnosis tools in such settings. Nevertheless, the majority of deep learning algorithms are resource hungry in terms of data and storage requirements. In this work, we propose a novel lightweight deep learning model for histological tumor classification through a Joint Sparsity-Quantization Aware Training framework. Extensive experiments were conducted to evaluate the proposed framework. Promising performance has been achieved compared to the most relevant state-of-the-art work with a classification accuracy of 94.26% and an average $5\times $ reduction in the memory footprint. This work aims at opening doors toward efficient point-of-care diagnostic devices suitable for environments with limited resources.",Training;Quantization (signal);Deep learning;Computational modeling;Transfer learning;Task analysis;Medical diagnostic imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292839,IEEE Journals,,,,,,
Cyclical Learning Rate Optimization on Deep Learning Model for Brain Tumor Segmentation,A. Fajar; R. Sarno; C. Fatichah; R. I. Susilo; G. Pangestu,IEEE Access,########,2023,"In recent years, deep learning has found widespread applications in tasks such as segmentation and classification. Fine-tuning hyperparameters is crucial to improve performance, with the learning rate being a key parameter. Various methods, including adaptive learning rates, learning rate scheduling, and cyclical learning rates, have been used to optimize learning rates. Cyclical learning rates offer significant benefits with minimal computational cost, as seen in previous research. This study introduces a novel approach to tuning the cyclical learning rate, which incorporates the exponential moving average. These methods are applied to the BraTS 2021 dataset for segmentation tasks, resulting in superior performance compared to the previous approach. Our proposed method reduces the epochs required to reach convergence by 19 and 54 epochs for U-Net and Dense U-net, respectively. For Res U-net, the epoch needed to convergence is 10 epochs more. However, the proposed method produces lower loss values with 0.707, 0.657, and 0.665 compared to the previous method with 0.712, 0.685, and 0.725 for U-net, Res U-net, and Dense U-net, respectively.",Training;Adaptive learning;Schedules;Image segmentation;Tumors;Task analysis;Market research;Three-dimensional displays;Deep learning;Information and communication technology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290895,IEEE Journals,,,,,,
Enhancing Brain Tumor Classification Through Customization of the Vision Transformer Learning,X. -K. Thai-Hoang; V. -D. Hoang,2023 International Workshop on Intelligent Systems (IWIS),23-Oct-23,2023,"In the medical diagnosis, the accuracy of decision-making holds immense significance. Healthcare professionals heavily rely on diagnostic results to guide treatment plans, as these decisions have a substantial impact on patients' health outcomes. An incorrect diagnosis can lead to detrimental consequences, including treatment delays or the administration of inappropriate treatments, thereby posing serious harm to the patient's well-being. The utilization of artificial intelligence (AI) in medical image recognition plays a crucial role in assisting various disease diagnosis systems. Improving accuracy of brain tumor classification is an important diagnosis and effective treatment planning. In this paper, we present an approach to improve brain tumor classification accuracy based on vision transformer approach and customizing head classifier and hyperparameter optimization for training. ViT model are a new type of neural network that can achieve state-of-the-art accuracy on image classification tasks. Our learning architecture approach reaches high accuracy with more than 95%. The experimental results demonstrated that the ViT model achieves a higher validation accuracy. This approach also show that the ViT model is more robust to noise and can generalize better to new data.",Training;Head;Brain modeling;Transformers;Planning;Medical diagnosis;Task analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10284696,IEEE Conferences,,,,,,
Modern Deep Learning Models for Skin Disease Diagnosis,H. Van Thanh; N. D. Quang; K. -H. Jo,2023 International Workshop on Intelligent Systems (IWIS),23-Oct-23,2023,"Skin cancer is a pressing global health concern, representing a significant challenge in terms of achieving accurate and timely diagnoses. The introduction of deep learning algorithms has heralded a new era of progress across various domains, particularly in the domain of skin disease diagnosis. In our research, we embark on a journey to meticulously evaluate the effectiveness of state-of-the-art deep learning models, utilizing the comprehensive HAM10000 dataset. Our overarching objective is to identify the foremost model for the diagnosis of skin cancer, leveraging the immense potential of modern machine learning techniques. By conducting a comprehensive and in-depth analysis of these models, our aim transcends the mere pursuit of technical excellence. We seek to make a substantial and meaningful contribution to the field of healthcare, particularly in the critical area of skin disease diagnosis. Our vision is to enhance the precision, reliability, and efficiency of diagnostic methodologies, ultimately leading to earlier detection, more effective treatments, and improved outcomes for patients grappling with skin cancer.",Deep learning;Measurement;Computational modeling;Pressing;Skin;Medical diagnosis;Object recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10284645,IEEE Conferences,,,,,,
Post-Hoc Explainability of BI-RADS Descriptors in a Multi-Task Framework for Breast Cancer Detection and Segmentation,M. Karimzadeh; A. Vakanski; M. Xian; B. Zhang,2023 IEEE 33rd International Workshop on Machine Learning for Signal Processing (MLSP),23-Oct-23,2023,"Despite recent medical advancements, breast cancer remains one of the most prevalent and deadly diseases among women. Although machine learning-based Computer-Aided Diagnosis (CAD) systems have shown potential to assist radiologists in analyzing medical images, the opaque nature of the best-performing CAD systems has raised concerns about their trustworthiness and interpretability. This paper proposes MT-BI-RADS, a novel explainable deep learning approach for tumor detection in Breast Ultrasound (BUS) images. The approach offers three levels of explanations to enable radiologists to comprehend the decision-making process in predicting tumor malignancy. Firstly, the proposed model outputs the BI-RADS categories used for BUS image analysis by radiologists. Secondly, the model employs multi-task learning to concurrently segment regions in images that correspond to tumors. Thirdly, the proposed approach outputs quantified contributions of each BI-RADS descriptor toward predicting the benign or malignant class using post-hoc explanations with Shapley Values.",Image segmentation;Solid modeling;Visualization;Ultrasonic imaging;Design automation;Signal processing;Multitasking,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286006,IEEE Conferences,,,,,,
Automatic Pneumonia Detection Through Chest X-Ray Image-Based,A. A. B. Ramli; Z. Zulkifli; S. Ahmad; N. Ghazali,2023 4th International Conference on Artificial Intelligence and Data Sciences (AiDAS),23-Oct-23,2023,"Pneumonia is a well-known inflammatory lung illness that needs to be diagnosed quickly and correctly to be effectively treated. Chest X-ray diagnosis of pneumonia is difficult, particularly in disadvantaged clinical settings. The confusing radiographic appearance of pneumonia frequently results in incorrect diagnoses and drawn-out diagnostic processes. Hence, this study aims to develop automated pneumonia detection prototype through X-ray image-based which can help in the early and efficient diagnosis of pneumonia, enabling timely treatment and potentially reducing the burden on radiologists or healthcare professionals. To accomplish this, Discrete Wavelet Transform (DWT) and bilateral filter are used to reduce noise from X-ray images. The classification was performed by using Convolutional Neural Networks (CNN) to generate evaluation metrics to determine the learning algorithm's performance. A set of 5,840 X-ray images of a normal chest and a pneumonia-infected chest were used for automation testing using CNN as models to generate the confusion matrix and performance accuracy. The trained CNN model shows encouraging results, categorising both pneumonia-positive and pneumonia-negative cases with outstanding accuracy, recall, and F1-score of 96.0%. This study shows the potential of automated pneumonia diagnosis utilising chest X-ray images, providing a helpful tool to help medical practitioners to make prompt and accurate assessments. The main drawback would be the absence of demographic information. Future suggestions include incorporating demographic data even more and enhancing the model's capacity to consider patients' histories. This study contributes to the development of automated pneumonia diagnosis by utilising deep learning techniques and improving diagnostic precision and effectiveness in clinical practice.",Training;Microorganisms;Pulmonary diseases;Computational modeling;Prototypes;Convolutional neural networks;History,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10284669,IEEE Conferences,,,,,,
Lung Cancer Intelligent Prediction System Using CNN,T. T. Alâ€“Shouka; K. M. A. Alheeti,2023 International Conference on Decision Aid Sciences and Applications (DASA),23-Oct-23,2023,"The mortality rate from Lung Cancer is high, and the disease is notoriously difficult to treat. It was the third most frequent mortality reason for women and the leading cause of male mortality, according to data compiled by the International Agency for Research on Cancer (IARC) Cancer Research Facility. Additionally, they provided rates of cancer occurrence and mortality for a total of 36 different cancers occurring in 185 different countries. Typically, early tumor detection is essential for effective therapy. Early cancers can be efficiently examined to help patients recover quickly. Traditional medical imaging techniques, such as X-rays, CT scans, MRIs, etc., offer little promise for lung tumor identification. CNNs are capable of exact image processing. This paper provides a probabilistic deep 2D CNN diagnosis method for lung cancer (CNNs). This algorithm is considered the best in image classification, which gives better results, making the proposed system more efficient. The accuracy values obtained are 86%, the recall of 92%, and the F1-score is 87%.",Computed tomography;Magnetic resonance imaging;Image processing;Lung cancer;Lung;Medical treatment;X-rays,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286799,IEEE Conferences,,,,,,
HMC-COVID-19: Hidden Markov Chains Model for COVID-19 Diagnosis,S. Goumiri; D. Benboudjema,2023 International Conference on Decision Aid Sciences and Applications (DASA),23-Oct-23,2023,"Markov chains are probabilistic models that are useful in different image processing tasks. This paper presents an approach based on Hidden Markov Chain (HMCs) to diagnose COVID-19, that we named HMC-COVID-19. To assess the performance of the proposed solution, we use a public dataset of chest X-RAY images. Our solution has been evaluated using the accuracy of prediction. The preliminary results are promising and can be further enhanced.",COVID-19;Deep learning;Pulmonary diseases;Hidden Markov models;Training data;Probabilistic logic;Data models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286601,IEEE Conferences,,,,,,
Alzheimer Disease Detection Using Advanced Transfer Learning Techniques on MRI Images,B. Mohamed Abdaelbassat; W. Hariri,2023 International Conference on Decision Aid Sciences and Applications (DASA),23-Oct-23,2023,"Alzheimer's disease (AD) is a neurological disorder that has a profound impact on millions of individuals globally. It is imperative to identify and diagnose AD at an early stage with precision, as this plays a crucial role in timely intervention and effective treatment. In recent years, the utilization of magnetic resonance imaging (MRI) in conjunction with deep learning has demonstrated promising results for assisting in the detection of Alzheimer's disease (AD). This study introduces a methodology for detecting AD by employing MRI images and convolutional neural networks (CNNs). Five pre-trained CNN models, including MobileNetV2, ResNet50, EfficientNet-B0, InceptionV3, and DenseNet-201, are employed for the transfer learning (TL) task. A comparative study and detailed analysis of the performances achieved by each model are presented. The models are trained on a large dataset of MRI images, consisting of both healthy and AD-affected brains with hyper-parameters tuning. During training, the CNNs learn to automatically identify patterns and discriminative features that differentiate between normal and AD-affected brain regions. To evaluate the effectiveness of the proposed method, a comprehensive set of experiments is conducted using publicly available datasets. From the experimental study, it is observed that the TL approach using the five pretrained CNN models achieves good performances in the two class classification problem of Very Mild Demented and Non Demented individuals. The results of this paper contribute to the growing field of computer-aided diagnosis for AD and emphasize the effectiveness of deep learning techniques, particularly CNNs, in the analysis of medical imaging data.",Deep learning;Training;Magnetic resonance imaging;Transfer learning;Brain modeling;Transformers;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286802,IEEE Conferences,,,,,,
Unveiling the Future of Oral Squamous Cell Carcinoma Diagnosis: An Innovative Hybrid AI Approach for Accurate Histopathological Image Analysis,I. U. Haq; M. Ahmed; M. Assam; Y. Y. Ghadi; A. Algarni,IEEE Access,30-Oct-23,2023,"Oral cancer poses a formidable global health threat, demanding urgent attention to combat its devastating impact. Timely detection of oral squamous cell carcinoma (OSCC) is pivotal for successful treatment and improved survival rates. However, manual histopathological analysis, reliant on the expertise of medical practitioners, can be time-consuming and vulnerable to subjective discrepancies. To surmount these challenges and elevate diagnostic outcomes, this research explores the transformative potential of artificial intelligence (AI) in OSCC diagnosis. Three distinct methodologies, namely Gabor + CatBoost, ResNet50 + CatBoost, and Gabor+ ResNet50 + CatBoost, were implemented to leverage the power of AI. By extracting 32 low-level features from the Gabor Filter and 100,532 high-level features from the ResNet50 model, the study adopts principal component analysis (PCA) to mitigate overfitting, retaining the top 4096 components. The extracted features underwent individual classification using CatBoost, followed by concatenation and image classification. Remarkably, the third strategy, which synergized Gabor filtering with ResNet50 feature extraction, along with CatBoost classification, demonstrated the most exceptional performance. Achieving an impressive accuracy of 94.92%, 95.51% precision, 84.30% sensitivity, 95.54% specificity, 94.90% F1 score, and 94.9% AUC, these AI-based approaches herald a new era of accurate and efficient OSCC diagnosis.",Cancer;Artificial intelligence;Feature extraction;Residual neural networks;Sensitivity;Medical diagnostic imaging;Training;Histopathology;Gabor filters,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10288489,IEEE Journals,,,,,,
Hybrid Metaheuristics With Deep Learning-Based Fusion Model for Biomedical Image Analysis,M. Obayya; M. K. Saeed; N. Alruwais; S. S. Alotaibi; M. Assiri; A. S. Salama,IEEE Access,26-Oct-23,2023,"Biomedical image analysis has played a pivotal role in modern healthcare by facilitating automated analysis and interpretation of medical images. Biomedical image classification is the process of automatically labelling or categorizing medical images based on their content. In recent years, this field has received considerable attention because of the abundance of bio-medical image data and the potential for deep learning (DL) algorithms to assist medical staff in identifying diseases and making treatment decisions. DL methods are mostly convolutional neural networks (CNN) has illustrated outstanding performance in analyzing and classifying biomedical images. Therefore, this study presents a new Hybrid Metaheuristics with Deep Learning based Fusion Model Biomedical Image Analysis (HMDL-MFMBIA) technique. The HMDL-MFMBIA technique initially performs image pre-processing and Swin-UNet-based segmentation. Besides, a fusion of multiple DL-based feature extractors takes place using Xception and Residual Network (ResNet) model. Moreover, a hybrid salp swarm algorithm (HSSA) was employed for the optimal hyperparameter selection of the DL models. Finally, the gated recurrent unit (GRU) algorithm can be exploited for the detection and classification of bio-medical images. A widespread of simulated is conducted to establish the enhanced biomedical image classification results of the HMDL-MFMBIA method. The simulation outcomes inferred the greater outcome of the HMDL-MFMBIA algorithm over other DL models.",Feature extraction;Medical diagnostic imaging;Biological system modeling;Classification algorithms;Image segmentation;Convolutional neural networks;Tuning;Biomedical image processing;Computer vision;Deep learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10288439,IEEE Journals,,,,,,
Enhancing Brain Tumor Diagnosis: Transitioning From Convolutional Neural Network to Involutional Neural Network,A. A. Asiri; A. Shaf; T. Ali; M. Zafar; M. A. Pasha; M. Irfan; S. Alqahtani; A. J. Alghamdi; A. H. Alghamdi; A. F. A. Alshamrani; M. Aleylyani; S. Alamri,IEEE Access,########,2023,"] Accurate classification of brain tumors is essential for effective medical diagnosis and treatment planning. Traditional approaches rely on convolutional neural networks (CNNs) for tumor detection, but they often suffer from high computational demands due to the large number of parameters. In this paper, we propose a novel approach for brain tumor classification using involutional neural networks (InvNets), which are designed to mitigate the parameter-intensive nature of CNNs. Unlike the spatial-agnostic and channel-specific convolution kernel, the involution kernel is location-specific and channel-agnostic. This location-specific operation allows the network to adapt to various visual patterns with respect to different spatial locations, enhancing its ability to capture intricate features within the medical images. Our study focuses on a four-class brain tumor classification problem, aiming to differentiate between different tumor types based on medical imaging data. In a comparative analysis, we demonstrate that conventional CNNs require over 4 million parameters, whereas our proposed InvNets require less than 0.2 million parameters, making them more efficient and resource-friendly. The evaluation of both CNNs and InvNets is carried out using standard performance matrices: accuracy, precision, recall, F1 score, and AUC-ROC values. Our findings reveal that the InvNets consistently outperform traditional CNNs. The InvNet architecture achieves an impressive 92% accuracy rate, showcasing its potential for accurate brain tumor classification. This improved accuracy, combined with the reduced parameter count, highlights the effectiveness of InvNets for medical image analysis tasks, especially in scenarios with limited computational resources.",Tumors;Brain modeling;Medical diagnostic imaging;Biological neural networks;Solid modeling;Computational modeling;Training;Convolutional neural networks;Image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10288488,IEEE Journals,,,,,,
Hybrid Approach for COVID-19 Severity Identification.,M. Mahfouz; G. Samuel; A. Mohammed,"2023 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)",20-Oct-23,2023,"Research effort have been dedicated to the utilization of deep learning techniques for the detection of COVID-19 and various forms of pneumonia. Although these efforts have produced commendable results in terms of performance, a prevalent focus has been on classifying different pneumonia types, with limited attention paid to assessing infection severity. In this context, our study offers a multifaceted contribution. Firstly, we propose a hybrid framework that combines a Convolutional Neural Network (CNN) with a fuzzy system to identify infection severity. The CNN-based model comprises two stacked layers of CNN models designed to distinguish between normal cases and various pneumonia types. Concurrently, the fuzzy system processes numerical laboratory test data and the CNN model's classification outcomes to determine the degree of infection severity. Experimental findings from experiments conducted on a meticulously curated dataset using diverse CNN models showcase that the VGG16 model achieved an impressive 92%. specificity and 88% accuracy in distinguishing between viral and bacterial pneumonia. Furthermore, our fuzzy model underwent evaluation with twelve actual medical cases, revealing its capacity for accurate assessment of recommended severity levels among patients. This approach facilitates healthcare specialists in quantifying the severity of viral pneumonia through a scoring system, enhancing the diagnostic process.",COVID-19;Fuzzy logic;Deep learning;Microorganisms;Pulmonary diseases;Computational modeling;Numerical models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10278298,IEEE Conferences,,,,,,
Comparing Autoencoder to Geometrical Features for Vascular Bifurcations Identification,I. Essadik; A. Nouri; R. Touahni; F. Autrusseau,2023 International Symposium on Image and Signal Processing and Analysis (ISPA),19-Oct-23,2023,"The cerebrovascular tree is a complex anatomical structure that plays a crucial role in the brain irrigation. A precise identification of the bifurcations in the vascular network is essential for understanding various cerebral pathologies. Traditional methods often require manual intervention and are sensitive to variations in data quality. In recent years, deep learning techniques, and particularly autoencoders, have shown promising performances for feature extraction and pattern recognition in a variety of domains. In this paper, we propose two novel approaches for vascular bifurcation identification based respectiveley on Autoencoder and geometrical features. The performance and effectiveness of each method in terms of classification of vascular bifurcations using medical imaging data is presented. The evaluation was performed on a sample database composed of 91 TOF-MRA, using various evaluation measures, including accuracy, F1 score and confusion matrix.",Deep learning;Pathology;Irrigation;Databases;Data integrity;Manuals;Bifurcation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10279589,IEEE Conferences,,,,,,
Sparrow Search Algorithm With Stacked Deep Learning Based Medical Image Analysis for Pancreatic Cancer Detection and Classification,J. V. N. Ramesh; T. Abirami; T. Gopalakrishnan; K. Narayanasamy; M. K. Ishak; F. K. Karim; S. M. Mostafa; A. Allakany,IEEE Access,18-Oct-23,2023,"Medical image analysis for pancreatic cancer (PC) classification and recognition is a vital domain of research and medical practices. PC is challenging to diagnose and treat; medical imaging approaches aid early diagnosis to analyse and treat, and employ of medical imaging approaches are support early diagnosis, correct analysis, and treatment planning. Computed Tomography (CT) scans are generally utilized to detect and classify PCs. Deep learning (DL) approaches have demonstrated the ability to support the diagnosis and detection of several medical conditions, containing PC. Convolutional Neural Networks (CNNs) are a kind of DL approach generally employed for image analysis that is trained to automatically learn and extract features in medical images. So, this study purposes a new Sparrow Search Algorithm with Stacked Deep Learning based Medical Image Analysis for Pancreatic Cancer Detection and Classification (SSASDL-PCDC) technique on CT images. The purpose of the study is to design an SSASDL-PCDC technique to achieve improved pancreatic cancer detection performance. In addition, the SSASDL-PCDC technique applies Harris Hawks Optimization (HHO) with a densely connected networks (DenseNet) model for the feature extraction process. Moreover, convolutional neural network with bi-directional long short-term memory (CNN-BiLSTM) approach was utilized for PC detection and classification. Furthermore, Sparrow Search Algorithm (SSA) is used to adjust the hyperparameter values of the CNN-BiLSTM technique. To evaluate the effectiveness of the SSASDL-PCDC technique, extensive experiments were executed on a comprehensive database of pancreatic CT images. The simulation outcome value depicted that the SSASDL-PCDC technique with maximum sensitivity of 99.26%, specificity of 99.26%, and accuracy of 99.26%.",Deep learning;Image analysis;Computed tomography;Pancreatic cancer;Feature extraction;Classification algorithms;Convolutional neural networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10287872,IEEE Journals,,,,,,
Improved Convolutional Neural Network Lung Cancer Classification Detection Method Based on Transfer Learning and Model Compression,L. Fangxing; P. Yaxin; C. Ju; T. T. Toe,2023 2nd International Conference on Artificial Intelligence and Computer Information Technology (AICIT),17-Oct-23,2023,"Lung cancer, one of the most life-threatening malignancies, has witnessed a significant increase in its incidence and mortality rates over the past few decades. Accurate determination of the histological type of lung cancer is crucial for developing appropriate treatment strategies. However, traditional medical imaging analysis, such as magnetic resonance imaging and computed tomography (CT), faces challenges due to sensor noise, patient variations, and pathological states, which may lead to misinterpretation and errors. In this regard, deep learning approaches can assist in rapidly distinguishing different types of lung cancer by learning complex patterns and features from medical imaging data, aiding clinicians in precise diagnosis. This research aims to address the limitations of existing lung cancer classification models, including suboptimal accuracy, slow inference speed, high computational and storage requirements, and vulnerability to overfitting. To achieve this, we propose an improved lung cancer classification and detection method based on deep learning. Our approach leverages transfer learning by utilizing pre-trained models, data augmentation techniques, and model compression methods, including global unstructured pruning and model distillation. Experimental results demonstrate that the teacher model achieves 100% accuracy, recall, precision, and F1-score on the dataset, with a loss of 0.00007575038. The distilled student model, after pruning 55.15% of its parameters, achieves a 52.5% reduction in size. It also attains 100% accuracy, recall, precision, and F1-score, with an average loss of 0.004158932.",Deep learning;Computational modeling;Computed tomography;Transfer learning;Lung cancer;Lung;Data augmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10277752,IEEE Conferences,,,,,,
Transfer-Based Deep Learning Technique for PCOS Detection Using Ultrasound Images,N. Kaur; G. Gupta; P. Kaur,"2023 International Conference on Network, Multimedia and Information Technology (NMITCON)",17-Oct-23,2023,"Polycystic ovary syndrome, or PCOS, is a complex problem of hormones that affects up to 10% of women of childbearing age. Correct treatment and control of PCOS depend on a correct diagnosis and classification of the disease. The majority of people who have PCOS are overweight, have more severe symptoms, have irregular periods, and have an increased amount of body hair. It is critical to acquire a diagnosis of polycystic ovary syndrome (PCOS) as quickly as possible so that you may begin treatment for the symptoms and reduce the associated health concerns. At the moment, physicians and radiologists employ ovarian ultrasonography to detect PCOD by determining the size of the eggs and calculating the number of eggs present in the ovary. This is one of the PCOS diagnosis variables that might be challenging. In this work, we suggest a way to classify people with PCOS by using ultrasound images of the ovaries and transfer learning-based deep learning models. Our model is trained on a large set of ultrasound images and was already trained using predefined InceptionV3 models on a set of medical images in general. We were able to classify PCOS with 99.48% accuracy, which shows that transfer learning is a good way to improve the accuracy of PCOS detection. Our experimental results demonstrate that transfer learning-based classification achieves superior performance in PCOS subtype classification compared to traditional machine learning approaches. Clinicians may be able to better diagnose and treat PCOS with the help of our suggested model.",Deep learning;Hair;Transfer learning;Sociology;Ultrasonography;Medical services;Statistics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10276245,IEEE Conferences,,,,,,
Evaluation of Modified VGG16 Learning Model for Classifying Skin Cancer Lesions,M. Naser Alaubaidy; A. Ismail Abdullah; F. Mousa Alwan,"2023 International Conference on Network, Multimedia and Information Technology (NMITCON)",17-Oct-23,2023,"One of the most serious types of cancer that is prevalent around the world is skin cancer. It is widely known that skin cancer characterized by uncontrolled cell and tissue growth, presenting a challenge in detecting or diagnosis skin cancer in early stage. Accurate segmentation of skin lesions in dermatoscopic images holds great importance in diagnosis and assessment of skin cancer. Thus, Fast, precise, and real-time algorithms are essential in effectively detecting skin cancer to aid physicians in their treatment decision-making process. In this paper, we evaluated the performance of the modified VGG16 with using three common convolutional neural networks(CNN) in detecting the skin cancer with dermoscopic skin lesions images. In this study, the Kaggle dataset is used to classify skin cancer images. To classify skin lesions, the model modified VGG16 is compared to the most common models such as VGG16, mobileNetV2 and Inception ResNet v2. All models are trained on the same dataset and their performance is evaluated using metrics such as: accuracy, precision, F1-score, and recall. Finally, The comparative analysis revealed a promising performance of the modified VGG16 model in detecting multiclass skin cancer, which highlights its promising potential in classifying.",Measurement;Analytical models;Image segmentation;Melanoma;Medical services;Skin;Real-time systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10276216,IEEE Conferences,,,,,,
Transparency in Medicine: How eXplainable AI is Revolutionizing Patient Care,H. Liz-LÃ³pez; J. Huertas-Tato; D. Camacho,"2023 International Conference on Network, Multimedia and Information Technology (NMITCON)",17-Oct-23,2023,"Deep Learning algorithms are considered â€œblack-boxâ€ algorithms because it is not possible to analyse how they find the final result. This greatly limits their application in several domains, especially in fields like medicine, where errors can harm patients. To overcome this limitation, explainable AI techniques have been developed that allow us to understand the features of the input that have been relevant to the system to find the result. Most authors do not pay enough attention to explainable AI techniques, creating very basic and uninformative representations. For this reason, we analyse different heatmap-based eXplainable AI techniques for different medical problems related to chest x-rays classification, depending on the classification problem: binary and mutilabel. In our methodology, we divide the techniques into two groups to address the explainability in Artificial Intelligence applied to medicine, and show five representative examples of different visualisation techniques.",Heating systems;Deep learning;Visualization;Artificial intelligence;Information technology;X-ray imaging;Biomedical imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10276126,IEEE Conferences,,,,,,
Super Resolution in Medical Imaging,A. U. Zanzaney; R. Hegde; L. Jain; V. Rao; C. K. Sharma,"2023 International Conference on Network, Multimedia and Information Technology (NMITCON)",17-Oct-23,2023,"Medical imaging is a technique and a procedure that involves imaging of the interior portion of a human body for clinical evaluation, medical intervention, and to show how certain organs or tissues are functioning (physiology). MRI scan images of any patient can degrade in quality if they are shared over the internet or in hardcopy. This paper intends to provide an application to obtain high-resolution medical images from lower resolution images. The healthcare sector requires services and data of the best possible quality and this application would help them in better diagnosis and treatment. This paper uses Deep Learning Models and Machine Learning techniques, particularly the Super Resolution Generative Adversarial Networks, to high-resolution Brain MRI images (128 x 128 dimensions) from low-resolution images (32 x 32 dimensions). A web application is developed using the Streamlit library that can also be accessed from a mobile (responsive User Interface) for the end-to-end implementation of the paper. The underlying architecture of the SRGAN model can be replicated to other use cases with suitable customization to achieve high accuracy in generation of super-resolution images. The future scope would include the implementation of a mobile application with real-time processing of generation of high-resolution images from lower resolution images.",Measurement;Deep learning;Training;Magnetic resonance imaging;Superresolution;User interfaces;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275786,IEEE Conferences,,,,,,
Real Time Oral Cavity Detection Leading to Oral Cancer using CNN,P. S. S. M; M. Shariff; S. D. P; V. M. H; S. K; A. S. Poornima,"2023 International Conference on Network, Multimedia and Information Technology (NMITCON)",17-Oct-23,2023,"Oral cancer presents a significant burden to global health, characterized by rising incidence rates and the associated mortality it entails. Timely identification and intervention hold immense potential in enhancing patient prognosis in the face of this disease. Nevertheless, conventional diagnostic approaches often hinge on subjective visual assessments by healthcare professionals, which can lead to potential delays in detection and diagnosis. The realm of medical image analysis has witnessed a transformative shift in recent times with the advent of deep learning methodologies. By harnessing the capabilities of convolutional neural networks (CNNs), scientists have made remarkable strides in employing these models to address diverse medical imaging tasks, including the early detection of cancerous conditions across different anatomical locations. In this paper, we propose a groundbreaking approach for the real-time identification of oral cavity conditions, with a specific emphasis on predicting the occurrence of oral cancer through the utilization of a deep learning framework. Our methodology synergistically integrates patient questionnaires and oral cavity images, culminating in a prediction model that enhances accuracy and reliability. Through this user-friendly system, individuals can undergo oral cancer diagnosis without invasive techniques, making early detection more accessible and potentially life-saving. The integration of communication with healthcare professionals further enhances the overall process, enabling users to seek timely guidance and consultation for better management of their oral health. This streamlined approach empowers individuals to actively participate in their healthcare, bridging the gap between medical technology and patient-centric care.",Deep learning;Visualization;Medical services;Streaming media;Real-time systems;Convolutional neural networks;Reliability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275851,IEEE Conferences,,,,,,
CAM loss based Network Activation Refinement for Chest X-ray Image based lung disease classification,X. Ai,"2023 5th International Conference on Electronics and Communication, Network and Computer Technology (ECNCT)",17-Oct-23,2023,"With the advance of deep learning technology, convolutional neural network (CNN) has been wildly used and achieved the state-of-the-art performances in the area of medical image classification. However, most existing medical image classification methods conduct their experiments on only one public dataset. When the performance of medical image classification methods within datasets greatly improves, researchers should focus more on domain adaptation tasks that are closer to practical applications. In this work, we proposed a CAM-loss guided network activation refinement method, which employed the box-level labeled lesion mask (LM) to enforce the network to activate near the lesions. A CAM-loss is proposed to evaluate the difference between CAM and LM. The training processing minimizes the classification loss and CAM-loss at the same time, to make sure the network knows not only what, but also why. When employing network activation refinement method, all CNNs tested in this work showed better cross-domain classification performances. When employing the proposed CAM-loss, the cross-domain classification performance can be further improved. The average performance improvement was about 4% for all three testing datasets. The experimental results show that when the network is guided to activate at lesion area, the baseline networks show better both within-domain and cross-domain classification performance.",Training;Deep learning;Pulmonary diseases;Lesions;Convolutional neural networks;Task analysis;X-ray imaging,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10281051,IEEE Conferences,,,,,,
Comparative Study of Deep Learning Techniques for Automated Classification of Lung Diseases,R. H; T. Kumaravel; P. Natesan; B. B M; S. Sangeetha; S. Dharanesh,2023 4th International Conference on Smart Electronics and Communication (ICOSEC),16-Oct-23,2023,"Accurate classification of lung requires overcoming limited availability of labeled data, particularly for rare conditions like COVID-19, and the risk of overfitting due to the high dimensionality of medical images. The use of transfer learning with VGG16, VGG19, and DenseNet201 architectures enables leveraging pre-trained models' knowledge of general image features, allowing effective learning on a smaller medical image dataset and reducing the risk of overfitting. This project proposes three deep learning methods based on the VGG16, VGG19, and DenseNet201 architectures. These approaches aim to automatically classify these three lung diseases using chest X-ray images. To enhance the data, preprocessing techniques and image augmentation methods were employed. The VGG16, VGG19, and DenseNet201 models were trained using transfer learning on this dataset, and performance was evaluated on a separate test set. The achieved accuracies for VGG16, VGG19, and DenseNet201 models were 95%, 96%, and 97%, respectively. These impressive accuracies demonstrate the capacity of the approaches for precise and early detection of lung diseases. Among the three architectures, DenseNet201 consistently exhibited the highest performance for all three disease classifications. In summary, the proposed deep learning techniques utilizing VGG16, VGG19, and DenseNet201 architectures offer promising tools for the automated classification of lung diseases based on chest X-ray images. Their implementation can greatly assist medical professionals in promptly diagnosing and treating these diseases. DenseNet201, in particular, showcased superior performance among the three architectures.",Deep learning;Surveys;Visualization;Pulmonary diseases;Transfer learning;Lung;Real-time systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10276053,IEEE Conferences,,,,,,
Leveraging Deep Learning for Accurate Detection and Precise Localization of Vertebral Fractures in Medical Imaging,M. L. Kumar; P. Sampath; P. V. V. S. Srinivas; D. Dommeti; S. R. Nallapati,2023 4th International Conference on Smart Electronics and Communication (ICOSEC),16-Oct-23,2023,"Cervical spine fractures present a serious risk to the patient's health and necessitate an early and precise diagnosis for the right course of action. Deep learning techniques have recently demonstrated promising results in the analysis of medical imaging, offering a potential remedy for automated fracture detection. An innovative method for identifying cervical spine fractures using deep learning algorithms is presented in this abstract. Our suggested approach makes use of a Convolutional Neural Network (CNN) architecture that was trained on a sizable dataset of CT scans of the cervical spine (neck) at both the level of a single vertebrae and the entire patient. To ensure that the model can distinguish between fracture patterns and healthy anatomy, the dataset includes both normal and fractured cases. To increase the model's robustness and generalizability, preprocessing methods like image normalization and augmentation are used. To maximize its performance in fracture detection, the CNN is trained using a combination of supervised learning and transfer learning strategies. The experiments are performed on a separate test set made up of a wide variety of cervical spine fracture cases in order to assess the model's performance. The outcomes show how effective our deep learning methodology is at detecting fractures, with a high accuracy rate and a low false-negative rate. Additionally, the model performs better than conventional manual interpretation by radiologists, demonstrating its potential to support clinical judgement. Rapid and automated analysis, reduced human error, and potentially improved patient outcomes are just a few benefits of the proposed deep learning-based cervical spine fracture detection system. Its incorporation into current radiology workflows may increase productivity and permit prompt interventions, which would improve patient care.",Deep learning;Computed tomography;Transfer learning;Supervised learning;Sensitivity and specificity;Radiology;Robustness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275972,IEEE Conferences,,,,,,
CNN-based Deep Learning Approach for MRI-based Brain Tumor Detection,K. Narayanasamy; Elangovan; S. K. L; M. Maragatharajan; D. Deepa,2023 4th International Conference on Smart Electronics and Communication (ICOSEC),16-Oct-23,2023,"Conventional brain tumor prediction method is a time-consuming and inaccurate process that involves manually evaluating a large number of MRI (magnetic resonance imaging) scan pictures in search of signs of brain cancer. This might have an effect on the patient's medical care. It may take a long time to analyse a large volume of scan data sets comprising brain pictures. Normal brain tissue and brain cancer tissue seem quite similar under the microscope, making it difficult to distinguish between the two during segmentation. A very accurate automated method for detecting cancer is crucial. This research study has developed a novel method that uses an algorithm to analyse 2D MRI human brain scans and detect brain Tumors. Combining conventional classification algorithms with deep learning methods, the approach makes use of a convolutional neural network (CNN). The model was successfully trained using a dataset of magnetic resonance imaging (MRI) scan images, including Tumors of varied sizes, locations, and intensities. Here, a SVM classifier (Support Vector Machine) and several activation methods are also used to check the results, including Softmax, RMSProp, and sigmoid. Our research showed that a CNN could attain an accuracy of 99.87%, which is higher than the present state-of-the-art findings. The CNN -based approach is meant to aid doctors in the precise identification of brain cancers in MRI scans. This innovation might greatly accelerate the pace at which patients are helped.",Support vector machines;Training;Deep learning;Magnetic resonance imaging;Brain cancer;Switches;Medical services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10276086,IEEE Conferences,,,,,,
Automated Models for the Classification of Magnetic Resonance Brain Tumour Images,S. Divya; A. Ali; N. Ibrahim; L. Padma Suresh,2023 28th International Conference on Automation and Computing (ICAC),16-Oct-23,2023,"Brain tumours are the second largest cause of cancer death in children under 15 and young adults until age 34. Also, among people over 65, these tumours are the second-fastest-growing cause of cancer death. Computer-assisted tumour diagnosis is challenging, and efforts to increase the accuracy of tumour classification and generalisation are continually being made despite the plethora of studies conducted. This study of automated multi-class brain tumour classification utilising Magnetic Resonance Images aims to design and develop three automatic brain tumour classification approaches to categorise the brain tumours as glioma, meningioma, and pituitary tumours, which assist clinicians in making brain tumour diagnoses and developing further treatment plans to save patient's life. This research proposes a transfer learning approach using ResNet 50, hand-crafted features with machine learning classifiers, and a hybrid firefly-optimised multi-class classifier for tumour classification. The hybrid methodology yields the highest classification accuracy of 99% using the Figshare dataset. Furthermore, using the Figshare dataset, the hybrid technique yields the highest sensitivity (recall) of 99% for meningioma and pituitary tumours, the highest precision of 100% for pituitary tumours, and the highest F1-measure of 99% for pituitary tumours.",Sensitivity;Automation;Computational modeling;Transfer learning;Magnetic resonance;Brain modeling;Tumors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275235,IEEE Conferences,,,,,,
Masked Autoencoders in Computer Vision: A Comprehensive Survey,Z. Zhou; X. Liu,IEEE Access,19-Oct-23,2023,"Masked autoencoders (MAE) is a deep learning method based on Transformer. Originally used for images, it has now been extended to video, audio, and some other temporal prediction tasks. In the field of computer vision, MAE performs well in classification, prediction, and target detection tasks. In terms of specific application, MAE has made many achievements in medical treatment, geography, 3D point cloud and machine troubleshooting. Since its introduction at the end of 2021, there have been more than 300 related preprints, and MAE has been significantly performed in tier one computer vision conferences during 2022 and 2023. In view of the current popularity of MAE and its future development prospects, we conduct a relatively comprehensive survey of MAE mainly covering officially published articles so far. We comb through and classify the improvements in MAE, demonstrating relatively representative applications in computer vision. Finally, as a summary, we discuss the possible future research directions and development areas based on the characteristics of MAE, hoping our work could be a reference for the future work of MAE.",Task analysis;Training;Visualization;Surveys;Transformers;Image reconstruction;Convolutional neural networks;Computer vision;Encoding;Deep learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10278410,IEEE Journals,,,,,,
Artificial Intelligence Chip Design for High-Speed Cardiac Arrhythmia Classification,Y. -H. Chen; C. -T. Wang; S. -Y. Lin; C. -S. Lai; B. Sheu,IEEE Nanotechnology Magazine,########,2023,"An artificial intelligence (AI)-enabled ECG chip (AI-ECG chip) for classifying continuous ECG signals is described. The AI-ECG chip employs a two-stage strategy. It integrates a QRS complex wave detection architecture for signal preprocessing and a two-layer deep-learning network for post-processing. TSMC $\text{180}~nm$180nm complementary metal-oxide semiconductor fabrication process was used to produce the AI-ECG chip, which can be operated at a maximum frequency of $\text{26.3}~MHz$26.3MHz while consuming $\text{3.11}~mW$3.11mW. Despite its compact $1.41 - m{m^2}$1.41-mm2 size. The AI-ECG chip can achieve arrhythmia detection accuracy of 90.56%. A salient feature of this chip is the ability to identify up to four different arrhythmias, thus offering a more extensive diagnostic range than most comparable chips. In summary, the AI-ECG chip achieves great balance among chip size, power efficiency, and detection capabilities. It is an attractive solution for portable ECG monitoring systems.",Electrocardiography;Arrhythmia;Very large scale integration;Registers;Neural networks;Detectors;AI accelerators;Artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10278501,IEEE Magazines,,,,,,
Exploring Deep Learning-Based MRI Radiomics for Brain Tumor Prognosis and Diagnosis,P. Jhansi Devi; A. Mahto; J. Aishwarya; A. Raghavendra; A. Kundu,2023 3rd Asian Conference on Innovation in Technology (ASIANCON),10-Oct-23,2023,"Advancements in deep learning (DL) techniques have brought a ray of hope in the battle against brain tumors by significantly improving their detection and segmentation in MRI scans. This research paper delves into recent studies that have introduced innovative DL models for brain tumor diagnosis and segmentation, achieving exceptional levels of accuracy, precision, recall, and AUC values. The VGG16, VGG19 and Hybrid-DANet architectures, among others, have emerged as beacons of promise, enabling automated tumor delineation and MR image segmentation. The integration of DL algorithms in clinical practice not only enhances surgical and radiotherapy planning but also plays a vital role in survival prediction and tumor subtyping. The discoveries of this study reveals the extraordinary capabilities of DL techniques in elevating brain tumor detection and segmentation accuracy, sensitivity, specificity, and F1-score. It highlights the power of embracing hyperparameter tuning to unlock optimal performance and redefine the landscape of brain tumor diagnosis and treatment. This profound progress instills a sense of warmth and optimism, as DL paves the way for more personalized and effective care for individuals battling brain tumors. With every step forward, DL empowers healthcare professionals and instills hope in patients and their loved ones, offering a brighter future where early detection and precise tumor segmentation become cornerstones of compassionate care.",Training;Deep learning;Image segmentation;Magnetic resonance imaging;Training data;Medical services;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10269927,IEEE Conferences,,,,,,
Prediction of Heart Disease Diagnosis Using Deep Learning Fusion Algorithm for Patient Monitoring System,T. Singh,2023 3rd Asian Conference on Innovation in Technology (ASIANCON),10-Oct-23,2023,"Deep learning algorithms have become increasingly popular in the field of medical diagnosis due to their ability to identify complex patterns and correlations in medical data in an efficient manner. In this paper, we present a novel deep learning fusion algorithm for the diagnosis of heart disease. The proposed algorithm combines convolutional neural network (CNN) and long short-term memory (LSTM) neural networks to extract both spatial and temporal features from electrocardiogram (ECG) data. The algorithm is then used to analyse the ECG signals of patients in order to accurately diagnose a heart disease. The performance of the proposed algorithm is evaluated on the MIT-BIH Arrhythmia Database, where it achieved an accuracy of 95.5%. The results suggest that the proposed algorithm could be used for the early and accurate diagnosis of heart disease.",Heart;Deep learning;Electrocardiography;Prediction algorithms;Feature extraction;Classification algorithms;Medical diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10269897,IEEE Conferences,,,,,,
Multiclass MRI Brain Tumour Classification with Deep Transfer Learning,C. R. Prasad; S. Mohammed; P. R. Rao; S. Kollem; S. Samala; S. Yalabaka,2023 3rd Asian Conference on Innovation in Technology (ASIANCON),10-Oct-23,2023,"A brain tumour is a dangerous form of cancer that happens when cells divide in an abnormal way. Recent advances in deep learning have helped the medical imaging sector in the diagnosis of numerous diseases. This paper presents Multiclass MRI Brain Tumour Classification with Deep Transfer Learning. In the proposed model, VGG-16 is employed as a deep transfer learning model. The dataset is collected from the Kaggle brain tumour MRI dataset, which is a combination of three popular brain tumour datasets such as figshare, SARTAJ, and Br35H datasets. The data are prepossessed by rescaling and random brightness and/or contrast by Â±20% before applying to the modified VGG-16 model. The proposed model employs minimum computational resources and achieves better results in accuracy, precision, recall, and F1 score.",Training;Technological innovation;Magnetic resonance imaging;Computational modeling;Transfer learning;Brightness;Brain modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10270572,IEEE Conferences,,,,,,